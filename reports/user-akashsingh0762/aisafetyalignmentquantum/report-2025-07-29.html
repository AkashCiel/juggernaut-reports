
    
        <h1>ðŸ¤– AI Research Report</h1>
        
            <strong>Date:</strong> 2025-07-29<br>
            <strong>Topics:</strong> ai safety research, ai alignment research, quantum computing<br>
            <strong>Papers Found:</strong> 150
        
        
        
            
                <h2>ðŸ¤– AI Summary</h2>
                <p>## ai safety research

In the realm of AI safety research, several papers highlight emerging trends and breakthroughs that address both the technical and ethical dimensions of deploying AI systems safely and responsibly. A significant trend is the development of self-evolving agents, as discussed in the survey by Gao et al., which emphasizes the need for AI systems that can adapt in real-time to new tasks and environments. This adaptability is crucial for maintaining safety in dynamic contexts, as static models may fail to respond appropriately to unforeseen changes or threats.

Moreover, the paper by Nayebi on corrigible agents introduces a novel framework with provable guarantees for maintaining safety in environments where agents can autonomously make decisions. This approach separates utility functions to ensure that safety and obedience dominate even when conflicting incentives arise, offering a structured path toward more reliable AI agents. Additionally, Li et al.s work on security tensors addresses the cross-modal safety gap in large visual-language models by extending text-based safety measures to visual inputs, which is critical for preventing harmful outputs from multimodal AI systems.

These developments underscore the importance of integrating adaptability, corrigibility, and cross-modal safety into AI systems to prevent adverse outcomes and enhance the reliability of AI applications in diverse and complex real-world scenarios. Collectively, these efforts are paving the way for AI systems that not only perform effectively but also adhere to safety standards crucial for widespread and secure adoption.

*Based on 50 research papers*

---

## quantum computing

The research landscape in quantum computing continues to evolve with significant advancements and applications across various domains. A noteworthy trend is the increasing exploration of quantum systems for computational tasks traditionally dominated by classical methods. For instance, the paper on **Quantum Walks on Arbitrary Spatial Networks with Rydberg Atoms** demonstrates how Rydberg atoms can serve as a promising platform for implementing quantum walks, which could potentially address complex graph-based problems with enhanced efficiency due to their strong, tunable interactions and native multi-qubit gates. This work underscores the potential of quantum computing in tackling network problems with quadratic speedup in spatial search algorithms.

Another pivotal development is the study of **Quantum Optical Shallow Networks**, which introduces a protocol for implementing shallow networks using quantum optics. This approach leverages single-photon states for encoding data and parameters, utilizing the Hong-Ou-Mandel effect to achieve the network output. Remarkably, this model maintains constant optical resources regardless of input features and neurons, indicating a significant resource efficiency that could have implications for scaling quantum networks.

The paper on **Quantum Simulation of Molecular Dynamics Processes** highlights the potential and current challenges of using quantum computers to model quantum molecular dynamics. While current quantum hardware encounters limitations due to noise, the development of shallower quantum circuits for initializing wave packets shows promise for improving hardware performance. This work emphasizes the bridging of quantum algorithms with classical simulators, reflecting ongoing efforts to harness quantum computing for complex simulations, a distinct advantage over classical methods.

Collectively, these studies illustrate the rapid progress in quantum computing technologies and methodologies, paving the way for breakthroughs in computational efficiency, scalability, and application scope. The implications of these advancements are profound, potentially revolutionizing fields such as cryptography, materials science, and complex system simulations, while also highlighting the need for continued innovation to overcome existing technological limitations.

*Based on 50 research papers*</p>
            
        
        
        <h2>ðŸ“š Research Papers</h2>
        
                
                    <h3><a href="http://dx.doi.org/10.1063/5.0278183" target="_blank">Monolithic optoelectronic circuit design for on-chip terahertz applications</a></h3>
                    <p><strong>Authors:</strong> Kateryna Kusyak, Benedikt Schulte, Toru Matsuyama, Gunda Kipp, Hope M. Bretscher, Matthew W. Day, Guido Meier, Alexander M. Potts, James W. McIver</p>
                    <p><strong>Published:</strong> 7/28/2025</p>
                    <p><strong>Categories:</strong> physics.optics, physics.ins-det</p>
                    <p><strong>Summary:</strong> We demonstrate a monolithic coplanar stripline platform for on-chip terahertz (THz) generation, transmission, and detection, addressing key challenges of mode purity, bandwidth, and referencing. Capacitive coupling of the photoconductive generator switch enforces pure odd-mode propagation, increases THz field strength, and extends the operational frequency range, achieving 0.05-1.4 THz. Our architecture enables fully monolithic fabrication with amorphous silicon switches, in situ field referencing, and galvanic isolation between generation and detection. Finite-element simulations and experiments confirm that suppressing parasitic modes improves signal integrity, providing a robust platform for high-fidelity THz spectroscopy, ultrafast electronics, and nanoscale quantum materials research.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2507.21046v1" target="_blank">A Survey of Self-Evolving Agents: On Path to Artificial Super Intelligence</a></h3>
                    <p><strong>Authors:</strong> Huan-ang Gao, Jiayi Geng, Wenyue Hua, Mengkang Hu, Xinzhe Juan, Hongzhang Liu, Shilong Liu, Jiahao Qiu, Xuan Qi, Yiran Wu, Hongru Wang, Han Xiao, Yuhang Zhou, Shaokun Zhang, Jiayi Zhang, Jinyu Xiang, Yixiong Fang, Qiwen Zhao, Dongrui Liu, Qihan Ren, Cheng Qian, Zhenghailong Wang, Minda Hu, Huazheng Wang, Qingyun Wu, Heng Ji, Mengdi Wang</p>
                    <p><strong>Published:</strong> 7/28/2025</p>
                    <p><strong>Categories:</strong> cs.AI</p>
                    <p><strong>Summary:</strong> Large Language Models (LLMs) have demonstrated strong capabilities but remain fundamentally static, unable to adapt their internal parameters to novel tasks, evolving knowledge domains, or dynamic interaction contexts. As LLMs are increasingly deployed in open-ended, interactive environments, this static nature has become a critical bottleneck, necessitating agents that can adaptively reason, act, and evolve in real time. This paradigm shift -- from scaling static models to developing self-evolving agents -- has sparked growing interest in architectures and methods enabling continual learning and adaptation from data, interactions, and experiences. This survey provides the first systematic and comprehensive review of self-evolving agents, organized around three foundational dimensions -- what to evolve, when to evolve, and how to evolve. We examine evolutionary mechanisms across agent components (e.g., models, memory, tools, architecture), categorize adaptation methods by stages (e.g., intra-test-time, inter-test-time), and analyze the algorithmic and architectural designs that guide evolutionary adaptation (e.g., scalar rewards, textual feedback, single-agent and multi-agent systems). Additionally, we analyze evaluation metrics and benchmarks tailored for self-evolving agents, highlight applications in domains such as coding, education, and healthcare, and identify critical challenges and research directions in safety, scalability, and co-evolutionary dynamics. By providing a structured framework for understanding and designing self-evolving agents, this survey establishes a roadmap for advancing adaptive agentic systems in both research and real-world deployments, ultimately shedding lights to pave the way for the realization of Artificial Super Intelligence (ASI), where agents evolve autonomously, performing at or beyond human-level intelligence across a wide array of tasks.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2507.21045v1" target="_blank">Reconstructing 4D Spatial Intelligence: A Survey</a></h3>
                    <p><strong>Authors:</strong> Yukang Cao, Jiahao Lu, Zhisheng Huang, Zhuowei Shen, Chengfeng Zhao, Fangzhou Hong, Zhaoxi Chen, Xin Li, Wenping Wang, Yuan Liu, Ziwei Liu</p>
                    <p><strong>Published:</strong> 7/28/2025</p>
                    <p><strong>Categories:</strong> cs.CV</p>
                    <p><strong>Summary:</strong> Reconstructing 4D spatial intelligence from visual observations has long been a central yet challenging task in computer vision, with broad real-world applications. These range from entertainment domains like movies, where the focus is often on reconstructing fundamental visual elements, to embodied AI, which emphasizes interaction modeling and physical realism. Fueled by rapid advances in 3D representations and deep learning architectures, the field has evolved quickly, outpacing the scope of previous surveys. Additionally, existing surveys rarely offer a comprehensive analysis of the hierarchical structure of 4D scene reconstruction. To address this gap, we present a new perspective that organizes existing methods into five progressive levels of 4D spatial intelligence: (1) Level 1 -- reconstruction of low-level 3D attributes (e.g., depth, pose, and point maps); (2) Level 2 -- reconstruction of 3D scene components (e.g., objects, humans, structures); (3) Level 3 -- reconstruction of 4D dynamic scenes; (4) Level 4 -- modeling of interactions among scene components; and (5) Level 5 -- incorporation of physical laws and constraints. We conclude the survey by discussing the key challenges at each level and highlighting promising directions for advancing toward even richer levels of 4D spatial intelligence. To track ongoing developments, we maintain an up-to-date project page: https://github.com/yukangcao/Awesome-4D-Spatial-Intelligence.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2507.21033v1" target="_blank">GPT-IMAGE-EDIT-1.5M: A Million-Scale, GPT-Generated Image Dataset</a></h3>
                    <p><strong>Authors:</strong> Yuhan Wang, Siwei Yang, Bingchen Zhao, Letian Zhang, Qing Liu, Yuyin Zhou, Cihang Xie</p>
                    <p><strong>Published:</strong> 7/28/2025</p>
                    <p><strong>Categories:</strong> cs.CV</p>
                    <p><strong>Summary:</strong> Recent advancements in large multimodal models like GPT-4o have set a new standard for high-fidelity, instruction-guided image editing. However, the proprietary nature of these models and their training data creates a significant barrier for open-source research. To bridge this gap, we introduce GPT-IMAGE-EDIT-1.5M, a publicly available, large-scale image-editing corpus containing more than 1.5 million high-quality triplets (instruction, source image, edited image). We systematically construct this dataset by leveraging the versatile capabilities of GPT-4o to unify and refine three popular image-editing datasets: OmniEdit, HQ-Edit, and UltraEdit. Specifically, our methodology involves 1) regenerating output images to enhance visual quality and instruction alignment, and 2) selectively rewriting prompts to improve semantic clarity. To validate the efficacy of our dataset, we fine-tune advanced open-source models on GPT-IMAGE-EDIT-1.5M. The empirical results are exciting, e.g., the fine-tuned FluxKontext achieves highly competitive performance across a comprehensive suite of benchmarks, including 7.24 on GEdit-EN, 3.80 on ImgEdit-Full, and 8.78 on Complex-Edit, showing stronger instruction following and higher perceptual quality while maintaining identity. These scores markedly exceed all previously published open-source methods and substantially narrow the gap to leading proprietary models. We hope the full release of GPT-IMAGE-EDIT-1.5M can help to catalyze further open research in instruction-guided image editing.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2507.21028v1" target="_blank">Multi-Agent-as-Judge: Aligning LLM-Agent-Based Automated Evaluation with Multi-Dimensional Human Evaluation</a></h3>
                    <p><strong>Authors:</strong> Jiaju Chen, Yuxuan Lu, Xiaojie Wang, Huimin Zeng, Jing Huang, Jiri Gesi, Ying Xu, Bingsheng Yao, Dakuo Wang</p>
                    <p><strong>Published:</strong> 7/28/2025</p>
                    <p><strong>Categories:</strong> cs.CL, 68T50</p>
                    <p><strong>Summary:</strong> Nearly all human work is collaborative; thus, the evaluation of real-world NLP applications often requires multiple dimensions that align with diverse human perspectives. As real human evaluator resources are often scarce and costly, the emerging LLM-as-a-judge paradigm sheds light on a promising approach to leverage LLM agents to believably simulate human evaluators. Yet, to date, existing LLM-as-a-judge approaches face two limitations: persona descriptions of agents are often arbitrarily designed, and the frameworks are not generalizable to other tasks. To address these challenges, we propose MAJ-EVAL, a Multi-Agent-as-Judge evaluation framework that can automatically construct multiple evaluator personas with distinct dimensions from relevant text documents (e.g., research papers), instantiate LLM agents with the personas, and engage in-group debates with multi-agents to Generate multi-dimensional feedback. Our evaluation experiments in both the educational and medical domains demonstrate that MAJ-EVAL can generate evaluation results that better align with human experts ratings compared with conventional automated evaluation metrics and existing LLM-as-a-judge methods.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2507.21018v1" target="_blank">Deep Learning for Skeleton Based Human Motion Rehabilitation Assessment: A Benchmark</a></h3>
                    <p><strong>Authors:</strong> Ali Ismail-Fawaz, Maxime Devanne, Stefano Berretti, Jonathan Weber, Germain Forestier</p>
                    <p><strong>Published:</strong> 7/28/2025</p>
                    <p><strong>Categories:</strong> cs.CV, cs.LG</p>
                    <p><strong>Summary:</strong> Automated assessment of human motion plays a vital role in rehabilitation, enabling objective evaluation of patient performance and progress. Unlike general human activity recognition, rehabilitation motion assessment focuses on analyzing the quality of movement within the same action class, requiring the detection of subtle deviations from ideal motion. Recent advances in deep learning and video-based skeleton extraction have opened new possibilities for accessible, scalable motion assessment using affordable devices such as smartphones or webcams. However, the field lacks standardized benchmarks, consistent evaluation protocols, and reproducible methodologies, limiting progress and comparability across studies. In this work, we address these gaps by (i) aggregating existing rehabilitation datasets into a unified archive called Rehab-Pile, (ii) proposing a general benchmarking framework for evaluating deep learning methods in this domain, and (iii) conducting extensive benchmarking of multiple architectures across classification and regression tasks. All datasets and implementations are released to the community to support transparency and reproducibility. This paper aims to establish a solid foundation for future research in automated rehabilitation assessment and foster the development of reliable, accessible, and personalized rehabilitation solutions. The datasets, source-code and results of this article are all publicly available.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2507.21012v1" target="_blank">User-Centered Design with AI in the Loop: A Case Study of Rapid User Interface Prototyping with Vibe Coding</a></h3>
                    <p><strong>Authors:</strong> Tianyi Li, Tanay Maheshwari, Alex Voelker</p>
                    <p><strong>Published:</strong> 7/28/2025</p>
                    <p><strong>Categories:</strong> cs.HC</p>
                    <p><strong>Summary:</strong> We present a case study of using generative user interfaces, or ``vibe coding, a method leveraging large language models (LLMs) for generating code via natural language prompts, to support rapid prototyping in user-centered design (UCD). Extending traditional UCD practices, we propose an AI-in-the-loop ideate-prototyping process. We share insights from an empirical experience integrating this process to develop an interactive data analytics interface for highway traffic engineers to effectively retrieve and analyze historical traffic data. With generative UIs, the team was able to elicit rich user feedback and test multiple alternative design ideas from user evaluation interviews and real-time collaborative sessions with domain experts. We discuss the advantages and pitfalls of vibe coding for bridging the gaps between design expertise and domain-specific expertise.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2507.21009v1" target="_blank">Memorization in Fine-Tuned Large Language Models</a></h3>
                    <p><strong>Authors:</strong> Danil Savine, Muni Sreenivas Pydi, Jamal Atif, Olivier CappÃ©</p>
                    <p><strong>Published:</strong> 7/28/2025</p>
                    <p><strong>Categories:</strong> cs.CL, cs.AI</p>
                    <p><strong>Summary:</strong> This study investigates the mechanisms and factors influencing memorization in fine-tuned large language models (LLMs), with a focus on the medical domain due to its privacy-sensitive nature. We examine how different aspects of the fine-tuning process affect a models propensity to memorize training data, using the PHEE dataset of pharmacovigilance events. Our research employs two main approaches: a membership inference attack to detect memorized data, and a generation task with prompted prefixes to assess verbatim reproduction. We analyze the impact of adapting different weight matrices in the transformer architecture, the relationship between perplexity and memorization, and the effect of increasing the rank in low-rank adaptation (LoRA) fine-tuning. Key findings include: (1) Value and Output matrices contribute more significantly to memorization compared to Query and Key matrices; (2) Lower perplexity in the fine-tuned model correlates with increased memorization; (3) Higher LoRA ranks lead to increased memorization, but with diminishing returns at higher ranks. These results provide insights into the trade-offs between model performance and privacy risks in fine-tuned LLMs. Our findings have implications for developing more effective and responsible strategies for adapting large language models while managing data privacy concerns.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2507.20997v1" target="_blank">Modular Delta Merging with Orthogonal Constraints: A Scalable Framework for Continual and Reversible Model Composition</a></h3>
                    <p><strong>Authors:</strong> Haris Khan, Shumaila Asif, Sadia Asif</p>
                    <p><strong>Published:</strong> 7/28/2025</p>
                    <p><strong>Categories:</strong> cs.LG, cs.AI</p>
                    <p><strong>Summary:</strong> In real-world machine learning deployments, models must be continually updated, composed, and when required, selectively undone. However, existing approaches to model merging and continual learning often suffer from task interference, catastrophic forgetting, or lack of reversibility. We propose Modular Delta Merging with Orthogonal Constraints (MDM-OC), a novel framework that enables scalable, interference-free, and reversible composition of fine-tuned models. Each task-specific model is encoded as a delta from a shared base and projected into an orthogonal subspace to eliminate conflict. These projected deltas are then merged via gradient-based optimization to form a unified model that retains performance across tasks. Our approach supports continual integration of new models, structured unmerging for compliance such as GDPR requirements, and model stability via elastic weight consolidation and synthetic replay. Extensive experiments on vision and natural language processing benchmarks demonstrate that MDM-OC outperforms prior baselines in accuracy, backward transfer, and unmerge fidelity, while remaining memory-efficient and computationally tractable. This framework offers a principled solution for modular and compliant AI system design.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2507.20994v1" target="_blank">Security Tensors as a Cross-Modal Bridge: Extending Text-Aligned Safety to Vision in LVLM</a></h3>
                    <p><strong>Authors:</strong> Shen Li, Liuyi Yao, Wujia Niu, Lan Zhang, Yaliang Li</p>
                    <p><strong>Published:</strong> 7/28/2025</p>
                    <p><strong>Categories:</strong> cs.CV, cs.AI</p>
                    <p><strong>Summary:</strong> Large visual-language models (LVLMs) integrate aligned large language models (LLMs) with visual modules to process multimodal inputs. However, the safety mechanisms developed for text-based LLMs do not naturally extend to visual modalities, leaving LVLMs vulnerable to harmful image inputs. To address this cross-modal safety gap, we introduce security tensors - trainable input vectors applied during inference through either the textual or visual modality. These tensors transfer textual safety alignment to visual processing without modifying the models parameters. They are optimized using a curated dataset containing (i) malicious image-text pairs requiring rejection, (ii) contrastive benign pairs with text structurally similar to malicious queries, with the purpose of being contrastive examples to guide visual reliance, and (iii) general benign samples preserving model functionality. Experimental results demonstrate that both textual and visual security tensors significantly enhance LVLMs ability to reject diverse harmful visual inputs while maintaining near-identical performance on benign tasks. Further internal analysis towards hidden-layer representations reveals that security tensors successfully activate the language modules textual safety layers in visual inputs, thereby effectively extending text-based safety to the visual modality.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2507.20987v1" target="_blank">JWB-DH-V1: Benchmark for Joint Whole-Body Talking Avatar and Speech Generation Version 1</a></h3>
                    <p><strong>Authors:</strong> Xinhan Di, Kristin Qi, Pengqian Yu</p>
                    <p><strong>Published:</strong> 7/28/2025</p>
                    <p><strong>Categories:</strong> cs.CV, cs.AI</p>
                    <p><strong>Summary:</strong> Recent advances in diffusion-based video generation have enabled photo-realistic short clips, but current methods still struggle to achieve multi-modal consistency when jointly generating whole-body motion and natural speech. Current approaches lack comprehensive evaluation frameworks that assess both visual and audio quality, and there are insufficient benchmarks for region-specific performance analysis. To address these gaps, we introduce the Joint Whole-Body Talking Avatar and Speech Generation Version I(JWB-DH-V1), comprising a large-scale multi-modal dataset with 10,000 unique identities across 2 million video samples, and an evaluation protocol for assessing joint audio-video generation of whole-body animatable avatars. Our evaluation of SOTA models reveals consistent performance disparities between face/hand-centric and whole-body performance, which incidates essential areas for future research. The dataset and evaluation tools are publicly available at https://github.com/deepreasonings/WholeBodyBenchmark.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2507.20976v1" target="_blank">Adapting Vehicle Detectors for Aerial Imagery to Unseen Domains with Weak Supervision</a></h3>
                    <p><strong>Authors:</strong> Xiao Fang, Minhyek Jeon, Zheyang Qin, Stanislav Panev, Celso de Melo, Shuowen Hu, Shayok Chakraborty, Fernando De la Torre</p>
                    <p><strong>Published:</strong> 7/28/2025</p>
                    <p><strong>Categories:</strong> cs.CV</p>
                    <p><strong>Summary:</strong> Detecting vehicles in aerial imagery is a critical task with applications in traffic monitoring, urban planning, and defense intelligence. Deep learning methods have provided state-of-the-art (SOTA) results for this application. However, a significant challenge arises when models trained on data from one geographic region fail to generalize effectively to other areas. Variability in factors such as environmental conditions, urban layouts, road networks, vehicle types, and image acquisition parameters (e.g., resolution, lighting, and angle) leads to domain shifts that degrade model performance. This paper proposes a novel method that uses generative AI to synthesize high-quality aerial images and their labels, improving detector training through data augmentation. Our key contribution is the development of a multi-stage, multi-modal knowledge transfer framework utilizing fine-tuned latent diffusion models (LDMs) to mitigate the distribution gap between the source and target environments. Extensive experiments across diverse aerial imagery domains show consistent performance improvements in AP50 over supervised learning on source domain data, weakly supervised adaptation methods, unsupervised domain adaptation methods, and open-set object detectors by 4-23%, 6-10%, 7-40%, and more than 50%, respectively. Furthermore, we introduce two newly annotated aerial datasets from New Zealand and Utah to support further research in this field. Project page is available at: https://humansensinglab.github.io/AGenDA</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2507.20973v1" target="_blank">Model-Agnostic Gender Bias Control for Text-to-Image Generation via Sparse Autoencoder</a></h3>
                    <p><strong>Authors:</strong> Chao Wu, Zhenyi Wang, Kangxian Xie, Naresh Kumar Devulapally, Vishnu Suresh Lokhande, Mingchen Gao</p>
                    <p><strong>Published:</strong> 7/28/2025</p>
                    <p><strong>Categories:</strong> cs.LG, cs.CV</p>
                    <p><strong>Summary:</strong> Text-to-image (T2I) diffusion models often exhibit gender bias, particularly by generating stereotypical associations between professions and gendered subjects. This paper presents SAE Debias, a lightweight and model-agnostic framework for mitigating such bias in T2I generation. Unlike prior approaches that rely on CLIP-based filtering or prompt engineering, which often require model-specific adjustments and offer limited control, SAE Debias operates directly within the feature space without retraining or architectural modifications. By leveraging a k-sparse autoencoder pre-trained on a gender bias dataset, the method identifies gender-relevant directions within the sparse latent space, capturing professional stereotypes. Specifically, a biased direction per profession is constructed from sparse latents and suppressed during inference to steer generations toward more gender-balanced outputs. Trained only once, the sparse autoencoder provides a reusable debiasing direction, offering effective control and interpretable insight into biased subspaces. Extensive evaluations across multiple T2I models, including Stable Diffusion 1.4, 1.5, 2.1, and SDXL, demonstrate that SAE Debias substantially reduces gender bias while preserving generation quality. To the best of our knowledge, this is the first work to apply sparse autoencoders for identifying and intervening in gender bias within T2I models. These findings contribute toward building socially responsible generative AI, providing an interpretable and model-agnostic tool to support fairness in text-to-image generation.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2507.20967v1" target="_blank">PROVCREATOR: Synthesizing Complex Heterogenous Graphs with Node and Edge Attributes</a></h3>
                    <p><strong>Authors:</strong> Tianhao Wang, Simon Klancher, Kunal Mukherjee, Josh Wiedemeier, Feng Chen, Murat Kantarcioglu, Kangkook Jee</p>
                    <p><strong>Published:</strong> 7/28/2025</p>
                    <p><strong>Categories:</strong> cs.LG</p>
                    <p><strong>Summary:</strong> The rise of graph-structured data has driven interest in graph learning and synthetic data generation. While successful in text and image domains, synthetic graph generation remains challenging -- especially for real-world graphs with complex, heterogeneous schemas. Existing research has focused mostly on homogeneous structures with simple attributes, limiting their usefulness and relevance for application domains requiring semantic fidelity. In this research, we introduce ProvCreator, a synthetic graph framework designed for complex heterogeneous graphs with high-dimensional node and edge attributes. ProvCreator formulates graph synthesis as a sequence generation task, enabling the use of transformer-based large language models. It features a versatile graph-to-sequence encoder-decoder that 1. losslessly encodes graph structure and attributes, 2. efficiently compresses large graphs for contextual modeling, and 3. supports end-to-end, learnable graph generation. To validate our research, we evaluate ProvCreator on two challenging domains: system provenance graphs in cybersecurity and knowledge graphs from IntelliGraph Benchmark Dataset. In both cases, ProvCreator captures intricate dependencies between structure and semantics, enabling the generation of realistic and privacy-aware synthetic datasets.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2507.20964v1" target="_blank">Core Safety Values for Provably Corrigible Agents</a></h3>
                    <p><strong>Authors:</strong> Aran Nayebi</p>
                    <p><strong>Published:</strong> 7/28/2025</p>
                    <p><strong>Categories:</strong> cs.AI, cs.CC, cs.GT, cs.LG, cs.MA</p>
                    <p><strong>Summary:</strong> We introduce the first implementable framework for corrigibility, with provable guarantees in multi-step, partially observed environments. Our framework replaces a single opaque reward with five *structurally separate* utility heads -- deference, switch-access preservation, truthfulness, low-impact behavior via a belief-based extension of Attainable Utility Preservation, and bounded task reward -- combined lexicographically by strict weight gaps. Theorem 1 proves exact single-round corrigibility in the partially observable off-switch game; Theorem 3 extends the guarantee to multi-step, self-spawning agents, showing that even if each head is \emph{learned} to mean-squared error $\varepsilon$ and the planner is $\varepsilon$-sub-optimal, the probability of violating \emph{any} safety property is bounded while still ensuring net human benefit. In contrast to Constitutional AI or RLHF/RLAIF, which merge all norms into one learned scalar, our separation makes obedience and impact-limits dominate even when incentives conflict. For open-ended settings where adversaries can modify the agent, we prove that deciding whether an arbitrary post-hack agent will ever violate corrigibility is undecidable by reduction to the halting problem, then carve out a finite-horizon ``decidable island where safety can be certified in randomized polynomial time and verified with privacy-preserving, constant-round zero-knowledge proofs. Consequently, the remaining challenge is the ordinary ML task of data coverage and generalization: reward-hacking risk is pushed into evaluation quality rather than hidden incentive leak-through, giving clearer implementation guidance for todays LLM assistants and future autonomous systems.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2507.20957v1" target="_blank">Your AI, Not Your View: The Bias of LLMs in Investment Analysis</a></h3>
                    <p><strong>Authors:</strong> Hoyoung Lee, Junhyuk Seo, Suhwan Park, Junhyeong Lee, Wonbin Ahn, Chanyeol Choi, Alejandro Lopez-Lira, Yongjae Lee</p>
                    <p><strong>Published:</strong> 7/28/2025</p>
                    <p><strong>Categories:</strong> q-fin.PM, cs.AI, cs.CL</p>
                    <p><strong>Summary:</strong> In finance, Large Language Models (LLMs) face frequent knowledge conflicts due to discrepancies between pre-trained parametric knowledge and real-time market data. These conflicts become particularly problematic when LLMs are deployed in real-world investment services, where misalignment between a models embedded preferences and those of the financial institution can lead to unreliable recommendations. Yet little research has examined what investment views LLMs actually hold. We propose an experimental framework to investigate such conflicts, offering the first quantitative analysis of confirmation bias in LLM-based investment analysis. Using hypothetical scenarios with balanced and imbalanced arguments, we extract models latent preferences and measure their persistence. Focusing on sector, size, and momentum, our analysis reveals distinct, model-specific tendencies. In particular, we observe a consistent preference for large-cap stocks and contrarian strategies across most models. These preferences often harden into confirmation bias, with models clinging to initial judgments despite counter-evidence.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2507.20934v1" target="_blank">Exploring text-to-image generation for historical document image retrieval</a></h3>
                    <p><strong>Authors:</strong> Melissa Cote, Alexandra Branzan Albu</p>
                    <p><strong>Published:</strong> 7/28/2025</p>
                    <p><strong>Categories:</strong> cs.CV</p>
                    <p><strong>Summary:</strong> Attribute-based document image retrieval (ABDIR) was recently proposed as an alternative to query-by-example (QBE) searches, the dominant document image retrieval (DIR) paradigm. One drawback of QBE searches is that they require sample query documents on hand that may not be available. ABDIR aims to offer users a flexible way to retrieve document images based on memorable visual features of document contents, describing document images with combinations of visual attributes determined via convolutional neural network (CNN)-based binary classifiers. We present an exploratory study of the use of generative AI to bridge the gap between QBE and ABDIR, focusing on historical documents as a use case for their diversity and uniqueness in visual features. We hypothesize that text-to-image (T2I) generation can be leveraged to create query document images using text prompts based on ABDIR-like attributes. We propose T2I-QBE, which uses Leonardo.Ai as the T2I generator with prompts that include a rough description of the desired document type and a list of the desired ABDIR-style attributes. This creates query images that are then used within the traditional QBE paradigm, which compares CNN-extracted query features to those of the document images in the dataset to retrieve the most relevant documents. Experiments on the HisIR19 dataset of historical documents confirm our hypothesis and suggest that T2I-QBE is a viable option for historical document image retrieval. To the authors knowledge, this is the first attempt at utilizing T2I generation for DIR.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2507.20930v1" target="_blank">FRED: Financial Retrieval-Enhanced Detection and Editing of Hallucinations in Language Models</a></h3>
                    <p><strong>Authors:</strong> Likun Tan, Kuan-Wei Huang, Kevin Wu</p>
                    <p><strong>Published:</strong> 7/28/2025</p>
                    <p><strong>Categories:</strong> cs.CL, cs.AI, cs.LG</p>
                    <p><strong>Summary:</strong> Hallucinations in large language models pose a critical challenge for applications requiring factual reliability, particularly in high-stakes domains such as finance. This work presents an effective approach for detecting and editing factually incorrect content in model-generated responses based on the provided context. Given a user-defined domain-specific error taxonomy, we construct a synthetic dataset by inserting tagged errors into financial question-answering corpora and then fine-tune four language models, Phi-4, Phi-4-mini, Qwen3-4B, and Qwen3-14B, to detect and edit these factual inaccuracies. Our best-performing model, fine-tuned Phi-4, achieves an 8% improvement in binary F1 score and a 30% gain in overall detection performance compared to OpenAI-o3. Notably, our fine-tuned Phi-4-mini model, despite having only 4 billion parameters, maintains competitive performance with just a 2% drop in binary detection and a 0.1% decline in overall detection compared to OpenAI-o3. Our work provides a practical solution for detecting and editing factual inconsistencies in financial text generation while introducing a generalizable framework that can enhance the trustworthiness and alignment of large language models across diverse applications beyond finance. Our code and data are available at https://github.com/pegasi-ai/fine-grained-editting.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2507.20911v1" target="_blank">Target-density formation in swarms with stochastic sensing and dynamics</a></h3>
                    <p><strong>Authors:</strong> Jason Hindes, George Stantchev, Klimka Szwaykowska Kasraie, Ira B. Schwartz</p>
                    <p><strong>Published:</strong> 7/28/2025</p>
                    <p><strong>Categories:</strong> nlin.AO, cond-mat.soft</p>
                    <p><strong>Summary:</strong> An important goal for swarming research is to create methods for predicting, controlling and designing swarms, which produce collective dynamics that solve a problem through emergent and stable pattern formation, without the need for constant intervention, and with a minimal number of parameters and controls. One such problem involves a swarm collectively producing a desired (target) density through local sensing, motion, and interactions in a domain. Here, we take a statistical physics perspective and develop and analyze a model wherein agents move in a stochastic walk over a networked domain, so as to reduce the error between the swarm density and the target, based on local, random, and uncertain measurements of the current density by the swarming agents. Using a combination of mean-field, small-fluctuation, and finite-number analysis, we are able to quantify how close and how fast a swarm comes to producing a target as a function of sensing uncertainty, stochastic collision rates, numbers of agents, and spatial variation of the target.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2507.20907v1" target="_blank">SCORPION: Addressing Scanner-Induced Variability in Histopathology</a></h3>
                    <p><strong>Authors:</strong> Jeongun Ryu, Heon Song, Seungeun Lee, Soo Ick Cho, Jiwon Shin, Kyunghyun Paeng, SÃ©rgio Pereira</p>
                    <p><strong>Published:</strong> 7/28/2025</p>
                    <p><strong>Categories:</strong> cs.CV, cs.AI</p>
                    <p><strong>Summary:</strong> Ensuring reliable model performance across diverse domains is a critical challenge in computational pathology. A particular source of variability in Whole-Slide Images is introduced by differences in digital scanners, thus calling for better scanner generalization. This is critical for the real-world adoption of computational pathology, where the scanning devices may differ per institution or hospital, and the model should not be dependent on scanner-induced details, which can ultimately affect the patients diagnosis and treatment planning. However, past efforts have primarily focused on standard domain generalization settings, evaluating on unseen scanners during training, without directly evaluating consistency across scanners for the same tissue. To overcome this limitation, we introduce SCORPION, a new dataset explicitly designed to evaluate model reliability under scanner variability. SCORPION includes 480 tissue samples, each scanned with 5 scanners, yielding 2,400 spatially aligned patches. This scanner-paired design allows for the isolation of scanner-induced variability, enabling a rigorous evaluation of model consistency while controlling for differences in tissue composition. Furthermore, we propose SimCons, a flexible framework that combines augmentation-based domain generalization techniques with a consistency loss to explicitly address scanner generalization. We empirically show that SimCons improves model consistency on varying scanners without compromising task-specific performance. By releasing the SCORPION dataset and proposing SimCons, we provide the research community with a crucial resource for evaluating and improving model consistency across diverse scanners, setting a new standard for reliability testing.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2507.20900v1" target="_blank">Music Arena: Live Evaluation for Text-to-Music</a></h3>
                    <p><strong>Authors:</strong> Yonghyun Kim, Wayne Chi, Anastasios N. Angelopoulos, Wei-Lin Chiang, Koichi Saito, Shinji Watanabe, Yuki Mitsufuji, Chris Donahue</p>
                    <p><strong>Published:</strong> 7/28/2025</p>
                    <p><strong>Categories:</strong> cs.SD, cs.AI, cs.MM</p>
                    <p><strong>Summary:</strong> We present Music Arena, an open platform for scalable human preference evaluation of text-to-music (TTM) models. Soliciting human preferences via listening studies is the gold standard for evaluation in TTM, but these studies are expensive to conduct and difficult to compare, as study protocols may differ across systems. Moreover, human preferences might help researchers align their TTM systems or improve automatic evaluation metrics, but an open and renewable source of preferences does not currently exist. We aim to fill these gaps by offering *live* evaluation for TTM. In Music Arena, real-world users input text prompts of their choosing and compare outputs from two TTM systems, and their preferences are used to compile a leaderboard. While Music Arena follows recent evaluation trends in other AI domains, we also design it with key features tailored to music: an LLM-based routing system to navigate the heterogeneous type signatures of TTM systems, and the collection of *detailed* preferences including listening data and natural language feedback. We also propose a rolling data release policy with user privacy guarantees, providing a renewable source of preference data and increasing platform transparency. Through its standardized evaluation protocol, transparent data access policies, and music-specific features, Music Arena not only addresses key challenges in the TTM ecosystem but also demonstrates how live evaluation can be thoughtfully adapted to unique characteristics of specific AI domains. Music Arena is available at: https://music-arena.org</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2507.20881v1" target="_blank">Endoscopic Depth Estimation Based on Deep Learning: A Survey</a></h3>
                    <p><strong>Authors:</strong> Ke Niu, Zeyun Liu, Xue Feng, Heng Li, Kaize Shi</p>
                    <p><strong>Published:</strong> 7/28/2025</p>
                    <p><strong>Categories:</strong> cs.CV, cs.GR</p>
                    <p><strong>Summary:</strong> Endoscopic depth estimation is a critical technology for improving the safety and precision of minimally invasive surgery. It has attracted considerable attention from researchers in medical imaging, computer vision, and robotics. Over the past decade, a large number of methods have been developed. Despite the existence of several related surveys, a comprehensive overview focusing on recent deep learning-based techniques is still limited. This paper endeavors to bridge this gap by systematically reviewing the state-of-the-art literature. Specifically, we provide a thorough survey of the field from three key perspectives: data, methods, and applications, covering a range of methods including both monocular and stereo approaches. We describe common performance evaluation metrics and summarize publicly available datasets. Furthermore, this review analyzes the specific challenges of endoscopic scenes and categorizes representative techniques based on their supervision strategies and network architectures. The application of endoscopic depth estimation in the important area of robot-assisted surgery is also reviewed. Finally, we outline potential directions for future research, such as domain adaptation, real-time implementation, and enhanced model generalization, thereby providing a valuable starting point for researchers to engage with and advance the field.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2507.20871v1" target="_blank">\textit{FedABC}: Attention-Based Client Selection for Federated Learning with Long-Term View</a></h3>
                    <p><strong>Authors:</strong> Wenxuan Ye, Xueli An, Junfan Wang, Xueqiang Yan, Georg Carle</p>
                    <p><strong>Published:</strong> 7/28/2025</p>
                    <p><strong>Categories:</strong> cs.NI, cs.LG</p>
                    <p><strong>Summary:</strong> Native AI support is a key objective in the evolution of 6G networks, with Federated Learning (FL) emerging as a promising paradigm. FL allows decentralized clients to collaboratively train an AI model without directly sharing their data, preserving privacy. Clients train local models on private data and share model updates, which a central server aggregates to refine the global model and redistribute it for the next iteration. However, client data heterogeneity slows convergence and reduces model accuracy, and frequent client participation imposes communication and computational burdens. To address these challenges, we propose \textit{FedABC}, an innovative client selection algorithm designed to take a long-term view in managing data heterogeneity and optimizing client participation. Inspired by attention mechanisms, \textit{FedABC} prioritizes informative clients by evaluating both model similarity and each models unique contributions to the global model. Moreover, considering the evolving demands of the global model, we formulate an optimization problem to guide \textit{FedABC} throughout the training process. Following the ``later-is-better principle, \textit{FedABC} adaptively adjusts the client selection threshold, encouraging greater participation in later training stages. Extensive simulations on CIFAR-10 demonstrate that \textit{FedABC} significantly outperforms existing approaches in model accuracy and client participation efficiency, achieving comparable performance with 32\% fewer clients than the classical FL algorithm \textit{FedAvg}, and 3.5\% higher accuracy with 2\% fewer clients than the state-of-the-art. This work marks a step toward deploying FL in heterogeneous, resource-constrained environments, thereby supporting native AI capabilities in 6G networks.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2507.20866v1" target="_blank">Neuromorphic Photonic Processing and Memory with Spiking Resonant Tunnelling Diode Neurons and Neural Networks</a></h3>
                    <p><strong>Authors:</strong> Dafydd Owen-Newns, Joshua Robertson, Giovanni Donati, Jose Figueiredo, Edward Wasige, Kathy Ludge, Bruno Romeira, Antonio Hurtado</p>
                    <p><strong>Published:</strong> 7/28/2025</p>
                    <p><strong>Categories:</strong> physics.comp-ph</p>
                    <p><strong>Summary:</strong> Neuromorphic computing-modelled after the functionality and efficiency of biological neural systems-offers promising new directions for advancing artificial intelligence and computational models. Photonic techniques for neuromorphic computing hardware are attracting increasing research interest, thanks to their potentials for ultra high bandwidths, low-crosstalk and high parallelism. Among these, approaches based upon resonant tunnelling diodes (RTDs) have recently gained attention as potential building blocks for next-generation light-enabled neuromorphic hardware, due to their capacity to replicate key neuronal behaviours such as excitable spiking and refractoriness, added to their potentials for high operational speeds, energy efficiency and compact footprints. In particular, their ability to function as opto-electronic spiking neurons makes them strong candidates for integration into novel event based neuromorphic computing systems. This work demonstrates the application of optically-triggered spiking RTD neurons to a multiplicity of applications and architectures, these include systems based upon single elements for multi-modal (photonic-electronic) fast rising edge-detection in time-series data, the construction of a two-layer feedforward artificial photonic spiking neural network (pSNN) using RTD neurons as the nonlinear nodes delivering excellent performance in complex dataset classification tasks, and a pSNN comprised of multiple coupled light-sensitive RTD spiking neurons that supports performance as an adjustable neuromorphic optical spiking memory system with a tunable storage time of spiking patterns.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2507.20862v1" target="_blank">Bi-cephalic self-attended model to classify Parkinsons disease patients with freezing of gait</a></h3>
                    <p><strong>Authors:</strong> Shomoita Jahid Mitin, Rodrigue Rizk, Maximilian Scherer, Thomas Koeglsperger, Daniel Lench, KC Santosh, Arun Singh</p>
                    <p><strong>Published:</strong> 7/28/2025</p>
                    <p><strong>Categories:</strong> cs.LG</p>
                    <p><strong>Summary:</strong> Parkinson Disease (PD) often results in motor and cognitive impairments, including gait dysfunction, particularly in patients with freezing of gait (FOG). Current detection methods are either subjective or reliant on specialized gait analysis tools. This study aims to develop an objective, data-driven, and multi-modal classification model to detect gait dysfunction in PD patients using resting-state EEG signals combined with demographic and clinical variables. We utilized a dataset of 124 participants: 42 PD patients with FOG (PDFOG+), 41 without FOG (PDFOG-), and 41 age-matched healthy controls. Features extracted from resting-state EEG and descriptive variables (age, education, disease duration) were used to train a novel Bi-cephalic Self-Attention Model (BiSAM). We tested three modalities: signal-only, descriptive-only, and multi-modal, across different EEG channel subsets (BiSAM-63, -16, -8, and -4). Signal-only and descriptive-only models showed limited performance, achieving a maximum accuracy of 55% and 68%, respectively. In contrast, the multi-modal models significantly outperformed both, with BiSAM-8 and BiSAM-4 achieving the highest classification accuracy of 88%. These results demonstrate the value of integrating EEG with objective descriptive features for robust PDFOG+ detection. This study introduces a multi-modal, attention-based architecture that objectively classifies PDFOG+ using minimal EEG channels and descriptive variables. This approach offers a scalable and efficient alternative to traditional assessments, with potential applications in routine clinical monitoring and early diagnosis of PD-related gait dysfunction.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2507.20850v1" target="_blank">Free Energy-Inspired Cognitive Risk Integration for AV Navigation in Pedestrian-Rich Environments</a></h3>
                    <p><strong>Authors:</strong> Meiting Dang, Yanping Wu, Yafei Wang, Dezong Zhao, David Flynn, Chongfeng Wei</p>
                    <p><strong>Published:</strong> 7/28/2025</p>
                    <p><strong>Categories:</strong> cs.RO, cs.AI</p>
                    <p><strong>Summary:</strong> Recent advances in autonomous vehicle (AV) behavior planning have shown impressive social interaction capabilities when interacting with other road users. However, achieving human-like prediction and decision-making in interactions with vulnerable road users remains a key challenge in complex multi-agent interactive environments. Existing research focuses primarily on crowd navigation for small mobile robots, which cannot be directly applied to AVs due to inherent differences in their decision-making strategies and dynamic boundaries. Moreover, pedestrians in these multi-agent simulations follow fixed behavior patterns that cannot dynamically respond to AV actions. To overcome these limitations, this paper proposes a novel framework for modeling interactions between the AV and multiple pedestrians. In this framework, a cognitive process modeling approach inspired by the Free Energy Principle is integrated into both the AV and pedestrian models to simulate more realistic interaction dynamics. Specifically, the proposed pedestrian Cognitive-Risk Social Force Model adjusts goal-directed and repulsive forces using a fused measure of cognitive uncertainty and physical risk to produce human-like trajectories. Meanwhile, the AV leverages this fused risk to construct a dynamic, risk-aware adjacency matrix for a Graph Convolutional Network within a Soft Actor-Critic architecture, allowing it to make more reasonable and informed decisions. Simulation results indicate that our proposed framework effectively improves safety, efficiency, and smoothness of AV navigation compared to the state-of-the-art method.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2507.20840v1" target="_blank">Towards Explainable Deep Clustering for Time Series Data</a></h3>
                    <p><strong>Authors:</strong> Udo Schlegel, Gabriel Marques Tavares, Thomas Seidl</p>
                    <p><strong>Published:</strong> 7/28/2025</p>
                    <p><strong>Categories:</strong> cs.LG</p>
                    <p><strong>Summary:</strong> Deep clustering uncovers hidden patterns and groups in complex time series data, yet its opaque decision-making limits use in safety-critical settings. This survey offers a structured overview of explainable deep clustering for time series, collecting current methods and their real-world applications. We thoroughly discuss and compare peer-reviewed and preprint papers through application domains across healthcare, finance, IoT, and climate science. Our analysis reveals that most work relies on autoencoder and attention architectures, with limited support for streaming, irregularly sampled, or privacy-preserved series, and interpretability is still primarily treated as an add-on. To push the field forward, we outline six research opportunities: (1) combining complex networks with built-in interpretability; (2) setting up clear, faithfulness-focused evaluation metrics for unsupervised explanations; (3) building explainers that adapt to live data streams; (4) crafting explanations tailored to specific domains; (5) adding human-in-the-loop methods that refine clusters and explanations together; and (6) improving our understanding of how time series clustering models work internally. By making interpretability a primary design goal rather than an afterthought, we propose the groundwork for the next generation of trustworthy deep clustering time series analytics.</p>
                
            
                
                    <h3><a href="http://dx.doi.org/10.3233/FAIA241288" target="_blank">Hanging Around: Cognitive Inspired Reasoning for Reactive Robotics</a></h3>
                    <p><strong>Authors:</strong> Mihai Pomarlan, Stefano De Giorgis, Rachel Ringe, Maria M. Hedblom, Nikolaos Tsiogkas</p>
                    <p><strong>Published:</strong> 7/28/2025</p>
                    <p><strong>Categories:</strong> cs.RO</p>
                    <p><strong>Summary:</strong> Situationally-aware artificial agents operating with competence in natural environments face several challenges: spatial awareness, object affordance detection, dynamic changes and unpredictability. A critical challenge is the agents ability to identify and monitor environmental elements pertinent to its objectives. Our research introduces a neurosymbolic modular architecture for reactive robotics. Our system combines a neural component performing object recognition over the environment and image processing techniques such as optical flow, with symbolic representation and reasoning. The reasoning system is grounded in the embodied cognition paradigm, via integrating image schematic knowledge in an ontological structure. The ontology is operatively used to create queries for the perception system, decide on actions, and infer entities capabilities derived from perceptual data. The combination of reasoning and image processing allows the agent to focus its perception for normal operation as well as discover new concepts for parts of objects involved in particular interactions. The discovered concepts allow the robot to autonomously acquire training data and adjust its subsymbolic perception to recognize the parts, as well as making planning for more complex tasks feasible by focusing search on those relevant object parts. We demonstrate our approach in a simulated world, in which an agent learns to recognize parts of objects involved in support relations. While the agent has no concept of handle initially, by observing examples of supported objects hanging from a hook it learns to recognize the parts involved in establishing support and becomes able to plan the establishment/destruction of the support relation. This underscores the agents capability to expand its knowledge through observation in a systematic way, and illustrates the potential of combining deep reasoning [...].</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2507.20818v1" target="_blank">Comparison of Magnetic Field Characteristics among 3 Tesla MRI Scanners: An Experimental Measurement Study</a></h3>
                    <p><strong>Authors:</strong> Francesco Girardello, Maria Antonietta DAvanzo, Massimo Mattozzi, Victorian Michele Ferro, Giuseppe Acri, Valentina Hartwig</p>
                    <p><strong>Published:</strong> 7/28/2025</p>
                    <p><strong>Categories:</strong> physics.med-ph</p>
                    <p><strong>Summary:</strong> Magnetic resonance imaging (MRI) scanners have advanced significantly, with a growing use of highfield 3 T systems. This evolution gives rise to safety concerns for healthcare personnel working in proximity to MRI equipment. While manufacturers provide theoretical Gauss line projections, these are typically derived under ideal open-environment conditions and may not reflect real-world installations. For this reason, identical MRI models can produce markedly different fringe field distributions depending on shielding and room configurations. The present study proposes an experimental methodology for the mapping of the fringe magnetic field in the vicinity of three 3 T MRI scanners. Field measurements were interpolated to generate threedimensional magnetic field maps. A comparative analysis was conducted, which revealed notable differences among the scanners. These differences serve to highlight the influence of site-specific factors on magnetic field propagation.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2507.20815v1" target="_blank">MVIAnalyzer: A Holistic Approach to Analyze Missing Value Imputation</a></h3>
                    <p><strong>Authors:</strong> Valerie Restat, Kai Tejkl, Uta StÃ¶rl</p>
                    <p><strong>Published:</strong> 7/28/2025</p>
                    <p><strong>Categories:</strong> cs.DB</p>
                    <p><strong>Summary:</strong> Missing values often limit the usage of data analysis or cause falsification of results. Therefore, methods of missing value imputation (MVI) are of great significance. However, in general, there is no universal, fair MVI method for different tasks. This work thus places MVI in the overall context of data analysis. For this purpose, we present the MVIAnalyzer, a generic framework for a holistic analysis of MVI. It considers the overall process up to the application and analysis of machine learning methods. The associated software is provided and can be used by other researchers for their own analyses. To this end, it further includes a missing value simulation with consideration of relevant parameters. The application of the MVIAnalyzer is demonstrated on data with different characteristics. An evaluation of the results shows the possibilities and limitations of different MVI methods. Since MVI is a very complex topic with different influencing variables, this paper additionally illustrates how the analysis can be supported by visualizations.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2507.20810v1" target="_blank">Why Flow Matching is Particle Swarm Optimization?</a></h3>
                    <p><strong>Authors:</strong> Kaichen Ouyang</p>
                    <p><strong>Published:</strong> 7/28/2025</p>
                    <p><strong>Categories:</strong> cs.NE, cs.AI, cs.LG</p>
                    <p><strong>Summary:</strong> This paper preliminarily investigates the duality between flow matching in generative models and particle swarm optimization (PSO) in evolutionary computation. Through theoretical analysis, we reveal the intrinsic connections between these two approaches in terms of their mathematical formulations and optimization mechanisms: the vector field learning in flow matching shares similar mathematical expressions with the velocity update rules in PSO; both methods follow the fundamental framework of progressive evolution from initial to target distributions; and both can be formulated as dynamical systems governed by ordinary differential equations. Our study demonstrates that flow matching can be viewed as a continuous generalization of PSO, while PSO provides a discrete implementation of swarm intelligence principles. This duality understanding establishes a theoretical foundation for developing novel hybrid algorithms and creates a unified framework for analyzing both methods. Although this paper only presents preliminary discussions, the revealed correspondences suggest several promising research directions, including improving swarm intelligence algorithms based on flow matching principles and enhancing generative models using swarm intelligence concepts.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2507.20805v1" target="_blank">Understanding Bias in Perceiving Dimensionality Reduction Projections</a></h3>
                    <p><strong>Authors:</strong> Seoyoung Doh, Hyeon Jeon, Sungbok Shin, Ghulam Jilani Quadri, Nam Wook Kim, Jinwook Seo</p>
                    <p><strong>Published:</strong> 7/28/2025</p>
                    <p><strong>Categories:</strong> cs.HC, cs.LG</p>
                    <p><strong>Summary:</strong> Selecting the dimensionality reduction technique that faithfully represents the structure is essential for reliable visual communication and analytics. In reality, however, practitioners favor projections for other attractions, such as aesthetics and visual saliency, over the projections structural faithfulness, a bias we define as visual interestingness. In this research, we conduct a user study that (1) verifies the existence of such bias and (2) explains why the bias exists. Our study suggests that visual interestingness biases practitioners preferences when selecting projections for analysis, and this bias intensifies with color-encoded labels and shorter exposure time. Based on our findings, we discuss strategies to mitigate bias in perceiving and interpreting DR projections.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2507.20800v1" target="_blank">LanternNet: A Novel Hub-and-Spoke System to Seek and Suppress Spotted Lanternfly Populations</a></h3>
                    <p><strong>Authors:</strong> Vinil Polepalli</p>
                    <p><strong>Published:</strong> 7/28/2025</p>
                    <p><strong>Categories:</strong> cs.RO, cs.AI, cs.CV</p>
                    <p><strong>Summary:</strong> The invasive spotted lanternfly (SLF) poses a significant threat to agriculture and ecosystems, causing widespread damage. Current control methods, such as egg scraping, pesticides, and quarantines, prove labor-intensive, environmentally hazardous, and inadequate for long-term SLF suppression. This research introduces LanternNet, a novel autonomous robotic Hub-and-Spoke system designed for scalable detection and suppression of SLF populations. A central, tree-mimicking hub utilizes a YOLOv8 computer vision model for precise SLF identification. Three specialized robotic spokes perform targeted tasks: pest neutralization, environmental monitoring, and navigation/mapping. Field deployment across multiple infested sites over 5 weeks demonstrated LanternNets efficacy. Quantitative analysis revealed significant reductions (p  0.01, paired t-tests) in SLF populations and corresponding improvements in tree health indicators across the majority of test sites. Compared to conventional methods, LanternNet offers substantial cost advantages and improved scalability. Furthermore, the systems adaptability for enhanced autonomy and targeting of other invasive species presents significant potential for broader ecological impact. LanternNet demonstrates the transformative potential of integrating robotics and AI for advanced invasive species management and improved environmental outcomes.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2507.20798v1" target="_blank">An Efficient Machine Learning Framework for Forest Height Estimation from Multi-Polarimetric Multi-Baseline SAR data</a></h3>
                    <p><strong>Authors:</strong> Francesca Razzano, Wenyu Yang, Sergio Vitale, Giampaolo Ferraioli, Silvia Liberata Ullo, Gilda Schirinzi</p>
                    <p><strong>Published:</strong> 7/28/2025</p>
                    <p><strong>Categories:</strong> cs.CV</p>
                    <p><strong>Summary:</strong> Accurate forest height estimation is crucial for climate change monitoring and carbon cycle assessment. Synthetic Aperture Radar (SAR), particularly in multi-channel configurations, has provided support for a long time in 3D forest structure reconstruction through model-based techniques. More recently, data-driven approaches using Machine Learning (ML) and Deep Learning (DL) have enabled new opportunities for forest parameter retrieval. This paper introduces FGump, a forest height estimation framework by gradient boosting using multi-channel SAR processing with LiDAR profiles as Ground Truth(GT). Unlike typical ML and DL approaches that require large datasets and complex architectures, FGump ensures a strong balance between accuracy and computational efficiency, using a limited set of hand-designed features and avoiding heavy preprocessing (e.g., calibration and/or quantization). Evaluated under both classification and regression paradigms, the proposed framework demonstrates that the regression formulation enables fine-grained, continuous estimations and avoids quantization artifacts by resulting in more precise measurements without rounding. Experimental results confirm that FGump outperforms State-of-the-Art (SOTA) AI-based and classical methods, achieving higher accuracy and significantly lower training and inference times, as demonstrated in our results.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2507.20796v1" target="_blank">Aligning Large Language Model Agents with Rational and Moral Preferences: A Supervised Fine-Tuning Approach</a></h3>
                    <p><strong>Authors:</strong> Wei Lu, Daniel L. Chen, Christian B. Hansen</p>
                    <p><strong>Published:</strong> 7/28/2025</p>
                    <p><strong>Categories:</strong> econ.GN, cs.AI, cs.LG, q-fin.EC</p>
                    <p><strong>Summary:</strong> Understanding how large language model (LLM) agents behave in strategic interactions is essential as these systems increasingly participate autonomously in economically and morally consequential decisions. We evaluate LLM preferences using canonical economic games, finding substantial deviations from human behavior. Models like GPT-4o show excessive cooperation and limited incentive sensitivity, while reasoning models, such as o3-mini, align more consistently with payoff-maximizing strategies. We propose a supervised fine-tuning pipeline that uses synthetic datasets derived from economic reasoning to align LLM agents with economic preferences, focusing on two stylized preference structures. In the first, utility depends only on individual payoffs (homo economicus), while utility also depends on a notion of Kantian universalizability in the second preference structure (homo moralis). We find that fine-tuning based on small datasets shifts LLM agent behavior toward the corresponding economic agent. We further assess the fine-tuned agents behavior in two applications: Moral dilemmas involving autonomous vehicles and algorithmic pricing in competitive markets. These examples illustrate how different normative objectives embedded via realizations from structured preference structures can influence market and moral outcomes. This work contributes a replicable, cost-efficient, and economically grounded pipeline to align AI preferences using moral-economic principles.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2507.20786v1" target="_blank">Automating Thematic Review of Prevention of Future Deaths Reports: Replicating the ONS Child Suicide Study using Large Language Models</a></h3>
                    <p><strong>Authors:</strong> Sam Osian, Arpan Dutta, Sahil Bhandari, Iain E. Buchan, Dan W. Joyce</p>
                    <p><strong>Published:</strong> 7/28/2025</p>
                    <p><strong>Categories:</strong> cs.CL</p>
                    <p><strong>Summary:</strong> Prevention of Future Deaths (PFD) reports, issued by coroners in England and Wales, flag systemic hazards that may lead to further loss of life. Analysis of these reports has previously been constrained by the manual effort required to identify and code relevant cases. In 2025, the Office for National Statistics (ONS) published a national thematic review of child-suicide PFD reports ($\leq$ 18 years), identifying 37 cases from January 2015 to November 2023 - a process based entirely on manual curation and coding. We evaluated whether a fully automated, open source text-to-table language-model pipeline (PFD Toolkit) could reproduce the ONSs identification and thematic analysis of child-suicide PFD reports, and assessed gains in efficiency and reliability. All 4,249 PFD reports published from July 2013 to November 2023 were processed via PFD Toolkits large language model pipelines. Automated screening identified cases where the coroner attributed death to suicide in individuals aged 18 or younger, and eligible reports were coded for recipient category and 23 concern sub-themes, replicating the ONS coding frame. PFD Toolkit identified 72 child-suicide PFD reports - almost twice the ONS count. Three blinded clinicians adjudicated a stratified sample of 144 reports to validate the child-suicide screening. Against the post-consensus clinical annotations, the LLM-based workflow showed substantial to almost-perfect agreement (Cohens $\kappa$ = 0.82, 95% CI: 0.66-0.98, raw agreement = 91%). The end-to-end script runtime was 8m 16s, transforming a process that previously took months into one that can be completed in minutes. This demonstrates that automated LLM analysis can reliably and efficiently replicate manual thematic reviews of coronial data, enabling scalable, reproducible, and timely insights for public health and safety. The PFD Toolkit is openly available for future research.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2507.20783v1" target="_blank">On The Role of Pretrained Language Models in General-Purpose Text Embeddings: A Survey</a></h3>
                    <p><strong>Authors:</strong> Meishan Zhang, Xin Zhang, Xinping Zhao, Shouzheng Huang, Baotian Hu, Min Zhang</p>
                    <p><strong>Published:</strong> 7/28/2025</p>
                    <p><strong>Categories:</strong> cs.CL</p>
                    <p><strong>Summary:</strong> Text embeddings have attracted growing interest due to their effectiveness across a wide range of natural language processing (NLP) tasks, such as retrieval, classification, clustering, bitext mining, and summarization. With the emergence of pretrained language models (PLMs), general-purpose text embeddings (GPTE) have gained significant traction for their ability to produce rich, transferable representations. The general architecture of GPTE typically leverages PLMs to derive dense text representations, which are then optimized through contrastive learning on large-scale pairwise datasets. In this survey, we provide a comprehensive overview of GPTE in the era of PLMs, focusing on the roles PLMs play in driving its development. We first examine the fundamental architecture and describe the basic roles of PLMs in GPTE, i.e., embedding extraction, expressivity enhancement, training strategies, learning objectives, and data construction. Then, we describe advanced roles enabled by PLMs, such as multilingual support, multimodal integration, code understanding, and scenario-specific adaptation. Finally, we highlight potential future research directions that move beyond traditional improvement goals, including ranking integration, safety considerations, bias mitigation, structural information incorporation, and the cognitive extension of embeddings. This survey aims to serve as a valuable reference for both newcomers and established researchers seeking to understand the current state and future potential of GPTE.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2507.20776v1" target="_blank">RingMo-Agent: A Unified Remote Sensing Foundation Model for Multi-Platform and Multi-Modal Reasoning</a></h3>
                    <p><strong>Authors:</strong> Huiyang Hu, Peijin Wang, Yingchao Feng, Kaiwen Wei, Wenxin Yin, Wenhui Diao, Mengyu Wang, Hanbo Bi, Kaiyue Kang, Tong Ling, Kun Fu, Xian Sun</p>
                    <p><strong>Published:</strong> 7/28/2025</p>
                    <p><strong>Categories:</strong> cs.CV</p>
                    <p><strong>Summary:</strong> Remote sensing (RS) images from multiple modalities and platforms exhibit diverse details due to differences in sensor characteristics and imaging perspectives. Existing vision-language research in RS largely relies on relatively homogeneous data sources. Moreover, they still remain limited to conventional visual perception tasks such as classification or captioning. As a result, these methods fail to serve as a unified and standalone framework capable of effectively handling RS imagery from diverse sources in real-world applications. To address these issues, we propose RingMo-Agent, a model designed to handle multi-modal and multi-platform data that performs perception and reasoning tasks based on user textual instructions. Compared with existing models, RingMo-Agent 1) is supported by a large-scale vision-language dataset named RS-VL3M, comprising over 3 million image-text pairs, spanning optical, SAR, and infrared (IR) modalities collected from both satellite and UAV platforms, covering perception and challenging reasoning tasks; 2) learns modality adaptive representations by incorporating separated embedding layers to construct isolated features for heterogeneous modalities and reduce cross-modal interference; 3) unifies task modeling by introducing task-specific tokens and employing a token-based high-dimensional hidden state decoding mechanism designed for long-horizon spatial tasks. Extensive experiments on various RS vision-language tasks demonstrate that RingMo-Agent not only proves effective in both visual understanding and sophisticated analytical tasks, but also exhibits strong generalizability across different platforms and sensing modalities.</p>
                
            
                
                    <h3><a href="http://dx.doi.org/10.1063/5.0279971" target="_blank">A Data-Driven Approach for Predicting Hydrodynamic Forces on Spherical Particles Using Volume Fraction Representations</a></h3>
                    <p><strong>Authors:</strong> Alexander Metelkin, Sam Jacob Jacob, Bernhard Vowinckel</p>
                    <p><strong>Published:</strong> 7/28/2025</p>
                    <p><strong>Categories:</strong> physics.flu-dyn, physics.comp-ph</p>
                    <p><strong>Summary:</strong> Particle-laden flows are simulated at various scales using numerical techniques that range from particle-resolved Direct Numerical Simulations (pr-DNS) for small-scale systems to Lagrange point-particle methods for laboratory-scale problems, and Euler-Euler approaches for larger-scale applications. Recent research has been particularly focused on the development of both physics-based and data-driven closures to enhance the accuracy of the Lagrangian point-particle approach by leveraging highly resolved data from pr-DNS. In this study, a data-driven methodology is presented for the prediction of hydrodynamic forces acting on spherical particles immersed in an ambient flow field, where neighboring particle information is represented by volume fractions. The volume fractions are computed on an auxiliary grid with cell sizes on the order of the particle diameter. The volume fraction values in the vicinity of each particle are used as input features for the data-driven model to predict the corresponding hydrodynamic forces and moments. The training data was generated by a series of pr-DNS of flow through arrays of randomly distributed, fixed-position particles at various Reynolds numbers and particle volume fractions. The data-driven model is built using Fully Connected Neural Networks (FCNN). Improved prediction accuracy of hydrodynamic forces and torques is demonstrated in comparison to FCNN models that rely on direct particle position inputs. In addition, the proposed volume-fraction-based approach exhibits greater flexibility than previously introduced models by accommodating systems with particles of different sizes and shapes.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2507.20755v1" target="_blank">Beyond Listenership: AI-Predicted Interventions Drive Improvements in Maternal Health Behaviours</a></h3>
                    <p><strong>Authors:</strong> Arpan Dasgupta, Sarvesh Gharat, Neha Madhiwalla, Aparna Hegde, Milind Tambe, Aparna Taneja</p>
                    <p><strong>Published:</strong> 7/28/2025</p>
                    <p><strong>Categories:</strong> cs.AI</p>
                    <p><strong>Summary:</strong> Automated voice calls with health information are a proven method for disseminating maternal and child health information among beneficiaries and are deployed in several programs around the world. However, these programs often suffer from beneficiary dropoffs and poor engagement. In previous work, through real-world trials, we showed that an AI model, specifically a restless bandit model, could identify beneficiaries who would benefit most from live service call interventions, preventing dropoffs and boosting engagement. However, one key question has remained open so far: does such improved listenership via AI-targeted interventions translate into beneficiaries improved knowledge and health behaviors? We present a first study that shows not only listenership improvements due to AI interventions, but also simultaneously links these improvements to health behavior changes. Specifically, we demonstrate that AI-scheduled interventions, which enhance listenership, lead to statistically significant improvements in beneficiaries health behaviors such as taking iron or calcium supplements in the postnatal period, as well as understanding of critical health topics during pregnancy and infancy. This underscores the potential of AI to drive meaningful improvements in maternal and child health.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2507.20739v1" target="_blank">Efficient Adjoint Petrov-Galerkin Reduced Order Models for fluid flows governed by the incompressible Navier-Stokes equations</a></h3>
                    <p><strong>Authors:</strong> Kamil David Sommer, Lucas Mieg, Siddharth Sharma, Romuald Skoda, Martin MÃ¶nnigmann</p>
                    <p><strong>Published:</strong> 7/28/2025</p>
                    <p><strong>Categories:</strong> eess.SY, cs.SY, physics.flu-dyn</p>
                    <p><strong>Summary:</strong> This research paper investigates the Adjoint Petrov-Galerkin (APG) method for reduced order models (ROM) and fluid dynamics governed by the incompressible Navier-Stokes equations. The Adjoint Petrov-Galerkin ROM, derived using the Mori-Zwanzig formalism, demonstrates superior accuracy and stability compared to standard Galerkin ROMs. However, challenges arise due to the time invariance of the test basis vectors, resulting in high computational requirements. To address this, we introduce a new efficient Adjoint Petrov-Galerkin (eAPG) ROM formulation, extending its application to the incompressible Navier-Stokes equations by exploiting the polynomial structure inherent in these equations. The offline and online phases partition eliminates the need for repeated test basis vector evaluations. This improves computational efficiency in comparison to the general Adjoint Petrov-Galerkin ROM formulation. A novel approach to augmenting the memory length, a critical factor influencing the stability and accuracy of the APG-ROM, is introduced, employing a data-driven optimization. Numerical results for the 3D turbulent flow around a circular cylinder demonstrate the efficacy of the proposed approach. Error measures and computational cost evaluations, considering metrics such as floating point operations and simulation time, provide a comprehensive analysis.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2507.20728v1" target="_blank">Learning the Value Systems of Societies from Preferences</a></h3>
                    <p><strong>Authors:</strong> AndrÃ©s Holgado-SÃ¡nchez, Holger Billhardt, Sascha Ossowski, Sara Degli-Esposti</p>
                    <p><strong>Published:</strong> 7/28/2025</p>
                    <p><strong>Categories:</strong> cs.AI, cs.CY, cs.LG</p>
                    <p><strong>Summary:</strong> Aligning AI systems with human values and the value-based preferences of various stakeholders (their value systems) is key in ethical AI. In value-aware AI systems, decision-making draws upon explicit computational representations of individual values (groundings) and their aggregation into value systems. As these are notoriously difficult to elicit and calibrate manually, value learning approaches aim to automatically derive computational models of an agents values and value system from demonstrations of human behaviour. Nonetheless, social science and humanities literature suggest that it is more adequate to conceive the value system of a society as a set of value systems of different groups, rather than as the simple aggregation of individual value systems. Accordingly, here we formalize the problem of learning the value systems of societies and propose a method to address it based on heuristic deep clustering. The method learns socially shared value groundings and a set of diverse value systems representing a given society by observing qualitative value-based preferences from a sample of agents. We evaluate the proposal in a use case with real data about travelling decisions.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2507.20721v1" target="_blank">AIComposer: Any Style and Content Image Composition via Feature Integration</a></h3>
                    <p><strong>Authors:</strong> Haowen Li, Zhenfeng Fan, Zhang Wen, Zhengzhou Zhu, Yunjin Li</p>
                    <p><strong>Published:</strong> 7/28/2025</p>
                    <p><strong>Categories:</strong> cs.CV</p>
                    <p><strong>Summary:</strong> Image composition has advanced significantly with large-scale pre-trained T2I diffusion models. Despite progress in same-domain composition, cross-domain composition remains under-explored. The main challenges are the stochastic nature of diffusion models and the style gap between input images, leading to failures and artifacts. Additionally, heavy reliance on text prompts limits practical applications. This paper presents the first cross-domain image composition method that does not require text prompts, allowing natural stylization and seamless compositions. Our method is efficient and robust, preserving the diffusion prior, as it involves minor steps for backward inversion and forward denoising without training the diffuser. Our method also uses a simple multilayer perceptron network to integrate CLIP features from foreground and background, manipulating diffusion with a local cross-attention strategy. It effectively preserves foreground content while enabling stable stylization without a pre-stylization network. Finally, we create a benchmark dataset with diverse contents and styles for fair evaluation, addressing the lack of testing datasets for cross-domain image composition. Our method outperforms state-of-the-art techniques in both qualitative and quantitative evaluations, significantly improving the LPIPS score by 30.5% and the CSD metric by 18.1%. We believe our method will advance future research and applications. Code and benchmark at https://github.com/sherlhw/AIComposer.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2507.20714v1" target="_blank">Prostate Cancer Classification Using Multimodal Feature Fusion and Explainable AI</a></h3>
                    <p><strong>Authors:</strong> Asma Sadia Khan, Fariba Tasnia Khan, Tanjim Mahmud, Salman Karim Khan, Rishita Chakma, Nahed Sharmen, Mohammad Shahadat Hossain, Karl Andersson</p>
                    <p><strong>Published:</strong> 7/28/2025</p>
                    <p><strong>Categories:</strong> cs.LG, cs.AI, q-bio.QM, stat.AP</p>
                    <p><strong>Summary:</strong> Prostate cancer, the second most prevalent male malignancy, requires advanced diagnostic tools. We propose an explainable AI system combining BERT (for textual clinical notes) and Random Forest (for numerical lab data) through a novel multimodal fusion strategy, achieving superior classification performance on PLCO-NIH dataset (98% accuracy, 99% AUC). While multimodal fusion is established, our work demonstrates that a simple yet interpretable BERT+RF pipeline delivers clinically significant improvements - particularly for intermediate cancer stages (Class 2/3 recall: 0.900 combined vs 0.824 numerical/0.725 textual). SHAP analysis provides transparent feature importance rankings, while ablation studies prove textual features complementary value. This accessible approach offers hospitals a balance of high performance (F1=89%), computational efficiency, and clinical interpretability - addressing critical needs in prostate cancer diagnostics.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2507.20711v1" target="_blank">Algorithmic Fairness: A Runtime Perspective</a></h3>
                    <p><strong>Authors:</strong> Filip Cano, Thomas A. Henzinger, Konstantin Kueffner</p>
                    <p><strong>Published:</strong> 7/28/2025</p>
                    <p><strong>Categories:</strong> cs.AI</p>
                    <p><strong>Summary:</strong> Fairness in AI is traditionally studied as a static property evaluated once, over a fixed dataset. However, real-world AI systems operate sequentially, with outcomes and environments evolving over time. This paper proposes a framework for analysing fairness as a runtime property. Using a minimal yet expressive model based on sequences of coin tosses with possibly evolving biases, we study the problems of monitoring and enforcing fairness expressed in either toss outcomes or coin biases. Since there is no one-size-fits-all solution for either problem, we provide a summary of monitoring and enforcement strategies, parametrised by environment dynamics, prediction horizon, and confidence thresholds. For both problems, we present general results under simple or minimal assumptions. We survey existing solutions for the monitoring problem for Markovian and additive dynamics, and existing solutions for the enforcement problem in static settings with known dynamics.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2507.20708v1" target="_blank">Exposing the Illusion of Fairness: Auditing Vulnerabilities to Distributional Manipulation Attacks</a></h3>
                    <p><strong>Authors:</strong> Valentin Lafargue, Adriana Laurindo Monteiro, Emmanuelle Claeys, Laurent Risser, Jean-Michel Loubes</p>
                    <p><strong>Published:</strong> 7/28/2025</p>
                    <p><strong>Categories:</strong> cs.LG, math.OC, stat.AP</p>
                    <p><strong>Summary:</strong> Proving the compliance of AI algorithms has become an important challenge with the growing deployment of such algorithms for real-life applications. Inspecting possible biased behaviors is mandatory to satisfy the constraints of the regulations of the EU Artificial Intelligences Act. Regulation-driven audits increasingly rely on global fairness metrics, with Disparate Impact being the most widely used. Yet such global measures depend highly on the distribution of the sample on which the measures are computed. We investigate first how to manipulate data samples to artificially satisfy fairness criteria, creating minimally perturbed datasets that remain statistically indistinguishable from the original distribution while satisfying prescribed fairness constraints. Then we study how to detect such manipulation. Our analysis (i) introduces mathematically sound methods for modifying empirical distributions under fairness constraints using entropic or optimal transport projections, (ii) examines how an auditee could potentially circumvent fairness inspections, and (iii) offers recommendations to help auditors detect such data manipulations. These results are validated through experiments on classical tabular datasets in bias detection.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2507.20707v1" target="_blank">A general framework for the OSRC-preconditioned EFIE in computational electromagnetics</a></h3>
                    <p><strong>Authors:</strong> Marion Darbas, Ignacia Fierro-Piccardo</p>
                    <p><strong>Published:</strong> 7/28/2025</p>
                    <p><strong>Categories:</strong> math.NA, cs.NA</p>
                    <p><strong>Summary:</strong> This work presents a comprehensive study of preconditioning strategies for the Electric Field Integral Equation (EFIE) using On-Surface Radiation Condition (OSRC) operators. We examine two distinct formulations -- the Magnetic-to-Electric (MtE) and Electric-to-Magnetic (EtM) maps -- used to precondition the EFIE, and we analyze their spectral properties, discretization behavior, and numerical performance. A central objective is to bridge the gap between theoretical development and practical implementation, identifying the strengths and limitations of each approach. Through numerical experiments on smooth, closed geometries, we show that the MtE formulation stands out as a cost-effective preconditioner in the context of Boundary Element Methods. We also offer implementation guidelines and propose improvements to address existing challenges. These findings provide a valuable reference for researchers and practitioners working with preconditioned boundary integral formulations in computational electromagnetics.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2507.20704v1" target="_blank">Text2VLM: Adapting Text-Only Datasets to Evaluate Alignment Training in Visual Language Models</a></h3>
                    <p><strong>Authors:</strong> Gabriel Downer, Sean Craven, Damian Ruck, Jake Thomas</p>
                    <p><strong>Published:</strong> 7/28/2025</p>
                    <p><strong>Categories:</strong> cs.CL, cs.AI, cs.CR</p>
                    <p><strong>Summary:</strong> The increasing integration of Visual Language Models (VLMs) into AI systems necessitates robust model alignment, especially when handling multimodal content that combines text and images. Existing evaluation datasets heavily lean towards text-only prompts, leaving visual vulnerabilities under evaluated. To address this gap, we propose \textbf{Text2VLM}, a novel multi-stage pipeline that adapts text-only datasets into multimodal formats, specifically designed to evaluate the resilience of VLMs against typographic prompt injection attacks. The Text2VLM pipeline identifies harmful content in the original text and converts it into a typographic image, creating a multimodal prompt for VLMs. Also, our evaluation of open-source VLMs highlights their increased susceptibility to prompt injection when visual inputs are introduced, revealing critical weaknesses in the current models alignment. This is in addition to a significant performance gap compared to closed-source frontier models. We validate Text2VLM through human evaluations, ensuring the alignment of extracted salient concepts; text summarization and output classification align with human expectations. Text2VLM provides a scalable tool for comprehensive safety assessment, contributing to the development of more robust safety mechanisms for VLMs. By enhancing the evaluation of multimodal vulnerabilities, Text2VLM plays a role in advancing the safe deployment of VLMs in diverse, real-world applications.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2507.20694v1" target="_blank">Optimization and Synthesis of Quantum Circuits with Global Gates</a></h3>
                    <p><strong>Authors:</strong> Alejandro Villoria, Henning Basold, Alfons Laarman</p>
                    <p><strong>Published:</strong> 7/28/2025</p>
                    <p><strong>Categories:</strong> quant-ph</p>
                    <p><strong>Summary:</strong> Compiling quantum circuits to account for hardware restrictions is an essential part of the quantum computing stack. Circuit compilation allows us to adapt algorithm descriptions into a sequence of operations supported by real quantum hardware, and has the potential to significantly improve their performance when optimization techniques are added to the process. One such optimization technique is reducing the number of quantum gates that are needed to execute a circuit. For instance, methods for reducing the number of non-Clifford gates or CNOT gates from a circuit is an extensive research area that has gathered significant interest over the years. For certain hardware platforms such as ion trap quantum computers, we can leverage some of their special properties to further reduce the cost of executing a quantum circuit in them. In this work we use global interactions, such as the Global M{\o}lmer-S{\o}rensen gate present in ion trap hardware, to optimize and synthesize quantum circuits. We design and implement an algorithm that is able to compile an arbitrary quantum circuit into another circuit that uses global gates as the entangling operation, while optimizing the number of global interactions needed. The algorithm is based on the ZX-calculus and uses an specialized circuit extraction routine that groups entangling gates into Global M{\o}lmer-S{\o}rensen gates. We benchmark the algorithm in a variety of circuits, and show how it improves their performance under state-of-the-art hardware considerations in comparison to a naive algorithm and the Qiskit optimizer.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2507.20688v1" target="_blank">Guard-GBDT: Efficient Privacy-Preserving Approximated GBDT Training on Vertical Dataset</a></h3>
                    <p><strong>Authors:</strong> Anxiao Song, Shujie Cui, Jianli Bai, Ke Cheng, Yulong Shen, Giovanni Russello</p>
                    <p><strong>Published:</strong> 7/28/2025</p>
                    <p><strong>Categories:</strong> cs.CR</p>
                    <p><strong>Summary:</strong> In light of increasing privacy concerns and stringent legal regulations, using secure multiparty computation (MPC) to enable collaborative GBDT model training among multiple data owners has garnered significant attention. Despite this, existing MPC-based GBDT frameworks face efficiency challenges due to high communication costs and the computation burden of non-linear operations, such as division and sigmoid calculations. In this work, we introduce Guard-GBDT, an innovative framework tailored for efficient and privacy-preserving GBDT training on vertical datasets. Guard-GBDT bypasses MPC-unfriendly division and sigmoid functions by using more streamlined approximations and reduces communication overhead by compressing the messages exchanged during gradient aggregation. We implement a prototype of Guard-GBDT and extensively evaluate its performance and accuracy on various real-world datasets. The results show that Guard-GBDT outperforms state-of-the-art HEP-XGB (CIKM21) and SiGBDT (ASIA CCS24) by up to $2.71\times$ and $12.21 \times$ on LAN network and up to $2.7\times$ and $8.2\times$ on WAN network. Guard-GBDT also achieves comparable accuracy with SiGBDT and plaintext XGBoost (better than HEP-XGB ), which exhibits a deviation of $\pm1\%$ to $\pm2\%$ only. Our implementation code is provided at https://github.com/XidianNSS/Guard-GBDT.git.</p>
                
            
                
                    <h3><a href="http://dx.doi.org/10.1063/5.0278183" target="_blank">Monolithic optoelectronic circuit design for on-chip terahertz applications</a></h3>
                    <p><strong>Authors:</strong> Kateryna Kusyak, Benedikt Schulte, Toru Matsuyama, Gunda Kipp, Hope M. Bretscher, Matthew W. Day, Guido Meier, Alexander M. Potts, James W. McIver</p>
                    <p><strong>Published:</strong> 7/28/2025</p>
                    <p><strong>Categories:</strong> physics.optics, physics.ins-det</p>
                    <p><strong>Summary:</strong> We demonstrate a monolithic coplanar stripline platform for on-chip terahertz (THz) generation, transmission, and detection, addressing key challenges of mode purity, bandwidth, and referencing. Capacitive coupling of the photoconductive generator switch enforces pure odd-mode propagation, increases THz field strength, and extends the operational frequency range, achieving 0.05-1.4 THz. Our architecture enables fully monolithic fabrication with amorphous silicon switches, in situ field referencing, and galvanic isolation between generation and detection. Finite-element simulations and experiments confirm that suppressing parasitic modes improves signal integrity, providing a robust platform for high-fidelity THz spectroscopy, ultrafast electronics, and nanoscale quantum materials research.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2507.21049v1" target="_blank">Rep-MTL: Unleashing the Power of Representation-level Task Saliency for Multi-Task Learning</a></h3>
                    <p><strong>Authors:</strong> Zedong Wang, Siyuan Li, Dan Xu</p>
                    <p><strong>Published:</strong> 7/28/2025</p>
                    <p><strong>Categories:</strong> cs.LG, cs.CV</p>
                    <p><strong>Summary:</strong> Despite the promise of Multi-Task Learning in leveraging complementary knowledge across tasks, existing multi-task optimization (MTO) techniques remain fixated on resolving conflicts via optimizer-centric loss scaling and gradient manipulation strategies, yet fail to deliver consistent gains. In this paper, we argue that the shared representation space, where task interactions naturally occur, offers rich information and potential for operations complementary to existing optimizers, especially for facilitating the inter-task complementarity, which is rarely explored in MTO. This intuition leads to Rep-MTL, which exploits the representation-level task saliency to quantify interactions between task-specific optimization and shared representation learning. By steering these saliencies through entropy-based penalization and sample-wise cross-task alignment, Rep-MTL aims to mitigate negative transfer by maintaining the effective training of individual tasks instead pure conflict-solving, while explicitly promoting complementary information sharing. Experiments are conducted on four challenging MTL benchmarks covering both task-shift and domain-shift scenarios. The results show that Rep-MTL, even paired with the basic equal weighting policy, achieves competitive performance gains with favorable efficiency. Beyond standard performance metrics, Power Law exponent analysis demonstrates Rep-MTLs efficacy in balancing task-specific learning and cross-task sharing. The project page is available at HERE.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2507.21046v1" target="_blank">A Survey of Self-Evolving Agents: On Path to Artificial Super Intelligence</a></h3>
                    <p><strong>Authors:</strong> Huan-ang Gao, Jiayi Geng, Wenyue Hua, Mengkang Hu, Xinzhe Juan, Hongzhang Liu, Shilong Liu, Jiahao Qiu, Xuan Qi, Yiran Wu, Hongru Wang, Han Xiao, Yuhang Zhou, Shaokun Zhang, Jiayi Zhang, Jinyu Xiang, Yixiong Fang, Qiwen Zhao, Dongrui Liu, Qihan Ren, Cheng Qian, Zhenghailong Wang, Minda Hu, Huazheng Wang, Qingyun Wu, Heng Ji, Mengdi Wang</p>
                    <p><strong>Published:</strong> 7/28/2025</p>
                    <p><strong>Categories:</strong> cs.AI</p>
                    <p><strong>Summary:</strong> Large Language Models (LLMs) have demonstrated strong capabilities but remain fundamentally static, unable to adapt their internal parameters to novel tasks, evolving knowledge domains, or dynamic interaction contexts. As LLMs are increasingly deployed in open-ended, interactive environments, this static nature has become a critical bottleneck, necessitating agents that can adaptively reason, act, and evolve in real time. This paradigm shift -- from scaling static models to developing self-evolving agents -- has sparked growing interest in architectures and methods enabling continual learning and adaptation from data, interactions, and experiences. This survey provides the first systematic and comprehensive review of self-evolving agents, organized around three foundational dimensions -- what to evolve, when to evolve, and how to evolve. We examine evolutionary mechanisms across agent components (e.g., models, memory, tools, architecture), categorize adaptation methods by stages (e.g., intra-test-time, inter-test-time), and analyze the algorithmic and architectural designs that guide evolutionary adaptation (e.g., scalar rewards, textual feedback, single-agent and multi-agent systems). Additionally, we analyze evaluation metrics and benchmarks tailored for self-evolving agents, highlight applications in domains such as coding, education, and healthcare, and identify critical challenges and research directions in safety, scalability, and co-evolutionary dynamics. By providing a structured framework for understanding and designing self-evolving agents, this survey establishes a roadmap for advancing adaptive agentic systems in both research and real-world deployments, ultimately shedding lights to pave the way for the realization of Artificial Super Intelligence (ASI), where agents evolve autonomously, performing at or beyond human-level intelligence across a wide array of tasks.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2507.21045v1" target="_blank">Reconstructing 4D Spatial Intelligence: A Survey</a></h3>
                    <p><strong>Authors:</strong> Yukang Cao, Jiahao Lu, Zhisheng Huang, Zhuowei Shen, Chengfeng Zhao, Fangzhou Hong, Zhaoxi Chen, Xin Li, Wenping Wang, Yuan Liu, Ziwei Liu</p>
                    <p><strong>Published:</strong> 7/28/2025</p>
                    <p><strong>Categories:</strong> cs.CV</p>
                    <p><strong>Summary:</strong> Reconstructing 4D spatial intelligence from visual observations has long been a central yet challenging task in computer vision, with broad real-world applications. These range from entertainment domains like movies, where the focus is often on reconstructing fundamental visual elements, to embodied AI, which emphasizes interaction modeling and physical realism. Fueled by rapid advances in 3D representations and deep learning architectures, the field has evolved quickly, outpacing the scope of previous surveys. Additionally, existing surveys rarely offer a comprehensive analysis of the hierarchical structure of 4D scene reconstruction. To address this gap, we present a new perspective that organizes existing methods into five progressive levels of 4D spatial intelligence: (1) Level 1 -- reconstruction of low-level 3D attributes (e.g., depth, pose, and point maps); (2) Level 2 -- reconstruction of 3D scene components (e.g., objects, humans, structures); (3) Level 3 -- reconstruction of 4D dynamic scenes; (4) Level 4 -- modeling of interactions among scene components; and (5) Level 5 -- incorporation of physical laws and constraints. We conclude the survey by discussing the key challenges at each level and highlighting promising directions for advancing toward even richer levels of 4D spatial intelligence. To track ongoing developments, we maintain an up-to-date project page: https://github.com/yukangcao/Awesome-4D-Spatial-Intelligence.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2507.21041v1" target="_blank">Primordial Black Hole Triggered Type Ia Supernovae I: Impact on Explosion Dynamics and Light Curves</a></h3>
                    <p><strong>Authors:</strong> Shing-Chi Leung, Seth Walther, Kenichi Nomoto, Alexander Kusenko</p>
                    <p><strong>Published:</strong> 7/28/2025</p>
                    <p><strong>Categories:</strong> astro-ph.HE, astro-ph.CO</p>
                    <p><strong>Summary:</strong> Primordial black holes (PBHs) in the asteroid-mass window are compelling dark matter candidates, made plausible by the existence of black holes and by the variety of mechanisms of their production in the early universe. If a PBH falls into a white dwarf (WD), the strong tidal forces can generate enough heat to trigger a thermonuclear runaway explosion, depending on the WD mass and the PBH orbital parameters. In this work, we investigate the WD explosion triggered by the passage of PBH. We perform 2D simulations of the WD undergoing thermonuclear explosion in this scenario, with the predicted ignition site as the parameter assuming the deflagration-detonation transition model. We study the explosion dynamics and predict the associated light curves and nucleosynthesis. We find that the model sequence predicts the light curves which align with the Phillips relation ($B_{\max}$ vs. $\Delta M_{15}$). Our models hint at a unifying approach in triggering Type Ia supernovae without involving two distinctive evolutionary tracks.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2507.21039v1" target="_blank">Data-parallel leading-order event generation in MadGraph5_aMC@NLO</a></h3>
                    <p><strong>Authors:</strong> Stephan HagebÃ¶ck, Daniele Massaro, Olivier Mattelaer, Stefan Roiser, Andrea Valassi, Zenny Wettersten</p>
                    <p><strong>Published:</strong> 7/28/2025</p>
                    <p><strong>Categories:</strong> hep-ph, hep-ex, physics.comp-ph</p>
                    <p><strong>Summary:</strong> The CUDACPP plugin for MadGraph5_aMC@NLO aims to accelerate leading order tree-level event generation by providing the MadEvent event generator with data-parallel helicity amplitudes. These amplitudes are written in templated C++ and CUDA, allowing them to be compiled for CPUs supporting SSE4, AVX2, and AVX-512 instruction sets as well as CUDA- and HIP-enabled GPUs. Using SIMD instruction sets, CUDACPP-generated amplitudes are shown to speed up linearly with SIMD register size, and GPU offloading is shown to provide acceleration beyond that of SIMD instructions. Additionally, the resulting speed-up in event generation perfectly aligns with predictions from measured runtime fractions spent in amplitude routines, and proper GPU utilisation can speed up high-multiplicity QCD processes by an order of magnitude when compared to optimal CPU usage in server-grade CPUs.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2507.21037v1" target="_blank">When Brain Foundation Model Meets Cauchy-Schwarz Divergence: A New Framework for Cross-Subject Motor Imagery Decoding</a></h3>
                    <p><strong>Authors:</strong> Jinzhou Wu, Baoping Tang, Qikang Li, Yi Wang, Cheng Li, Shujian Yu</p>
                    <p><strong>Published:</strong> 7/28/2025</p>
                    <p><strong>Categories:</strong> cs.LG</p>
                    <p><strong>Summary:</strong> Decoding motor imagery (MI) electroencephalogram (EEG) signals, a key non-invasive brain-computer interface (BCI) paradigm for controlling external systems, has been significantly advanced by deep learning. However, MI-EEG decoding remains challenging due to substantial inter-subject variability and limited labeled target data, which necessitate costly calibration for new users. Many existing multi-source domain adaptation (MSDA) methods indiscriminately incorporate all available source domains, disregarding the large inter-subject differences in EEG signals, which leads to negative transfer and excessive computational costs. Moreover, while many approaches focus on feature distribution alignment, they often neglect the explicit dependence between features and decision-level outputs, limiting their ability to preserve discriminative structures. To address these gaps, we propose a novel MSDA framework that leverages a pretrained large Brain Foundation Model (BFM) for dynamic and informed source subject selection, ensuring only relevant sources contribute to adaptation. Furthermore, we employ Cauchy-Schwarz (CS) and Conditional CS (CCS) divergences to jointly perform feature-level and decision-level alignment, enhancing domain invariance while maintaining class discriminability. Extensive evaluations on two benchmark MI-EEG datasets demonstrate that our framework outperforms a broad range of state-of-the-art baselines. Additional experiments with a large source pool validate the scalability and efficiency of BFM-guided selection, which significantly reduces training time without sacrificing performance.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2507.21033v1" target="_blank">GPT-IMAGE-EDIT-1.5M: A Million-Scale, GPT-Generated Image Dataset</a></h3>
                    <p><strong>Authors:</strong> Yuhan Wang, Siwei Yang, Bingchen Zhao, Letian Zhang, Qing Liu, Yuyin Zhou, Cihang Xie</p>
                    <p><strong>Published:</strong> 7/28/2025</p>
                    <p><strong>Categories:</strong> cs.CV</p>
                    <p><strong>Summary:</strong> Recent advancements in large multimodal models like GPT-4o have set a new standard for high-fidelity, instruction-guided image editing. However, the proprietary nature of these models and their training data creates a significant barrier for open-source research. To bridge this gap, we introduce GPT-IMAGE-EDIT-1.5M, a publicly available, large-scale image-editing corpus containing more than 1.5 million high-quality triplets (instruction, source image, edited image). We systematically construct this dataset by leveraging the versatile capabilities of GPT-4o to unify and refine three popular image-editing datasets: OmniEdit, HQ-Edit, and UltraEdit. Specifically, our methodology involves 1) regenerating output images to enhance visual quality and instruction alignment, and 2) selectively rewriting prompts to improve semantic clarity. To validate the efficacy of our dataset, we fine-tune advanced open-source models on GPT-IMAGE-EDIT-1.5M. The empirical results are exciting, e.g., the fine-tuned FluxKontext achieves highly competitive performance across a comprehensive suite of benchmarks, including 7.24 on GEdit-EN, 3.80 on ImgEdit-Full, and 8.78 on Complex-Edit, showing stronger instruction following and higher perceptual quality while maintaining identity. These scores markedly exceed all previously published open-source methods and substantially narrow the gap to leading proprietary models. We hope the full release of GPT-IMAGE-EDIT-1.5M can help to catalyze further open research in instruction-guided image editing.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2507.21028v1" target="_blank">Multi-Agent-as-Judge: Aligning LLM-Agent-Based Automated Evaluation with Multi-Dimensional Human Evaluation</a></h3>
                    <p><strong>Authors:</strong> Jiaju Chen, Yuxuan Lu, Xiaojie Wang, Huimin Zeng, Jing Huang, Jiri Gesi, Ying Xu, Bingsheng Yao, Dakuo Wang</p>
                    <p><strong>Published:</strong> 7/28/2025</p>
                    <p><strong>Categories:</strong> cs.CL, 68T50</p>
                    <p><strong>Summary:</strong> Nearly all human work is collaborative; thus, the evaluation of real-world NLP applications often requires multiple dimensions that align with diverse human perspectives. As real human evaluator resources are often scarce and costly, the emerging LLM-as-a-judge paradigm sheds light on a promising approach to leverage LLM agents to believably simulate human evaluators. Yet, to date, existing LLM-as-a-judge approaches face two limitations: persona descriptions of agents are often arbitrarily designed, and the frameworks are not generalizable to other tasks. To address these challenges, we propose MAJ-EVAL, a Multi-Agent-as-Judge evaluation framework that can automatically construct multiple evaluator personas with distinct dimensions from relevant text documents (e.g., research papers), instantiate LLM agents with the personas, and engage in-group debates with multi-agents to Generate multi-dimensional feedback. Our evaluation experiments in both the educational and medical domains demonstrate that MAJ-EVAL can generate evaluation results that better align with human experts ratings compared with conventional automated evaluation metrics and existing LLM-as-a-judge methods.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2507.21018v1" target="_blank">Deep Learning for Skeleton Based Human Motion Rehabilitation Assessment: A Benchmark</a></h3>
                    <p><strong>Authors:</strong> Ali Ismail-Fawaz, Maxime Devanne, Stefano Berretti, Jonathan Weber, Germain Forestier</p>
                    <p><strong>Published:</strong> 7/28/2025</p>
                    <p><strong>Categories:</strong> cs.CV, cs.LG</p>
                    <p><strong>Summary:</strong> Automated assessment of human motion plays a vital role in rehabilitation, enabling objective evaluation of patient performance and progress. Unlike general human activity recognition, rehabilitation motion assessment focuses on analyzing the quality of movement within the same action class, requiring the detection of subtle deviations from ideal motion. Recent advances in deep learning and video-based skeleton extraction have opened new possibilities for accessible, scalable motion assessment using affordable devices such as smartphones or webcams. However, the field lacks standardized benchmarks, consistent evaluation protocols, and reproducible methodologies, limiting progress and comparability across studies. In this work, we address these gaps by (i) aggregating existing rehabilitation datasets into a unified archive called Rehab-Pile, (ii) proposing a general benchmarking framework for evaluating deep learning methods in this domain, and (iii) conducting extensive benchmarking of multiple architectures across classification and regression tasks. All datasets and implementations are released to the community to support transparency and reproducibility. This paper aims to establish a solid foundation for future research in automated rehabilitation assessment and foster the development of reliable, accessible, and personalized rehabilitation solutions. The datasets, source-code and results of this article are all publicly available.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2507.21016v1" target="_blank">Predicting Cognition from fMRI:A Comparative Study of Graph, Transformer, and Kernel Models Across Task and Rest Conditions</a></h3>
                    <p><strong>Authors:</strong> Jagruti Patel, Mikkel SchÃ¶ttner, Thomas A. W. Bolton, Patric Hagmann</p>
                    <p><strong>Published:</strong> 7/28/2025</p>
                    <p><strong>Categories:</strong> cs.LG, q-bio.NC</p>
                    <p><strong>Summary:</strong> Predicting cognition from neuroimaging data in healthy individuals offers insights into the neural mechanisms underlying cognitive abilities, with potential applications in precision medicine and early detection of neurological and psychiatric conditions. This study systematically benchmarked classical machine learning (Kernel Ridge Regression (KRR)) and advanced deep learning (DL) models (Graph Neural Networks (GNN) and Transformer-GNN (TGNN)) for cognitive prediction using Resting-state (RS), Working Memory, and Language task fMRI data from the Human Connectome Project Young Adult dataset. Our results, based on R2 scores, Pearson correlation coefficient, and mean absolute error, revealed that task-based fMRI, eliciting neural responses directly tied to cognition, outperformed RS fMRI in predicting cognitive behavior. Among the methods compared, a GNN combining structural connectivity (SC) and functional connectivity (FC) consistently achieved the highest performance across all fMRI modalities; however, its advantage over KRR using FC alone was not statistically significant. The TGNN, designed to model temporal dynamics with SC as a prior, performed competitively with FC-based approaches for task-fMRI but struggled with RS data, where its performance aligned with the lower-performing GNN that directly used fMRI time-series data as node features. These findings emphasize the importance of selecting appropriate model architectures and feature representations to fully leverage the spatial and temporal richness of neuroimaging data. This study highlights the potential of multimodal graph-aware DL models to combine SC and FC for cognitive prediction, as well as the promise of Transformer-based approaches for capturing temporal dynamics. By providing a comprehensive comparison of models, this work serves as a guide for advancing brain-behavior modeling using fMRI, SC and DL.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2507.21012v1" target="_blank">User-Centered Design with AI in the Loop: A Case Study of Rapid User Interface Prototyping with Vibe Coding</a></h3>
                    <p><strong>Authors:</strong> Tianyi Li, Tanay Maheshwari, Alex Voelker</p>
                    <p><strong>Published:</strong> 7/28/2025</p>
                    <p><strong>Categories:</strong> cs.HC</p>
                    <p><strong>Summary:</strong> We present a case study of using generative user interfaces, or ``vibe coding, a method leveraging large language models (LLMs) for generating code via natural language prompts, to support rapid prototyping in user-centered design (UCD). Extending traditional UCD practices, we propose an AI-in-the-loop ideate-prototyping process. We share insights from an empirical experience integrating this process to develop an interactive data analytics interface for highway traffic engineers to effectively retrieve and analyze historical traffic data. With generative UIs, the team was able to elicit rich user feedback and test multiple alternative design ideas from user evaluation interviews and real-time collaborative sessions with domain experts. We discuss the advantages and pitfalls of vibe coding for bridging the gaps between design expertise and domain-specific expertise.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2507.21009v1" target="_blank">Memorization in Fine-Tuned Large Language Models</a></h3>
                    <p><strong>Authors:</strong> Danil Savine, Muni Sreenivas Pydi, Jamal Atif, Olivier CappÃ©</p>
                    <p><strong>Published:</strong> 7/28/2025</p>
                    <p><strong>Categories:</strong> cs.CL, cs.AI</p>
                    <p><strong>Summary:</strong> This study investigates the mechanisms and factors influencing memorization in fine-tuned large language models (LLMs), with a focus on the medical domain due to its privacy-sensitive nature. We examine how different aspects of the fine-tuning process affect a models propensity to memorize training data, using the PHEE dataset of pharmacovigilance events. Our research employs two main approaches: a membership inference attack to detect memorized data, and a generation task with prompted prefixes to assess verbatim reproduction. We analyze the impact of adapting different weight matrices in the transformer architecture, the relationship between perplexity and memorization, and the effect of increasing the rank in low-rank adaptation (LoRA) fine-tuning. Key findings include: (1) Value and Output matrices contribute more significantly to memorization compared to Query and Key matrices; (2) Lower perplexity in the fine-tuned model correlates with increased memorization; (3) Higher LoRA ranks lead to increased memorization, but with diminishing returns at higher ranks. These results provide insights into the trade-offs between model performance and privacy risks in fine-tuned LLMs. Our findings have implications for developing more effective and responsible strategies for adapting large language models while managing data privacy concerns.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2507.20997v1" target="_blank">Modular Delta Merging with Orthogonal Constraints: A Scalable Framework for Continual and Reversible Model Composition</a></h3>
                    <p><strong>Authors:</strong> Haris Khan, Shumaila Asif, Sadia Asif</p>
                    <p><strong>Published:</strong> 7/28/2025</p>
                    <p><strong>Categories:</strong> cs.LG, cs.AI</p>
                    <p><strong>Summary:</strong> In real-world machine learning deployments, models must be continually updated, composed, and when required, selectively undone. However, existing approaches to model merging and continual learning often suffer from task interference, catastrophic forgetting, or lack of reversibility. We propose Modular Delta Merging with Orthogonal Constraints (MDM-OC), a novel framework that enables scalable, interference-free, and reversible composition of fine-tuned models. Each task-specific model is encoded as a delta from a shared base and projected into an orthogonal subspace to eliminate conflict. These projected deltas are then merged via gradient-based optimization to form a unified model that retains performance across tasks. Our approach supports continual integration of new models, structured unmerging for compliance such as GDPR requirements, and model stability via elastic weight consolidation and synthetic replay. Extensive experiments on vision and natural language processing benchmarks demonstrate that MDM-OC outperforms prior baselines in accuracy, backward transfer, and unmerge fidelity, while remaining memory-efficient and computationally tractable. This framework offers a principled solution for modular and compliant AI system design.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2507.20994v1" target="_blank">Security Tensors as a Cross-Modal Bridge: Extending Text-Aligned Safety to Vision in LVLM</a></h3>
                    <p><strong>Authors:</strong> Shen Li, Liuyi Yao, Wujia Niu, Lan Zhang, Yaliang Li</p>
                    <p><strong>Published:</strong> 7/28/2025</p>
                    <p><strong>Categories:</strong> cs.CV, cs.AI</p>
                    <p><strong>Summary:</strong> Large visual-language models (LVLMs) integrate aligned large language models (LLMs) with visual modules to process multimodal inputs. However, the safety mechanisms developed for text-based LLMs do not naturally extend to visual modalities, leaving LVLMs vulnerable to harmful image inputs. To address this cross-modal safety gap, we introduce security tensors - trainable input vectors applied during inference through either the textual or visual modality. These tensors transfer textual safety alignment to visual processing without modifying the models parameters. They are optimized using a curated dataset containing (i) malicious image-text pairs requiring rejection, (ii) contrastive benign pairs with text structurally similar to malicious queries, with the purpose of being contrastive examples to guide visual reliance, and (iii) general benign samples preserving model functionality. Experimental results demonstrate that both textual and visual security tensors significantly enhance LVLMs ability to reject diverse harmful visual inputs while maintaining near-identical performance on benign tasks. Further internal analysis towards hidden-layer representations reveals that security tensors successfully activate the language modules textual safety layers in visual inputs, thereby effectively extending text-based safety to the visual modality.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2507.20987v1" target="_blank">JWB-DH-V1: Benchmark for Joint Whole-Body Talking Avatar and Speech Generation Version 1</a></h3>
                    <p><strong>Authors:</strong> Xinhan Di, Kristin Qi, Pengqian Yu</p>
                    <p><strong>Published:</strong> 7/28/2025</p>
                    <p><strong>Categories:</strong> cs.CV, cs.AI</p>
                    <p><strong>Summary:</strong> Recent advances in diffusion-based video generation have enabled photo-realistic short clips, but current methods still struggle to achieve multi-modal consistency when jointly generating whole-body motion and natural speech. Current approaches lack comprehensive evaluation frameworks that assess both visual and audio quality, and there are insufficient benchmarks for region-specific performance analysis. To address these gaps, we introduce the Joint Whole-Body Talking Avatar and Speech Generation Version I(JWB-DH-V1), comprising a large-scale multi-modal dataset with 10,000 unique identities across 2 million video samples, and an evaluation protocol for assessing joint audio-video generation of whole-body animatable avatars. Our evaluation of SOTA models reveals consistent performance disparities between face/hand-centric and whole-body performance, which incidates essential areas for future research. The dataset and evaluation tools are publicly available at https://github.com/deepreasonings/WholeBodyBenchmark.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2507.20985v1" target="_blank">Behavioral Study of Dashboard Mechanisms</a></h3>
                    <p><strong>Authors:</strong> Paula Kayongo, Jessica Hullman, Jason Hartline</p>
                    <p><strong>Published:</strong> 7/28/2025</p>
                    <p><strong>Categories:</strong> cs.GT</p>
                    <p><strong>Summary:</strong> Visualization dashboards are increasingly used in strategic settings like auctions to enhance decision-making and reduce strategic confusion. This paper presents behavioral experiments evaluating how different dashboard designs affect bid optimization in reverse first-price auctions. Additionally, we assess how dashboard designs impact the auction designers ability to accurately infer bidders preferences within the dashboard mechanism framework. We compare visualizations of the bid allocation rule, commonly deployed in practice, to alternatives that display expected utility. We find that utility-based visualizations significantly improve bidding by reducing cognitive demands on bidders. However, even with improved dashboards, bidders systematically under-shade their bids, driven by an implicit preference for certain wins in uncertain settings. As a result, dashboard-based mechanisms that assume fully rational or risk-neutral bidder responses to dashboards can produce significant estimation errors when inferring private preferences, which may lead to suboptimal allocations in practice. Explicitly modeling agents behavioral responses to dashboards substantially improves inference accuracy, highlighting the need to align visualization design and econometric inference assumptions in practice.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2507.20980v1" target="_blank">LargeMvC-Net: Anchor-based Deep Unfolding Network for Large-scale Multi-view Clustering</a></h3>
                    <p><strong>Authors:</strong> Shide Du, Chunming Wu, Zihan Fang, Wendi Zhao, Yilin Wu, Changwei Wang, Shiping Wang</p>
                    <p><strong>Published:</strong> 7/28/2025</p>
                    <p><strong>Categories:</strong> cs.CV, stat.CO, stat.ML</p>
                    <p><strong>Summary:</strong> Deep anchor-based multi-view clustering methods enhance the scalability of neural networks by utilizing representative anchors to reduce the computational complexity of large-scale clustering. Despite their scalability advantages, existing approaches often incorporate anchor structures in a heuristic or task-agnostic manner, either through post-hoc graph construction or as auxiliary components for message passing. Such designs overlook the core structural demands of anchor-based clustering, neglecting key optimization principles. To bridge this gap, we revisit the underlying optimization problem of large-scale anchor-based multi-view clustering and unfold its iterative solution into a novel deep network architecture, termed LargeMvC-Net. The proposed model decomposes the anchor-based clustering process into three modules: RepresentModule, NoiseModule, and AnchorModule, corresponding to representation learning, noise suppression, and anchor indicator estimation. Each module is derived by unfolding a step of the original optimization procedure into a dedicated network component, providing structural clarity and optimization traceability. In addition, an unsupervised reconstruction loss aligns each view with the anchor-induced latent space, encouraging consistent clustering structures across views. Extensive experiments on several large-scale multi-view benchmarks show that LargeMvC-Net consistently outperforms state-of-the-art methods in terms of both effectiveness and scalability.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2507.20976v1" target="_blank">Adapting Vehicle Detectors for Aerial Imagery to Unseen Domains with Weak Supervision</a></h3>
                    <p><strong>Authors:</strong> Xiao Fang, Minhyek Jeon, Zheyang Qin, Stanislav Panev, Celso de Melo, Shuowen Hu, Shayok Chakraborty, Fernando De la Torre</p>
                    <p><strong>Published:</strong> 7/28/2025</p>
                    <p><strong>Categories:</strong> cs.CV</p>
                    <p><strong>Summary:</strong> Detecting vehicles in aerial imagery is a critical task with applications in traffic monitoring, urban planning, and defense intelligence. Deep learning methods have provided state-of-the-art (SOTA) results for this application. However, a significant challenge arises when models trained on data from one geographic region fail to generalize effectively to other areas. Variability in factors such as environmental conditions, urban layouts, road networks, vehicle types, and image acquisition parameters (e.g., resolution, lighting, and angle) leads to domain shifts that degrade model performance. This paper proposes a novel method that uses generative AI to synthesize high-quality aerial images and their labels, improving detector training through data augmentation. Our key contribution is the development of a multi-stage, multi-modal knowledge transfer framework utilizing fine-tuned latent diffusion models (LDMs) to mitigate the distribution gap between the source and target environments. Extensive experiments across diverse aerial imagery domains show consistent performance improvements in AP50 over supervised learning on source domain data, weakly supervised adaptation methods, unsupervised domain adaptation methods, and open-set object detectors by 4-23%, 6-10%, 7-40%, and more than 50%, respectively. Furthermore, we introduce two newly annotated aerial datasets from New Zealand and Utah to support further research in this field. Project page is available at: https://humansensinglab.github.io/AGenDA</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2507.20973v1" target="_blank">Model-Agnostic Gender Bias Control for Text-to-Image Generation via Sparse Autoencoder</a></h3>
                    <p><strong>Authors:</strong> Chao Wu, Zhenyi Wang, Kangxian Xie, Naresh Kumar Devulapally, Vishnu Suresh Lokhande, Mingchen Gao</p>
                    <p><strong>Published:</strong> 7/28/2025</p>
                    <p><strong>Categories:</strong> cs.LG, cs.CV</p>
                    <p><strong>Summary:</strong> Text-to-image (T2I) diffusion models often exhibit gender bias, particularly by generating stereotypical associations between professions and gendered subjects. This paper presents SAE Debias, a lightweight and model-agnostic framework for mitigating such bias in T2I generation. Unlike prior approaches that rely on CLIP-based filtering or prompt engineering, which often require model-specific adjustments and offer limited control, SAE Debias operates directly within the feature space without retraining or architectural modifications. By leveraging a k-sparse autoencoder pre-trained on a gender bias dataset, the method identifies gender-relevant directions within the sparse latent space, capturing professional stereotypes. Specifically, a biased direction per profession is constructed from sparse latents and suppressed during inference to steer generations toward more gender-balanced outputs. Trained only once, the sparse autoencoder provides a reusable debiasing direction, offering effective control and interpretable insight into biased subspaces. Extensive evaluations across multiple T2I models, including Stable Diffusion 1.4, 1.5, 2.1, and SDXL, demonstrate that SAE Debias substantially reduces gender bias while preserving generation quality. To the best of our knowledge, this is the first work to apply sparse autoencoders for identifying and intervening in gender bias within T2I models. These findings contribute toward building socially responsible generative AI, providing an interpretable and model-agnostic tool to support fairness in text-to-image generation.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2507.20968v1" target="_blank">From Entanglement to Alignment: Representation Space Decomposition for Unsupervised Time Series Domain Adaptation</a></h3>
                    <p><strong>Authors:</strong> Rongyao Cai, Ming Jin, Qingsong Wen, Kexin Zhang</p>
                    <p><strong>Published:</strong> 7/28/2025</p>
                    <p><strong>Categories:</strong> cs.LG, cs.AI</p>
                    <p><strong>Summary:</strong> Domain shift poses a fundamental challenge in time series analysis, where models trained on source domain often fail dramatically when applied in target domain with different yet similar distributions. While current unsupervised domain adaptation (UDA) methods attempt to align cross-domain feature distributions, they typically treat features as indivisible entities, ignoring their intrinsic compositions that governs domain adaptation. We introduce DARSD, a novel UDA framework with theoretical explainability that explicitly realizes UDA tasks from the perspective of representation space decomposition. Our core insight is that effective domain adaptation requires not just alignment, but principled disentanglement of transferable knowledge from mixed representations. DARSD consists three synergistic components: (I) An adversarial learnable common invariant basis that projects original features into a domain-invariant subspace while preserving semantic content; (II) A prototypical pseudo-labeling mechanism that dynamically separates target features based on confidence, hindering error accumulation; (III) A hybrid contrastive optimization strategy that simultaneously enforces feature clustering and consistency while mitigating emerging distribution gaps. Comprehensive experiments conducted on four benchmark datasets (WISDM, HAR, HHAR, and MFD) demonstrate DARSDs superiority against 12 UDA algorithms, achieving optimal performance in 35 out of 53 cross-domain scenarios.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2507.20967v1" target="_blank">PROVCREATOR: Synthesizing Complex Heterogenous Graphs with Node and Edge Attributes</a></h3>
                    <p><strong>Authors:</strong> Tianhao Wang, Simon Klancher, Kunal Mukherjee, Josh Wiedemeier, Feng Chen, Murat Kantarcioglu, Kangkook Jee</p>
                    <p><strong>Published:</strong> 7/28/2025</p>
                    <p><strong>Categories:</strong> cs.LG</p>
                    <p><strong>Summary:</strong> The rise of graph-structured data has driven interest in graph learning and synthetic data generation. While successful in text and image domains, synthetic graph generation remains challenging -- especially for real-world graphs with complex, heterogeneous schemas. Existing research has focused mostly on homogeneous structures with simple attributes, limiting their usefulness and relevance for application domains requiring semantic fidelity. In this research, we introduce ProvCreator, a synthetic graph framework designed for complex heterogeneous graphs with high-dimensional node and edge attributes. ProvCreator formulates graph synthesis as a sequence generation task, enabling the use of transformer-based large language models. It features a versatile graph-to-sequence encoder-decoder that 1. losslessly encodes graph structure and attributes, 2. efficiently compresses large graphs for contextual modeling, and 3. supports end-to-end, learnable graph generation. To validate our research, we evaluate ProvCreator on two challenging domains: system provenance graphs in cybersecurity and knowledge graphs from IntelliGraph Benchmark Dataset. In both cases, ProvCreator captures intricate dependencies between structure and semantics, enabling the generation of realistic and privacy-aware synthetic datasets.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2507.20964v1" target="_blank">Core Safety Values for Provably Corrigible Agents</a></h3>
                    <p><strong>Authors:</strong> Aran Nayebi</p>
                    <p><strong>Published:</strong> 7/28/2025</p>
                    <p><strong>Categories:</strong> cs.AI, cs.CC, cs.GT, cs.LG, cs.MA</p>
                    <p><strong>Summary:</strong> We introduce the first implementable framework for corrigibility, with provable guarantees in multi-step, partially observed environments. Our framework replaces a single opaque reward with five *structurally separate* utility heads -- deference, switch-access preservation, truthfulness, low-impact behavior via a belief-based extension of Attainable Utility Preservation, and bounded task reward -- combined lexicographically by strict weight gaps. Theorem 1 proves exact single-round corrigibility in the partially observable off-switch game; Theorem 3 extends the guarantee to multi-step, self-spawning agents, showing that even if each head is \emph{learned} to mean-squared error $\varepsilon$ and the planner is $\varepsilon$-sub-optimal, the probability of violating \emph{any} safety property is bounded while still ensuring net human benefit. In contrast to Constitutional AI or RLHF/RLAIF, which merge all norms into one learned scalar, our separation makes obedience and impact-limits dominate even when incentives conflict. For open-ended settings where adversaries can modify the agent, we prove that deciding whether an arbitrary post-hack agent will ever violate corrigibility is undecidable by reduction to the halting problem, then carve out a finite-horizon ``decidable island where safety can be certified in randomized polynomial time and verified with privacy-preserving, constant-round zero-knowledge proofs. Consequently, the remaining challenge is the ordinary ML task of data coverage and generalization: reward-hacking risk is pushed into evaluation quality rather than hidden incentive leak-through, giving clearer implementation guidance for todays LLM assistants and future autonomous systems.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2507.20957v1" target="_blank">Your AI, Not Your View: The Bias of LLMs in Investment Analysis</a></h3>
                    <p><strong>Authors:</strong> Hoyoung Lee, Junhyuk Seo, Suhwan Park, Junhyeong Lee, Wonbin Ahn, Chanyeol Choi, Alejandro Lopez-Lira, Yongjae Lee</p>
                    <p><strong>Published:</strong> 7/28/2025</p>
                    <p><strong>Categories:</strong> q-fin.PM, cs.AI, cs.CL</p>
                    <p><strong>Summary:</strong> In finance, Large Language Models (LLMs) face frequent knowledge conflicts due to discrepancies between pre-trained parametric knowledge and real-time market data. These conflicts become particularly problematic when LLMs are deployed in real-world investment services, where misalignment between a models embedded preferences and those of the financial institution can lead to unreliable recommendations. Yet little research has examined what investment views LLMs actually hold. We propose an experimental framework to investigate such conflicts, offering the first quantitative analysis of confirmation bias in LLM-based investment analysis. Using hypothetical scenarios with balanced and imbalanced arguments, we extract models latent preferences and measure their persistence. Focusing on sector, size, and momentum, our analysis reveals distinct, model-specific tendencies. In particular, we observe a consistent preference for large-cap stocks and contrarian strategies across most models. These preferences often harden into confirmation bias, with models clinging to initial judgments despite counter-evidence.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2507.20953v1" target="_blank">Mask-Free Audio-driven Talking Face Generation for Enhanced Visual Quality and Identity Preservation</a></h3>
                    <p><strong>Authors:</strong> Dogucan Yaman, Fevziye Irem Eyiokur, Leonard BÃ¤rmann, HazÄ±m Kemal Ekenel, Alexander Waibel</p>
                    <p><strong>Published:</strong> 7/28/2025</p>
                    <p><strong>Categories:</strong> cs.CV</p>
                    <p><strong>Summary:</strong> Audio-Driven Talking Face Generation aims at generating realistic videos of talking faces, focusing on accurate audio-lip synchronization without deteriorating any identity-related visual details. Recent state-of-the-art methods are based on inpainting, meaning that the lower half of the input face is masked, and the model fills the masked region by generating lips aligned with the given audio. Hence, to preserve identity-related visual details from the lower half, these approaches additionally require an unmasked identity reference image randomly selected from the same video. However, this common masking strategy suffers from (1) information loss in the input faces, significantly affecting the networks ability to preserve visual quality and identity details, (2) variation between identity reference and input image degrading reconstruction performance, and (3) the identity reference negatively impacting the model, causing unintended copying of elements unaligned with the audio. To address these issues, we propose a mask-free talking face generation approach while maintaining the 2D-based face editing task. Instead of masking the lower half, we transform the input images to have closed mouths, using a two-step landmark-based approach trained in an unpaired manner. Subsequently, we provide these edited but unmasked faces to a lip adaptation model alongside the audio to generate appropriate lip movements. Thus, our approach needs neither masked input images nor identity reference images. We conduct experiments on the benchmark LRS2 and HDTF datasets and perform various ablation studies to validate our contributions.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2507.20934v1" target="_blank">Exploring text-to-image generation for historical document image retrieval</a></h3>
                    <p><strong>Authors:</strong> Melissa Cote, Alexandra Branzan Albu</p>
                    <p><strong>Published:</strong> 7/28/2025</p>
                    <p><strong>Categories:</strong> cs.CV</p>
                    <p><strong>Summary:</strong> Attribute-based document image retrieval (ABDIR) was recently proposed as an alternative to query-by-example (QBE) searches, the dominant document image retrieval (DIR) paradigm. One drawback of QBE searches is that they require sample query documents on hand that may not be available. ABDIR aims to offer users a flexible way to retrieve document images based on memorable visual features of document contents, describing document images with combinations of visual attributes determined via convolutional neural network (CNN)-based binary classifiers. We present an exploratory study of the use of generative AI to bridge the gap between QBE and ABDIR, focusing on historical documents as a use case for their diversity and uniqueness in visual features. We hypothesize that text-to-image (T2I) generation can be leveraged to create query document images using text prompts based on ABDIR-like attributes. We propose T2I-QBE, which uses Leonardo.Ai as the T2I generator with prompts that include a rough description of the desired document type and a list of the desired ABDIR-style attributes. This creates query images that are then used within the traditional QBE paradigm, which compares CNN-extracted query features to those of the document images in the dataset to retrieve the most relevant documents. Experiments on the HisIR19 dataset of historical documents confirm our hypothesis and suggest that T2I-QBE is a viable option for historical document image retrieval. To the authors knowledge, this is the first attempt at utilizing T2I generation for DIR.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2507.20930v1" target="_blank">FRED: Financial Retrieval-Enhanced Detection and Editing of Hallucinations in Language Models</a></h3>
                    <p><strong>Authors:</strong> Likun Tan, Kuan-Wei Huang, Kevin Wu</p>
                    <p><strong>Published:</strong> 7/28/2025</p>
                    <p><strong>Categories:</strong> cs.CL, cs.AI, cs.LG</p>
                    <p><strong>Summary:</strong> Hallucinations in large language models pose a critical challenge for applications requiring factual reliability, particularly in high-stakes domains such as finance. This work presents an effective approach for detecting and editing factually incorrect content in model-generated responses based on the provided context. Given a user-defined domain-specific error taxonomy, we construct a synthetic dataset by inserting tagged errors into financial question-answering corpora and then fine-tune four language models, Phi-4, Phi-4-mini, Qwen3-4B, and Qwen3-14B, to detect and edit these factual inaccuracies. Our best-performing model, fine-tuned Phi-4, achieves an 8% improvement in binary F1 score and a 30% gain in overall detection performance compared to OpenAI-o3. Notably, our fine-tuned Phi-4-mini model, despite having only 4 billion parameters, maintains competitive performance with just a 2% drop in binary detection and a 0.1% decline in overall detection compared to OpenAI-o3. Our work provides a practical solution for detecting and editing factual inconsistencies in financial text generation while introducing a generalizable framework that can enhance the trustworthiness and alignment of large language models across diverse applications beyond finance. Our code and data are available at https://github.com/pegasi-ai/fine-grained-editting.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2507.20920v1" target="_blank">RIS-LAD: A Benchmark and Model for Referring Low-Altitude Drone Image Segmentation</a></h3>
                    <p><strong>Authors:</strong> Kai Ye, YingShi Luan, Zhudi Chen, Guangyue Meng, Pingyang Dai, Liujuan Cao</p>
                    <p><strong>Published:</strong> 7/28/2025</p>
                    <p><strong>Categories:</strong> cs.CV</p>
                    <p><strong>Summary:</strong> Referring Image Segmentation (RIS), which aims to segment specific objects based on natural language descriptions, plays an essential role in vision-language understanding. Despite its progress in remote sensing applications, RIS in Low-Altitude Drone (LAD) scenarios remains underexplored. Existing datasets and methods are typically designed for high-altitude and static-view imagery. They struggle to handle the unique characteristics of LAD views, such as diverse viewpoints and high object density. To fill this gap, we present RIS-LAD, the first fine-grained RIS benchmark tailored for LAD scenarios. This dataset comprises 13,871 carefully annotated image-text-mask triplets collected from realistic drone footage, with a focus on small, cluttered, and multi-viewpoint scenes. It highlights new challenges absent in previous benchmarks, such as category drift caused by tiny objects and object drift under crowded same-class objects. To tackle these issues, we propose the Semantic-Aware Adaptive Reasoning Network (SAARN). Rather than uniformly injecting all linguistic features, SAARN decomposes and routes semantic information to different stages of the network. Specifically, the Category-Dominated Linguistic Enhancement (CDLE) aligns visual features with object categories during early encoding, while the Adaptive Reasoning Fusion Module (ARFM) dynamically selects semantic cues across scales to improve reasoning in complex scenes. The experimental evaluation reveals that RIS-LAD presents substantial challenges to state-of-the-art RIS algorithms, and also demonstrates the effectiveness of our proposed model in addressing these challenges. The dataset and code will be publicly released soon at: https://github.com/AHideoKuzeA/RIS-LAD/.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2507.20913v1" target="_blank">HAMLET-FFD: Hierarchical Adaptive Multi-modal Learning Embeddings Transformation for Face Forgery Detection</a></h3>
                    <p><strong>Authors:</strong> Jialei Cui, Jianwei Du, Yanzhe Li, Lei Gao, Hui Jiang, Chenfu Bao</p>
                    <p><strong>Published:</strong> 7/28/2025</p>
                    <p><strong>Categories:</strong> cs.CV, cs.AI</p>
                    <p><strong>Summary:</strong> The rapid evolution of face manipulation techniques poses a critical challenge for face forgery detection: cross-domain generalization. Conventional methods, which rely on simple classification objectives, often fail to learn domain-invariant representations. We propose HAMLET-FFD, a cognitively inspired Hierarchical Adaptive Multi-modal Learning framework that tackles this challenge via bidirectional cross-modal reasoning. Building on contrastive vision-language models such as CLIP, HAMLET-FFD introduces a knowledge refinement loop that iteratively assesses authenticity by integrating visual evidence with conceptual cues, emulating expert forensic analysis. A key innovation is a bidirectional fusion mechanism in which textual authenticity embeddings guide the aggregation of hierarchical visual features, while modulated visual features refine text embeddings to generate image-adaptive prompts. This closed-loop process progressively aligns visual observations with semantic priors to enhance authenticity assessment. By design, HAMLET-FFD freezes all pretrained parameters, serving as an external plugin that preserves CLIPs original capabilities. Extensive experiments demonstrate its superior generalization to unseen manipulations across multiple benchmarks, and visual analyses reveal a division of labor among embeddings, with distinct representations specializing in fine-grained artifact recognition.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2507.20911v1" target="_blank">Target-density formation in swarms with stochastic sensing and dynamics</a></h3>
                    <p><strong>Authors:</strong> Jason Hindes, George Stantchev, Klimka Szwaykowska Kasraie, Ira B. Schwartz</p>
                    <p><strong>Published:</strong> 7/28/2025</p>
                    <p><strong>Categories:</strong> nlin.AO, cond-mat.soft</p>
                    <p><strong>Summary:</strong> An important goal for swarming research is to create methods for predicting, controlling and designing swarms, which produce collective dynamics that solve a problem through emergent and stable pattern formation, without the need for constant intervention, and with a minimal number of parameters and controls. One such problem involves a swarm collectively producing a desired (target) density through local sensing, motion, and interactions in a domain. Here, we take a statistical physics perspective and develop and analyze a model wherein agents move in a stochastic walk over a networked domain, so as to reduce the error between the swarm density and the target, based on local, random, and uncertain measurements of the current density by the swarming agents. Using a combination of mean-field, small-fluctuation, and finite-number analysis, we are able to quantify how close and how fast a swarm comes to producing a target as a function of sensing uncertainty, stochastic collision rates, numbers of agents, and spatial variation of the target.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2507.20907v1" target="_blank">SCORPION: Addressing Scanner-Induced Variability in Histopathology</a></h3>
                    <p><strong>Authors:</strong> Jeongun Ryu, Heon Song, Seungeun Lee, Soo Ick Cho, Jiwon Shin, Kyunghyun Paeng, SÃ©rgio Pereira</p>
                    <p><strong>Published:</strong> 7/28/2025</p>
                    <p><strong>Categories:</strong> cs.CV, cs.AI</p>
                    <p><strong>Summary:</strong> Ensuring reliable model performance across diverse domains is a critical challenge in computational pathology. A particular source of variability in Whole-Slide Images is introduced by differences in digital scanners, thus calling for better scanner generalization. This is critical for the real-world adoption of computational pathology, where the scanning devices may differ per institution or hospital, and the model should not be dependent on scanner-induced details, which can ultimately affect the patients diagnosis and treatment planning. However, past efforts have primarily focused on standard domain generalization settings, evaluating on unseen scanners during training, without directly evaluating consistency across scanners for the same tissue. To overcome this limitation, we introduce SCORPION, a new dataset explicitly designed to evaluate model reliability under scanner variability. SCORPION includes 480 tissue samples, each scanned with 5 scanners, yielding 2,400 spatially aligned patches. This scanner-paired design allows for the isolation of scanner-induced variability, enabling a rigorous evaluation of model consistency while controlling for differences in tissue composition. Furthermore, we propose SimCons, a flexible framework that combines augmentation-based domain generalization techniques with a consistency loss to explicitly address scanner generalization. We empirically show that SimCons improves model consistency on varying scanners without compromising task-specific performance. By releasing the SCORPION dataset and proposing SimCons, we provide the research community with a crucial resource for evaluating and improving model consistency across diverse scanners, setting a new standard for reliability testing.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2507.20900v1" target="_blank">Music Arena: Live Evaluation for Text-to-Music</a></h3>
                    <p><strong>Authors:</strong> Yonghyun Kim, Wayne Chi, Anastasios N. Angelopoulos, Wei-Lin Chiang, Koichi Saito, Shinji Watanabe, Yuki Mitsufuji, Chris Donahue</p>
                    <p><strong>Published:</strong> 7/28/2025</p>
                    <p><strong>Categories:</strong> cs.SD, cs.AI, cs.MM</p>
                    <p><strong>Summary:</strong> We present Music Arena, an open platform for scalable human preference evaluation of text-to-music (TTM) models. Soliciting human preferences via listening studies is the gold standard for evaluation in TTM, but these studies are expensive to conduct and difficult to compare, as study protocols may differ across systems. Moreover, human preferences might help researchers align their TTM systems or improve automatic evaluation metrics, but an open and renewable source of preferences does not currently exist. We aim to fill these gaps by offering *live* evaluation for TTM. In Music Arena, real-world users input text prompts of their choosing and compare outputs from two TTM systems, and their preferences are used to compile a leaderboard. While Music Arena follows recent evaluation trends in other AI domains, we also design it with key features tailored to music: an LLM-based routing system to navigate the heterogeneous type signatures of TTM systems, and the collection of *detailed* preferences including listening data and natural language feedback. We also propose a rolling data release policy with user privacy guarantees, providing a renewable source of preference data and increasing platform transparency. Through its standardized evaluation protocol, transparent data access policies, and music-specific features, Music Arena not only addresses key challenges in the TTM ecosystem but also demonstrates how live evaluation can be thoughtfully adapted to unique characteristics of specific AI domains. Music Arena is available at: https://music-arena.org</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2507.20881v1" target="_blank">Endoscopic Depth Estimation Based on Deep Learning: A Survey</a></h3>
                    <p><strong>Authors:</strong> Ke Niu, Zeyun Liu, Xue Feng, Heng Li, Kaize Shi</p>
                    <p><strong>Published:</strong> 7/28/2025</p>
                    <p><strong>Categories:</strong> cs.CV, cs.GR</p>
                    <p><strong>Summary:</strong> Endoscopic depth estimation is a critical technology for improving the safety and precision of minimally invasive surgery. It has attracted considerable attention from researchers in medical imaging, computer vision, and robotics. Over the past decade, a large number of methods have been developed. Despite the existence of several related surveys, a comprehensive overview focusing on recent deep learning-based techniques is still limited. This paper endeavors to bridge this gap by systematically reviewing the state-of-the-art literature. Specifically, we provide a thorough survey of the field from three key perspectives: data, methods, and applications, covering a range of methods including both monocular and stereo approaches. We describe common performance evaluation metrics and summarize publicly available datasets. Furthermore, this review analyzes the specific challenges of endoscopic scenes and categorizes representative techniques based on their supervision strategies and network architectures. The application of endoscopic depth estimation in the important area of robot-assisted surgery is also reviewed. Finally, we outline potential directions for future research, such as domain adaptation, real-time implementation, and enhanced model generalization, thereby providing a valuable starting point for researchers to engage with and advance the field.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2507.20880v1" target="_blank">JAM: A Tiny Flow-based Song Generator with Fine-grained Controllability and Aesthetic Alignment</a></h3>
                    <p><strong>Authors:</strong> Renhang Liu, Chia-Yu Hung, Navonil Majumder, Taylor Gautreaux, Amir Ali Bagherzadeh, Chuan Li, Dorien Herremans, Soujanya Poria</p>
                    <p><strong>Published:</strong> 7/28/2025</p>
                    <p><strong>Categories:</strong> cs.SD, cs.AI</p>
                    <p><strong>Summary:</strong> Diffusion and flow-matching models have revolutionized automatic text-to-audio generation in recent times. These models are increasingly capable of generating high quality and faithful audio outputs capturing to speech and acoustic events. However, there is still much room for improvement in creative audio generation that primarily involves music and songs. Recent open lyrics-to-song models, such as, DiffRhythm, ACE-Step, and LeVo, have set an acceptable standard in automatic song generation for recreational use. However, these models lack fine-grained word-level controllability often desired by musicians in their workflows. To the best of our knowledge, our flow-matching-based JAM is the first effort toward endowing word-level timing and duration control in song generation, allowing fine-grained vocal control. To enhance the quality of generated songs to better align with human preferences, we implement aesthetic alignment through Direct Preference Optimization, which iteratively refines the model using a synthetic dataset, eliminating the need or manual data annotations. Furthermore, we aim to standardize the evaluation of such lyrics-to-song models through our public evaluation dataset JAME. We show that JAM outperforms the existing models in terms of the music-specific attributes.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2507.20871v1" target="_blank">\textit{FedABC}: Attention-Based Client Selection for Federated Learning with Long-Term View</a></h3>
                    <p><strong>Authors:</strong> Wenxuan Ye, Xueli An, Junfan Wang, Xueqiang Yan, Georg Carle</p>
                    <p><strong>Published:</strong> 7/28/2025</p>
                    <p><strong>Categories:</strong> cs.NI, cs.LG</p>
                    <p><strong>Summary:</strong> Native AI support is a key objective in the evolution of 6G networks, with Federated Learning (FL) emerging as a promising paradigm. FL allows decentralized clients to collaboratively train an AI model without directly sharing their data, preserving privacy. Clients train local models on private data and share model updates, which a central server aggregates to refine the global model and redistribute it for the next iteration. However, client data heterogeneity slows convergence and reduces model accuracy, and frequent client participation imposes communication and computational burdens. To address these challenges, we propose \textit{FedABC}, an innovative client selection algorithm designed to take a long-term view in managing data heterogeneity and optimizing client participation. Inspired by attention mechanisms, \textit{FedABC} prioritizes informative clients by evaluating both model similarity and each models unique contributions to the global model. Moreover, considering the evolving demands of the global model, we formulate an optimization problem to guide \textit{FedABC} throughout the training process. Following the ``later-is-better principle, \textit{FedABC} adaptively adjusts the client selection threshold, encouraging greater participation in later training stages. Extensive simulations on CIFAR-10 demonstrate that \textit{FedABC} significantly outperforms existing approaches in model accuracy and client participation efficiency, achieving comparable performance with 32\% fewer clients than the classical FL algorithm \textit{FedAvg}, and 3.5\% higher accuracy with 2\% fewer clients than the state-of-the-art. This work marks a step toward deploying FL in heterogeneous, resource-constrained environments, thereby supporting native AI capabilities in 6G networks.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2507.20866v1" target="_blank">Neuromorphic Photonic Processing and Memory with Spiking Resonant Tunnelling Diode Neurons and Neural Networks</a></h3>
                    <p><strong>Authors:</strong> Dafydd Owen-Newns, Joshua Robertson, Giovanni Donati, Jose Figueiredo, Edward Wasige, Kathy Ludge, Bruno Romeira, Antonio Hurtado</p>
                    <p><strong>Published:</strong> 7/28/2025</p>
                    <p><strong>Categories:</strong> physics.comp-ph</p>
                    <p><strong>Summary:</strong> Neuromorphic computing-modelled after the functionality and efficiency of biological neural systems-offers promising new directions for advancing artificial intelligence and computational models. Photonic techniques for neuromorphic computing hardware are attracting increasing research interest, thanks to their potentials for ultra high bandwidths, low-crosstalk and high parallelism. Among these, approaches based upon resonant tunnelling diodes (RTDs) have recently gained attention as potential building blocks for next-generation light-enabled neuromorphic hardware, due to their capacity to replicate key neuronal behaviours such as excitable spiking and refractoriness, added to their potentials for high operational speeds, energy efficiency and compact footprints. In particular, their ability to function as opto-electronic spiking neurons makes them strong candidates for integration into novel event based neuromorphic computing systems. This work demonstrates the application of optically-triggered spiking RTD neurons to a multiplicity of applications and architectures, these include systems based upon single elements for multi-modal (photonic-electronic) fast rising edge-detection in time-series data, the construction of a two-layer feedforward artificial photonic spiking neural network (pSNN) using RTD neurons as the nonlinear nodes delivering excellent performance in complex dataset classification tasks, and a pSNN comprised of multiple coupled light-sensitive RTD spiking neurons that supports performance as an adjustable neuromorphic optical spiking memory system with a tunable storage time of spiking patterns.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2507.20862v1" target="_blank">Bi-cephalic self-attended model to classify Parkinsons disease patients with freezing of gait</a></h3>
                    <p><strong>Authors:</strong> Shomoita Jahid Mitin, Rodrigue Rizk, Maximilian Scherer, Thomas Koeglsperger, Daniel Lench, KC Santosh, Arun Singh</p>
                    <p><strong>Published:</strong> 7/28/2025</p>
                    <p><strong>Categories:</strong> cs.LG</p>
                    <p><strong>Summary:</strong> Parkinson Disease (PD) often results in motor and cognitive impairments, including gait dysfunction, particularly in patients with freezing of gait (FOG). Current detection methods are either subjective or reliant on specialized gait analysis tools. This study aims to develop an objective, data-driven, and multi-modal classification model to detect gait dysfunction in PD patients using resting-state EEG signals combined with demographic and clinical variables. We utilized a dataset of 124 participants: 42 PD patients with FOG (PDFOG+), 41 without FOG (PDFOG-), and 41 age-matched healthy controls. Features extracted from resting-state EEG and descriptive variables (age, education, disease duration) were used to train a novel Bi-cephalic Self-Attention Model (BiSAM). We tested three modalities: signal-only, descriptive-only, and multi-modal, across different EEG channel subsets (BiSAM-63, -16, -8, and -4). Signal-only and descriptive-only models showed limited performance, achieving a maximum accuracy of 55% and 68%, respectively. In contrast, the multi-modal models significantly outperformed both, with BiSAM-8 and BiSAM-4 achieving the highest classification accuracy of 88%. These results demonstrate the value of integrating EEG with objective descriptive features for robust PDFOG+ detection. This study introduces a multi-modal, attention-based architecture that objectively classifies PDFOG+ using minimal EEG channels and descriptive variables. This approach offers a scalable and efficient alternative to traditional assessments, with potential applications in routine clinical monitoring and early diagnosis of PD-related gait dysfunction.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2507.20850v1" target="_blank">Free Energy-Inspired Cognitive Risk Integration for AV Navigation in Pedestrian-Rich Environments</a></h3>
                    <p><strong>Authors:</strong> Meiting Dang, Yanping Wu, Yafei Wang, Dezong Zhao, David Flynn, Chongfeng Wei</p>
                    <p><strong>Published:</strong> 7/28/2025</p>
                    <p><strong>Categories:</strong> cs.RO, cs.AI</p>
                    <p><strong>Summary:</strong> Recent advances in autonomous vehicle (AV) behavior planning have shown impressive social interaction capabilities when interacting with other road users. However, achieving human-like prediction and decision-making in interactions with vulnerable road users remains a key challenge in complex multi-agent interactive environments. Existing research focuses primarily on crowd navigation for small mobile robots, which cannot be directly applied to AVs due to inherent differences in their decision-making strategies and dynamic boundaries. Moreover, pedestrians in these multi-agent simulations follow fixed behavior patterns that cannot dynamically respond to AV actions. To overcome these limitations, this paper proposes a novel framework for modeling interactions between the AV and multiple pedestrians. In this framework, a cognitive process modeling approach inspired by the Free Energy Principle is integrated into both the AV and pedestrian models to simulate more realistic interaction dynamics. Specifically, the proposed pedestrian Cognitive-Risk Social Force Model adjusts goal-directed and repulsive forces using a fused measure of cognitive uncertainty and physical risk to produce human-like trajectories. Meanwhile, the AV leverages this fused risk to construct a dynamic, risk-aware adjacency matrix for a Graph Convolutional Network within a Soft Actor-Critic architecture, allowing it to make more reasonable and informed decisions. Simulation results indicate that our proposed framework effectively improves safety, efficiency, and smoothness of AV navigation compared to the state-of-the-art method.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2507.20840v1" target="_blank">Towards Explainable Deep Clustering for Time Series Data</a></h3>
                    <p><strong>Authors:</strong> Udo Schlegel, Gabriel Marques Tavares, Thomas Seidl</p>
                    <p><strong>Published:</strong> 7/28/2025</p>
                    <p><strong>Categories:</strong> cs.LG</p>
                    <p><strong>Summary:</strong> Deep clustering uncovers hidden patterns and groups in complex time series data, yet its opaque decision-making limits use in safety-critical settings. This survey offers a structured overview of explainable deep clustering for time series, collecting current methods and their real-world applications. We thoroughly discuss and compare peer-reviewed and preprint papers through application domains across healthcare, finance, IoT, and climate science. Our analysis reveals that most work relies on autoencoder and attention architectures, with limited support for streaming, irregularly sampled, or privacy-preserved series, and interpretability is still primarily treated as an add-on. To push the field forward, we outline six research opportunities: (1) combining complex networks with built-in interpretability; (2) setting up clear, faithfulness-focused evaluation metrics for unsupervised explanations; (3) building explainers that adapt to live data streams; (4) crafting explanations tailored to specific domains; (5) adding human-in-the-loop methods that refine clusters and explanations together; and (6) improving our understanding of how time series clustering models work internally. By making interpretability a primary design goal rather than an afterthought, we propose the groundwork for the next generation of trustworthy deep clustering time series analytics.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2507.20837v1" target="_blank">Designing topological cluster synchronization patterns with the Dirac operator</a></h3>
                    <p><strong>Authors:</strong> Ahmed A. A. Zaid, Ginestra Bianconi</p>
                    <p><strong>Published:</strong> 7/28/2025</p>
                    <p><strong>Categories:</strong> nlin.AO, cond-mat.dis-nn, cond-mat.stat-mech, math-ph, math.DS, math.MP</p>
                    <p><strong>Summary:</strong> Designing stable cluster synchronization patterns is a fundamental challenge in nonlinear dynamics of networks with great relevance to understanding neuronal and brain dynamics. So far, cluster synchronizion have been studied exclusively in a node-based dynamical approach, according to which oscillators are associated only with the nodes of the network. Here, we propose a topological synchronization dynamics model based on the use of the Topological Dirac operator, which allows us to design cluster synchronization patterns for topological oscillators associated with both nodes and edges of a network. In particular, by modulating the ground state of the free energy associated with the dynamical model we construct topological cluster synchronization patterns. These are aligned with the eigenstates of the Topological Dirac Equation that provide a very useful decomposition of the dynamical state of node and edge signals associated with the network. We use linear stability analysis to predict the stability of the topological cluster synchronization patterns and provide numerical evidence of the ability to design several stable topological cluster synchronization states on random graphs and on stochastic block models.</p>
                
            
                
                    <h3><a href="http://dx.doi.org/10.3233/FAIA241288" target="_blank">Hanging Around: Cognitive Inspired Reasoning for Reactive Robotics</a></h3>
                    <p><strong>Authors:</strong> Mihai Pomarlan, Stefano De Giorgis, Rachel Ringe, Maria M. Hedblom, Nikolaos Tsiogkas</p>
                    <p><strong>Published:</strong> 7/28/2025</p>
                    <p><strong>Categories:</strong> cs.RO</p>
                    <p><strong>Summary:</strong> Situationally-aware artificial agents operating with competence in natural environments face several challenges: spatial awareness, object affordance detection, dynamic changes and unpredictability. A critical challenge is the agents ability to identify and monitor environmental elements pertinent to its objectives. Our research introduces a neurosymbolic modular architecture for reactive robotics. Our system combines a neural component performing object recognition over the environment and image processing techniques such as optical flow, with symbolic representation and reasoning. The reasoning system is grounded in the embodied cognition paradigm, via integrating image schematic knowledge in an ontological structure. The ontology is operatively used to create queries for the perception system, decide on actions, and infer entities capabilities derived from perceptual data. The combination of reasoning and image processing allows the agent to focus its perception for normal operation as well as discover new concepts for parts of objects involved in particular interactions. The discovered concepts allow the robot to autonomously acquire training data and adjust its subsymbolic perception to recognize the parts, as well as making planning for more complex tasks feasible by focusing search on those relevant object parts. We demonstrate our approach in a simulated world, in which an agent learns to recognize parts of objects involved in support relations. While the agent has no concept of handle initially, by observing examples of supported objects hanging from a hook it learns to recognize the parts involved in establishing support and becomes able to plan the establishment/destruction of the support relation. This underscores the agents capability to expand its knowledge through observation in a systematic way, and illustrates the potential of combining deep reasoning [...].</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2507.20829v1" target="_blank">Phase structure of below-threshold harmonics in aligned molecules: a few-level model system</a></h3>
                    <p><strong>Authors:</strong> Samuel SchÃ¶pa, Falk-Erik Wiechmann, Franziska Fennel, Dieter Bauer</p>
                    <p><strong>Published:</strong> 7/28/2025</p>
                    <p><strong>Categories:</strong> quant-ph, physics.atom-ph, physics.optics</p>
                    <p><strong>Summary:</strong> We utilize few-level model systems to analyze the polarization and phase properties of below-threshold harmonics (BTH) in aligned molecules. In a two-level system (TLS), we find that the phase of emitted harmonics undergoes a distinct change. For harmonics with photon energies below the transition between the dominant field-dressed states, the phase alternates by $\pi$ between successive odd harmonic orders but remains constant above. Exploiting this behavior, we construct a four-level model composed of two uncoupled TLS subsystems aligned along orthogonal directions. We demonstrate that with selected transition frequencies lower-order harmonics follow the polarization of the linearly polarized driving field while higher-order harmonics exhibit a mirrored polarization. The model predicts that aligned systems with orthogonal transition dipoles may show analogous phase and polarization features in the BTH regime.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2507.20815v1" target="_blank">MVIAnalyzer: A Holistic Approach to Analyze Missing Value Imputation</a></h3>
                    <p><strong>Authors:</strong> Valerie Restat, Kai Tejkl, Uta StÃ¶rl</p>
                    <p><strong>Published:</strong> 7/28/2025</p>
                    <p><strong>Categories:</strong> cs.DB</p>
                    <p><strong>Summary:</strong> Missing values often limit the usage of data analysis or cause falsification of results. Therefore, methods of missing value imputation (MVI) are of great significance. However, in general, there is no universal, fair MVI method for different tasks. This work thus places MVI in the overall context of data analysis. For this purpose, we present the MVIAnalyzer, a generic framework for a holistic analysis of MVI. It considers the overall process up to the application and analysis of machine learning methods. The associated software is provided and can be used by other researchers for their own analyses. To this end, it further includes a missing value simulation with consideration of relevant parameters. The application of the MVIAnalyzer is demonstrated on data with different characteristics. An evaluation of the results shows the possibilities and limitations of different MVI methods. Since MVI is a very complex topic with different influencing variables, this paper additionally illustrates how the analysis can be supported by visualizations.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2507.20810v1" target="_blank">Why Flow Matching is Particle Swarm Optimization?</a></h3>
                    <p><strong>Authors:</strong> Kaichen Ouyang</p>
                    <p><strong>Published:</strong> 7/28/2025</p>
                    <p><strong>Categories:</strong> cs.NE, cs.AI, cs.LG</p>
                    <p><strong>Summary:</strong> This paper preliminarily investigates the duality between flow matching in generative models and particle swarm optimization (PSO) in evolutionary computation. Through theoretical analysis, we reveal the intrinsic connections between these two approaches in terms of their mathematical formulations and optimization mechanisms: the vector field learning in flow matching shares similar mathematical expressions with the velocity update rules in PSO; both methods follow the fundamental framework of progressive evolution from initial to target distributions; and both can be formulated as dynamical systems governed by ordinary differential equations. Our study demonstrates that flow matching can be viewed as a continuous generalization of PSO, while PSO provides a discrete implementation of swarm intelligence principles. This duality understanding establishes a theoretical foundation for developing novel hybrid algorithms and creates a unified framework for analyzing both methods. Although this paper only presents preliminary discussions, the revealed correspondences suggest several promising research directions, including improving swarm intelligence algorithms based on flow matching principles and enhancing generative models using swarm intelligence concepts.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2507.20805v1" target="_blank">Understanding Bias in Perceiving Dimensionality Reduction Projections</a></h3>
                    <p><strong>Authors:</strong> Seoyoung Doh, Hyeon Jeon, Sungbok Shin, Ghulam Jilani Quadri, Nam Wook Kim, Jinwook Seo</p>
                    <p><strong>Published:</strong> 7/28/2025</p>
                    <p><strong>Categories:</strong> cs.HC, cs.LG</p>
                    <p><strong>Summary:</strong> Selecting the dimensionality reduction technique that faithfully represents the structure is essential for reliable visual communication and analytics. In reality, however, practitioners favor projections for other attractions, such as aesthetics and visual saliency, over the projections structural faithfulness, a bias we define as visual interestingness. In this research, we conduct a user study that (1) verifies the existence of such bias and (2) explains why the bias exists. Our study suggests that visual interestingness biases practitioners preferences when selecting projections for analysis, and this bias intensifies with color-encoded labels and shorter exposure time. Based on our findings, we discuss strategies to mitigate bias in perceiving and interpreting DR projections.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2507.20800v1" target="_blank">LanternNet: A Novel Hub-and-Spoke System to Seek and Suppress Spotted Lanternfly Populations</a></h3>
                    <p><strong>Authors:</strong> Vinil Polepalli</p>
                    <p><strong>Published:</strong> 7/28/2025</p>
                    <p><strong>Categories:</strong> cs.RO, cs.AI, cs.CV</p>
                    <p><strong>Summary:</strong> The invasive spotted lanternfly (SLF) poses a significant threat to agriculture and ecosystems, causing widespread damage. Current control methods, such as egg scraping, pesticides, and quarantines, prove labor-intensive, environmentally hazardous, and inadequate for long-term SLF suppression. This research introduces LanternNet, a novel autonomous robotic Hub-and-Spoke system designed for scalable detection and suppression of SLF populations. A central, tree-mimicking hub utilizes a YOLOv8 computer vision model for precise SLF identification. Three specialized robotic spokes perform targeted tasks: pest neutralization, environmental monitoring, and navigation/mapping. Field deployment across multiple infested sites over 5 weeks demonstrated LanternNets efficacy. Quantitative analysis revealed significant reductions (p  0.01, paired t-tests) in SLF populations and corresponding improvements in tree health indicators across the majority of test sites. Compared to conventional methods, LanternNet offers substantial cost advantages and improved scalability. Furthermore, the systems adaptability for enhanced autonomy and targeting of other invasive species presents significant potential for broader ecological impact. LanternNet demonstrates the transformative potential of integrating robotics and AI for advanced invasive species management and improved environmental outcomes.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2507.20798v1" target="_blank">An Efficient Machine Learning Framework for Forest Height Estimation from Multi-Polarimetric Multi-Baseline SAR data</a></h3>
                    <p><strong>Authors:</strong> Francesca Razzano, Wenyu Yang, Sergio Vitale, Giampaolo Ferraioli, Silvia Liberata Ullo, Gilda Schirinzi</p>
                    <p><strong>Published:</strong> 7/28/2025</p>
                    <p><strong>Categories:</strong> cs.CV</p>
                    <p><strong>Summary:</strong> Accurate forest height estimation is crucial for climate change monitoring and carbon cycle assessment. Synthetic Aperture Radar (SAR), particularly in multi-channel configurations, has provided support for a long time in 3D forest structure reconstruction through model-based techniques. More recently, data-driven approaches using Machine Learning (ML) and Deep Learning (DL) have enabled new opportunities for forest parameter retrieval. This paper introduces FGump, a forest height estimation framework by gradient boosting using multi-channel SAR processing with LiDAR profiles as Ground Truth(GT). Unlike typical ML and DL approaches that require large datasets and complex architectures, FGump ensures a strong balance between accuracy and computational efficiency, using a limited set of hand-designed features and avoiding heavy preprocessing (e.g., calibration and/or quantization). Evaluated under both classification and regression paradigms, the proposed framework demonstrates that the regression formulation enables fine-grained, continuous estimations and avoids quantization artifacts by resulting in more precise measurements without rounding. Experimental results confirm that FGump outperforms State-of-the-Art (SOTA) AI-based and classical methods, achieving higher accuracy and significantly lower training and inference times, as demonstrated in our results.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2507.20796v1" target="_blank">Aligning Large Language Model Agents with Rational and Moral Preferences: A Supervised Fine-Tuning Approach</a></h3>
                    <p><strong>Authors:</strong> Wei Lu, Daniel L. Chen, Christian B. Hansen</p>
                    <p><strong>Published:</strong> 7/28/2025</p>
                    <p><strong>Categories:</strong> econ.GN, cs.AI, cs.LG, q-fin.EC</p>
                    <p><strong>Summary:</strong> Understanding how large language model (LLM) agents behave in strategic interactions is essential as these systems increasingly participate autonomously in economically and morally consequential decisions. We evaluate LLM preferences using canonical economic games, finding substantial deviations from human behavior. Models like GPT-4o show excessive cooperation and limited incentive sensitivity, while reasoning models, such as o3-mini, align more consistently with payoff-maximizing strategies. We propose a supervised fine-tuning pipeline that uses synthetic datasets derived from economic reasoning to align LLM agents with economic preferences, focusing on two stylized preference structures. In the first, utility depends only on individual payoffs (homo economicus), while utility also depends on a notion of Kantian universalizability in the second preference structure (homo moralis). We find that fine-tuning based on small datasets shifts LLM agent behavior toward the corresponding economic agent. We further assess the fine-tuned agents behavior in two applications: Moral dilemmas involving autonomous vehicles and algorithmic pricing in competitive markets. These examples illustrate how different normative objectives embedded via realizations from structured preference structures can influence market and moral outcomes. This work contributes a replicable, cost-efficient, and economically grounded pipeline to align AI preferences using moral-economic principles.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2507.20786v1" target="_blank">Automating Thematic Review of Prevention of Future Deaths Reports: Replicating the ONS Child Suicide Study using Large Language Models</a></h3>
                    <p><strong>Authors:</strong> Sam Osian, Arpan Dutta, Sahil Bhandari, Iain E. Buchan, Dan W. Joyce</p>
                    <p><strong>Published:</strong> 7/28/2025</p>
                    <p><strong>Categories:</strong> cs.CL</p>
                    <p><strong>Summary:</strong> Prevention of Future Deaths (PFD) reports, issued by coroners in England and Wales, flag systemic hazards that may lead to further loss of life. Analysis of these reports has previously been constrained by the manual effort required to identify and code relevant cases. In 2025, the Office for National Statistics (ONS) published a national thematic review of child-suicide PFD reports ($\leq$ 18 years), identifying 37 cases from January 2015 to November 2023 - a process based entirely on manual curation and coding. We evaluated whether a fully automated, open source text-to-table language-model pipeline (PFD Toolkit) could reproduce the ONSs identification and thematic analysis of child-suicide PFD reports, and assessed gains in efficiency and reliability. All 4,249 PFD reports published from July 2013 to November 2023 were processed via PFD Toolkits large language model pipelines. Automated screening identified cases where the coroner attributed death to suicide in individuals aged 18 or younger, and eligible reports were coded for recipient category and 23 concern sub-themes, replicating the ONS coding frame. PFD Toolkit identified 72 child-suicide PFD reports - almost twice the ONS count. Three blinded clinicians adjudicated a stratified sample of 144 reports to validate the child-suicide screening. Against the post-consensus clinical annotations, the LLM-based workflow showed substantial to almost-perfect agreement (Cohens $\kappa$ = 0.82, 95% CI: 0.66-0.98, raw agreement = 91%). The end-to-end script runtime was 8m 16s, transforming a process that previously took months into one that can be completed in minutes. This demonstrates that automated LLM analysis can reliably and efficiently replicate manual thematic reviews of coronial data, enabling scalable, reproducible, and timely insights for public health and safety. The PFD Toolkit is openly available for future research.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2507.20783v1" target="_blank">On The Role of Pretrained Language Models in General-Purpose Text Embeddings: A Survey</a></h3>
                    <p><strong>Authors:</strong> Meishan Zhang, Xin Zhang, Xinping Zhao, Shouzheng Huang, Baotian Hu, Min Zhang</p>
                    <p><strong>Published:</strong> 7/28/2025</p>
                    <p><strong>Categories:</strong> cs.CL</p>
                    <p><strong>Summary:</strong> Text embeddings have attracted growing interest due to their effectiveness across a wide range of natural language processing (NLP) tasks, such as retrieval, classification, clustering, bitext mining, and summarization. With the emergence of pretrained language models (PLMs), general-purpose text embeddings (GPTE) have gained significant traction for their ability to produce rich, transferable representations. The general architecture of GPTE typically leverages PLMs to derive dense text representations, which are then optimized through contrastive learning on large-scale pairwise datasets. In this survey, we provide a comprehensive overview of GPTE in the era of PLMs, focusing on the roles PLMs play in driving its development. We first examine the fundamental architecture and describe the basic roles of PLMs in GPTE, i.e., embedding extraction, expressivity enhancement, training strategies, learning objectives, and data construction. Then, we describe advanced roles enabled by PLMs, such as multilingual support, multimodal integration, code understanding, and scenario-specific adaptation. Finally, we highlight potential future research directions that move beyond traditional improvement goals, including ranking integration, safety considerations, bias mitigation, structural information incorporation, and the cognitive extension of embeddings. This survey aims to serve as a valuable reference for both newcomers and established researchers seeking to understand the current state and future potential of GPTE.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2507.21053v1" target="_blank">Flow Matching Policy Gradients</a></h3>
                    <p><strong>Authors:</strong> David McAllister, Songwei Ge, Brent Yi, Chung Min Kim, Ethan Weber, Hongsuk Choi, Haiwen Feng, Angjoo Kanazawa</p>
                    <p><strong>Published:</strong> 7/28/2025</p>
                    <p><strong>Categories:</strong> cs.LG, cs.RO</p>
                    <p><strong>Summary:</strong> Flow-based generative models, including diffusion models, excel at modeling continuous distributions in high-dimensional spaces. In this work, we introduce Flow Policy Optimization (FPO), a simple on-policy reinforcement learning algorithm that brings flow matching into the policy gradient framework. FPO casts policy optimization as maximizing an advantage-weighted ratio computed from the conditional flow matching loss, in a manner compatible with the popular PPO-clip framework. It sidesteps the need for exact likelihood computation while preserving the generative capabilities of flow-based models. Unlike prior approaches for diffusion-based reinforcement learning that bind training to a specific sampling method, FPO is agnostic to the choice of diffusion or flow integration at both training and inference time. We show that FPO can train diffusion-style policies from scratch in a variety of continuous control tasks. We find that flow-based models can capture multimodal action distributions and achieve higher performance than Gaussian policies, particularly in under-conditioned settings.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2507.21051v1" target="_blank">Certification of nonobjective information by Popescu-Rohrlich box fraction and distinguishing quantum theory</a></h3>
                    <p><strong>Authors:</strong> Chellasamy Jebarathinam</p>
                    <p><strong>Published:</strong> 7/28/2025</p>
                    <p><strong>Categories:</strong> quant-ph</p>
                    <p><strong>Summary:</strong> It is demonstrated that identifying information-theoretic limitations of quantum Bell nonlocality alone cannot completely distinguish quantum theory from generalized nonsignaling theories. To this end, an information-theoretic concept of certifying nonobjective information by the Popescu-Rohrlich box fraction is employed. Furthermore, in the aforementioned demonstration, a partial answer to the question of what distinguishes quantum theory from generalized nonsignaling theories emerges beyond the one provided by the principle of information causality alone. This is accomplished by demonstrating that postquantum models identified by the information causality are isolated by the emergence of the Popescu-Rohrlich box fraction of nonobjective information in Bell-local boxes of a generalized nonsignaling theory, over the two other generalized nonsignaling theories that have simplicial local state spaces.</p>
                
            
                
                    <h3><a href="http://dx.doi.org/10.1063/5.0278183" target="_blank">Monolithic optoelectronic circuit design for on-chip terahertz applications</a></h3>
                    <p><strong>Authors:</strong> Kateryna Kusyak, Benedikt Schulte, Toru Matsuyama, Gunda Kipp, Hope M. Bretscher, Matthew W. Day, Guido Meier, Alexander M. Potts, James W. McIver</p>
                    <p><strong>Published:</strong> 7/28/2025</p>
                    <p><strong>Categories:</strong> physics.optics, physics.ins-det</p>
                    <p><strong>Summary:</strong> We demonstrate a monolithic coplanar stripline platform for on-chip terahertz (THz) generation, transmission, and detection, addressing key challenges of mode purity, bandwidth, and referencing. Capacitive coupling of the photoconductive generator switch enforces pure odd-mode propagation, increases THz field strength, and extends the operational frequency range, achieving 0.05-1.4 THz. Our architecture enables fully monolithic fabrication with amorphous silicon switches, in situ field referencing, and galvanic isolation between generation and detection. Finite-element simulations and experiments confirm that suppressing parasitic modes improves signal integrity, providing a robust platform for high-fidelity THz spectroscopy, ultrafast electronics, and nanoscale quantum materials research.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2507.21049v1" target="_blank">Rep-MTL: Unleashing the Power of Representation-level Task Saliency for Multi-Task Learning</a></h3>
                    <p><strong>Authors:</strong> Zedong Wang, Siyuan Li, Dan Xu</p>
                    <p><strong>Published:</strong> 7/28/2025</p>
                    <p><strong>Categories:</strong> cs.LG, cs.CV</p>
                    <p><strong>Summary:</strong> Despite the promise of Multi-Task Learning in leveraging complementary knowledge across tasks, existing multi-task optimization (MTO) techniques remain fixated on resolving conflicts via optimizer-centric loss scaling and gradient manipulation strategies, yet fail to deliver consistent gains. In this paper, we argue that the shared representation space, where task interactions naturally occur, offers rich information and potential for operations complementary to existing optimizers, especially for facilitating the inter-task complementarity, which is rarely explored in MTO. This intuition leads to Rep-MTL, which exploits the representation-level task saliency to quantify interactions between task-specific optimization and shared representation learning. By steering these saliencies through entropy-based penalization and sample-wise cross-task alignment, Rep-MTL aims to mitigate negative transfer by maintaining the effective training of individual tasks instead pure conflict-solving, while explicitly promoting complementary information sharing. Experiments are conducted on four challenging MTL benchmarks covering both task-shift and domain-shift scenarios. The results show that Rep-MTL, even paired with the basic equal weighting policy, achieves competitive performance gains with favorable efficiency. Beyond standard performance metrics, Power Law exponent analysis demonstrates Rep-MTLs efficacy in balancing task-specific learning and cross-task sharing. The project page is available at HERE.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2507.21045v1" target="_blank">Reconstructing 4D Spatial Intelligence: A Survey</a></h3>
                    <p><strong>Authors:</strong> Yukang Cao, Jiahao Lu, Zhisheng Huang, Zhuowei Shen, Chengfeng Zhao, Fangzhou Hong, Zhaoxi Chen, Xin Li, Wenping Wang, Yuan Liu, Ziwei Liu</p>
                    <p><strong>Published:</strong> 7/28/2025</p>
                    <p><strong>Categories:</strong> cs.CV</p>
                    <p><strong>Summary:</strong> Reconstructing 4D spatial intelligence from visual observations has long been a central yet challenging task in computer vision, with broad real-world applications. These range from entertainment domains like movies, where the focus is often on reconstructing fundamental visual elements, to embodied AI, which emphasizes interaction modeling and physical realism. Fueled by rapid advances in 3D representations and deep learning architectures, the field has evolved quickly, outpacing the scope of previous surveys. Additionally, existing surveys rarely offer a comprehensive analysis of the hierarchical structure of 4D scene reconstruction. To address this gap, we present a new perspective that organizes existing methods into five progressive levels of 4D spatial intelligence: (1) Level 1 -- reconstruction of low-level 3D attributes (e.g., depth, pose, and point maps); (2) Level 2 -- reconstruction of 3D scene components (e.g., objects, humans, structures); (3) Level 3 -- reconstruction of 4D dynamic scenes; (4) Level 4 -- modeling of interactions among scene components; and (5) Level 5 -- incorporation of physical laws and constraints. We conclude the survey by discussing the key challenges at each level and highlighting promising directions for advancing toward even richer levels of 4D spatial intelligence. To track ongoing developments, we maintain an up-to-date project page: https://github.com/yukangcao/Awesome-4D-Spatial-Intelligence.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2507.21043v1" target="_blank">Topological chiral superconductivity from antiferromagnetic correlations in moirÃ© bands with extreme spin-orbit coupling</a></h3>
                    <p><strong>Authors:</strong> Chenyuan Li, Fang Xie, Jennifer Cano, Qimiao Si</p>
                    <p><strong>Published:</strong> 7/28/2025</p>
                    <p><strong>Categories:</strong> cond-mat.supr-con, cond-mat.str-el</p>
                    <p><strong>Summary:</strong> Motivated by the strong-correlation phenomenology observed near the superconducting phase in twisted bilayer WSe$_2$, we study multi-orbital $t$-$J$ models that are derived from different parameter regimes. The models contain effective antiferromagnetic interactions that are influenced by the strong underlying spin-orbit coupling. The possible superconducting pairing states are investigated in these models. We find that the preferred pairing order parameters are associated with the $^{1,2}E$ representations of the three-fold rotation symmetry operator $C_3$, with the $p\pm i p$ component intermixing with the $d\pm id$ component. The chiral superconducting states are shown to be topological, based on the Wilson loops of the corresponding Bogoliubov quasiparticles. We discuss the implications of our findings for experimental observations, as well as the new connections our results uncover between the moir\{e} superconductivity and its counterpart in bulk quantum materials.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2507.21042v1" target="_blank">Approximate solutions to the shrinking core model</a></h3>
                    <p><strong>Authors:</strong> Cristian Moreno Pulido, Rachael Olwande, Tim Myers, Francesc Font</p>
                    <p><strong>Published:</strong> 7/28/2025</p>
                    <p><strong>Categories:</strong> cond-mat.other</p>
                    <p><strong>Summary:</strong> The shrinking core model describes the reaction of a spherical solid particle with a surrounding fluid. In this work, we revisit the SCM by deriving it from the underlying physical processes and performing a careful non-dimensionalisation, which highlights the limitations of the commonly used pseudo-steady-state approximation, particularly in liquid-solid systems where fluid and solid densities are comparable. To address these limitations, we derive approximate analytical solutions using a perturbation method that improves upon the pseudo-steady-state model. We also obtain a small-time solution capturing early transient behavior. A semi-implicit finite difference scheme is implemented to solve the full model numerically and benchmark the analytical approximations. We demonstrate that the perturbation solution provides significantly improved accuracy over the pseudo-steady-state model, especially in diffusion-limited regimes. Finally, we propose a simple fitting procedure combining the perturbation with the early-time solutions to estimate physical parameters from experimental data at minimal computational cost.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2507.21039v1" target="_blank">Data-parallel leading-order event generation in MadGraph5_aMC@NLO</a></h3>
                    <p><strong>Authors:</strong> Stephan HagebÃ¶ck, Daniele Massaro, Olivier Mattelaer, Stefan Roiser, Andrea Valassi, Zenny Wettersten</p>
                    <p><strong>Published:</strong> 7/28/2025</p>
                    <p><strong>Categories:</strong> hep-ph, hep-ex, physics.comp-ph</p>
                    <p><strong>Summary:</strong> The CUDACPP plugin for MadGraph5_aMC@NLO aims to accelerate leading order tree-level event generation by providing the MadEvent event generator with data-parallel helicity amplitudes. These amplitudes are written in templated C++ and CUDA, allowing them to be compiled for CPUs supporting SSE4, AVX2, and AVX-512 instruction sets as well as CUDA- and HIP-enabled GPUs. Using SIMD instruction sets, CUDACPP-generated amplitudes are shown to speed up linearly with SIMD register size, and GPU offloading is shown to provide acceleration beyond that of SIMD instructions. Additionally, the resulting speed-up in event generation perfectly aligns with predictions from measured runtime fractions spent in amplitude routines, and proper GPU utilisation can speed up high-multiplicity QCD processes by an order of magnitude when compared to optimal CPU usage in server-grade CPUs.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2507.21037v1" target="_blank">When Brain Foundation Model Meets Cauchy-Schwarz Divergence: A New Framework for Cross-Subject Motor Imagery Decoding</a></h3>
                    <p><strong>Authors:</strong> Jinzhou Wu, Baoping Tang, Qikang Li, Yi Wang, Cheng Li, Shujian Yu</p>
                    <p><strong>Published:</strong> 7/28/2025</p>
                    <p><strong>Categories:</strong> cs.LG</p>
                    <p><strong>Summary:</strong> Decoding motor imagery (MI) electroencephalogram (EEG) signals, a key non-invasive brain-computer interface (BCI) paradigm for controlling external systems, has been significantly advanced by deep learning. However, MI-EEG decoding remains challenging due to substantial inter-subject variability and limited labeled target data, which necessitate costly calibration for new users. Many existing multi-source domain adaptation (MSDA) methods indiscriminately incorporate all available source domains, disregarding the large inter-subject differences in EEG signals, which leads to negative transfer and excessive computational costs. Moreover, while many approaches focus on feature distribution alignment, they often neglect the explicit dependence between features and decision-level outputs, limiting their ability to preserve discriminative structures. To address these gaps, we propose a novel MSDA framework that leverages a pretrained large Brain Foundation Model (BFM) for dynamic and informed source subject selection, ensuring only relevant sources contribute to adaptation. Furthermore, we employ Cauchy-Schwarz (CS) and Conditional CS (CCS) divergences to jointly perform feature-level and decision-level alignment, enhancing domain invariance while maintaining class discriminability. Extensive evaluations on two benchmark MI-EEG datasets demonstrate that our framework outperforms a broad range of state-of-the-art baselines. Additional experiments with a large source pool validate the scalability and efficiency of BFM-guided selection, which significantly reduces training time without sacrificing performance.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2507.21036v1" target="_blank">Quantum optical shallow networks</a></h3>
                    <p><strong>Authors:</strong> Simone Roncallo, Angela Rosy Morgillo, Seth Lloyd, Chiara Macchiavello, Lorenzo Maccone</p>
                    <p><strong>Published:</strong> 7/28/2025</p>
                    <p><strong>Categories:</strong> quant-ph</p>
                    <p><strong>Summary:</strong> Classical shallow networks are universal approximators. Given a sufficient number of neurons, they can reproduce any continuous function to arbitrary precision, with a resource cost that scales linearly in both the input size and the number of trainable parameters. In this work, we present a quantum optical protocol that implements a shallow network with an arbitrary number of neurons. Both the input data and the parameters are encoded into single-photon states. Leveraging the Hong-Ou-Mandel effect, the network output is determined by the coincidence rates measured when the photons interfere at a beam splitter, with multiple neurons prepared as a mixture of single-photon states. Remarkably, once trained, our model requires constant optical resources regardless of the number of input features and neurons.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2507.21033v1" target="_blank">GPT-IMAGE-EDIT-1.5M: A Million-Scale, GPT-Generated Image Dataset</a></h3>
                    <p><strong>Authors:</strong> Yuhan Wang, Siwei Yang, Bingchen Zhao, Letian Zhang, Qing Liu, Yuyin Zhou, Cihang Xie</p>
                    <p><strong>Published:</strong> 7/28/2025</p>
                    <p><strong>Categories:</strong> cs.CV</p>
                    <p><strong>Summary:</strong> Recent advancements in large multimodal models like GPT-4o have set a new standard for high-fidelity, instruction-guided image editing. However, the proprietary nature of these models and their training data creates a significant barrier for open-source research. To bridge this gap, we introduce GPT-IMAGE-EDIT-1.5M, a publicly available, large-scale image-editing corpus containing more than 1.5 million high-quality triplets (instruction, source image, edited image). We systematically construct this dataset by leveraging the versatile capabilities of GPT-4o to unify and refine three popular image-editing datasets: OmniEdit, HQ-Edit, and UltraEdit. Specifically, our methodology involves 1) regenerating output images to enhance visual quality and instruction alignment, and 2) selectively rewriting prompts to improve semantic clarity. To validate the efficacy of our dataset, we fine-tune advanced open-source models on GPT-IMAGE-EDIT-1.5M. The empirical results are exciting, e.g., the fine-tuned FluxKontext achieves highly competitive performance across a comprehensive suite of benchmarks, including 7.24 on GEdit-EN, 3.80 on ImgEdit-Full, and 8.78 on Complex-Edit, showing stronger instruction following and higher perceptual quality while maintaining identity. These scores markedly exceed all previously published open-source methods and substantially narrow the gap to leading proprietary models. We hope the full release of GPT-IMAGE-EDIT-1.5M can help to catalyze further open research in instruction-guided image editing.</p>
                
            
                
                    <h3><a href="http://dx.doi.org/10.3847/1538-4357/adf430" target="_blank">Revisiting the Perseus Cluster I: Resolving the Si/S/Ar/Ca ratios by Stellar Convection</a></h3>
                    <p><strong>Authors:</strong> Shing-Chi Leung, Kenichi Nomoto, Aurora Simionescu</p>
                    <p><strong>Published:</strong> 7/28/2025</p>
                    <p><strong>Categories:</strong> astro-ph.HE, astro-ph.GA</p>
                    <p><strong>Summary:</strong> Chemical abundance measurements from stars in the Milky Way to the intragalactic medium in the Perseus Cluster have challenged the spherical explosion models. Models in the literature cannot closely match the observed element ratios, where Si, S are overproduced and Ar, Ca are underproduced. In this article, we explore the impact of the model parameters during the evolution of massive stars on the final explosive nucleosynthesis. We investigate the effects of a parametrized model of the convective process, including the mixing length parameter and the semi-convection parameter, on the production of Si-group elements. We search for the value pair that can reduce the discrepancy in the models. We conclude that a mixing length parameter of 2.2 and semi-convection parameter of 0.03 are required to fit these criteria. Using this updated value pair, we compute a sequence of massive star models from $M_{\rm ZAMS} = $ 15 -- 40 $M_{\odot}$. The high resolution data from future observations such as XRISM will provide further details on less constrained processes in stellar evolution and supernova explosion. Future comparison with supernova models of various progenitor metallicity will further shed light on the supernova population and their relative rates on cosmological scales.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2507.21031v1" target="_blank">Second order free cumulants: product, commutator, and anti-commutator</a></h3>
                    <p><strong>Authors:</strong> Daniel Munoz George, Daniel Perales</p>
                    <p><strong>Published:</strong> 7/28/2025</p>
                    <p><strong>Categories:</strong> math.OA, math.CO, 46L54, 05A18, 05A05</p>
                    <p><strong>Summary:</strong> Given two second order free random variables $a$ and $b$, we study the second order free cumulants of their product $ab$, their commutator $ab-ba$, and their anti-commutator $ab+ba$. Let $(\kappa_n^a)_{n\geq 1}$ and $(\kappa_{n,m}^a)_{n,m\geq 1}$ denote the sequence of free cumulants of first and second order, respectively, of a random variable $a$ in a second order non-commutative probability space $(\mathcal{A},\varphi,\varphi^2)$. Given $a$ and $b$ two second order freely independent random variables, we provide formulas to compute each of the cumulants $(\kappa_{n,m}^{ab})_{n,m\geq 1}$, $(\kappa_{n,m}^{ab-ba})_{n,m\geq 1}$, and $(\kappa_{n,m}^{ab+ba})_{n,m\geq 1}$ in terms of the individual cumulants $(\kappa_{n}^{a})_{n\geq 1}$, $(\kappa_{n,m}^{a})_{n,m\geq 1}$, $(\kappa_{n}^{b})_{n\geq 1}$, and $(\kappa_{n,m}^{b})_{n,m\geq 1}$. For $n=m=1$ our formulas read: \begin{align*} \kappa_{1,1}^{ab} = \kappa_{2}^{a}\kappa_{2}^{b} +\kappa_{1,1}^{a}(\kappa_{1}^{b})^2+\kappa_{1,1}^{b}(\kappa_{1}^{a})^2,\\ \kappa_{1,1}^{ab-ba} = 2\kappa_{2}^{a}\kappa_{2}^{b},\\ \kappa_{1,1}^{ab+ba} = 2\kappa_{2}^{a}\kappa_{2}^{b} +4\kappa_{1,1}^{a}(\kappa_{1}^{b})^2+4\kappa_{1,1}^{b}(\kappa_{1}^{a})^2. \end{align*} In general, our formulas express the cumulants $\kappa_{n,m}^{ab}$, $\kappa_{n,m}^{ab-ba}$, and $\kappa_{n,m}^{ab+ba}$ as sums indexed by special subsets of non-crossing partitioned permutations. The formulas for the commutator and anti-commutator where not studied before, while the formula for the product was only known in the case the where the individual second order free cumulants vanish. As an application, we compute explicitly the cumulants of the anti-commutator and product of two second order free semicircular variables.</p>
                
            
                
                    <h3><a href="http://dx.doi.org/10.1021/acs.jpca.5c02029" target="_blank">Quantum Simulation of Molecular Dynamics Processes -- A Benchmark Study Using Classical Simulator and Present-Day Quantum Hardware</a></h3>
                    <p><strong>Authors:</strong> Tamila Kuanysheva, Brian Kendrick, Lukasz Cincio, Dmitri Babikov</p>
                    <p><strong>Published:</strong> 7/28/2025</p>
                    <p><strong>Categories:</strong> quant-ph</p>
                    <p><strong>Summary:</strong> We explore how the fundamental problems in quantum molecular dynamics can be modelled using classical simulators (emulators) of quantum computers and the actual quantum hardware available to us today. The list of problems we tackle includes propagation of a free wave packet, vibration of a harmonic oscillator, and tunneling through a barrier. Each of these problems starts with the initial wave packet setup. Although Qiskit provides a general method for initializing wavefunctions, in most cases it generates deep quantum circuits. While these circuits perform well on noiseless simulators, they suffer from excessive noise on quantum hardware. To overcome this issue, we designed a shallower quantum circuit for preparing a Gaussian-like initial wave packet, which improves the performance on real hardware. Next, quantum circuits are implemented to apply the kinetic and potential energy operators for the evolution of a wavefunction over time. The results of our modelling on classical emulators of quantum hardware agree perfectly with the results obtained using the traditional (classical) methods. This serves as a benchmark and demonstrates that the quantum algorithms and Qiskit codes we developed are accurate. However, the results obtained on the actual quantum hardware available today, such as IBMs superconducting qubits and IonQs trapped ions, indicate large discrepancies due to hardware limitations. This work highlights both the potential and challenges of using quantum computers to solve fundamental quantum molecular dynamics problems.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2507.21028v1" target="_blank">Multi-Agent-as-Judge: Aligning LLM-Agent-Based Automated Evaluation with Multi-Dimensional Human Evaluation</a></h3>
                    <p><strong>Authors:</strong> Jiaju Chen, Yuxuan Lu, Xiaojie Wang, Huimin Zeng, Jing Huang, Jiri Gesi, Ying Xu, Bingsheng Yao, Dakuo Wang</p>
                    <p><strong>Published:</strong> 7/28/2025</p>
                    <p><strong>Categories:</strong> cs.CL, 68T50</p>
                    <p><strong>Summary:</strong> Nearly all human work is collaborative; thus, the evaluation of real-world NLP applications often requires multiple dimensions that align with diverse human perspectives. As real human evaluator resources are often scarce and costly, the emerging LLM-as-a-judge paradigm sheds light on a promising approach to leverage LLM agents to believably simulate human evaluators. Yet, to date, existing LLM-as-a-judge approaches face two limitations: persona descriptions of agents are often arbitrarily designed, and the frameworks are not generalizable to other tasks. To address these challenges, we propose MAJ-EVAL, a Multi-Agent-as-Judge evaluation framework that can automatically construct multiple evaluator personas with distinct dimensions from relevant text documents (e.g., research papers), instantiate LLM agents with the personas, and engage in-group debates with multi-agents to Generate multi-dimensional feedback. Our evaluation experiments in both the educational and medical domains demonstrate that MAJ-EVAL can generate evaluation results that better align with human experts ratings compared with conventional automated evaluation metrics and existing LLM-as-a-judge methods.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2507.21021v1" target="_blank">Behavior-Specific Filtering for Enhanced Pig Behavior Classification in Precision Livestock Farming</a></h3>
                    <p><strong>Authors:</strong> Zhen Zhang, Dong Sam Ha, Gota Morota, Sook Shin</p>
                    <p><strong>Published:</strong> 7/28/2025</p>
                    <p><strong>Categories:</strong> cs.LG</p>
                    <p><strong>Summary:</strong> This study proposes a behavior-specific filtering method to improve behavior classification accuracy in Precision Livestock Farming. While traditional filtering methods, such as wavelet denoising, achieved an accuracy of 91.58%, they apply uniform processing to all behaviors. In contrast, the proposed behavior-specific filtering method combines Wavelet Denoising with a Low Pass Filter, tailored to active and inactive pig behaviors, and achieved a peak accuracy of 94.73%. These results highlight the effectiveness of behavior-specific filtering in enhancing animal behavior monitoring, supporting better health management and farm efficiency.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2507.21018v1" target="_blank">Deep Learning for Skeleton Based Human Motion Rehabilitation Assessment: A Benchmark</a></h3>
                    <p><strong>Authors:</strong> Ali Ismail-Fawaz, Maxime Devanne, Stefano Berretti, Jonathan Weber, Germain Forestier</p>
                    <p><strong>Published:</strong> 7/28/2025</p>
                    <p><strong>Categories:</strong> cs.CV, cs.LG</p>
                    <p><strong>Summary:</strong> Automated assessment of human motion plays a vital role in rehabilitation, enabling objective evaluation of patient performance and progress. Unlike general human activity recognition, rehabilitation motion assessment focuses on analyzing the quality of movement within the same action class, requiring the detection of subtle deviations from ideal motion. Recent advances in deep learning and video-based skeleton extraction have opened new possibilities for accessible, scalable motion assessment using affordable devices such as smartphones or webcams. However, the field lacks standardized benchmarks, consistent evaluation protocols, and reproducible methodologies, limiting progress and comparability across studies. In this work, we address these gaps by (i) aggregating existing rehabilitation datasets into a unified archive called Rehab-Pile, (ii) proposing a general benchmarking framework for evaluating deep learning methods in this domain, and (iii) conducting extensive benchmarking of multiple architectures across classification and regression tasks. All datasets and implementations are released to the community to support transparency and reproducibility. This paper aims to establish a solid foundation for future research in automated rehabilitation assessment and foster the development of reliable, accessible, and personalized rehabilitation solutions. The datasets, source-code and results of this article are all publicly available.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2507.21015v1" target="_blank">Learning Transferable Facial Emotion Representations from Large-Scale Semantically Rich Captions</a></h3>
                    <p><strong>Authors:</strong> Licai Sun, Xingxun Jiang, Haoyu Chen, Yante Li, Zheng Lian, Biu Liu, Yuan Zong, Wenming Zheng, Jukka M. LeppÃ¤nen, Guoying Zhao</p>
                    <p><strong>Published:</strong> 7/28/2025</p>
                    <p><strong>Categories:</strong> cs.CV</p>
                    <p><strong>Summary:</strong> Current facial emotion recognition systems are predominately trained to predict a fixed set of predefined categories or abstract dimensional values. This constrained form of supervision hinders generalization and applicability, as it reduces the rich and nuanced spectrum of emotions into oversimplified labels or scales. In contrast, natural language provides a more flexible, expressive, and interpretable way to represent emotions, offering a much broader source of supervision. Yet, leveraging semantically rich natural language captions as supervisory signals for facial emotion representation learning remains relatively underexplored, primarily due to two key challenges: 1) the lack of large-scale caption datasets with rich emotional semantics, and 2) the absence of effective frameworks tailored to harness such rich supervision. To this end, we introduce EmoCap100K, a large-scale facial emotion caption dataset comprising over 100,000 samples, featuring rich and structured semantic descriptions that capture both global affective states and fine-grained local facial behaviors. Building upon this dataset, we further propose EmoCapCLIP, which incorporates a joint global-local contrastive learning framework enhanced by a cross-modal guided positive mining module. This design facilitates the comprehensive exploitation of multi-level caption information while accommodating semantic similarities between closely related expressions. Extensive evaluations on over 20 benchmarks covering five tasks demonstrate the superior performance of our method, highlighting the promise of learning facial emotion representations from large-scale semantically rich captions. The code and data will be available at https://github.com/sunlicai/EmoCapCLIP.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2507.21014v1" target="_blank">Probing Partonic Evolution and Hadronization via Balance Functions and Correlations of Charmed Hadrons</a></h3>
                    <p><strong>Authors:</strong> Oveis Sheibani, Claude Pruneau, Victor Gonzalez, Sumit Basu, Alexandru Florin Dobrin, Yash Patley, Basanta Nandi, Sadhana Dash</p>
                    <p><strong>Published:</strong> 7/28/2025</p>
                    <p><strong>Categories:</strong> hep-ex, hep-ph</p>
                    <p><strong>Summary:</strong> Predictions of charm correlation functions and more specifically balance functions are presented in proton--proton (pp) collisions at sqrt(s_NN) = 13 TeV based on the PYTHIA 8.3 event generator. Correlations are computed for identical and cross-species charmed hadrons in both minimum bias and high-pT biased collisions. We study the strength of correlations as a function of the number of balanced flavors and investigate the impact of variations of PYTHIA parameters controlling the Lund string fragmentation on the shape and strength of the correlation functions. The feasibility of measurements of the charm balance function presented is discussed in the context of the future LHC experiments.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2507.21011v1" target="_blank">Quantum Walks on Arbitrary Spatial Networks with Rydberg Atoms</a></h3>
                    <p><strong>Authors:</strong> Gabriel Almeida, Raul Santos, Lara Janiurek, Yasser Omar</p>
                    <p><strong>Published:</strong> 7/28/2025</p>
                    <p><strong>Categories:</strong> quant-ph, cond-mat.quant-gas, physics.app-ph, physics.atom-ph</p>
                    <p><strong>Summary:</strong> Rydberg atoms provide a highly promising platform for quantum computation, leveraging their strong tunable interactions to encode and manipulate information in the electronic states of individual atoms. Key advantages of Rydberg atoms include scalability, reconfigurable connectivity, and native multi-qubit gates, making them particularly well-suited for addressing complex network problems. These problems can often be framed as graph-based tasks, which can be efficiently addressed using quantum walks. In this work, we propose a general implementation of staggered quantum walks with Rydberg atoms, with a particular focus on spatial networks. We also present an efficient algorithm for constructing the tessellations required for the staggered quantum walk. Finally, we demonstrate that our proposal achieves quadratic speedup in spatial search algorithms.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2507.21009v1" target="_blank">Memorization in Fine-Tuned Large Language Models</a></h3>
                    <p><strong>Authors:</strong> Danil Savine, Muni Sreenivas Pydi, Jamal Atif, Olivier CappÃ©</p>
                    <p><strong>Published:</strong> 7/28/2025</p>
                    <p><strong>Categories:</strong> cs.CL, cs.AI</p>
                    <p><strong>Summary:</strong> This study investigates the mechanisms and factors influencing memorization in fine-tuned large language models (LLMs), with a focus on the medical domain due to its privacy-sensitive nature. We examine how different aspects of the fine-tuning process affect a models propensity to memorize training data, using the PHEE dataset of pharmacovigilance events. Our research employs two main approaches: a membership inference attack to detect memorized data, and a generation task with prompted prefixes to assess verbatim reproduction. We analyze the impact of adapting different weight matrices in the transformer architecture, the relationship between perplexity and memorization, and the effect of increasing the rank in low-rank adaptation (LoRA) fine-tuning. Key findings include: (1) Value and Output matrices contribute more significantly to memorization compared to Query and Key matrices; (2) Lower perplexity in the fine-tuned model correlates with increased memorization; (3) Higher LoRA ranks lead to increased memorization, but with diminishing returns at higher ranks. These results provide insights into the trade-offs between model performance and privacy risks in fine-tuned LLMs. Our findings have implications for developing more effective and responsible strategies for adapting large language models while managing data privacy concerns.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2507.21007v1" target="_blank">High-precision bootstrap of multi-matrix quantum mechanics</a></h3>
                    <p><strong>Authors:</strong> Henry W. Lin, Zechuan Zheng</p>
                    <p><strong>Published:</strong> 7/28/2025</p>
                    <p><strong>Categories:</strong> hep-th, hep-lat</p>
                    <p><strong>Summary:</strong> We consider matrix quantum mechanics with multiple bosonic matrices, including those obtained from dimensional reduction of Yang-Mills theories. Using the matrix bootstrap, we study simple observables like $\langle \mathop{tr} X^2 \rangle$ in the confining phase of the theory in the infinite $N$ limit. By leveraging the symmetries of these models and using non-linear relaxation, we consider constraints up to level 14, e.g., constraints from traces of words of length $\le 14$. Our results are more precise than large $N$, continuum extrapolations of lattice Monte Carlo simulations, including an estimate of certain simple observables up to 8 significant digits.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2507.21006v1" target="_blank">Explicit and Effectively Symmetric Runge-Kutta Methods</a></h3>
                    <p><strong>Authors:</strong> Daniil Shmelev, Kurusch Ebrahimi-Fard, Nikolas Tapia, Cristopher Salvi</p>
                    <p><strong>Published:</strong> 7/28/2025</p>
                    <p><strong>Categories:</strong> math.NA, cs.NA, math.CA, math.RA, 16T05, 65L05, 65L06, 05C05</p>
                    <p><strong>Summary:</strong> Symmetry is a key property of numerical methods. The geometric properties of symmetric schemes make them an attractive option for integrating Hamiltonian systems, whilst their ability to exactly recover the initial condition without the need to store the entire solution trajectory makes them ideal for the efficient implementation of Neural ODEs. In this work, we present a Hopf algebraic approach to the study of symmetric B-series methods. We show that every B-series method can be written as the composition of a symmetric and antisymmetric component, and explore the structure of this decomposition for Runge-Kutta schemes. A major bottleneck of symmetric Runge-Kutta schemes is their implicit nature, which requires solving a nonlinear system at each step. By introducing a new set of order conditions which minimise the antisymmetric component of a scheme, we derive what we call Explicit and Effectively Symmetric (EES) schemes -- a new class of explicit Runge-Kutta schemes with near-symmetric properties. We present examples of second-order EES schemes and demonstrate that, despite their low order, these schemes readily outperform higher-order explicit schemes such as RK4 and RK5, and achieve results comparable to implicit symmetric schemes at a significantly lower computational cost.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2507.21002v1" target="_blank">Spectral distribution of sparse Gaussian Ensembles of Real Asymmetric Matrices</a></h3>
                    <p><strong>Authors:</strong> Ratul Dutta, Pragya Shukla</p>
                    <p><strong>Published:</strong> 7/28/2025</p>
                    <p><strong>Categories:</strong> cond-mat.dis-nn, cond-mat.stat-mech, math-ph, math.MP, quant-ph</p>
                    <p><strong>Summary:</strong> Theoretical analysis of biological and artificial neural networks e.g. modelling of synaptic or weight matrices necessitate consideration of the generic real-asymmetric matrix ensembles, those with varying order of matrix elements e.g. a sparse structure or a banded structure. We pursue the complexity parameter approach to analyze the spectral statistics of the multiparametric Gaussian ensembles of real asymmetric matrices and derive the ensemble averaged spectral densities for real as well as complex eigenvalues. Considerations of the matrix elements with arbitrary choice of mean and variances render us the freedom to model the desired sparsity in the ensemble. Our formulation provides a common mathematical formulation of the spectral statistics for a wide range of sparse real-asymmetric ensembles and also reveals, thereby, a deep rooted universality among them.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2507.21001v1" target="_blank">Anomalous fluctuations of Bose-Einstein condensates in optical lattices</a></h3>
                    <p><strong>Authors:</strong> Zahra Jalali-Mola, Niklas KÃ¤ming, Luca Asteria, Utso Bhattacharya, Ravindra W. Chhajlany, Klaus Sengstock, Maciej Lewenstein, Tobias Grass, Christof Weitenberg</p>
                    <p><strong>Published:</strong> 7/28/2025</p>
                    <p><strong>Categories:</strong> cond-mat.quant-gas, quant-ph</p>
                    <p><strong>Summary:</strong> Fluctuations are fundamental in physics and important for understanding and characterizing phase transitions. In this spirit, the phase transition to the Bose-Einstein condensate (BEC) is of specific importance. Whereas fluctuations of the condensate particle number in atomic BECs have been studied in continuous systems, experimental and theoretical studies for lattice systems were so far missing. Here, we explore the condensate particle number fluctuations in an optical lattice BEC across the phase transition in a combined experimental and theoretical study. We present both experimental data using ultracold $^{87}$Rb atoms and numerical simulations based on a hybrid approach combining the Bogoliubov quasiparticle framework with a master equation analysis for modeling the system. We find strongly anomalous fluctuations, where the variance of the condensate number $\delta N_{\rm BEC}^2$ scales with the total atom number as $N^{1+\gamma}$ with an exponent around $\gamma_{\rm theo}=0.74$ and $\gamma_{\rm exp}=0.62$, which we attribute to the 2D/3D crossover geometry and the interactions. Our study highlights the importance of the trap geometry on the character of fluctuations and on fundamental quantum mechanical properties.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2507.20999v1" target="_blank">LoRA-PAR: A Flexible Dual-System LoRA Partitioning Approach to Efficient LLM Fine-Tuning</a></h3>
                    <p><strong>Authors:</strong> Yining Huang, Bin Li, Keke Tang, Meilian Chen</p>
                    <p><strong>Published:</strong> 7/28/2025</p>
                    <p><strong>Categories:</strong> cs.LG, cs.CL</p>
                    <p><strong>Summary:</strong> Large-scale generative models like DeepSeek-R1 and OpenAI-O1 benefit substantially from chain-of-thought (CoT) reasoning, yet pushing their performance typically requires vast data, large model sizes, and full-parameter fine-tuning. While parameter-efficient fine-tuning (PEFT) helps reduce cost, most existing approaches primarily address domain adaptation or layer-wise allocation rather than explicitly tailoring data and parameters to different response demands. Inspired by Thinking, Fast and Slow, which characterizes two distinct modes of thought-System 1 (fast, intuitive, often automatic) and System 2 (slower, more deliberative and analytic)-we draw an analogy that different subregions of an LLMs parameters might similarly specialize for tasks that demand quick, intuitive responses versus those requiring multi-step logical reasoning. Therefore, we propose LoRA-PAR, a dual-system LoRA framework that partitions both data and parameters by System 1 or System 2 demands, using fewer yet more focused parameters for each task. Specifically, we classify task data via multi-model role-playing and voting, and partition parameters based on importance scoring, then adopt a two-stage fine-tuning strategy of training System 1 tasks with supervised fine-tuning (SFT) to enhance knowledge and intuition and refine System 2 tasks with reinforcement learning (RL) to reinforce deeper logical deliberation next. Extensive experiments show that the two-stage fine-tuning strategy, SFT and RL, lowers active parameter usage while matching or surpassing SOTA PEFT baselines.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2507.20998v1" target="_blank">Efficient Memristive Spiking Neural Networks Architecture with Supervised In-Situ STDP Method</a></h3>
                    <p><strong>Authors:</strong> Santlal Prajapati, Susmita Sur-Kolay, Soumyadeep Dutta</p>
                    <p><strong>Published:</strong> 7/28/2025</p>
                    <p><strong>Categories:</strong> cs.ET, cs.NE</p>
                    <p><strong>Summary:</strong> Memristor-based Spiking Neural Networks (SNNs) with temporal spike encoding enable ultra-low-energy computation, making them ideal for battery-powered intelligent devices. This paper presents a circuit-level memristive spiking neural network (SNN) architecture trained using a proposed novel supervised in-situ learning algorithm inspired by spike-timing-dependent plasticity (STDP). The proposed architecture efficiently implements lateral inhibition and the refractory period, eliminating the need for external microcontrollers or ancillary control hardware. All synapses of the winning neurons are updated in parallel, enhancing training efficiency. The modular design ensures scalability with respect to input data dimensions and output class count. The SNN is evaluated in LTspice for pattern recognition (using 5x3 binary images) and classification tasks using the Iris and Breast Cancer Wisconsin (BCW) datasets. During testing, the system achieved perfect pattern recognition and high classification accuracies of 99.11\% (Iris) and 97.9\% (BCW). Additionally, it has demonstrated robustness, maintaining an average recognition rate of 93.4\% under 20\% input noise. The impact of stuck-at-conductance faults and memristor device variations was also analyzed.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2507.20996v1" target="_blank">Improving Adversarial Robustness Through Adaptive Learning-Driven Multi-Teacher Knowledge Distillation</a></h3>
                    <p><strong>Authors:</strong> Hayat Ullah, Syed Muhammad Talha Zaidi, Arslan Munir</p>
                    <p><strong>Published:</strong> 7/28/2025</p>
                    <p><strong>Categories:</strong> cs.CV</p>
                    <p><strong>Summary:</strong> Convolutional neural networks (CNNs) excel in computer vision but are susceptible to adversarial attacks, crafted perturbations designed to mislead predictions. Despite advances in adversarial training, a gap persists between model accuracy and robustness. To mitigate this issue, in this paper, we present a multi-teacher adversarial robustness distillation using an adaptive learning strategy. Specifically, our proposed method first trained multiple clones of a baseline CNN model using an adversarial training strategy on a pool of perturbed data acquired through different adversarial attacks. Once trained, these adversarially trained models are used as teacher models to supervise the learning of a student model on clean data using multi-teacher knowledge distillation. To ensure an effective robustness distillation, we design an adaptive learning strategy that controls the knowledge contribution of each model by assigning weights as per their prediction precision. Distilling knowledge from adversarially pre-trained teacher models not only enhances the learning capabilities of the student model but also empowers it with the capacity to withstand different adversarial attacks, despite having no exposure to adversarial data. To verify our claims, we extensively evaluated our proposed method on MNIST-Digits and Fashion-MNIST datasets across diverse experimental settings. The obtained results exhibit the efficacy of our multi-teacher adversarial distillation and adaptive learning strategy, enhancing CNNs adversarial robustness against various adversarial attacks.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2507.20995v1" target="_blank">VArsity: Can Large Language Models Keep Power Engineering Students in Phase?</a></h3>
                    <p><strong>Authors:</strong> Samuel Talkington, Daniel K. Molzahn</p>
                    <p><strong>Published:</strong> 7/28/2025</p>
                    <p><strong>Categories:</strong> cs.CY, cs.SY, eess.SY</p>
                    <p><strong>Summary:</strong> This paper provides an educational case study regarding our experience in deploying ChatGPT Large Language Models (LLMs) in the Spring 2025 and Fall 2023 offerings of ECE 4320: Power System Analysis and Control at Georgia Tech. As part of course assessments, students were tasked with identifying, explaining, and correcting errors in the ChatGPT outputs corresponding to power factor correction problems. While most students successfully identified the errors in the outputs from the GPT-4 version of ChatGPT used in Fall 2023, students found the errors from the ChatGPT o1 version much more difficult to identify in Spring 2025. As shown in this case study, the role of LLMs in pedagogy, assessment, and learning in power engineering classrooms is an important topic deserving further investigation.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2507.20994v1" target="_blank">Security Tensors as a Cross-Modal Bridge: Extending Text-Aligned Safety to Vision in LVLM</a></h3>
                    <p><strong>Authors:</strong> Shen Li, Liuyi Yao, Wujia Niu, Lan Zhang, Yaliang Li</p>
                    <p><strong>Published:</strong> 7/28/2025</p>
                    <p><strong>Categories:</strong> cs.CV, cs.AI</p>
                    <p><strong>Summary:</strong> Large visual-language models (LVLMs) integrate aligned large language models (LLMs) with visual modules to process multimodal inputs. However, the safety mechanisms developed for text-based LLMs do not naturally extend to visual modalities, leaving LVLMs vulnerable to harmful image inputs. To address this cross-modal safety gap, we introduce security tensors - trainable input vectors applied during inference through either the textual or visual modality. These tensors transfer textual safety alignment to visual processing without modifying the models parameters. They are optimized using a curated dataset containing (i) malicious image-text pairs requiring rejection, (ii) contrastive benign pairs with text structurally similar to malicious queries, with the purpose of being contrastive examples to guide visual reliance, and (iii) general benign samples preserving model functionality. Experimental results demonstrate that both textual and visual security tensors significantly enhance LVLMs ability to reject diverse harmful visual inputs while maintaining near-identical performance on benign tasks. Further internal analysis towards hidden-layer representations reveals that security tensors successfully activate the language modules textual safety layers in visual inputs, thereby effectively extending text-based safety to the visual modality.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2507.20992v1" target="_blank">Heat Capacity and the Violation of Scaling Laws in Gravitational System</a></h3>
                    <p><strong>Authors:</strong> Shi-Bei Kong</p>
                    <p><strong>Published:</strong> 7/28/2025</p>
                    <p><strong>Categories:</strong> gr-qc, hep-th</p>
                    <p><strong>Summary:</strong> In this paper, we examine the scaling laws in gravitational system from the perspective of free energy landscape. It has been found that for some special black holes, their critical exponents are beyond mean field theory, and more surprisingly violate the scaling laws. We find that the main reason for the violation of the scaling laws is that the heat capacity at constant volume for these black holes is 0, so the critical exponent $\alpha$ is often treated as 0 and can not be derived from the scaling hypothesis while another three critical exponents can. We also find that the strange critical exponents means there is a symmetry violation in the order parameter.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2507.20990v1" target="_blank">PyBird-JAX: Accelerated inference in large-scale structure with model-independent emulation of one-loop galaxy power spectra</a></h3>
                    <p><strong>Authors:</strong> Alexander Reeves, Pierre Zhang, Henry Zheng</p>
                    <p><strong>Published:</strong> 7/28/2025</p>
                    <p><strong>Categories:</strong> astro-ph.CO, astro-ph.IM</p>
                    <p><strong>Summary:</strong> We present $\texttt{PyBird-JAX}$, a differentiable, $\texttt{JAX}$-based implementation of $\texttt{PyBird}$, using internal neural network emulators to accelerate computationally costly operations for rapid large-scale structure (LSS) analysis. $\texttt{PyBird-JAX}$ computes one-loop EFTofLSS predictions for redshift-space galaxy power spectrum multipoles in 1.2 ms on a CPU and 0.2 ms on a GPU, achieving 3-4 orders of magnitude speed-up over $\texttt{PyBird}$. The emulators take a compact spline-based representation of the input linear power spectrum $P(k)$ as feature vectors, making the approach applicable to a wide range of cosmological models. We rigorously validate its accuracy against large-volume simulations and on BOSS data, including cosmologies not explicitly represented in the training set. Leveraging automatic differentiation, $\texttt{PyBird-JAX}$ supports Fisher forecasting, Taylor expansion of model predictions, gradient-based searches, and vectorised ensemble sampling. Interfaced with a variety of samplers and Boltzmann solvers, $\texttt{PyBird-JAX}$ provides a high-performance, end-to-end inference pipeline. Combined with a symbolic-$P(k)$ generator, a typical Stage-4 LSS MCMC converges in minutes on a GPU. Our results demonstrate that $\texttt{PyBird-JAX}$ delivers the precision and speed required for upcoming LSS surveys, opening the door to accelerated cosmological inference with minimal accuracy loss and no pretraining. In a companion paper [1], we put $\texttt{PyBird-JAX}$ to use in achieving LSS marginalised constraints free from volume projection effects through non-flat measures.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2507.20987v1" target="_blank">JWB-DH-V1: Benchmark for Joint Whole-Body Talking Avatar and Speech Generation Version 1</a></h3>
                    <p><strong>Authors:</strong> Xinhan Di, Kristin Qi, Pengqian Yu</p>
                    <p><strong>Published:</strong> 7/28/2025</p>
                    <p><strong>Categories:</strong> cs.CV, cs.AI</p>
                    <p><strong>Summary:</strong> Recent advances in diffusion-based video generation have enabled photo-realistic short clips, but current methods still struggle to achieve multi-modal consistency when jointly generating whole-body motion and natural speech. Current approaches lack comprehensive evaluation frameworks that assess both visual and audio quality, and there are insufficient benchmarks for region-specific performance analysis. To address these gaps, we introduce the Joint Whole-Body Talking Avatar and Speech Generation Version I(JWB-DH-V1), comprising a large-scale multi-modal dataset with 10,000 unique identities across 2 million video samples, and an evaluation protocol for assessing joint audio-video generation of whole-body animatable avatars. Our evaluation of SOTA models reveals consistent performance disparities between face/hand-centric and whole-body performance, which incidates essential areas for future research. The dataset and evaluation tools are publicly available at https://github.com/deepreasonings/WholeBodyBenchmark.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2507.20985v1" target="_blank">Behavioral Study of Dashboard Mechanisms</a></h3>
                    <p><strong>Authors:</strong> Paula Kayongo, Jessica Hullman, Jason Hartline</p>
                    <p><strong>Published:</strong> 7/28/2025</p>
                    <p><strong>Categories:</strong> cs.GT</p>
                    <p><strong>Summary:</strong> Visualization dashboards are increasingly used in strategic settings like auctions to enhance decision-making and reduce strategic confusion. This paper presents behavioral experiments evaluating how different dashboard designs affect bid optimization in reverse first-price auctions. Additionally, we assess how dashboard designs impact the auction designers ability to accurately infer bidders preferences within the dashboard mechanism framework. We compare visualizations of the bid allocation rule, commonly deployed in practice, to alternatives that display expected utility. We find that utility-based visualizations significantly improve bidding by reducing cognitive demands on bidders. However, even with improved dashboards, bidders systematically under-shade their bids, driven by an implicit preference for certain wins in uncertain settings. As a result, dashboard-based mechanisms that assume fully rational or risk-neutral bidder responses to dashboards can produce significant estimation errors when inferring private preferences, which may lead to suboptimal allocations in practice. Explicitly modeling agents behavioral responses to dashboards substantially improves inference accuracy, highlighting the need to align visualization design and econometric inference assumptions in practice.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2507.20984v1" target="_blank">SmallThinker: A Family of Efficient Large Language Models Natively Trained for Local Deployment</a></h3>
                    <p><strong>Authors:</strong> Yixin Song, Zhenliang Xue, Dongliang Wei, Feiyang Chen, Jianxiang Gao, Junchen Liu, Hangyu Liang, Guangshuo Qin, Chengrong Tian, Bo Wen, Longyu Zhao, Xinrui Zheng, Zeyu Mi, Haibo Chen</p>
                    <p><strong>Published:</strong> 7/28/2025</p>
                    <p><strong>Categories:</strong> cs.LG, cs.AI</p>
                    <p><strong>Summary:</strong> While frontier large language models (LLMs) continue to push capability boundaries, their deployment remains confined to GPU-powered cloud infrastructure. We challenge this paradigm with SmallThinker, a family of LLMs natively designed - not adapted - for the unique constraints of local devices: weak computational power, limited memory, and slow storage. Unlike traditional approaches that mainly compress existing models built for clouds, we architect SmallThinker from the ground up to thrive within these limitations. Our innovation lies in a deployment-aware architecture that transforms constraints into design principles. First, We introduce a two-level sparse structure combining fine-grained Mixture-of-Experts (MoE) with sparse feed-forward networks, drastically reducing computational demands without sacrificing model capacity. Second, to conquer the I/O bottleneck of slow storage, we design a pre-attention router that enables our co-designed inference engine to prefetch expert parameters from storage while computing attention, effectively hiding storage latency that would otherwise cripple on-device inference. Third, for memory efficiency, we utilize NoPE-RoPE hybrid sparse attention mechanism to slash KV cache requirements. We release SmallThinker-4B-A0.6B and SmallThinker-21B-A3B, which achieve state-of-the-art performance scores and even outperform larger LLMs. Remarkably, our co-designed system mostly eliminates the need for expensive GPU hardware: with Q4_0 quantization, both models exceed 20 tokens/s on ordinary consumer CPUs, while consuming only 1GB and 8GB of memory respectively. SmallThinker is publicly available at hf.co/PowerInfer/SmallThinker-4BA0.6B-Instruct and hf.co/PowerInfer/SmallThinker-21BA3B-Instruct.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2507.20983v1" target="_blank">Fiducial observers and the thermal atmosphere in the black hole quantum throat</a></h3>
                    <p><strong>Authors:</strong> Thomas G. Mertens, Thomas Tappeiner, Bruno de S. L. Torres</p>
                    <p><strong>Published:</strong> 7/28/2025</p>
                    <p><strong>Categories:</strong> hep-th, gr-qc</p>
                    <p><strong>Summary:</strong> We propose a construction of fiducial observers in the throat region of near-extremal black holes within the framework of JT quantum gravity, leading to a notion of local observers in a highly quantum regime of the gravitational field. The construction is based on an earlier proposal for light-ray anchoring to the asymptotic boundary and is uniquely fixed at the semiclassical level by demanding that the notion of time translations for an observer at the asymptotic boundary of JT gravity should be extended into the bulk as the flow of a conformal isometry. Since conformal isometries are a necessary condition for geometric modular flow, our construction is amenable as a candidate geometric gravitational dressing that may be interpreted via the modular crossed product, potentially connecting our choice of dressing with recent developments on the literature on local observables in quantum gravity. Taking this definition beyond the semiclassical regime, we compute quantum gravitational wormhole contributions to the black hole thermal atmosphere, directly producing a finite thermal entropy and leading to a quantum description of the stretched horizon in this model.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2507.20981v1" target="_blank">Stochastic gradient with least-squares control variates</a></h3>
                    <p><strong>Authors:</strong> Fabio Nobile, Matteo Raviola, Nathan Schaeffer</p>
                    <p><strong>Published:</strong> 7/28/2025</p>
                    <p><strong>Categories:</strong> math.OC, cs.NA, math.NA</p>
                    <p><strong>Summary:</strong> The stochastic gradient descent (SGD) method is a widely used approach for solving stochastic optimization problems, but its convergence is typically slow. Existing variance reduction techniques, such as SAGA, improve convergence by leveraging stored gradient information; however, they are restricted to settings where the objective functional is a finite sum, and their performance degrades when the number of terms in the sum is large. In this work, we propose a novel approach which is well suited when the objective is given by an expectation over random variables with a continuous probability distribution. Our method constructs a control variate by fitting a linear model to past gradient evaluations using weighted discrete least-squares, effectively reducing variance while preserving computational efficiency. We establish theoretical sublinear convergence guarantees for strongly convex objectives and demonstrate the methods effectiveness through numerical experiments on random PDE-constrained optimization problems.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2507.20980v1" target="_blank">LargeMvC-Net: Anchor-based Deep Unfolding Network for Large-scale Multi-view Clustering</a></h3>
                    <p><strong>Authors:</strong> Shide Du, Chunming Wu, Zihan Fang, Wendi Zhao, Yilin Wu, Changwei Wang, Shiping Wang</p>
                    <p><strong>Published:</strong> 7/28/2025</p>
                    <p><strong>Categories:</strong> cs.CV, stat.CO, stat.ML</p>
                    <p><strong>Summary:</strong> Deep anchor-based multi-view clustering methods enhance the scalability of neural networks by utilizing representative anchors to reduce the computational complexity of large-scale clustering. Despite their scalability advantages, existing approaches often incorporate anchor structures in a heuristic or task-agnostic manner, either through post-hoc graph construction or as auxiliary components for message passing. Such designs overlook the core structural demands of anchor-based clustering, neglecting key optimization principles. To bridge this gap, we revisit the underlying optimization problem of large-scale anchor-based multi-view clustering and unfold its iterative solution into a novel deep network architecture, termed LargeMvC-Net. The proposed model decomposes the anchor-based clustering process into three modules: RepresentModule, NoiseModule, and AnchorModule, corresponding to representation learning, noise suppression, and anchor indicator estimation. Each module is derived by unfolding a step of the original optimization procedure into a dedicated network component, providing structural clarity and optimization traceability. In addition, an unsupervised reconstruction loss aligns each view with the anchor-induced latent space, encouraging consistent clustering structures across views. Extensive experiments on several large-scale multi-view benchmarks show that LargeMvC-Net consistently outperforms state-of-the-art methods in terms of both effectiveness and scalability.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2507.20979v1" target="_blank">Ultralight boson constraints from gravitational wave observations of spinning binary black holes</a></h3>
                    <p><strong>Authors:</strong> P. S. Aswathi, William E. East, Nils Siemonsen, Ling Sun, Dana Jones</p>
                    <p><strong>Published:</strong> 7/28/2025</p>
                    <p><strong>Categories:</strong> gr-qc, astro-ph.HE, hep-ph</p>
                    <p><strong>Summary:</strong> In the presence of an ultralight scalar or vector boson, a spinning black hole will be spun down through the superradiant instability. We use spin measurements from gravitational wave observations of binary black holes, in particular the heavy binary black hole merger event GW231123, along with the lower-mass GW190517 event, to constrain the existence of ultralight bosons. We disfavor scalars with masses in the range of $[0.55, 11]\times 10^{-13}$ eV and vectors in the range of $[0.11, 18]\times 10^{-13}$ eV, making only a conservative assumption that the black hole lifetimes are greater than $10^5$ years. The lower ends of these ranges, where the exclusion confidence is the highest, were not previously excluded by spin measurements from electromagnetic or gravitational wave observations. We map these constraints to axion and dark photon models with interactions.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2507.20976v1" target="_blank">Adapting Vehicle Detectors for Aerial Imagery to Unseen Domains with Weak Supervision</a></h3>
                    <p><strong>Authors:</strong> Xiao Fang, Minhyek Jeon, Zheyang Qin, Stanislav Panev, Celso de Melo, Shuowen Hu, Shayok Chakraborty, Fernando De la Torre</p>
                    <p><strong>Published:</strong> 7/28/2025</p>
                    <p><strong>Categories:</strong> cs.CV</p>
                    <p><strong>Summary:</strong> Detecting vehicles in aerial imagery is a critical task with applications in traffic monitoring, urban planning, and defense intelligence. Deep learning methods have provided state-of-the-art (SOTA) results for this application. However, a significant challenge arises when models trained on data from one geographic region fail to generalize effectively to other areas. Variability in factors such as environmental conditions, urban layouts, road networks, vehicle types, and image acquisition parameters (e.g., resolution, lighting, and angle) leads to domain shifts that degrade model performance. This paper proposes a novel method that uses generative AI to synthesize high-quality aerial images and their labels, improving detector training through data augmentation. Our key contribution is the development of a multi-stage, multi-modal knowledge transfer framework utilizing fine-tuned latent diffusion models (LDMs) to mitigate the distribution gap between the source and target environments. Extensive experiments across diverse aerial imagery domains show consistent performance improvements in AP50 over supervised learning on source domain data, weakly supervised adaptation methods, unsupervised domain adaptation methods, and open-set object detectors by 4-23%, 6-10%, 7-40%, and more than 50%, respectively. Furthermore, we introduce two newly annotated aerial datasets from New Zealand and Utah to support further research in this field. Project page is available at: https://humansensinglab.github.io/AGenDA</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2507.20973v1" target="_blank">Model-Agnostic Gender Bias Control for Text-to-Image Generation via Sparse Autoencoder</a></h3>
                    <p><strong>Authors:</strong> Chao Wu, Zhenyi Wang, Kangxian Xie, Naresh Kumar Devulapally, Vishnu Suresh Lokhande, Mingchen Gao</p>
                    <p><strong>Published:</strong> 7/28/2025</p>
                    <p><strong>Categories:</strong> cs.LG, cs.CV</p>
                    <p><strong>Summary:</strong> Text-to-image (T2I) diffusion models often exhibit gender bias, particularly by generating stereotypical associations between professions and gendered subjects. This paper presents SAE Debias, a lightweight and model-agnostic framework for mitigating such bias in T2I generation. Unlike prior approaches that rely on CLIP-based filtering or prompt engineering, which often require model-specific adjustments and offer limited control, SAE Debias operates directly within the feature space without retraining or architectural modifications. By leveraging a k-sparse autoencoder pre-trained on a gender bias dataset, the method identifies gender-relevant directions within the sparse latent space, capturing professional stereotypes. Specifically, a biased direction per profession is constructed from sparse latents and suppressed during inference to steer generations toward more gender-balanced outputs. Trained only once, the sparse autoencoder provides a reusable debiasing direction, offering effective control and interpretable insight into biased subspaces. Extensive evaluations across multiple T2I models, including Stable Diffusion 1.4, 1.5, 2.1, and SDXL, demonstrate that SAE Debias substantially reduces gender bias while preserving generation quality. To the best of our knowledge, this is the first work to apply sparse autoencoders for identifying and intervening in gender bias within T2I models. These findings contribute toward building socially responsible generative AI, providing an interpretable and model-agnostic tool to support fairness in text-to-image generation.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2507.20964v1" target="_blank">Core Safety Values for Provably Corrigible Agents</a></h3>
                    <p><strong>Authors:</strong> Aran Nayebi</p>
                    <p><strong>Published:</strong> 7/28/2025</p>
                    <p><strong>Categories:</strong> cs.AI, cs.CC, cs.GT, cs.LG, cs.MA</p>
                    <p><strong>Summary:</strong> We introduce the first implementable framework for corrigibility, with provable guarantees in multi-step, partially observed environments. Our framework replaces a single opaque reward with five *structurally separate* utility heads -- deference, switch-access preservation, truthfulness, low-impact behavior via a belief-based extension of Attainable Utility Preservation, and bounded task reward -- combined lexicographically by strict weight gaps. Theorem 1 proves exact single-round corrigibility in the partially observable off-switch game; Theorem 3 extends the guarantee to multi-step, self-spawning agents, showing that even if each head is \emph{learned} to mean-squared error $\varepsilon$ and the planner is $\varepsilon$-sub-optimal, the probability of violating \emph{any} safety property is bounded while still ensuring net human benefit. In contrast to Constitutional AI or RLHF/RLAIF, which merge all norms into one learned scalar, our separation makes obedience and impact-limits dominate even when incentives conflict. For open-ended settings where adversaries can modify the agent, we prove that deciding whether an arbitrary post-hack agent will ever violate corrigibility is undecidable by reduction to the halting problem, then carve out a finite-horizon ``decidable island where safety can be certified in randomized polynomial time and verified with privacy-preserving, constant-round zero-knowledge proofs. Consequently, the remaining challenge is the ordinary ML task of data coverage and generalization: reward-hacking risk is pushed into evaluation quality rather than hidden incentive leak-through, giving clearer implementation guidance for todays LLM assistants and future autonomous systems.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2507.20963v1" target="_blank">GTAD: Global Temporal Aggregation Denoising Learning for 3D Semantic Occupancy Prediction</a></h3>
                    <p><strong>Authors:</strong> Tianhao Li, Yang Li, Mengtian Li, Yisheng Deng, Weifeng Ge</p>
                    <p><strong>Published:</strong> 7/28/2025</p>
                    <p><strong>Categories:</strong> cs.CV</p>
                    <p><strong>Summary:</strong> Accurately perceiving dynamic environments is a fundamental task for autonomous driving and robotic systems. Existing methods inadequately utilize temporal information, relying mainly on local temporal interactions between adjacent frames and failing to leverage global sequence information effectively. To address this limitation, we investigate how to effectively aggregate global temporal features from temporal sequences, aiming to achieve occupancy representations that efficiently utilize global temporal information from historical observations. For this purpose, we propose a global temporal aggregation denoising network named GTAD, introducing a global temporal information aggregation framework as a new paradigm for holistic 3D scene understanding. Our method employs an in-model latent denoising network to aggregate local temporal features from the current moment and global temporal features from historical sequences. This approach enables the effective perception of both fine-grained temporal information from adjacent frames and global temporal patterns from historical observations. As a result, it provides a more coherent and comprehensive understanding of the environment. Extensive experiments on the nuScenes and Occ3D-nuScenes benchmark and ablation studies demonstrate the superiority of our method.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2507.20957v1" target="_blank">Your AI, Not Your View: The Bias of LLMs in Investment Analysis</a></h3>
                    <p><strong>Authors:</strong> Hoyoung Lee, Junhyuk Seo, Suhwan Park, Junhyeong Lee, Wonbin Ahn, Chanyeol Choi, Alejandro Lopez-Lira, Yongjae Lee</p>
                    <p><strong>Published:</strong> 7/28/2025</p>
                    <p><strong>Categories:</strong> q-fin.PM, cs.AI, cs.CL</p>
                    <p><strong>Summary:</strong> In finance, Large Language Models (LLMs) face frequent knowledge conflicts due to discrepancies between pre-trained parametric knowledge and real-time market data. These conflicts become particularly problematic when LLMs are deployed in real-world investment services, where misalignment between a models embedded preferences and those of the financial institution can lead to unreliable recommendations. Yet little research has examined what investment views LLMs actually hold. We propose an experimental framework to investigate such conflicts, offering the first quantitative analysis of confirmation bias in LLM-based investment analysis. Using hypothetical scenarios with balanced and imbalanced arguments, we extract models latent preferences and measure their persistence. Focusing on sector, size, and momentum, our analysis reveals distinct, model-specific tendencies. In particular, we observe a consistent preference for large-cap stocks and contrarian strategies across most models. These preferences often harden into confirmation bias, with models clinging to initial judgments despite counter-evidence.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2507.20956v1" target="_blank">Mind the Gap: Conformative Decoding to Improve Output Diversity of Instruction-Tuned Large Language Models</a></h3>
                    <p><strong>Authors:</strong> Max Peeperkorn, Tom Kouwenhoven, Dan Brown, Anna Jordanous</p>
                    <p><strong>Published:</strong> 7/28/2025</p>
                    <p><strong>Categories:</strong> cs.CL, cs.AI</p>
                    <p><strong>Summary:</strong> Instruction-tuning large language models (LLMs) reduces the diversity of their outputs, which has implications for many tasks, particularly for creative tasks. This paper investigates the ``diversity gap for a writing prompt narrative generation task. This gap emerges as measured by current diversity metrics for various open-weight and open-source LLMs. The results show significant decreases in diversity due to instruction-tuning. We explore the diversity loss at each fine-tuning stage for the OLMo and OLMo 2 models to further understand how output diversity is affected. The results indicate that DPO has the most substantial impact on diversity. Motivated by these findings, we present a new decoding strategy, conformative decoding, which guides an instruct model using its more diverse base model to reintroduce output diversity. We show that conformative decoding typically increases diversity and even maintains or improves quality.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2507.20954v1" target="_blank">PySHRED: A Python package for SHallow REcurrent Decoding for sparse sensing, model reduction and scientific discovery</a></h3>
                    <p><strong>Authors:</strong> David Ye, Jan Williams, Mars Gao, Stefano Riva, Matteo Tomasetto, David Zoro, J. Nathan Kutz</p>
                    <p><strong>Published:</strong> 7/28/2025</p>
                    <p><strong>Categories:</strong> cs.LG, cs.CE, math.DS, nlin.CD</p>
                    <p><strong>Summary:</strong> SHallow REcurrent Decoders (SHRED) provide a deep learning strategy for modeling high-dimensional dynamical systems and/or spatiotemporal data from dynamical system snapshot observations. PySHRED is a Python package that implements SHRED and several of its major extensions, including for robust sensing, reduced order modeling and physics discovery. In this paper, we introduce the version 1.0 release of PySHRED, which includes data preprocessors and a number of cutting-edge SHRED methods specifically designed to handle real-world data that may be noisy, multi-scale, parameterized, prohibitively high-dimensional, and strongly nonlinear. The package is easy to install, thoroughly-documented, supplemented with extensive code examples, and modularly-structured to support future additions. The entire codebase is released under the MIT license and is available at https://github.com/pyshred-dev/pyshred.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2507.20953v1" target="_blank">Mask-Free Audio-driven Talking Face Generation for Enhanced Visual Quality and Identity Preservation</a></h3>
                    <p><strong>Authors:</strong> Dogucan Yaman, Fevziye Irem Eyiokur, Leonard BÃ¤rmann, HazÄ±m Kemal Ekenel, Alexander Waibel</p>
                    <p><strong>Published:</strong> 7/28/2025</p>
                    <p><strong>Categories:</strong> cs.CV</p>
                    <p><strong>Summary:</strong> Audio-Driven Talking Face Generation aims at generating realistic videos of talking faces, focusing on accurate audio-lip synchronization without deteriorating any identity-related visual details. Recent state-of-the-art methods are based on inpainting, meaning that the lower half of the input face is masked, and the model fills the masked region by generating lips aligned with the given audio. Hence, to preserve identity-related visual details from the lower half, these approaches additionally require an unmasked identity reference image randomly selected from the same video. However, this common masking strategy suffers from (1) information loss in the input faces, significantly affecting the networks ability to preserve visual quality and identity details, (2) variation between identity reference and input image degrading reconstruction performance, and (3) the identity reference negatively impacting the model, causing unintended copying of elements unaligned with the audio. To address these issues, we propose a mask-free talking face generation approach while maintaining the 2D-based face editing task. Instead of masking the lower half, we transform the input images to have closed mouths, using a two-step landmark-based approach trained in an unpaired manner. Subsequently, we provide these edited but unmasked faces to a lip adaptation model alongside the audio to generate appropriate lip movements. Thus, our approach needs neither masked input images nor identity reference images. We conduct experiments on the benchmark LRS2 and HDTF datasets and perform various ablation studies to validate our contributions.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2507.20951v1" target="_blank">Partially Observable Monte-Carlo Graph Search</a></h3>
                    <p><strong>Authors:</strong> Yang You, Vincent Thomas, Alex Schutz, Robert Skilton, Nick Hawes, Olivier Buffet</p>
                    <p><strong>Published:</strong> 7/28/2025</p>
                    <p><strong>Categories:</strong> cs.AI, cs.RO</p>
                    <p><strong>Summary:</strong> Currently, large partially observable Markov decision processes (POMDPs) are often solved by sampling-based online methods which interleave planning and execution phases. However, a pre-computed offline policy is more desirable in POMDP applications with time or energy constraints. But previous offline algorithms are not able to scale up to large POMDPs. In this article, we propose a new sampling-based algorithm, the partially observable Monte-Carlo graph search (POMCGS) to solve large POMDPs offline. Different from many online POMDP methods, which progressively develop a tree while performing (Monte-Carlo) simulations, POMCGS folds this search tree on the fly to construct a policy graph, so that computations can be drastically reduced, and users can analyze and validate the policy prior to embedding and executing it. Moreover, POMCGS, together with action progressive widening and observation clustering methods provided in this article, is able to address certain continuous POMDPs. Through experiments, we demonstrate that POMCGS can generate policies on the most challenging POMDPs, which cannot be computed by previous offline algorithms, and these policies values are competitive compared with the state-of-the-art online POMDP algorithms.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2507.20950v1" target="_blank">Witness the High-Dimensional Quantum Steering via Majorization Lattice</a></h3>
                    <p><strong>Authors:</strong> Ma-Cheng Yang, Cong-Feng Qiao</p>
                    <p><strong>Published:</strong> 7/28/2025</p>
                    <p><strong>Categories:</strong> quant-ph</p>
                    <p><strong>Summary:</strong> Quantum steering enables one party to influence another remote quantum state by local measurement. While steering is fundamental to many quantum information tasks, the existing detection methods in the literature are mainly constrained to either specific measurement scenario or low-dimensional systems. In this work, we propose a majorization lattice framework for steering detection, which is capable of exploring the steering in arbitrary dimension and measurement setting. Steering inequalities for two-qubit states, high-dimensional Werner states and isotropic states are obtained, which set even stringent bars than what have reached yet. Notably, the known high-dimensional results turn out to be some kind of approximate limits of the new approach.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2507.20946v1" target="_blank">Component groups for non-supercuspidal $L$-parameters for $p$-adic $\mathrm{SL}_3$</a></h3>
                    <p><strong>Authors:</strong> Kwangho Choiy, Doyon Kim, Razan Taha, Pan Yan</p>
                    <p><strong>Published:</strong> 7/28/2025</p>
                    <p><strong>Categories:</strong> math.RT, math.NT, 22E50 (Primary) 22E35, 11F70 (Secondary)</p>
                    <p><strong>Summary:</strong> We explicitly classify all the component groups associated to the non-supercuspidal, tempered $L$-parameters of $\mathrm{SL}_3(F)$ for a $p$-adic field $F$ of characteristic $0$ by direct case-by-case computations in $\mathrm{PGL}_3(\mathbb{C})$, following earlier work for $\mathrm{SL}_2$ by Labesse-Langlands and Shelstad.</p>
                
            
        
        
            <p><em>Generated by AI News Agent</em></p>
        
    

