
    
        <h1>🤖 AI Research Report</h1>
        
            <strong>Date:</strong> 2025-08-04<br>
            <strong>Topics:</strong> ai safety research, ai alignment research, quantum computing<br>
            <strong>Papers Found:</strong> 105
        
        
        
            
                <h2>🤖 AI Summary</h2>
                <p>## ai safety research

The research papers provided cover a broad range of topics, with several relevant insights and trends in AI safety research. A key theme is the need for robust and reliable AI systems, as demonstrated by papers focusing on semantic segmentation performance in AI safety applications, such as those employed in public safety. The challenges of data heterogeneity, class imbalance, and the detection of small, safety-critical features underscore the necessity for standardization in data labeling and model training to ensure reliable AI performance in safety-critical scenarios (Paper 1).

AI safety is also addressed in the context of LLMs and their deployment in sensitive and inclusive environments. Papers on LLMs emphasize the importance of handling gender-neutral pronouns (Paper 4) and the necessity of robust confidence estimation frameworks to ensure trustworthy outputs (Paper 45). These studies highlight the ongoing need to improve AI models to avoid biases and inaccuracies that could lead to harmful or unfair outcomes. Additionally, the exploration of agentic frameworks and retrieval-augmented generation in AI-driven decision-making (Paper 13) and radiology question answering (Paper 13) point to advancements in enhancing factuality and reducing hallucinations, which are critical for maintaining safety and reliability in AI systems used in healthcare and other sensitive fields.

In summary, the papers reveal an overarching trend towards developing AI systems that not only perform effectively but also align with ethical standards and safety requirements. This includes advancements in data handling, interpretability, and trustworthiness, which are essential for deploying AI in real-world applications where safety is paramount. Future research directions emphasize the need for improved training methodologies, enhanced model interpretability, and the integration of ethical considerations into AI system design to ensure safe and reliable AI deployment across various domains.

*Based on 50 research papers*

---

## ai alignment research

The research papers provided focus on various domains such as satellite communication, language models, legal systems, and others, yet they share a common underlying theme of enhancing AIs capability and ethical alignment with human values—a central theme in AI alignment research.

One prominent trend is the improvement of AI systems ability to handle complex, nuanced tasks while maintaining ethical standards and cultural sensitivity. For instance, the study of large language models (LLMs) in handling nonbinary pronouns highlights the ongoing efforts in inclusive AI, which is crucial for ensuring that AI systems align with diverse human values and fairness. Similarly, the evaluation of LLMs in Persian cultural contexts (MELAC) underscores the importance of linguistic and cultural alignment in AI models, which is vital for their global applicability and acceptance.

Additionally, the use of explainable AI (XAI) techniques in educational and clinical settings, as seen in the studies on student evaluation and medical reasoning, reflects the growing emphasis on transparency and accountability in AI decision-making processes. These efforts aim to align AI systems more closely with human reasoning and decision-making processes, thereby increasing trust and usability.

Furthermore, the exploration of agentic frameworks in radiology question answering and the development of retrieval-augmented generation (RAG) frameworks in legal systems (NyayaRAG) demonstrate the potential of AI to enhance decision-making in critical domains while ensuring alignment with expert knowledge and legal standards. These advancements highlight the importance of integrating AI with domain-specific knowledge to improve its reliability and alignment with human expertise.

Overall, these studies illustrate significant progress in improving AIs inclusivity, transparency, and integration with human systems, which are crucial aspects of AI alignment research. The implication is a more ethical, reliable, and context-aware deployment of AI technologies, paving the way for their responsible integration into various aspects of society.

*Based on 50 research papers*

---

## quantum computing

While none of the provided research papers focus explicitly on quantum computing, the paper titled **Entanglement swapping for partially entangled qudits and the role of quantum complementarity** by Diego S. Starke et al. is most relevant to this field. This study extends the entanglement swapping protocol to partially entangled qudit states, offering insights into quantum communication and computation. The authors analyze the protocol using complete complementarity relations, demonstrating that the average distributed entanglement is bounded by the initial entanglements of the input pairs. This work reveals that initial states with no local quantum coherence are sufficient to explore the protocols key characteristics, simplifying the process. The findings suggest that the bounds on distributed entanglement could be tightened, paving the way for more efficient quantum communication protocols. This research highlights the importance of entanglement in quantum systems and its potential to enhance quantum communication networks, a critical aspect of quantum computing.

The other papers, while intriguing, focus on different topics such as visual navigation, semantic segmentation in 3D point-cloud data, large language models, and dark matter detection, which intersect less directly with the core themes of quantum computing. However, the continual advancements in these diverse areas of research could indirectly benefit quantum computing through advancements in data processing, machine learning, and sensor technologies, which are increasingly important in the practical implementation and scaling of quantum systems.

*Based on 5 research papers*</p>
            
        
        
        <h2>📚 Research Papers</h2>
        
                
                    <h3><a href="http://arxiv.org/abs/2508.00822v1" target="_blank">Cross-Dataset Semantic Segmentation Performance Analysis: Unifying NIST Point Cloud City Datasets for 3D Deep Learning</a></h3>
                    <p><strong>Authors:</strong> Alexander Nikitas Dimopoulos, Joseph Grasso</p>
                    <p><strong>Published:</strong> 8/1/2025</p>
                    <p><strong>Categories:</strong> cs.CV</p>
                    <p><strong>Summary:</strong> This study analyzes semantic segmentation performance across heterogeneously labeled point-cloud datasets relevant to public safety applications, including pre-incident planning systems derived from lidar scans. Using NISTs Point Cloud City dataset (Enfield and Memphis collections), we investigate challenges in unifying differently labeled 3D data. Our methodology employs a graded schema with the KPConv architecture, evaluating performance through IoU metrics on safety-relevant features. Results indicate performance variability: geometrically large objects (e.g. stairs, windows) achieve higher segmentation performance, suggesting potential for navigational context, while smaller safety-critical features exhibit lower recognition rates. Performance is impacted by class imbalance and the limited geometric distinction of smaller objects in typical lidar scans, indicating limitations in detecting certain safety-relevant features using current point-cloud methods. Key identified challenges include insufficient labeled data, difficulties in unifying class labels across datasets, and the need for standardization. Potential directions include automated labeling and multi-dataset learning strategies. We conclude that reliable point-cloud semantic segmentation for public safety necessitates standardized annotation protocols and improved labeling techniques to address data heterogeneity and the detection of small, safety-critical elements.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.00800v1" target="_blank">Multibeam High Throughput Satellite: Hardware Foundation, Resource Allocation, and Precoding</a></h3>
                    <p><strong>Authors:</strong> Rui Chen, Wen-Xuan Long, Bing-Qian Wang, Yuan He, Rui-Jin Sun, Nan Cheng, Gan Zheng, Dusit Niyato</p>
                    <p><strong>Published:</strong> 8/1/2025</p>
                    <p><strong>Categories:</strong> eess.SP</p>
                    <p><strong>Summary:</strong> With its wide coverage and uninterrupted service, satellite communication is a critical technology for next-generation 6G communications. High throughput satellite (HTS) systems, utilizing multipoint beam and frequency multiplexing techniques, enable satellite communication capacity of up to Tbps to meet the growing traffic demand. Therefore, it is imperative to review the-state-of-the-art of multibeam HTS systems and identify their associated challenges and perspectives. Firstly, we summarize the multibeam HTS hardware foundations, including ground station systems, on-board payloads, and user terminals. Subsequently, we review the flexible on-board radio resource allocation approaches of bandwidth, power, time slot, and joint allocation schemes of HTS systems to optimize resource utilization and cater to non-uniform service demand. Additionally, we survey multibeam precoding methods for the HTS system to achieve full-frequency reuse and interference cancellation, which are classified according to different deployments such as single gateway precoding, multiple gateway precoding, on-board precoding, and hybrid on-board/on-ground precoding. Finally, we disscuss the challenges related to Q/V band link outage, time and frequency synchronization of gateways, the accuracy of channel state information (CSI), payload light-weight development, and the application of deep learning (DL). Research on these topics will contribute to enhancing the performance of HTS systems and finally delivering high-speed data to areas underserved by terrestrial networks.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.00796v1" target="_blank">UV flux variation study in contact binary VW Cephei</a></h3>
                    <p><strong>Authors:</strong> Anurag Baruah, Mayukh Pahari</p>
                    <p><strong>Published:</strong> 8/1/2025</p>
                    <p><strong>Categories:</strong> astro-ph.SR</p>
                    <p><strong>Summary:</strong> Despite many attempts, the origin of UV emission line and continuum in contact binary stars remains unclear. We present a substantial UV spectroscopic analysis of VW Cephei, a late-type contact binary system, using 46 low-resolution spectra from the International Ultraviolet Explorer (IUE) in the wavelength range 1150-1978 \r{A}. By modelling continuum and emissions lines in individual spectra, we report the significant detection of OIII] (1660 and 1666 \r{A}) and SiIV (1393 and 1402 \r{A}) line complexes. We observe that UV fluxes for both continuum and emission lines like CIV, OIII], CII and SiIV vary significantly (fractional rms variability up to 45%) from hours to years. In addition, line widths also change by hundreds of kilometres/sec. The UV flux variabilities observed in the continuum bands and line emissions are uncorrelated. However, most of the flux values follow the binary orbital period observed from optical data. Our analysis indicates that, while the variation in continuum flux may be attributed to a heated photosphere, the line width measurements indicate that the emission lines are likely formed in the dynamical clouds associated with Roche lobe overflow. We estimate the mass transfer rate of $ \dot{M} = (0.82 \pm 0.01) \times 10^{-7} \ M_{\odot} {yr^{-1}}$ from UV line fluxes, which is in good agreement with optical studies.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.00788v1" target="_blank">Do They Understand Them? An Updated Evaluation on Nonbinary Pronoun Handling in Large Language Models</a></h3>
                    <p><strong>Authors:</strong> Xushuo Tang, Yi Ding, Zhengyi Yang, Yin Chen, Yongrui Gu, Wenke Yang, Mingchen Ju, Xin Cao, Yongfei Liu, Wenjie Zhang</p>
                    <p><strong>Published:</strong> 8/1/2025</p>
                    <p><strong>Categories:</strong> cs.CL, cs.AI</p>
                    <p><strong>Summary:</strong> Large language models (LLMs) are increasingly deployed in sensitive contexts where fairness and inclusivity are critical. Pronoun usage, especially concerning gender-neutral and neopronouns, remains a key challenge for responsible AI. Prior work, such as the MISGENDERED benchmark, revealed significant limitations in earlier LLMs handling of inclusive pronouns, but was constrained to outdated models and limited evaluations. In this study, we introduce MISGENDERED+, an extended and updated benchmark for evaluating LLMs pronoun fidelity. We benchmark five representative LLMs, GPT-4o, Claude 4, DeepSeek-V3, Qwen Turbo, and Qwen2.5, across zero-shot, few-shot, and gender identity inference. Our results show notable improvements compared with previous studies, especially in binary and gender-neutral pronoun accuracy. However, accuracy on neopronouns and reverse inference tasks remains inconsistent, underscoring persistent gaps in identity-sensitive reasoning. We discuss implications, model-specific observations, and avenues for future inclusive AI research.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.00785v1" target="_blank">Explainable AI and Machine Learning for Exam-based Student Evaluation: Causal and Predictive Analysis of Socio-academic and Economic Factors</a></h3>
                    <p><strong>Authors:</strong> Bushra Akter, Md Biplob Hosen, Sabbir Ahmed, Mehrin Anannya, Md. Farhad Hossain</p>
                    <p><strong>Published:</strong> 8/1/2025</p>
                    <p><strong>Categories:</strong> cs.LG</p>
                    <p><strong>Summary:</strong> Academic performance depends on a multivariable nexus of socio-academic and financial factors. This study investigates these influences to develop effective strategies for optimizing students CGPA. To achieve this, we reviewed various literature to identify key influencing factors and constructed an initial hypothetical causal graph based on the findings. Additionally, an online survey was conducted, where 1,050 students participated, providing comprehensive data for analysis. Rigorous data preprocessing techniques, including cleaning and visualization, ensured data quality before analysis. Causal analysis validated the relationships among variables, offering deeper insights into their direct and indirect effects on CGPA. Regression models were implemented for CGPA prediction, while classification models categorized students based on performance levels. Ridge Regression demonstrated strong predictive accuracy, achieving a Mean Absolute Error of 0.12 and a Mean Squared Error of 0.023. Random Forest outperformed in classification, attaining an F1-score near perfection and an accuracy of 98.68%. Explainable AI techniques such as SHAP, LIME, and Interpret enhanced model interpretability, highlighting critical factors such as study hours, scholarships, parental education, and prior academic performance. The study culminated in the development of a web-based application that provides students with personalized insights, allowing them to predict academic performance, identify areas for improvement, and make informed decisions to enhance their outcomes.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.00780v1" target="_blank">CUbesat Solar Polarimeter (CUSP) Sensitivity Estimation and Performance Optimization using Geant4</a></h3>
                    <p><strong>Authors:</strong> Abhay Kumar, Giovanni Lombardi, Giovanni De Cesare, Nicolas De Angelis, Sergio Fabiani, Ettore Del Monte, Andrea Alimenti, Riccardo Campana, Enrico Costa, Paolo Soffitta, Mauro Centrone, Sergio Di Cosimo, Giuseppe Di Persio, Alessandro Lacerenza, Pasqualino Loffredo, Fabio Muleri, Paolo Romano, Alda Rubini, Emanuele Scalise, Enrico Silva, Davide Albanesi, Ilaria Baffo, Daniele Brienza, Valerio Campamaggiore, Giovanni Cucinella, Andrea Curatolo, Giulia de Iulis, Andrea Del Re, Vito Di Bari, Simone Di Filippo, Immacolata Donnarumma, Pierluigi Fanelli, Nicolas Gagliardi, Paolo Leonetti, Matteo Merge, Gabriele Minervini, Dario Modenini, Andrea Negri, Daniele Pecorella, Massimo Perelli, Alice Ponti, Francesca Sbop, Paolo Tortora, Alessandro Turchi, Valerio Vagelli, Emanuele Zaccagnino, Alessandro Zambardi, Costantino Zazza</p>
                    <p><strong>Published:</strong> 8/1/2025</p>
                    <p><strong>Categories:</strong> astro-ph.SR, astro-ph.HE, astro-ph.IM</p>
                    <p><strong>Summary:</strong> The CUbesat Solar Polarimeter (CUSP) aims to measure the linear polarization of solar flares in the 25-100 keV X-ray band using a Compton scattering polarimeter. CUSP will allow us to study the magnetic reconnection and particle acceleration in the flaring magnetic structures of our star by providing high-sensitivity polarization measurements. CUSP is a project in the framework of the Alcor Program of the Italian Space Agency aimed to develop innovative CubeSat technologies and missions. As part of CUSPs Phase B study, which began in December 2024 and will continue for one year, we present the development status of the Geant4 based simulator to accurately simulate the detectors response and initial results on the sensitivity of the instrument. Geant4 Monte Carlo simulation is used to assess the physical interactions of the source photons with the detector and the passive materials. We implemented a detailed CUSP Mass Model within Geant4 to simulate and estimate the instruments sensitivity, correcting the geometric effects of the instrument. We also evaluated the effect of backscattering shielding on the sensitivity to optimize the mass model of the instrument.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.00778v1" target="_blank">τ-Ring: A Smart Ring Platform for Multimodal Physiological and Behavioral Sensing</a></h3>
                    <p><strong>Authors:</strong> Jiankai Tang, Zhe He, Mingyu Zhang, Wei Geng, Chengchi Zhou, Weinan Shi, Yuanchun Shi, Yuntao Wang</p>
                    <p><strong>Published:</strong> 8/1/2025</p>
                    <p><strong>Categories:</strong> cs.CE</p>
                    <p><strong>Summary:</strong> Smart rings have emerged as uniquely convenient devices for continuous physiological and behavioral sensing, offering unobtrusive, constant access to metrics such as heart rate, motion, and skin temperature. Yet most commercial solutions remain proprietary, hindering reproducibility and slowing innovation in wearable research. We introduce {\tau}-Ring, a commercial-ready platform that bridges this gap through: (i) accessible hardware combining time-synchronized multi-channel PPG, 6-axis IMU, temperature sensing, NFC, and on-board storage; (ii) adjustable firmware that lets researchers rapidly reconfigure sampling rates, power modes, and wireless protocols; and (iii) a fully open-source Android software suite that supports both real-time streaming and 8-hour offline logging. Together, these features enable out-of-the-box, reproducible acquisition of rich physiological and behavioral datasets, accelerating prototyping and standardizing experimentation. We validate the platform with demonstration studies in heart-rate monitoring and ring-based handwriting recognition. Source code is available at GitHub: https://github.com/thuhci/OpenRing.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.00771v1" target="_blank">Latin American network on electromagnetic effects in strongly interacting matter: Contribution to the update of the Latin American Strategy for High Energy, Cosmology and Astroparticle Physics</a></h3>
                    <p><strong>Authors:</strong> Ana Mizher, Alejandro Ayala</p>
                    <p><strong>Published:</strong> 8/1/2025</p>
                    <p><strong>Categories:</strong> hep-ph, hep-ex, hep-th</p>
                    <p><strong>Summary:</strong> An accurate characterization of the quark-gluon plasma requires understanding of how electromagnetic effects affect the processes mediated by the strong force. All the scenarios in which the plasma emerges, either in nature or in the laboratory, involve strong electromagnetic fields. The early universe, compact astrophysical objects, or ultra-relativistic heavy-ion collisions harbor the most intense fields we know. Researches from the Latin America region have made a substantial contribution on this subject and the \lq\lq Latin American Network on Electromagnetic Effects in Strongly Interacting Matter aims to cluster efforts to address open questions related to these systems, boosting collaborations and interaction among its members and connecting Latin American institutions with institutions from the rest of the world. In face of the upcoming experimental programs and new facilities, our mission is to bring together experimentalists, phenomenologists and theorists to better explore the properties of strongly interacting matter in the presence of intense electromagnetic fields. This document describes succinctly the recent contributions from researchers of the Latin American region to the subject, as well as our activities and perspectives for the future.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.00755v1" target="_blank">AI-Driven Collaborative Satellite Object Detection for Space Sustainability</a></h3>
                    <p><strong>Authors:</strong> Peng Hu, Wenxuan Zhang</p>
                    <p><strong>Published:</strong> 8/1/2025</p>
                    <p><strong>Categories:</strong> eess.IV, cs.CV</p>
                    <p><strong>Summary:</strong> The growing density of satellites in low-Earth orbit (LEO) presents serious challenges to space sustainability, primarily due to the increased risk of in-orbit collisions. Traditional ground-based tracking systems are constrained by latency and coverage limitations, underscoring the need for onboard, vision-based space object detection (SOD) capabilities. In this paper, we propose a novel satellite clustering framework that enables the collaborative execution of deep learning (DL)-based SOD tasks across multiple satellites. To support this approach, we construct a high-fidelity dataset simulating imaging scenarios for clustered satellite formations. A distance-aware viewpoint selection strategy is introduced to optimize detection performance, and recent DL models are used for evaluation. Experimental results show that the clustering-based method achieves competitive detection accuracy compared to single-satellite and existing approaches, while maintaining a low size, weight, and power (SWaP) footprint. These findings underscore the potential of distributed, AI-enabled in-orbit systems to enhance space situational awareness and contribute to long-term space sustainability.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.00749v1" target="_blank">Dynamic Symbolic Execution for Semantic Difference Analysis of Component and Connector Architectures</a></h3>
                    <p><strong>Authors:</strong> Johanna Grahl, Bernhard Rumpe, Max Stachon, Sebastian Stüber</p>
                    <p><strong>Published:</strong> 8/1/2025</p>
                    <p><strong>Categories:</strong> cs.SE, cs.FL, cs.SC, 68N30, D.2.4</p>
                    <p><strong>Summary:</strong> In the context of model-driven development, ensuring the correctness and consistency of evolving models is paramount. This paper investigates the application of Dynamic Symbolic Execution (DSE) for semantic difference analysis of component-and-connector architectures, specifically utilizing MontiArc models. We have enhanced the existing MontiArc-to-Java generator to gather both symbolic and concrete execution data at runtime, encompassing transition conditions, visited states, and internal variables of automata. This data facilitates the identification of significant execution traces that provide critical insights into system behavior. We evaluate various execution strategies based on the criteria of runtime efficiency, minimality, and completeness, establishing a framework for assessing the applicability of DSE in semantic difference analysis. Our findings indicate that while DSE shows promise for analyzing component and connector architectures, scalability remains a primary limitation, suggesting further research is needed to enhance its practical utility in larger systems.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.00748v1" target="_blank">Is It Really You? Exploring Biometric Verification Scenarios in Photorealistic Talking-Head Avatar Videos</a></h3>
                    <p><strong>Authors:</strong> Laura Pedrouzo-Rodriguez, Pedro Delgado-DeRobles, Luis F. Gomez, Ruben Tolosana, Ruben Vera-Rodriguez, Aythami Morales, Julian Fierrez</p>
                    <p><strong>Published:</strong> 8/1/2025</p>
                    <p><strong>Categories:</strong> cs.CV, cs.AI, cs.CR, cs.MM</p>
                    <p><strong>Summary:</strong> Photorealistic talking-head avatars are becoming increasingly common in virtual meetings, gaming, and social platforms. These avatars allow for more immersive communication, but they also introduce serious security risks. One emerging threat is impersonation: an attacker can steal a users avatar-preserving their appearance and voice-making it nearly impossible to detect its fraudulent usage by sight or sound alone. In this paper, we explore the challenge of biometric verification in such avatar-mediated scenarios. Our main question is whether an individuals facial motion patterns can serve as reliable behavioral biometrics to verify their identity when the avatars visual appearance is a facsimile of its owner. To answer this question, we introduce a new dataset of realistic avatar videos created using a state-of-the-art one-shot avatar generation model, GAGAvatar, with genuine and impostor avatar videos. We also propose a lightweight, explainable spatio-temporal Graph Convolutional Network architecture with temporal attention pooling, that uses only facial landmarks to model dynamic facial gestures. Experimental results demonstrate that facial motion cues enable meaningful identity verification with AUC values approaching 80%. The proposed benchmark and biometric system are available for the research community in order to bring attention to the urgent need for more advanced behavioral biometric defenses in avatar-based communication systems.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.00744v1" target="_blank">Rethinking Backbone Design for Lightweight 3D Object Detection in LiDAR</a></h3>
                    <p><strong>Authors:</strong> Adwait Chandorkar, Hasan Tercan, Tobias Meisen</p>
                    <p><strong>Published:</strong> 8/1/2025</p>
                    <p><strong>Categories:</strong> cs.CV</p>
                    <p><strong>Summary:</strong> Recent advancements in LiDAR-based 3D object detection have significantly accelerated progress toward the realization of fully autonomous driving in real-world environments. Despite achieving high detection performance, most of the approaches still rely on a VGG-based or ResNet-based backbone for feature exploration, which increases the model complexity. Lightweight backbone design is well-explored for 2D object detection, but research on 3D object detection still remains limited. In this work, we introduce Dense Backbone, a lightweight backbone that combines the benefits of high processing speed, lightweight architecture, and robust detection accuracy. We adapt multiple SoTA 3d object detectors, such as PillarNet, with our backbone and show that with our backbone, these models retain most of their detection capability at a significantly reduced computational cost. To our knowledge, this is the first dense-layer-based backbone tailored specifically for 3D object detection from point cloud data. DensePillarNet, our adaptation of PillarNet, achieves a 29% reduction in model parameters and a 28% reduction in latency with just a 2% drop in detection accuracy on the nuScenes test set. Furthermore, Dense Backbones plug-and-play design allows straightforward integration into existing architectures, requiring no modifications to other network components.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.00743v1" target="_blank">Agentic large language models improve retrieval-based radiology question answering</a></h3>
                    <p><strong>Authors:</strong> Sebastian Wind, Jeta Sopa, Daniel Truhn, Mahshad Lotfinia, Tri-Thien Nguyen, Keno Bressem, Lisa Adams, Mirabela Rusu, Harald Köstler, Gerhard Wellein, Andreas Maier, Soroosh Tayebi Arasteh</p>
                    <p><strong>Published:</strong> 8/1/2025</p>
                    <p><strong>Categories:</strong> cs.CL, cs.AI, cs.LG</p>
                    <p><strong>Summary:</strong> Clinical decision-making in radiology increasingly benefits from artificial intelligence (AI), particularly through large language models (LLMs). However, traditional retrieval-augmented generation (RAG) systems for radiology question answering (QA) typically rely on single-step retrieval, limiting their ability to handle complex clinical reasoning tasks. Here we propose an agentic RAG framework enabling LLMs to autonomously decompose radiology questions, iteratively retrieve targeted clinical evidence from Radiopaedia, and dynamically synthesize evidence-based responses. We evaluated 24 LLMs spanning diverse architectures, parameter scales (0.5B to 670B), and training paradigms (general-purpose, reasoning-optimized, clinically fine-tuned), using 104 expert-curated radiology questions from previously established RSNA-RadioQA and ExtendedQA datasets. Agentic retrieval significantly improved mean diagnostic accuracy over zero-shot prompting (73% vs. 64%; P200B parameters) demonstrated minimal changes (2% improvement). Additionally, agentic retrieval reduced hallucinations (mean 9.4%) and retrieved clinically relevant context in 46% of cases, substantially aiding factual grounding. Even clinically fine-tuned models exhibited meaningful improvements (e.g., MedGemma-27B improved from 71% to 81%), indicating complementary roles of retrieval and fine-tuning. These results highlight the potential of agentic frameworks to enhance factuality and diagnostic accuracy in radiology QA, particularly among mid-sized LLMs, warranting future studies to validate their clinical utility.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.00742v1" target="_blank">Applying Psychometrics to Large Language Model Simulated Populations: Recreating the HEXACO Personality Inventory Experiment with Generative Agents</a></h3>
                    <p><strong>Authors:</strong> Sarah Mercer, Daniel P. Martin, Phil Swatton</p>
                    <p><strong>Published:</strong> 8/1/2025</p>
                    <p><strong>Categories:</strong> cs.CL, cs.LG</p>
                    <p><strong>Summary:</strong> Generative agents powered by Large Language Models demonstrate human-like characteristics through sophisticated natural language interactions. Their ability to assume roles and personalities based on predefined character biographies has positioned them as cost-effective substitutes for human participants in social science research. This paper explores the validity of such persona-based agents in representing human populations; we recreate the HEXACO personality inventory experiment by surveying 310 GPT-4 powered agents, conducting factor analysis on their responses, and comparing these results to the original findings presented by Ashton, Lee,  Goldberg in 2004. Our results found 1) a coherent and reliable personality structure was recoverable from the agents responses demonstrating partial alignment to the HEXACO framework. 2) the derived personality dimensions were consistent and reliable within GPT-4, when coupled with a sufficiently curated population, and 3) cross-model analysis revealed variability in personality profiling, suggesting model-specific biases and limitations. We discuss the practical considerations and challenges encountered during the experiment. This study contributes to the ongoing discourse on the potential benefits and limitations of using generative agents in social science research and provides useful guidance on designing consistent and representative agent personas to maximise coverage and representation of human personality traits.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.00741v1" target="_blank">Out-of-Context Abduction: LLMs Make Inferences About Procedural Data Leveraging Declarative Facts in Earlier Training Data</a></h3>
                    <p><strong>Authors:</strong> Sohaib Imran, Rob Lamb, Peter M. Atkinson</p>
                    <p><strong>Published:</strong> 8/1/2025</p>
                    <p><strong>Categories:</strong> cs.CL, cs.AI</p>
                    <p><strong>Summary:</strong> Large language models (LLMs) are trained on large corpora, yet it is unclear whether they can reason about the information present within their training data. We design experiments to study out-of-context abduction in LLMs, the ability to infer the most plausible explanations for observations using relevant facts present in training data. We train treatment LLMs on names and behavior descriptions of fictitious chatbots, but not on examples of dialogue with the chatbots. We find that OpenAIs GPT 4o LLM can correctly infer at least one chatbots name after observing example responses characteristic of that chatbot. We also find that previously training GPT 4o on descriptions of a chatbots behavior allows it to display behaviors more characteristic of the chatbot when iteratively trained to display such behaviors. Our results have implications for situational awareness in LLMs and, therefore, for AI safety.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.00738v1" target="_blank">Tool-Assisted Conformance Checking to Reference Process Models</a></h3>
                    <p><strong>Authors:</strong> Bernhard Rumpe, Max Stachon, Sebastian Stüber, Valdes Voufo</p>
                    <p><strong>Published:</strong> 8/1/2025</p>
                    <p><strong>Categories:</strong> cs.SE, cs.FL, 68N30, D.2.4</p>
                    <p><strong>Summary:</strong> Reference models convey best practices and standards. The reference frameworks necessitate conformance checks to ensure adherence to established guidelines and principles, which is crucial for maintaining quality and consistency in various processes. This paper explores automated conformance checks for concrete process models against reference models using causal dependency analysis of tasks and events. Existing notions of conformance checking for process models focus on verifying process execution traces and lack the expressiveness and automation needed for semantic model comparison, leaving this question unresolved. We integrate our approach into a broader semantic framework for defining reference model conformance. We outline an algorithm for reference process model conformance checking, evaluate it through a case study, and discuss its strengths and limitations. Our research provides a tool-assisted solution enhancing accuracy and flexibility in process model conformance verification.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.00737v1" target="_blank">How LLMs are Shaping the Future of Virtual Reality</a></h3>
                    <p><strong>Authors:</strong> Süeda Özkaya, Santiago Berrezueta-Guzman, Stefan Wagner</p>
                    <p><strong>Published:</strong> 8/1/2025</p>
                    <p><strong>Categories:</strong> cs.HC, cs.AI</p>
                    <p><strong>Summary:</strong> The integration of Large Language Models (LLMs) into Virtual Reality (VR) games marks a paradigm shift in the design of immersive, adaptive, and intelligent digital experiences. This paper presents a comprehensive review of recent research at the intersection of LLMs and VR, examining how these models are transforming narrative generation, non-player character (NPC) interactions, accessibility, personalization, and game mastering. Drawing from an analysis of 62 peer reviewed studies published between 2018 and 2025, we identify key application domains ranging from emotionally intelligent NPCs and procedurally generated storytelling to AI-driven adaptive systems and inclusive gameplay interfaces. We also address the major challenges facing this convergence, including real-time performance constraints, memory limitations, ethical risks, and scalability barriers. Our findings highlight that while LLMs significantly enhance realism, creativity, and user engagement in VR environments, their effective deployment requires robust design strategies that integrate multimodal interaction, hybrid AI architectures, and ethical safeguards. The paper concludes by outlining future research directions in multimodal AI, affective computing, reinforcement learning, and open-source development, aiming to guide the responsible advancement of intelligent and inclusive VR systems.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.00723v1" target="_blank">Why Do Decision Makers (Not) Use AI? A Cross-Domain Analysis of Factors Impacting AI Adoption</a></h3>
                    <p><strong>Authors:</strong> Rebecca Yu, Valerie Chen, Ameet Talwalkar, Hoda Heidari</p>
                    <p><strong>Published:</strong> 8/1/2025</p>
                    <p><strong>Categories:</strong> cs.HC</p>
                    <p><strong>Summary:</strong> Growing excitement around deploying AI across various domains calls for a careful assessment of how human decision-makers interact with AI-powered systems. In particular, it is essential to understand when decision-makers voluntarily choose to consult AI tools, which we term decision-maker adoption. We interviewed experts across four domains -- medicine, law, journalism, and the public sector -- to explore current AI use cases and perceptions of adoption. From these interviews, we identify key factors that shape decision-maker adoption of AI tools: the decision-makers background, perceptions of the AI, consequences for the decision-maker, and perceived implications for other stakeholders. We translate these factors into an AI adoption sheet to analyze how decision-makers approach adoption choices through comparative, cross-domain case studies, highlighting how our factors help explain inter-domain differences in adoption. Our findings offer practical guidance for supporting the responsible and context-aware deployment of AI by better accounting for the decision-makers perspective.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.00718v1" target="_blank">Democratizing Tabular Data Access with an Open$\unicode{x2013}$Source Synthetic$\unicode{x2013}$Data SDK</a></h3>
                    <p><strong>Authors:</strong> Ivona Krchova, Mariana Vargas Vieyra, Mario Scriminaci, Andrey Sidorenko</p>
                    <p><strong>Published:</strong> 8/1/2025</p>
                    <p><strong>Categories:</strong> cs.LG</p>
                    <p><strong>Summary:</strong> Machine learning development critically depends on access to high-quality data. However, increasing restrictions due to privacy, proprietary interests, and ethical concerns have created significant barriers to data accessibility. Synthetic data offers a viable solution by enabling safe, broad data usage without compromising sensitive information. This paper presents the MOSTLY AI Synthetic Data Software Development Kit (SDK), an open-source toolkit designed specifically for synthesizing high-quality tabular data. The SDK integrates robust features such as differential privacy guarantees, fairness-aware data generation, and automated quality assurance into a flexible and accessible Python interface. Leveraging the TabularARGN autoregressive framework, the SDK supports diverse data types and complex multi-table and sequential datasets, delivering competitive performance with notable improvements in speed and usability. Currently deployed both as a cloud service and locally installable software, the SDK has seen rapid adoption, highlighting its practicality in addressing real-world data bottlenecks and promoting widespread data democratization.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.00717v1" target="_blank">Generative AI in Higher Education: Evidence from an Elite College</a></h3>
                    <p><strong>Authors:</strong> Zara Contractor, Germán Reyes</p>
                    <p><strong>Published:</strong> 8/1/2025</p>
                    <p><strong>Categories:</strong> econ.GN, q-fin.EC</p>
                    <p><strong>Summary:</strong> Generative AI is transforming higher education, yet systematic evidence on student adoption remains limited. Using novel survey data from a selective U.S. college, we document over 80 percent of students using AI academically within two years of ChatGPTs release. Adoption varies across disciplines, demographics, and achievement levels, highlighting AIs potential to reshape educational inequalities. Students predominantly use AI for augmenting learning (e.g., explanations, feedback), but also to automate tasks (e.g., essay generation). Positive perceptions of AIs educational benefits strongly predict adoption. Institutional policies can influence usage patterns but risk creating unintended disparate impacts across student groups due to uneven compliance.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.00713v1" target="_blank">Controllability of diffusive Lotka-Volterra strongly competitive systems under boundary constrained controls</a></h3>
                    <p><strong>Authors:</strong> Elisa Affili, Enrique Zuazua</p>
                    <p><strong>Published:</strong> 8/1/2025</p>
                    <p><strong>Categories:</strong> math.AP, math.OC, 35K57, 93B05, 35G60, 92D40</p>
                    <p><strong>Summary:</strong> We investigate the controllability of the competition-diffusion Lotka-Volterra system. Our primary focus is on the one-dimensional setting with Dirichlet boundary controls, interpreted as ecological management policies regulating the density of species at the habitat boundaries and satisfying bilateral constraints. We show that the system can be steered from any initial state to a constant steady state representing the extinction of the less competitive species. In contrast, we prove that controllability toward a steady state where the more competitive species vanishes is generally not achievable when the inter-species competition rates are too unbalanced. This obstruction is due to the existence of barrier solutions, which we explicitly construct based on the spectral properties of the associated reaction-diffusion operators. Our theoretical results are illustrated through numerical simulations and are accompanied by a discussion of open problems and potential directions for future research.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.00709v1" target="_blank">NyayaRAG: Realistic Legal Judgment Prediction with RAG under the Indian Common Law System</a></h3>
                    <p><strong>Authors:</strong> Shubham Kumar Nigam, Balaramamahanthi Deepak Patnaik, Shivam Mishra, Ajay Varghese Thomas, Noel Shallum, Kripabandhu Ghosh, Arnab Bhattacharya</p>
                    <p><strong>Published:</strong> 8/1/2025</p>
                    <p><strong>Categories:</strong> cs.CL, cs.AI, cs.IR, cs.LG</p>
                    <p><strong>Summary:</strong> Legal Judgment Prediction (LJP) has emerged as a key area in AI for law, aiming to automate judicial outcome forecasting and enhance interpretability in legal reasoning. While previous approaches in the Indian context have relied on internal case content such as facts, issues, and reasoning, they often overlook a core element of common law systems, which is reliance on statutory provisions and judicial precedents. In this work, we propose NyayaRAG, a Retrieval-Augmented Generation (RAG) framework that simulates realistic courtroom scenarios by providing models with factual case descriptions, relevant legal statutes, and semantically retrieved prior cases. NyayaRAG evaluates the effectiveness of these combined inputs in predicting court decisions and generating legal explanations using a domain-specific pipeline tailored to the Indian legal system. We assess performance across various input configurations using both standard lexical and semantic metrics as well as LLM-based evaluators such as G-Eval. Our results show that augmenting factual inputs with structured legal knowledge significantly improves both predictive accuracy and explanation quality.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.00704v1" target="_blank">XANES absorption spectra of penta-graphene and penta-SiC2 with different terminations: a computational study</a></h3>
                    <p><strong>Authors:</strong> Andrea Pedrielli, Tommaso Morresi, Simone Taioli</p>
                    <p><strong>Published:</strong> 8/1/2025</p>
                    <p><strong>Categories:</strong> cond-mat.mtrl-sci, cond-mat.mes-hall</p>
                    <p><strong>Summary:</strong> In recent research, penta-graphene and penta-SiC2 have emerged as innovative 2D materials consisting exclusively of pentagons. However, there is still a significant gap in the theoretical characterization of these materials, which hinders progress in their synthesis and potential technological applications. This study aims to close this gap by investigating the X-ray absorption near-edge spectroscopy (XANES) of these materials through ab initio calculations. In particular, we analyze the XANES spectra of penta-graphene in its pristine, hydrogenated, and hydroxylated states, and we investigate the effects of substitution by a single silicon in both penta-graphene and pentagraphane. In addition, we calculate the XANES spectra for pristine and hydrogenated penta-SiC2. This work sets the stage for the possible identification of penta-graphene and penta-SiC2 phases by X-ray spectroscopy at the experimental level and lays the foundation for the future engineering of the absorption properties of these materials in optical devices.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.00701v1" target="_blank">D3: Training-Free AI-Generated Video Detection Using Second-Order Features</a></h3>
                    <p><strong>Authors:</strong> Chende Zheng, Ruiqi suo, Chenhao Lin, Zhengyu Zhao, Le Yang, Shuai Liu, Minghui Yang, Cong Wang, Chao Shen</p>
                    <p><strong>Published:</strong> 8/1/2025</p>
                    <p><strong>Categories:</strong> cs.CV, cs.AI</p>
                    <p><strong>Summary:</strong> The evolution of video generation techniques, such as Sora, has made it increasingly easy to produce high-fidelity AI-generated videos, raising public concern over the dissemination of synthetic content. However, existing detection methodologies remain limited by their insufficient exploration of temporal artifacts in synthetic videos. To bridge this gap, we establish a theoretical framework through second-order dynamical analysis under Newtonian mechanics, subsequently extending the Second-order Central Difference features tailored for temporal artifact detection. Building on this theoretical foundation, we reveal a fundamental divergence in second-order feature distributions between real and AI-generated videos. Concretely, we propose Detection by Difference of Differences (D3), a novel training-free detection method that leverages the above second-order temporal discrepancies. We validate the superiority of our D3 on 4 open-source datasets (Gen-Video, VideoPhy, EvalCrafter, VidProM), 40 subsets in total. For example, on GenVideo, D3 outperforms the previous best method by 10.39% (absolute) mean Average Precision. Additional experiments on time cost and post-processing operations demonstrate D3s exceptional computational efficiency and strong robust performance. Our code is available at https://github.com/Zig-HS/D3.</p>
                
            
                
                    <h3><a href="http://dx.doi.org/10.7717/peerj-cs.3045" target="_blank">Classification of Psychiatry Clinical Notes by Diagnosis: A Deep Learning and Machine Learning Approach</a></h3>
                    <p><strong>Authors:</strong> Sergio Rubio-Martín, María Teresa García-Ordás, Antonio Serrano-García, Clara Margarita Franch-Pato, Arturo Crespo-Álvaro, José Alberto Benítez-Andrades</p>
                    <p><strong>Published:</strong> 8/1/2025</p>
                    <p><strong>Categories:</strong> cs.LG, cs.CL</p>
                    <p><strong>Summary:</strong> The classification of clinical notes into specific diagnostic categories is critical in healthcare, especially for mental health conditions like Anxiety and Adjustment Disorder. In this study, we compare the performance of various Artificial Intelligence models, including both traditional Machine Learning approaches (Random Forest, Support Vector Machine, K-nearest neighbors, Decision Tree, and eXtreme Gradient Boost) and Deep Learning models (DistilBERT and SciBERT), to classify clinical notes into these two diagnoses. Additionally, we implemented three oversampling strategies: No Oversampling, Random Oversampling, and Synthetic Minority Oversampling Technique (SMOTE), to assess their impact on model performance. Hyperparameter tuning was also applied to optimize model accuracy. Our results indicate that oversampling techniques had minimal impact on model performance overall. The only exception was SMOTE, which showed a positive effect specifically with BERT-based models. However, hyperparameter optimization significantly improved accuracy across the models, enhancing their ability to generalize and perform on the dataset. The Decision Tree and eXtreme Gradient Boost models achieved the highest accuracy among machine learning approaches, both reaching 96%, while the DistilBERT and SciBERT models also attained 96% accuracy in the deep learning category. These findings underscore the importance of hyperparameter tuning in maximizing model performance. This study contributes to the ongoing research on AI-assisted diagnostic tools in mental health by providing insights into the efficacy of different model architectures and data balancing methods.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.00679v1" target="_blank">Segment First, Retrieve Better: Realistic Legal Search via Rhetorical Role-Based Queries</a></h3>
                    <p><strong>Authors:</strong> Shubham Kumar Nigam, Tanmay Dubey, Noel Shallum, Arnab Bhattacharya</p>
                    <p><strong>Published:</strong> 8/1/2025</p>
                    <p><strong>Categories:</strong> cs.CL, cs.AI, cs.IR, cs.LG</p>
                    <p><strong>Summary:</strong> Legal precedent retrieval is a cornerstone of the common law system, governed by the principle of stare decisis, which demands consistency in judicial decisions. However, the growing complexity and volume of legal documents challenge traditional retrieval methods. TraceRetriever mirrors real-world legal search by operating with limited case information, extracting only rhetorically significant segments instead of requiring complete documents. Our pipeline integrates BM25, Vector Database, and Cross-Encoder models, combining initial results through Reciprocal Rank Fusion before final re-ranking. Rhetorical annotations are generated using a Hierarchical BiLSTM CRF classifier trained on Indian judgments. Evaluated on IL-PCR and COLIEE 2025 datasets, TraceRetriever addresses growing document volume challenges while aligning with practical search constraints, reliable and scalable foundation for precedent retrieval enhancing legal research when only partial case knowledge is available.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.00674v1" target="_blank">Context-Aware Visualization for Explainable AI Recommendations in Social Media: A Vision for User-Aligned Explanations</a></h3>
                    <p><strong>Authors:</strong> Banan Alkhateeb, Ellis Solaiman</p>
                    <p><strong>Published:</strong> 8/1/2025</p>
                    <p><strong>Categories:</strong> cs.AI, cs.HC, cs.LG</p>
                    <p><strong>Summary:</strong> Social media platforms today strive to improve user experience through AI recommendations, yet the value of such recommendations vanishes as users do not understand the reasons behind them. This issue arises because explainability in social media is general and lacks alignment with user-specific needs. In this vision paper, we outline a user-segmented and context-aware explanation layer by proposing a visual explanation system with diverse explanation methods. The proposed system is framed by the variety of user needs and contexts, showing explanations in different visualized forms, including a technically detailed version for AI experts and a simplified one for lay users. Our framework is the first to jointly adapt explanation style (visual vs. numeric) and granularity (expert vs. lay) inside a single pipeline. A public pilot with 30 X users will validate its impact on decision-making and trust.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.00669v1" target="_blank">Medical Reasoning in the Era of LLMs: A Systematic Review of Enhancement Techniques and Applications</a></h3>
                    <p><strong>Authors:</strong> Wenxuan Wang, Zizhan Ma, Meidan Ding, Shiyi Zheng, Shengyuan Liu, Jie Liu, Jiaming Ji, Wenting Chen, Xiang Li, Linlin Shen, Yixuan Yuan</p>
                    <p><strong>Published:</strong> 8/1/2025</p>
                    <p><strong>Categories:</strong> cs.CL, cs.AI, cs.CV, cs.LG</p>
                    <p><strong>Summary:</strong> The proliferation of Large Language Models (LLMs) in medicine has enabled impressive capabilities, yet a critical gap remains in their ability to perform systematic, transparent, and verifiable reasoning, a cornerstone of clinical practice. This has catalyzed a shift from single-step answer generation to the development of LLMs explicitly designed for medical reasoning. This paper provides the first systematic review of this emerging field. We propose a taxonomy of reasoning enhancement techniques, categorized into training-time strategies (e.g., supervised fine-tuning, reinforcement learning) and test-time mechanisms (e.g., prompt engineering, multi-agent systems). We analyze how these techniques are applied across different data modalities (text, image, code) and in key clinical applications such as diagnosis, education, and treatment planning. Furthermore, we survey the evolution of evaluation benchmarks from simple accuracy metrics to sophisticated assessments of reasoning quality and visual interpretability. Based on an analysis of 60 seminal studies from 2022-2025, we conclude by identifying critical challenges, including the faithfulness-plausibility gap and the need for native multimodal reasoning, and outlining future directions toward building efficient, robust, and sociotechnically responsible medical AI.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.00668v1" target="_blank">Advancing Quantum Information Science Pre-College Education: The Case for Learning Sciences Collaboration</a></h3>
                    <p><strong>Authors:</strong> Raquel Coelho, Roy Pea, Christian Schunn, Jinglei Cheng, Junyu Liu</p>
                    <p><strong>Published:</strong> 8/1/2025</p>
                    <p><strong>Categories:</strong> physics.ed-ph, cs.AI, cs.CY, quant-ph</p>
                    <p><strong>Summary:</strong> As quantum information science advances and the need for pre-college engagement grows, a critical question remains: How can young learners be prepared to participate in a field so radically different from what they have encountered before? This paper argues that meeting this challenge will require strong interdisciplinary collaboration with the Learning Sciences (LS), a field dedicated to understanding how people learn and designing theory-guided environments to support learning. Drawing on lessons from previous STEM education efforts, we discuss two key contributions of the learning sciences to quantum information science (QIS) education. The first is design-based research, the signature methodology of learning sciences, which can inform the development, refinement, and scaling of effective QIS learning experiences. The second is a framework for reshaping how learners reason about, learn and participate in QIS practices through shifts in knowledge representations that provide new forms of engagement and associated learning. We call for a two-way partnership between quantum information science and the learning sciences, one that not only supports learning in quantum concepts and practices but also improves our understanding of how to teach and support learning in highly complex domains. We also consider potential questions involved in bridging these disciplinary communities and argue that the theoretical and practical benefits justify the effort.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.00665v1" target="_blank">Transparent Adaptive Learning via Data-Centric Multimodal Explainable AI</a></h3>
                    <p><strong>Authors:</strong> Maryam Mosleh, Marie Devlin, Ellis Solaiman</p>
                    <p><strong>Published:</strong> 8/1/2025</p>
                    <p><strong>Categories:</strong> cs.AI, cs.HC, cs.LG</p>
                    <p><strong>Summary:</strong> Artificial intelligence-driven adaptive learning systems are reshaping education through data-driven adaptation of learning experiences. Yet many of these systems lack transparency, offering limited insight into how decisions are made. Most explainable AI (XAI) techniques focus on technical outputs but neglect user roles and comprehension. This paper proposes a hybrid framework that integrates traditional XAI techniques with generative AI models and user personalisation to generate multimodal, personalised explanations tailored to user needs. We redefine explainability as a dynamic communication process tailored to user roles and learning goals. We outline the frameworks design, key XAI limitations in education, and research directions on accuracy, fairness, and personalisation. Our aim is to move towards explainable AI that enhances transparency while supporting user-centred experiences.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.00654v1" target="_blank">LEO: An Open-Source Platform for Linking OMERO with Lab Notebooks and Heterogeneous Metadata Sources</a></h3>
                    <p><strong>Authors:</strong> Rodrigo Escobar Díaz Guerrero, Jamile Mohammad Jafari, Tobias Meyer-Zedler, Michael Schmitt, Juergen Popp, Thomas Bocklitz</p>
                    <p><strong>Published:</strong> 8/1/2025</p>
                    <p><strong>Categories:</strong> cs.CE, cs.SE</p>
                    <p><strong>Summary:</strong> In the interdisciplinary field of microscopy research, managing and integrating large volumes of data stored across disparate platforms remains a major challenge. Data types such as bioimages, experimental records, and spectral information are often maintained in separate repositories, each following different management standards. However, linking these data sources across the research lifecycle is essential to align with the FAIR principles of data management: Findability, Accessibility, Interoperability, and Reusability. Despite this need, there is a notable lack of tools capable of effectively integrating and linking data from heterogeneous sources. To address this gap, we present LEO (Linking Electronic Lab Notebooks with OMERO), a web-based platform designed to create and manage links between distributed data systems. LEO was initially developed to link objects between Electronic Lab Notebooks (ELNs) and OMERO, but its functionality has since been extended through a plugin-based architecture, allowing the integration of additional data sources. This extensibility makes LEO a scalable and flexible solution for a wide range of microscopy research workflows.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.00652v1" target="_blank">The Manipulative Power of Voice Characteristics: Investigating Deceptive Patterns in Mandarin Chinese Female Synthetic Speech</a></h3>
                    <p><strong>Authors:</strong> Shuning Zhang, Han Chen, Yabo Wang, Yiqun Xu, Jiaqi Bai, Yuanyuan Wu, Shixuan Li, Xin Yi, Chunhui Wang, Hewu Li</p>
                    <p><strong>Published:</strong> 8/1/2025</p>
                    <p><strong>Categories:</strong> cs.HC</p>
                    <p><strong>Summary:</strong> Pervasive voice interaction enables deceptive patterns through subtle voice characteristics, yet empirical investigation into this manipulation lags behind, especially within major non-English language contexts. Addressing this gap, our study presents the first systematic investigation into voice characteristic-based dark patterns employing female synthetic voices in Mandarin Chinese. This focus is crucial given the prevalence of female personas in commercial assistants and the prosodic significance in the Chinese language. Guided by the conceptual framework identifying key influencing factors, we systematically evaluate effectiveness variations by manipulating voice characteristics (five characteristics, three intensities) across different scenarios (shopping vs. question-answering) with different commercial aims. A preliminary study (N=24) validated the experimental materials and the main study (N=36) revealed significant behavioral manipulation (up to +2027.6%). Crucially, the analysis showed that effectiveness varied significantly with voice characteristics and scenario, mediated by user perception (of tone, intonation, timbre) and user demographics (individual preferences, though limited demographic impact). These interconnected findings offer evidence-based insights for ethical design.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.00650v1" target="_blank">Evac-Cast: An Interpretable Machine-Learning Framework for Evacuation Forecasts Across Hurricanes and Wildfires</a></h3>
                    <p><strong>Authors:</strong> Bo Li, Chenyue Liu, Ali Mostafavi</p>
                    <p><strong>Published:</strong> 8/1/2025</p>
                    <p><strong>Categories:</strong> physics.soc-ph</p>
                    <p><strong>Summary:</strong> Evacuation is critical for disaster safety, yet agencies lack timely, accurate, and transparent tools for evacuation prediction. This study introduces Evac-Cast, an interpretable machine learning framework that predicts tract-level evacuation rates using over 20 features derived from four dimensions: hazard intensity, community vulnerability, evacuation readiness, and built environment. Using an XGBoost model trained on multi-source, large-scale datasets for two hurricanes (Ian 2022, Milton 2024) and two wildfires (Kincade 2019, Palisades--Eaton 2025), Evac-Cast achieves mean absolute errors of 4.5% and 3.5% for hurricane and wildfire events, respectively. SHAP analysis reveals a consistent feature importance hierarchy across hazards, led by hazard intensity. Notably, the models perform well without explicit psychosocial variables, suggesting that macro-level proxies effectively encode behavioral signals traditionally captured through time-consuming surveys. This work offers a survey-free, high-resolution approach for predicting and understanding evacuation in hazard events, which could serve as a data-driven tool to support decision-making in emergency management.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.00645v1" target="_blank">SmartFlow: A CFD-solver-agnostic deep reinforcement learning framework for computational fluid dynamics on HPC platforms</a></h3>
                    <p><strong>Authors:</strong> Maochao Xiao, Yuning Wang, Felix Rodach, Bernat Font, Marius Kurz, Pol Suárez, Di Zhou, Francisco Alcántara-Ávila, Ting Zhu, Junle Liu, Ricard Montalà, Jiawei Chen, Jean Rabault, Oriol Lehmkuhl, Andrea Beck, Johan Larsson, Ricardo Vinuesa, Sergio Pirozzoli</p>
                    <p><strong>Published:</strong> 8/1/2025</p>
                    <p><strong>Categories:</strong> physics.flu-dyn, physics.comp-ph</p>
                    <p><strong>Summary:</strong> Deep reinforcement learning (DRL) is emerging as a powerful tool for fluid-dynamics research, encompassing active flow control, autonomous navigation, turbulence modeling and discovery of novel numerical schemes. We introduce SmartFlow, a CFD-solver-agnostic framework for both single- and multi-agent DRL algorithms that can easily integrate with MPI-parallel CPU and GPU-accelerated solvers. Built on Relexi and SmartSOD2D, SmartFlow uses the SmartSim infrastructure library and our newly developed SmartRedis-MPI library to enable asynchronous, low-latency, in-memory communication between CFD solvers and Python-based DRL algorithms. SmartFlow leverages PyTorchs Stable-Baselines3 for training, which provides a modular, Gym-like environment API. We demonstrate its versatility via three case studies: single-agent synthetic-jet control for drag reduction in a cylinder flow simulated by the high-order FLEXI solver, multi-agent cylinder wake control using the GPU-accelerated spectral-element code SOD2D, and multi-agent wall-model learning for large-eddy simulation with the finite-difference solver CaLES. SmartFlows CFD-solver-agnostic design and seamless HPC integration is promising to accelerate RL-driven fluid-mechanics studies.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.00639v1" target="_blank">Minimum Data, Maximum Impact: 20 annotated samples for explainable lung nodule classification</a></h3>
                    <p><strong>Authors:</strong> Luisa Gallée, Catharina Silvia Lisson, Christoph Gerhard Lisson, Daniela Drees, Felix Weig, Daniel Vogele, Meinrad Beer, Michael Götz</p>
                    <p><strong>Published:</strong> 8/1/2025</p>
                    <p><strong>Categories:</strong> cs.CV</p>
                    <p><strong>Summary:</strong> Classification models that provide human-interpretable explanations enhance clinicians trust and usability in medical image diagnosis. One research focus is the integration and prediction of pathology-related visual attributes used by radiologists alongside the diagnosis, aligning AI decision-making with clinical reasoning. Radiologists use attributes like shape and texture as established diagnostic criteria and mirroring these in AI decision-making both enhances transparency and enables explicit validation of model outputs. However, the adoption of such models is limited by the scarcity of large-scale medical image datasets annotated with these attributes. To address this challenge, we propose synthesizing attribute-annotated data using a generative model. We enhance the Diffusion Model with attribute conditioning and train it using only 20 attribute-labeled lung nodule samples from the LIDC-IDRI dataset. Incorporating its generated images into the training of an explainable model boosts performance, increasing attribute prediction accuracy by 13.4% and target prediction accuracy by 1.8% compared to training with only the small real attribute-annotated dataset. This work highlights the potential of synthetic data to overcome dataset limitations, enhancing the applicability of explainable models in medical image analysis.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.00637v1" target="_blank">Cyber-Physical Co-Simulation of Load Frequency Control under Load-Altering Attacks</a></h3>
                    <p><strong>Authors:</strong> Michał Forystek, Andrew D. Syrmakesis, Alkistis Kontou, Panos Kotsampopoulos, Nikos D. Hatziargyriou, Charalambos Konstantinou</p>
                    <p><strong>Published:</strong> 8/1/2025</p>
                    <p><strong>Categories:</strong> eess.SY, cs.CR, cs.SY</p>
                    <p><strong>Summary:</strong> Integrating Information and Communications Technology (ICT) devices into the power grid brings many benefits. However, it also exposes the grid to new potential cyber threats. Many control and protection mechanisms, such as Load Frequency Control (LFC), responsible for maintaining nominal frequency during load fluctuations and Under Frequency Load Shedding (UFLS) disconnecting portion of the load during an emergency, are dependent on information exchange through the communication network. The recently emerging Load Altering Attacks (LAAs) utilize a botnet of high-wattage devices to introduce load fluctuation. In their dynamic form (DLAAs), they manipulate the load in response to live grid frequency measurements for increased efficiency, posing a notable threat to grid stability. Recognizing the importance of communication networks in power grid cyber security research, this paper presents an open-source co-simulation environment that models the power grid with the corresponding communication network, implementing grid protective mechanisms. This setup allows the comprehensive analysis of the attacks in concrete LFC and UFLS scenarios.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.00632v1" target="_blank">Multi-Agent Game Generation and Evaluation via Audio-Visual Recordings</a></h3>
                    <p><strong>Authors:</strong> Alexia Jolicoeur-Martineau</p>
                    <p><strong>Published:</strong> 8/1/2025</p>
                    <p><strong>Categories:</strong> cs.AI, cs.MA, cs.MM</p>
                    <p><strong>Summary:</strong> While AI excels at generating text, audio, images, and videos, creating interactive audio-visual content such as video games remains challenging. Current LLMs can generate JavaScript games and animations, but lack automated evaluation metrics and struggle with complex content that normally requires teams of humans working for many months (multi-shot, multi-agents) using assets made by artists. To tackle these issues, we built a new metric and a multi-agent system. We propose AVR-Eval, a relative metric for multimedia content quality using Audio-Visual Recordings (AVRs). An omni-modal model (processing text, video, and audio) compares the AVRs of two contents, with a text model reviewing evaluations to determine superiority. We show that AVR-Eval properly identifies good from broken or mismatched content. We built AVR-Agent, a multi-agent system generating JavaScript code from a bank of multimedia assets (audio, images, 3D models). The coding agent selects relevant assets, generates multiple initial codes, uses AVR-Eval to identify the best version, and iteratively improves it through omni-modal agent feedback from the AVR. We run experiments on games and animations with AVR-Eval (win rate of content A against B). We find that content generated by AVR-Agent has a significantly higher win rate against content made through one-shot generation. However, models struggle to leverage custom assets and AVR feedback effectively, showing no higher win rate. This reveals a critical gap: while humans benefit from high-quality assets and audio-visual feedback, current coding models do not seem to utilize these resources as effectively, highlighting fundamental differences between human and machine content creation approaches.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.00630v1" target="_blank">MCeT: Behavioral Model Correctness Evaluation using Large Language Models</a></h3>
                    <p><strong>Authors:</strong> Khaled Ahmed, Jialing Song, Boqi Chen, Ou Wei, Bingzhou Zheng</p>
                    <p><strong>Published:</strong> 8/1/2025</p>
                    <p><strong>Categories:</strong> cs.SE</p>
                    <p><strong>Summary:</strong> Behavioral model diagrams, e.g., sequence diagrams, are an essential form of documentation that are typically designed by system engineers from requirements documentation, either fully manually or assisted by design tools. With the growing use of Large Language Models (LLM) as AI modeling assistants, more automation will be involved in generating diagrams. This necessitates the advancement of automatic model correctness evaluation tools. Such a tool can be used to evaluate both manually and AI automatically generated models; to provide feedback to system engineers, and enable AI assistants to self-evaluate and self-enhance their generated models. In this paper, we propose MCeT, the first fully automated tool to evaluate the correctness of a behavioral model, sequence diagrams in particular, against its corresponding requirements text and produce a list of issues that the model has. We utilize LLMs for the correctness evaluation tasks as they have shown outstanding natural language understanding ability. However, we show that directly asking an LLM to compare a diagram to requirements finds less than 35% of issues that experienced engineers can find. We propose to supplement the direct check with a fine-grained, multi-perspective approach; we split the diagram into atomic, non-divisible interactions, and split the requirements text into atomic, self-contained items. We compare the diagram with atomic requirements and each diagram-atom with the requirements. We also propose a self-consistency checking approach that combines perspectives to mitigate LLM hallucinated issues. Our combined approach improves upon the precision of the direct approach from 0.58 to 0.81 in a dataset of real requirements. Moreover, the approach finds 90% more issues that the experienced engineers found than the direct approach, and reports an average of 6 new issues per diagram.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.00627v1" target="_blank">IAMAP: Unlocking Deep Learning in QGIS for non-coders and limited computing resources</a></h3>
                    <p><strong>Authors:</strong> Paul Tresson, Pierre Le Coz, Hadrien Tulet, Anthony Malkassian, Maxime Réjou Méchain</p>
                    <p><strong>Published:</strong> 8/1/2025</p>
                    <p><strong>Categories:</strong> cs.LG, I.4.9; I.4.6</p>
                    <p><strong>Summary:</strong> Remote sensing has entered a new era with the rapid development of artificial intelligence approaches. However, the implementation of deep learning has largely remained restricted to specialists and has been impractical because it often requires (i) large reference datasets for model training and validation; (ii) substantial computing resources; and (iii) strong coding skills. Here, we introduce IAMAP, a user-friendly QGIS plugin that addresses these three challenges in an easy yet flexible way. IAMAP builds on recent advancements in self-supervised learning strategies, which now provide robust feature extractors, often referred to as foundation models. These generalist models can often be reliably used in few-shot or zero-shot scenarios (i.e., with little to no fine-tuning). IAMAPs interface allows users to streamline several key steps in remote sensing image analysis: (i) extracting image features using a wide range of deep learning architectures; (ii) reducing dimensionality with built-in algorithms; (iii) performing clustering on features or their reduced representations; (iv) generating feature similarity maps; and (v) calibrating and validating supervised machine learning models for prediction. By enabling non-AI specialists to leverage the high-quality features provided by recent deep learning approaches without requiring GPU capacity or extensive reference datasets, IAMAP contributes to the democratization of computationally efficient and energy-conscious deep learning methods.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.00625v1" target="_blank">OpenScout v1.1 mobile robot: a case study on open hardware continuation</a></h3>
                    <p><strong>Authors:</strong> Bartosz Krawczyk, Ahmed Elbary, Robbie Cato, Jagdish Patil, Kaung Myat, Anyeh Ndi-Tah, Nivetha Sakthivel, Mark Crampton, Gautham Das, Charles Fox</p>
                    <p><strong>Published:</strong> 8/1/2025</p>
                    <p><strong>Categories:</strong> cs.RO</p>
                    <p><strong>Summary:</strong> OpenScout is an Open Source Hardware (OSH) mobile robot for research and industry. It is extended to v1.1 which includes simplified, cheaper and more powerful onboard compute hardware; a simulated ROS2 interface; and a Gazebo simulation. Changes, their rationale, project methodology, and results are reported as an OSH case study.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.00619v1" target="_blank">DACTYL: Diverse Adversarial Corpus of Texts Yielded from Large Language Models</a></h3>
                    <p><strong>Authors:</strong> Shantanu Thorat, Andrew Caines</p>
                    <p><strong>Published:</strong> 8/1/2025</p>
                    <p><strong>Categories:</strong> cs.CL, cs.LG</p>
                    <p><strong>Summary:</strong> Existing AIG (AI-generated) text detectors struggle in real-world settings despite succeeding in internal testing, suggesting that they may not be robust enough. We rigorously examine the machine-learning procedure to build these detectors to address this. Most current AIG text detection datasets focus on zero-shot generations, but little work has been done on few-shot or one-shot generations, where LLMs are given human texts as an example. In response, we introduce the Diverse Adversarial Corpus of Texts Yielded from Language models (DACTYL), a challenging AIG text detection dataset focusing on one-shot/few-shot generations. We also include texts from domain-specific continued-pre-trained (CPT) language models, where we fully train all parameters using a memory-efficient optimization approach. Many existing AIG text detectors struggle significantly on our dataset, indicating a potential vulnerability to one-shot/few-shot and CPT-generated texts. We also train our own classifiers using two approaches: standard binary cross-entropy (BCE) optimization and a more recent approach, deep X-risk optimization (DXO). While BCE-trained classifiers marginally outperform DXO classifiers on the DACTYL test set, the latter excels on out-of-distribution (OOD) texts. In our mock deployment scenario in student essay detection with an OOD student essay dataset, the best DXO classifier outscored the best BCE-trained classifier by 50.56 macro-F1 score points at the lowest false positive rates for both. Our results indicate that DXO classifiers generalize better without overfitting to the test set. Our experiments highlight several areas of improvement for AIG text detectors.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.00614v1" target="_blank">Prompting Science Report 3: Ill pay you or Ill kill you -- but will you care?</a></h3>
                    <p><strong>Authors:</strong> Lennart Meincke, Ethan Mollick, Lilach Mollick, Dan Shapiro</p>
                    <p><strong>Published:</strong> 8/1/2025</p>
                    <p><strong>Categories:</strong> cs.CL, cs.AI</p>
                    <p><strong>Summary:</strong> This is the third in a series of short reports that seek to help business, education, and policy leaders understand the technical details of working with AI through rigorous testing. In this report, we investigate two commonly held prompting beliefs: a) offering to tip the AI model and b) threatening the AI model. Tipping was a commonly shared tactic for improving AI performance and threats have been endorsed by Google Founder Sergey Brin (All-In, May 2025, 8:20) who observed that models tend to do better if you threaten them, a claim we subject to empirical testing here. We evaluate model performance on GPQA (Rein et al. 2024) and MMLU-Pro (Wang et al. 2024). We demonstrate two things: - Threatening or tipping a model generally has no significant effect on benchmark performance. - Prompt variations can significantly affect performance on a per-question level. However, it is hard to know in advance whether a particular prompting approach will help or harm the LLMs ability to answer any particular question. Taken together, this suggests that simple prompting variations might not be as effective as previously assumed, especially for difficult problems. However, as reported previously (Meincke et al. 2025a), prompting approaches can yield significantly different results for individual questions.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.00605v1" target="_blank">GHTM: A Graph based Hybrid Topic Modeling Approach in Low-Resource Bengali Language</a></h3>
                    <p><strong>Authors:</strong> Farhana Haque, Md. Abdur Rahman, Sumon Ahmed</p>
                    <p><strong>Published:</strong> 8/1/2025</p>
                    <p><strong>Categories:</strong> cs.CL</p>
                    <p><strong>Summary:</strong> Topic modeling is a Natural Language Processing (NLP) technique that is used to identify latent themes and extract topics from text corpora by grouping similar documents based on their most significant keywords. Although widely researched in English, topic modeling remains understudied in Bengali due to its morphological complexity, lack of adequate resources and initiatives. In this contribution, a novel Graph Convolutional Network (GCN) based model called GHTM (Graph-Based Hybrid Topic Model) is proposed. This model represents input vectors of documents as nodes in the graph, which GCN uses to produce semantically rich embeddings. The embeddings are then decomposed using Non-negative Matrix Factorization (NMF) to get the topical representations of the underlying themes of the text corpus. This study compares the proposed model against a wide range of Bengali topic modeling techniques, from traditional methods such as LDA, LSA, and NMF to contemporary frameworks such as BERTopic and Top2Vec on three Bengali datasets. The experimental results demonstrate the effectiveness of the proposed model by outperforming other models in topic coherence and diversity. In addition, we introduce a novel Bengali dataset called NCTBText sourced from Bengali textbook materials to enrich and diversify the predominantly newspaper-centric Bengali corpora.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.00604v1" target="_blank">Composable OS Kernel Architectures for Autonomous Intelligence</a></h3>
                    <p><strong>Authors:</strong> Rajpreet Singh, Vidhi Kothari</p>
                    <p><strong>Published:</strong> 8/1/2025</p>
                    <p><strong>Categories:</strong> cs.OS, cs.AI</p>
                    <p><strong>Summary:</strong> As intelligent systems permeate edge devices, cloud infrastructure, and embedded real-time environments, this research proposes a new OS kernel architecture for intelligent systems, transforming kernels from static resource managers to adaptive, AI-integrated platforms. Key contributions include: (1) treating Loadable Kernel Modules (LKMs) as AI-oriented computation units for fast sensory and cognitive processing in kernel space; (2) expanding the Linux kernel into an AI-native environment with built-in deep learning inference, floating-point acceleration, and real-time adaptive scheduling for efficient ML workloads; and (3) introducing a Neurosymbolic kernel design leveraging Category Theory and Homotopy Type Theory to unify symbolic reasoning and differentiable logic within OS internals. Together, these approaches enable operating systems to proactively anticipate and adapt to the cognitive needs of autonomous intelligent applications.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.00600v1" target="_blank">A Context-Aware Dual-Metric Framework for Confidence Estimation in Large Language Models</a></h3>
                    <p><strong>Authors:</strong> Mingruo Yuan, Shuyi Zhang, Ben Kao</p>
                    <p><strong>Published:</strong> 8/1/2025</p>
                    <p><strong>Categories:</strong> cs.CL, cs.LG</p>
                    <p><strong>Summary:</strong> Accurate confidence estimation is essential for trustworthy large language models (LLMs) systems, as it empowers the user to determine when to trust outputs and enables reliable deployment in safety-critical applications. Current confidence estimation methods for LLMs neglect the relevance between responses and contextual information, a crucial factor in output quality evaluation, particularly in scenarios where background knowledge is provided. To bridge this gap, we propose CRUX (Context-aware entropy Reduction and Unified consistency eXamination), the first framework that integrates context faithfulness and consistency for confidence estimation via two novel metrics. First, contextual entropy reduction represents data uncertainty with the information gain through contrastive sampling with and without context. Second, unified consistency examination captures potential model uncertainty through the global consistency of the generated answers with and without context. Experiments across three benchmark datasets (CoQA, SQuAD, QuAC) and two domain-specific datasets (BioASQ, EduQG) demonstrate CRUXs effectiveness, achieving the highest AUROC than existing baselines.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.00591v1" target="_blank">Wukong Framework for Not Safe For Work Detection in Text-to-Image systems</a></h3>
                    <p><strong>Authors:</strong> Mingrui Liu, Sixiao Zhang, Cheng Long</p>
                    <p><strong>Published:</strong> 8/1/2025</p>
                    <p><strong>Categories:</strong> cs.CV, cs.AI, cs.CR</p>
                    <p><strong>Summary:</strong> Text-to-Image (T2I) generation is a popular AI-generated content (AIGC) technology enabling diverse and creative image synthesis. However, some outputs may contain Not Safe For Work (NSFW) content (e.g., violence), violating community guidelines. Detecting NSFW content efficiently and accurately, known as external safeguarding, is essential. Existing external safeguards fall into two types: text filters, which analyze user prompts but overlook T2I model-specific variations and are prone to adversarial attacks; and image filters, which analyze final generated images but are computationally costly and introduce latency. Diffusion models, the foundation of modern T2I systems like Stable Diffusion, generate images through iterative denoising using a U-Net architecture with ResNet and Transformer blocks. We observe that: (1) early denoising steps define the semantic layout of the image, and (2) cross-attention layers in U-Net are crucial for aligning text and image regions. Based on these insights, we propose Wukong, a transformer-based NSFW detection framework that leverages intermediate outputs from early denoising steps and reuses U-Nets pre-trained cross-attention parameters. Wukong operates within the diffusion process, enabling early detection without waiting for full image generation. We also introduce a new dataset containing prompts, seeds, and image-specific NSFW labels, and evaluate Wukong on this and two public benchmarks. Results show that Wukong significantly outperforms text-based safeguards and achieves comparable accuracy of image filters, while offering much greater efficiency.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.00590v1" target="_blank">A Novel Modeling Framework and Data Product for Extended VIIRS-like Artificial Nighttime Light Image Reconstruction (1986-2024)</a></h3>
                    <p><strong>Authors:</strong> Yihe Tian, Kwan Man Cheng, Zhengbo Zhang, Tao Zhang, Suju Li, Dongmei Yan, Bing Xu</p>
                    <p><strong>Published:</strong> 8/1/2025</p>
                    <p><strong>Categories:</strong> cs.CV, eess.IV</p>
                    <p><strong>Summary:</strong> Artificial Night-Time Light (NTL) remote sensing is a vital proxy for quantifying the intensity and spatial distribution of human activities. Although the NPP-VIIRS sensor provides high-quality NTL observations, its temporal coverage, which begins in 2012, restricts long-term time-series studies that extend to earlier periods. Despite the progress in extending VIIRS-like NTL time-series, current methods still suffer from two significant shortcomings: the underestimation of light intensity and the structural omission. To overcome these limitations, we propose a novel reconstruction framework consisting of a two-stage process: construction and refinement. The construction stage features a Hierarchical Fusion Decoder (HFD) designed to enhance the fidelity of the initial reconstruction. The refinement stage employs a Dual Feature Refiner (DFR), which leverages high-resolution impervious surface masks to guide and enhance fine-grained structural details. Based on this framework, we developed the Extended VIIRS-like Artificial Nighttime Light (EVAL) product for China, extending the standard data record backwards by 26 years to begin in 1986. Quantitative evaluation shows that EVAL significantly outperforms existing state-of-the-art products, boosting the $\text{R}^2$ from 0.68 to 0.80 while lowering the RMSE from 1.27 to 0.99. Furthermore, EVAL exhibits excellent temporal consistency and maintains a high correlation with socioeconomic parameters, confirming its reliability for long-term analysis. The resulting EVAL dataset provides a valuable new resource for the research community and is publicly available at https://doi.org/10.11888/HumanNat.tpdc.302930.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.00589v1" target="_blank">Context-based Motion Retrieval using Open Vocabulary Methods for Autonomous Driving</a></h3>
                    <p><strong>Authors:</strong> Stefan Englmeier, Max A. Büttner, Katharina Winter, Fabian B. Flohr</p>
                    <p><strong>Published:</strong> 8/1/2025</p>
                    <p><strong>Categories:</strong> cs.CV, cs.CL, cs.IR, cs.RO, 68T45, 68P20, 68T10, 68T50, 68T07, 68T40, I.2.10; I.4.8; I.2.9; H.3.3</p>
                    <p><strong>Summary:</strong> Autonomous driving systems must operate reliably in safety-critical scenarios, particularly those involving unusual or complex behavior by Vulnerable Road Users (VRUs). Identifying these edge cases in driving datasets is essential for robust evaluation and generalization, but retrieving such rare human behavior scenarios within the long tail of large-scale datasets is challenging. To support targeted evaluation of autonomous driving systems in diverse, human-centered scenarios, we propose a novel context-aware motion retrieval framework. Our method combines Skinned Multi-Person Linear (SMPL)-based motion sequences and corresponding video frames before encoding them into a shared multimodal embedding space aligned with natural language. Our approach enables the scalable retrieval of human behavior and their context through text queries. This work also introduces our dataset WayMoCo, an extension of the Waymo Open Dataset. It contains automatically labeled motion and scene context descriptions derived from generated pseudo-ground-truth SMPL sequences and corresponding image data. Our approach outperforms state-of-the-art models by up to 27.5% accuracy in motion-context retrieval, when evaluated on the WayMoCo dataset.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.00580v1" target="_blank">OmniUnet: A Multimodal Network for Unstructured Terrain Segmentation on Planetary Rovers Using RGB, Depth, and Thermal Imagery</a></h3>
                    <p><strong>Authors:</strong> Raul Castilla-Arquillo, Carlos Perez-del-Pulgar, Levin Gerdes, Alfonso Garcia-Cerezo, Miguel A. Olivares-Mendez</p>
                    <p><strong>Published:</strong> 8/1/2025</p>
                    <p><strong>Categories:</strong> cs.RO, cs.AI</p>
                    <p><strong>Summary:</strong> Robot navigation in unstructured environments requires multimodal perception systems that can support safe navigation. Multimodality enables the integration of complementary information collected by different sensors. However, this information must be processed by machine learning algorithms specifically designed to leverage heterogeneous data. Furthermore, it is necessary to identify which sensor modalities are most informative for navigation in the target environment. In Martian exploration, thermal imagery has proven valuable for assessing terrain safety due to differences in thermal behaviour between soil types. This work presents OmniUnet, a transformer-based neural network architecture for semantic segmentation using RGB, depth, and thermal (RGB-D-T) imagery. A custom multimodal sensor housing was developed using 3D printing and mounted on the Martian Rover Testbed for Autonomy (MaRTA) to collect a multimodal dataset in the Bardenas semi-desert in northern Spain. This location serves as a representative environment of the Martian surface, featuring terrain types such as sand, bedrock, and compact soil. A subset of this dataset was manually labeled to support supervised training of the network. The model was evaluated both quantitatively and qualitatively, achieving a pixel accuracy of 80.37% and demonstrating strong performance in segmenting complex unstructured terrain. Inference tests yielded an average prediction time of 673 ms on a resource-constrained computer (Jetson Orin Nano), confirming its suitability for on-robot deployment. The software implementation of the network and the labeled dataset have been made publicly available to support future research in multimodal terrain perception for planetary robotics.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.00576v1" target="_blank">MultiSHAP: A Shapley-Based Framework for Explaining Cross-Modal Interactions in Multimodal AI Models</a></h3>
                    <p><strong>Authors:</strong> Zhanliang Wang, Kai Wang</p>
                    <p><strong>Published:</strong> 8/1/2025</p>
                    <p><strong>Categories:</strong> cs.AI</p>
                    <p><strong>Summary:</strong> Multimodal AI models have achieved impressive performance in tasks that require integrating information from multiple modalities, such as vision and language. However, their black-box nature poses a major barrier to deployment in high-stakes applications where interpretability and trustworthiness are essential. How to explain cross-modal interactions in multimodal AI models remains a major challenge. While existing model explanation methods, such as attention map and Grad-CAM, offer coarse insights into cross-modal relationships, they cannot precisely quantify the synergistic effects between modalities, and are limited to open-source models with accessible internal weights. Here we introduce MultiSHAP, a model-agnostic interpretability framework that leverages the Shapley Interaction Index to attribute multimodal predictions to pairwise interactions between fine-grained visual and textual elements (such as image patches and text tokens), while being applicable to both open- and closed-source models. Our approach provides: (1) instance-level explanations that reveal synergistic and suppressive cross-modal effects for individual samples - why the model makes a specific prediction on this input, and (2) dataset-level explanation that uncovers generalizable interaction patterns across samples - how the model integrates information across modalities. Experiments on public multimodal benchmarks confirm that MultiSHAP faithfully captures cross-modal reasoning mechanisms, while real-world case studies demonstrate its practical utility. Our framework is extensible beyond two modalities, offering a general solution for interpreting complex multimodal AI models.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.00800v1" target="_blank">Multibeam High Throughput Satellite: Hardware Foundation, Resource Allocation, and Precoding</a></h3>
                    <p><strong>Authors:</strong> Rui Chen, Wen-Xuan Long, Bing-Qian Wang, Yuan He, Rui-Jin Sun, Nan Cheng, Gan Zheng, Dusit Niyato</p>
                    <p><strong>Published:</strong> 8/1/2025</p>
                    <p><strong>Categories:</strong> eess.SP</p>
                    <p><strong>Summary:</strong> With its wide coverage and uninterrupted service, satellite communication is a critical technology for next-generation 6G communications. High throughput satellite (HTS) systems, utilizing multipoint beam and frequency multiplexing techniques, enable satellite communication capacity of up to Tbps to meet the growing traffic demand. Therefore, it is imperative to review the-state-of-the-art of multibeam HTS systems and identify their associated challenges and perspectives. Firstly, we summarize the multibeam HTS hardware foundations, including ground station systems, on-board payloads, and user terminals. Subsequently, we review the flexible on-board radio resource allocation approaches of bandwidth, power, time slot, and joint allocation schemes of HTS systems to optimize resource utilization and cater to non-uniform service demand. Additionally, we survey multibeam precoding methods for the HTS system to achieve full-frequency reuse and interference cancellation, which are classified according to different deployments such as single gateway precoding, multiple gateway precoding, on-board precoding, and hybrid on-board/on-ground precoding. Finally, we disscuss the challenges related to Q/V band link outage, time and frequency synchronization of gateways, the accuracy of channel state information (CSI), payload light-weight development, and the application of deep learning (DL). Research on these topics will contribute to enhancing the performance of HTS systems and finally delivering high-speed data to areas underserved by terrestrial networks.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.00796v1" target="_blank">UV flux variation study in contact binary VW Cephei</a></h3>
                    <p><strong>Authors:</strong> Anurag Baruah, Mayukh Pahari</p>
                    <p><strong>Published:</strong> 8/1/2025</p>
                    <p><strong>Categories:</strong> astro-ph.SR</p>
                    <p><strong>Summary:</strong> Despite many attempts, the origin of UV emission line and continuum in contact binary stars remains unclear. We present a substantial UV spectroscopic analysis of VW Cephei, a late-type contact binary system, using 46 low-resolution spectra from the International Ultraviolet Explorer (IUE) in the wavelength range 1150-1978 \r{A}. By modelling continuum and emissions lines in individual spectra, we report the significant detection of OIII] (1660 and 1666 \r{A}) and SiIV (1393 and 1402 \r{A}) line complexes. We observe that UV fluxes for both continuum and emission lines like CIV, OIII], CII and SiIV vary significantly (fractional rms variability up to 45%) from hours to years. In addition, line widths also change by hundreds of kilometres/sec. The UV flux variabilities observed in the continuum bands and line emissions are uncorrelated. However, most of the flux values follow the binary orbital period observed from optical data. Our analysis indicates that, while the variation in continuum flux may be attributed to a heated photosphere, the line width measurements indicate that the emission lines are likely formed in the dynamical clouds associated with Roche lobe overflow. We estimate the mass transfer rate of $ \dot{M} = (0.82 \pm 0.01) \times 10^{-7} \ M_{\odot} {yr^{-1}}$ from UV line fluxes, which is in good agreement with optical studies.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.00788v1" target="_blank">Do They Understand Them? An Updated Evaluation on Nonbinary Pronoun Handling in Large Language Models</a></h3>
                    <p><strong>Authors:</strong> Xushuo Tang, Yi Ding, Zhengyi Yang, Yin Chen, Yongrui Gu, Wenke Yang, Mingchen Ju, Xin Cao, Yongfei Liu, Wenjie Zhang</p>
                    <p><strong>Published:</strong> 8/1/2025</p>
                    <p><strong>Categories:</strong> cs.CL, cs.AI</p>
                    <p><strong>Summary:</strong> Large language models (LLMs) are increasingly deployed in sensitive contexts where fairness and inclusivity are critical. Pronoun usage, especially concerning gender-neutral and neopronouns, remains a key challenge for responsible AI. Prior work, such as the MISGENDERED benchmark, revealed significant limitations in earlier LLMs handling of inclusive pronouns, but was constrained to outdated models and limited evaluations. In this study, we introduce MISGENDERED+, an extended and updated benchmark for evaluating LLMs pronoun fidelity. We benchmark five representative LLMs, GPT-4o, Claude 4, DeepSeek-V3, Qwen Turbo, and Qwen2.5, across zero-shot, few-shot, and gender identity inference. Our results show notable improvements compared with previous studies, especially in binary and gender-neutral pronoun accuracy. However, accuracy on neopronouns and reverse inference tasks remains inconsistent, underscoring persistent gaps in identity-sensitive reasoning. We discuss implications, model-specific observations, and avenues for future inclusive AI research.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.00785v1" target="_blank">Explainable AI and Machine Learning for Exam-based Student Evaluation: Causal and Predictive Analysis of Socio-academic and Economic Factors</a></h3>
                    <p><strong>Authors:</strong> Bushra Akter, Md Biplob Hosen, Sabbir Ahmed, Mehrin Anannya, Md. Farhad Hossain</p>
                    <p><strong>Published:</strong> 8/1/2025</p>
                    <p><strong>Categories:</strong> cs.LG</p>
                    <p><strong>Summary:</strong> Academic performance depends on a multivariable nexus of socio-academic and financial factors. This study investigates these influences to develop effective strategies for optimizing students CGPA. To achieve this, we reviewed various literature to identify key influencing factors and constructed an initial hypothetical causal graph based on the findings. Additionally, an online survey was conducted, where 1,050 students participated, providing comprehensive data for analysis. Rigorous data preprocessing techniques, including cleaning and visualization, ensured data quality before analysis. Causal analysis validated the relationships among variables, offering deeper insights into their direct and indirect effects on CGPA. Regression models were implemented for CGPA prediction, while classification models categorized students based on performance levels. Ridge Regression demonstrated strong predictive accuracy, achieving a Mean Absolute Error of 0.12 and a Mean Squared Error of 0.023. Random Forest outperformed in classification, attaining an F1-score near perfection and an accuracy of 98.68%. Explainable AI techniques such as SHAP, LIME, and Interpret enhanced model interpretability, highlighting critical factors such as study hours, scholarships, parental education, and prior academic performance. The study culminated in the development of a web-based application that provides students with personalized insights, allowing them to predict academic performance, identify areas for improvement, and make informed decisions to enhance their outcomes.</p>
                
            
                
                    <h3><a href="http://dx.doi.org/10.1145/3746027.3755705" target="_blank">SpA2V: Harnessing Spatial Auditory Cues for Audio-driven Spatially-aware Video Generation</a></h3>
                    <p><strong>Authors:</strong> Kien T. Pham, Yingqing He, Yazhou Xing, Qifeng Chen, Long Chen</p>
                    <p><strong>Published:</strong> 8/1/2025</p>
                    <p><strong>Categories:</strong> cs.GR, cs.AI, cs.CV, cs.MM, cs.SD, eess.AS</p>
                    <p><strong>Summary:</strong> Audio-driven video generation aims to synthesize realistic videos that align with input audio recordings, akin to the human ability to visualize scenes from auditory input. However, existing approaches predominantly focus on exploring semantic information, such as the classes of sounding sources present in the audio, limiting their ability to generate videos with accurate content and spatial composition. In contrast, we humans can not only naturally identify the semantic categories of sounding sources but also determine their deeply encoded spatial attributes, including locations and movement directions. This useful information can be elucidated by considering specific spatial indicators derived from the inherent physical properties of sound, such as loudness or frequency. As prior methods largely ignore this factor, we present SpA2V, the first framework explicitly exploits these spatial auditory cues from audios to generate videos with high semantic and spatial correspondence. SpA2V decomposes the generation process into two stages: 1) Audio-guided Video Planning: We meticulously adapt a state-of-the-art MLLM for a novel task of harnessing spatial and semantic cues from input audio to construct Video Scene Layouts (VSLs). This serves as an intermediate representation to bridge the gap between the audio and video modalities. 2) Layout-grounded Video Generation: We develop an efficient and effective approach to seamlessly integrate VSLs as conditional guidance into pre-trained diffusion models, enabling VSL-grounded video generation in a training-free manner. Extensive experiments demonstrate that SpA2V excels in generating realistic videos with semantic and spatial alignment to the input audios.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.00780v1" target="_blank">CUbesat Solar Polarimeter (CUSP) Sensitivity Estimation and Performance Optimization using Geant4</a></h3>
                    <p><strong>Authors:</strong> Abhay Kumar, Giovanni Lombardi, Giovanni De Cesare, Nicolas De Angelis, Sergio Fabiani, Ettore Del Monte, Andrea Alimenti, Riccardo Campana, Enrico Costa, Paolo Soffitta, Mauro Centrone, Sergio Di Cosimo, Giuseppe Di Persio, Alessandro Lacerenza, Pasqualino Loffredo, Fabio Muleri, Paolo Romano, Alda Rubini, Emanuele Scalise, Enrico Silva, Davide Albanesi, Ilaria Baffo, Daniele Brienza, Valerio Campamaggiore, Giovanni Cucinella, Andrea Curatolo, Giulia de Iulis, Andrea Del Re, Vito Di Bari, Simone Di Filippo, Immacolata Donnarumma, Pierluigi Fanelli, Nicolas Gagliardi, Paolo Leonetti, Matteo Merge, Gabriele Minervini, Dario Modenini, Andrea Negri, Daniele Pecorella, Massimo Perelli, Alice Ponti, Francesca Sbop, Paolo Tortora, Alessandro Turchi, Valerio Vagelli, Emanuele Zaccagnino, Alessandro Zambardi, Costantino Zazza</p>
                    <p><strong>Published:</strong> 8/1/2025</p>
                    <p><strong>Categories:</strong> astro-ph.SR, astro-ph.HE, astro-ph.IM</p>
                    <p><strong>Summary:</strong> The CUbesat Solar Polarimeter (CUSP) aims to measure the linear polarization of solar flares in the 25-100 keV X-ray band using a Compton scattering polarimeter. CUSP will allow us to study the magnetic reconnection and particle acceleration in the flaring magnetic structures of our star by providing high-sensitivity polarization measurements. CUSP is a project in the framework of the Alcor Program of the Italian Space Agency aimed to develop innovative CubeSat technologies and missions. As part of CUSPs Phase B study, which began in December 2024 and will continue for one year, we present the development status of the Geant4 based simulator to accurately simulate the detectors response and initial results on the sensitivity of the instrument. Geant4 Monte Carlo simulation is used to assess the physical interactions of the source photons with the detector and the passive materials. We implemented a detailed CUSP Mass Model within Geant4 to simulate and estimate the instruments sensitivity, correcting the geometric effects of the instrument. We also evaluated the effect of backscattering shielding on the sensitivity to optimize the mass model of the instrument.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.00778v1" target="_blank">τ-Ring: A Smart Ring Platform for Multimodal Physiological and Behavioral Sensing</a></h3>
                    <p><strong>Authors:</strong> Jiankai Tang, Zhe He, Mingyu Zhang, Wei Geng, Chengchi Zhou, Weinan Shi, Yuanchun Shi, Yuntao Wang</p>
                    <p><strong>Published:</strong> 8/1/2025</p>
                    <p><strong>Categories:</strong> cs.CE</p>
                    <p><strong>Summary:</strong> Smart rings have emerged as uniquely convenient devices for continuous physiological and behavioral sensing, offering unobtrusive, constant access to metrics such as heart rate, motion, and skin temperature. Yet most commercial solutions remain proprietary, hindering reproducibility and slowing innovation in wearable research. We introduce {\tau}-Ring, a commercial-ready platform that bridges this gap through: (i) accessible hardware combining time-synchronized multi-channel PPG, 6-axis IMU, temperature sensing, NFC, and on-board storage; (ii) adjustable firmware that lets researchers rapidly reconfigure sampling rates, power modes, and wireless protocols; and (iii) a fully open-source Android software suite that supports both real-time streaming and 8-hour offline logging. Together, these features enable out-of-the-box, reproducible acquisition of rich physiological and behavioral datasets, accelerating prototyping and standardizing experimentation. We validate the platform with demonstration studies in heart-rate monitoring and ring-based handwriting recognition. Source code is available at GitHub: https://github.com/thuhci/OpenRing.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.00771v1" target="_blank">Latin American network on electromagnetic effects in strongly interacting matter: Contribution to the update of the Latin American Strategy for High Energy, Cosmology and Astroparticle Physics</a></h3>
                    <p><strong>Authors:</strong> Ana Mizher, Alejandro Ayala</p>
                    <p><strong>Published:</strong> 8/1/2025</p>
                    <p><strong>Categories:</strong> hep-ph, hep-ex, hep-th</p>
                    <p><strong>Summary:</strong> An accurate characterization of the quark-gluon plasma requires understanding of how electromagnetic effects affect the processes mediated by the strong force. All the scenarios in which the plasma emerges, either in nature or in the laboratory, involve strong electromagnetic fields. The early universe, compact astrophysical objects, or ultra-relativistic heavy-ion collisions harbor the most intense fields we know. Researches from the Latin America region have made a substantial contribution on this subject and the \lq\lq Latin American Network on Electromagnetic Effects in Strongly Interacting Matter aims to cluster efforts to address open questions related to these systems, boosting collaborations and interaction among its members and connecting Latin American institutions with institutions from the rest of the world. In face of the upcoming experimental programs and new facilities, our mission is to bring together experimentalists, phenomenologists and theorists to better explore the properties of strongly interacting matter in the presence of intense electromagnetic fields. This document describes succinctly the recent contributions from researchers of the Latin American region to the subject, as well as our activities and perspectives for the future.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.00769v1" target="_blank">Designing cultured tissue moulds using evolutionary strategies</a></h3>
                    <p><strong>Authors:</strong> Allison E. Andrews, Hugh Dickinson, James P. Hague</p>
                    <p><strong>Published:</strong> 8/1/2025</p>
                    <p><strong>Categories:</strong> physics.bio-ph, q-bio.TO</p>
                    <p><strong>Summary:</strong> There is an unmet need for artificial intelligence techniques that can speed up the design of growth strategies for cultured tissues. Cultured tissue is increasingly important for a range of applications such as cultivated meat, pharmaceutical assays and regenerative medicine. In this paper, we introduce a method based around evolutionary strategies, machine learning and biophysical simulations that can be used to speed up the process of identifying new tissue growth strategies for these diverse applications. We demonstrate the method by designing tethering strategies to grow tissues containing various cell types with desirable properties such as high cellular alignment and uniform density.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.00756v1" target="_blank">LeakyCLIP: Extracting Training Data from CLIP</a></h3>
                    <p><strong>Authors:</strong> Yunhao Chen, Shujie Wang, Xin Wang, Xingjun Ma</p>
                    <p><strong>Published:</strong> 8/1/2025</p>
                    <p><strong>Categories:</strong> cs.CR</p>
                    <p><strong>Summary:</strong> Understanding the memorization and privacy leakage risks in Contrastive Language--Image Pretraining (CLIP) is critical for ensuring the security of multimodal models. Recent studies have demonstrated the feasibility of extracting sensitive training examples from diffusion models, with conditional diffusion models exhibiting a stronger tendency to memorize and leak information. In this work, we investigate data memorization and extraction risks in CLIP through the lens of CLIP inversion, a process that aims to reconstruct training images from text prompts. To this end, we introduce \textbf{LeakyCLIP}, a novel attack framework designed to achieve high-quality, semantically accurate image reconstruction from CLIP embeddings. We identify three key challenges in CLIP inversion: 1) non-robust features, 2) limited visual semantics in text embeddings, and 3) low reconstruction fidelity. To address these challenges, LeakyCLIP employs 1) adversarial fine-tuning to enhance optimization smoothness, 2) linear transformation-based embedding alignment, and 3) Stable Diffusion-based refinement to improve fidelity. Empirical results demonstrate the superiority of LeakyCLIP, achieving over 358% improvement in Structural Similarity Index Measure (SSIM) for ViT-B-16 compared to baseline methods on LAION-2B subset. Furthermore, we uncover a pervasive leakage risk, showing that training data membership can even be successfully inferred from the metrics of low-fidelity reconstructions. Our work introduces a practical method for CLIP inversion while offering novel insights into the nature and scope of privacy risks in multimodal models.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.00755v1" target="_blank">AI-Driven Collaborative Satellite Object Detection for Space Sustainability</a></h3>
                    <p><strong>Authors:</strong> Peng Hu, Wenxuan Zhang</p>
                    <p><strong>Published:</strong> 8/1/2025</p>
                    <p><strong>Categories:</strong> eess.IV, cs.CV</p>
                    <p><strong>Summary:</strong> The growing density of satellites in low-Earth orbit (LEO) presents serious challenges to space sustainability, primarily due to the increased risk of in-orbit collisions. Traditional ground-based tracking systems are constrained by latency and coverage limitations, underscoring the need for onboard, vision-based space object detection (SOD) capabilities. In this paper, we propose a novel satellite clustering framework that enables the collaborative execution of deep learning (DL)-based SOD tasks across multiple satellites. To support this approach, we construct a high-fidelity dataset simulating imaging scenarios for clustered satellite formations. A distance-aware viewpoint selection strategy is introduced to optimize detection performance, and recent DL models are used for evaluation. Experimental results show that the clustering-based method achieves competitive detection accuracy compared to single-satellite and existing approaches, while maintaining a low size, weight, and power (SWaP) footprint. These findings underscore the potential of distributed, AI-enabled in-orbit systems to enhance space situational awareness and contribute to long-term space sustainability.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.00750v1" target="_blank">SU-ESRGAN: Semantic and Uncertainty-Aware ESRGAN for Super-Resolution of Satellite and Drone Imagery with Fine-Tuning for Cross Domain Evaluation</a></h3>
                    <p><strong>Authors:</strong> Prerana Ramkumar</p>
                    <p><strong>Published:</strong> 8/1/2025</p>
                    <p><strong>Categories:</strong> cs.CV, cs.LG, eess.IV</p>
                    <p><strong>Summary:</strong> Generative Adversarial Networks (GANs) have achieved realistic super-resolution (SR) of images however, they lack semantic consistency and per-pixel confidence, limiting their credibility in critical remote sensing applications such as disaster response, urban planning and agriculture. This paper introduces Semantic and Uncertainty-Aware ESRGAN (SU-ESRGAN), the first SR framework designed for satellite imagery to integrate the ESRGAN, segmentation loss via DeepLabv3 for class detail preservation and Monte Carlo dropout to produce pixel-wise uncertainty maps. The SU-ESRGAN produces results (PSNR, SSIM, LPIPS) comparable to the Baseline ESRGAN on aerial imagery. This novel model is valuable in satellite systems or UAVs that use wide field-of-view (FoV) cameras, trading off spatial resolution for coverage. The modular design allows integration in UAV data pipelines for on-board or post-processing SR to enhance imagery resulting due to motion blur, compression and sensor limitations. Further, the model is fine-tuned to evaluate its performance on cross domain applications. The tests are conducted on two drone based datasets which differ in altitude and imaging perspective. Performance evaluation of the fine-tuned models show a stronger adaptation to the Aerial Maritime Drone Dataset, whose imaging characteristics align with the training data, highlighting the importance of domain-aware training in SR-applications.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.00749v1" target="_blank">Dynamic Symbolic Execution for Semantic Difference Analysis of Component and Connector Architectures</a></h3>
                    <p><strong>Authors:</strong> Johanna Grahl, Bernhard Rumpe, Max Stachon, Sebastian Stüber</p>
                    <p><strong>Published:</strong> 8/1/2025</p>
                    <p><strong>Categories:</strong> cs.SE, cs.FL, cs.SC, 68N30, D.2.4</p>
                    <p><strong>Summary:</strong> In the context of model-driven development, ensuring the correctness and consistency of evolving models is paramount. This paper investigates the application of Dynamic Symbolic Execution (DSE) for semantic difference analysis of component-and-connector architectures, specifically utilizing MontiArc models. We have enhanced the existing MontiArc-to-Java generator to gather both symbolic and concrete execution data at runtime, encompassing transition conditions, visited states, and internal variables of automata. This data facilitates the identification of significant execution traces that provide critical insights into system behavior. We evaluate various execution strategies based on the criteria of runtime efficiency, minimality, and completeness, establishing a framework for assessing the applicability of DSE in semantic difference analysis. Our findings indicate that while DSE shows promise for analyzing component and connector architectures, scalability remains a primary limitation, suggesting further research is needed to enhance its practical utility in larger systems.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.00748v1" target="_blank">Is It Really You? Exploring Biometric Verification Scenarios in Photorealistic Talking-Head Avatar Videos</a></h3>
                    <p><strong>Authors:</strong> Laura Pedrouzo-Rodriguez, Pedro Delgado-DeRobles, Luis F. Gomez, Ruben Tolosana, Ruben Vera-Rodriguez, Aythami Morales, Julian Fierrez</p>
                    <p><strong>Published:</strong> 8/1/2025</p>
                    <p><strong>Categories:</strong> cs.CV, cs.AI, cs.CR, cs.MM</p>
                    <p><strong>Summary:</strong> Photorealistic talking-head avatars are becoming increasingly common in virtual meetings, gaming, and social platforms. These avatars allow for more immersive communication, but they also introduce serious security risks. One emerging threat is impersonation: an attacker can steal a users avatar-preserving their appearance and voice-making it nearly impossible to detect its fraudulent usage by sight or sound alone. In this paper, we explore the challenge of biometric verification in such avatar-mediated scenarios. Our main question is whether an individuals facial motion patterns can serve as reliable behavioral biometrics to verify their identity when the avatars visual appearance is a facsimile of its owner. To answer this question, we introduce a new dataset of realistic avatar videos created using a state-of-the-art one-shot avatar generation model, GAGAvatar, with genuine and impostor avatar videos. We also propose a lightweight, explainable spatio-temporal Graph Convolutional Network architecture with temporal attention pooling, that uses only facial landmarks to model dynamic facial gestures. Experimental results demonstrate that facial motion cues enable meaningful identity verification with AUC values approaching 80%. The proposed benchmark and biometric system are available for the research community in order to bring attention to the urgent need for more advanced behavioral biometric defenses in avatar-based communication systems.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.00744v1" target="_blank">Rethinking Backbone Design for Lightweight 3D Object Detection in LiDAR</a></h3>
                    <p><strong>Authors:</strong> Adwait Chandorkar, Hasan Tercan, Tobias Meisen</p>
                    <p><strong>Published:</strong> 8/1/2025</p>
                    <p><strong>Categories:</strong> cs.CV</p>
                    <p><strong>Summary:</strong> Recent advancements in LiDAR-based 3D object detection have significantly accelerated progress toward the realization of fully autonomous driving in real-world environments. Despite achieving high detection performance, most of the approaches still rely on a VGG-based or ResNet-based backbone for feature exploration, which increases the model complexity. Lightweight backbone design is well-explored for 2D object detection, but research on 3D object detection still remains limited. In this work, we introduce Dense Backbone, a lightweight backbone that combines the benefits of high processing speed, lightweight architecture, and robust detection accuracy. We adapt multiple SoTA 3d object detectors, such as PillarNet, with our backbone and show that with our backbone, these models retain most of their detection capability at a significantly reduced computational cost. To our knowledge, this is the first dense-layer-based backbone tailored specifically for 3D object detection from point cloud data. DensePillarNet, our adaptation of PillarNet, achieves a 29% reduction in model parameters and a 28% reduction in latency with just a 2% drop in detection accuracy on the nuScenes test set. Furthermore, Dense Backbones plug-and-play design allows straightforward integration into existing architectures, requiring no modifications to other network components.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.00743v1" target="_blank">Agentic large language models improve retrieval-based radiology question answering</a></h3>
                    <p><strong>Authors:</strong> Sebastian Wind, Jeta Sopa, Daniel Truhn, Mahshad Lotfinia, Tri-Thien Nguyen, Keno Bressem, Lisa Adams, Mirabela Rusu, Harald Köstler, Gerhard Wellein, Andreas Maier, Soroosh Tayebi Arasteh</p>
                    <p><strong>Published:</strong> 8/1/2025</p>
                    <p><strong>Categories:</strong> cs.CL, cs.AI, cs.LG</p>
                    <p><strong>Summary:</strong> Clinical decision-making in radiology increasingly benefits from artificial intelligence (AI), particularly through large language models (LLMs). However, traditional retrieval-augmented generation (RAG) systems for radiology question answering (QA) typically rely on single-step retrieval, limiting their ability to handle complex clinical reasoning tasks. Here we propose an agentic RAG framework enabling LLMs to autonomously decompose radiology questions, iteratively retrieve targeted clinical evidence from Radiopaedia, and dynamically synthesize evidence-based responses. We evaluated 24 LLMs spanning diverse architectures, parameter scales (0.5B to 670B), and training paradigms (general-purpose, reasoning-optimized, clinically fine-tuned), using 104 expert-curated radiology questions from previously established RSNA-RadioQA and ExtendedQA datasets. Agentic retrieval significantly improved mean diagnostic accuracy over zero-shot prompting (73% vs. 64%; P200B parameters) demonstrated minimal changes (2% improvement). Additionally, agentic retrieval reduced hallucinations (mean 9.4%) and retrieved clinically relevant context in 46% of cases, substantially aiding factual grounding. Even clinically fine-tuned models exhibited meaningful improvements (e.g., MedGemma-27B improved from 71% to 81%), indicating complementary roles of retrieval and fine-tuning. These results highlight the potential of agentic frameworks to enhance factuality and diagnostic accuracy in radiology QA, particularly among mid-sized LLMs, warranting future studies to validate their clinical utility.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.00742v1" target="_blank">Applying Psychometrics to Large Language Model Simulated Populations: Recreating the HEXACO Personality Inventory Experiment with Generative Agents</a></h3>
                    <p><strong>Authors:</strong> Sarah Mercer, Daniel P. Martin, Phil Swatton</p>
                    <p><strong>Published:</strong> 8/1/2025</p>
                    <p><strong>Categories:</strong> cs.CL, cs.LG</p>
                    <p><strong>Summary:</strong> Generative agents powered by Large Language Models demonstrate human-like characteristics through sophisticated natural language interactions. Their ability to assume roles and personalities based on predefined character biographies has positioned them as cost-effective substitutes for human participants in social science research. This paper explores the validity of such persona-based agents in representing human populations; we recreate the HEXACO personality inventory experiment by surveying 310 GPT-4 powered agents, conducting factor analysis on their responses, and comparing these results to the original findings presented by Ashton, Lee,  Goldberg in 2004. Our results found 1) a coherent and reliable personality structure was recoverable from the agents responses demonstrating partial alignment to the HEXACO framework. 2) the derived personality dimensions were consistent and reliable within GPT-4, when coupled with a sufficiently curated population, and 3) cross-model analysis revealed variability in personality profiling, suggesting model-specific biases and limitations. We discuss the practical considerations and challenges encountered during the experiment. This study contributes to the ongoing discourse on the potential benefits and limitations of using generative agents in social science research and provides useful guidance on designing consistent and representative agent personas to maximise coverage and representation of human personality traits.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.00741v1" target="_blank">Out-of-Context Abduction: LLMs Make Inferences About Procedural Data Leveraging Declarative Facts in Earlier Training Data</a></h3>
                    <p><strong>Authors:</strong> Sohaib Imran, Rob Lamb, Peter M. Atkinson</p>
                    <p><strong>Published:</strong> 8/1/2025</p>
                    <p><strong>Categories:</strong> cs.CL, cs.AI</p>
                    <p><strong>Summary:</strong> Large language models (LLMs) are trained on large corpora, yet it is unclear whether they can reason about the information present within their training data. We design experiments to study out-of-context abduction in LLMs, the ability to infer the most plausible explanations for observations using relevant facts present in training data. We train treatment LLMs on names and behavior descriptions of fictitious chatbots, but not on examples of dialogue with the chatbots. We find that OpenAIs GPT 4o LLM can correctly infer at least one chatbots name after observing example responses characteristic of that chatbot. We also find that previously training GPT 4o on descriptions of a chatbots behavior allows it to display behaviors more characteristic of the chatbot when iteratively trained to display such behaviors. Our results have implications for situational awareness in LLMs and, therefore, for AI safety.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.00738v1" target="_blank">Tool-Assisted Conformance Checking to Reference Process Models</a></h3>
                    <p><strong>Authors:</strong> Bernhard Rumpe, Max Stachon, Sebastian Stüber, Valdes Voufo</p>
                    <p><strong>Published:</strong> 8/1/2025</p>
                    <p><strong>Categories:</strong> cs.SE, cs.FL, 68N30, D.2.4</p>
                    <p><strong>Summary:</strong> Reference models convey best practices and standards. The reference frameworks necessitate conformance checks to ensure adherence to established guidelines and principles, which is crucial for maintaining quality and consistency in various processes. This paper explores automated conformance checks for concrete process models against reference models using causal dependency analysis of tasks and events. Existing notions of conformance checking for process models focus on verifying process execution traces and lack the expressiveness and automation needed for semantic model comparison, leaving this question unresolved. We integrate our approach into a broader semantic framework for defining reference model conformance. We outline an algorithm for reference process model conformance checking, evaluate it through a case study, and discuss its strengths and limitations. Our research provides a tool-assisted solution enhancing accuracy and flexibility in process model conformance verification.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.00737v1" target="_blank">How LLMs are Shaping the Future of Virtual Reality</a></h3>
                    <p><strong>Authors:</strong> Süeda Özkaya, Santiago Berrezueta-Guzman, Stefan Wagner</p>
                    <p><strong>Published:</strong> 8/1/2025</p>
                    <p><strong>Categories:</strong> cs.HC, cs.AI</p>
                    <p><strong>Summary:</strong> The integration of Large Language Models (LLMs) into Virtual Reality (VR) games marks a paradigm shift in the design of immersive, adaptive, and intelligent digital experiences. This paper presents a comprehensive review of recent research at the intersection of LLMs and VR, examining how these models are transforming narrative generation, non-player character (NPC) interactions, accessibility, personalization, and game mastering. Drawing from an analysis of 62 peer reviewed studies published between 2018 and 2025, we identify key application domains ranging from emotionally intelligent NPCs and procedurally generated storytelling to AI-driven adaptive systems and inclusive gameplay interfaces. We also address the major challenges facing this convergence, including real-time performance constraints, memory limitations, ethical risks, and scalability barriers. Our findings highlight that while LLMs significantly enhance realism, creativity, and user engagement in VR environments, their effective deployment requires robust design strategies that integrate multimodal interaction, hybrid AI architectures, and ethical safeguards. The paper concludes by outlining future research directions in multimodal AI, affective computing, reinforcement learning, and open-source development, aiming to guide the responsible advancement of intelligent and inclusive VR systems.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.00733v1" target="_blank">AudioGen-Omni: A Unified Multimodal Diffusion Transformer for Video-Synchronized Audio, Speech, and Song Generation</a></h3>
                    <p><strong>Authors:</strong> Le Wang, Jun Wang, Feng Deng, Chen Zhang, Kun Gai, Di Zhang</p>
                    <p><strong>Published:</strong> 8/1/2025</p>
                    <p><strong>Categories:</strong> cs.SD, cs.CV, cs.MM, eess.AS</p>
                    <p><strong>Summary:</strong> We present AudioGen-Omni - a unified approach based on multimodal diffusion transformers (MMDit), capable of generating high-fidelity audio, speech, and songs coherently synchronized with the input video. AudioGen-Omni introduces a novel joint training paradigm that seamlessly integrates large-scale video-text-audio corpora, enabling a model capable of generating semantically rich, acoustically diverse audio conditioned on multimodal inputs and adaptable to a wide range of audio generation tasks. AudioGen-Omni employs a unified lyrics-transcription encoder that encodes graphemes and phonemes from both sung and spoken inputs into dense frame-level representations. Dense frame-level representations are fused using an AdaLN-based joint attention mechanism enhanced with phase-aligned anisotropic positional infusion (PAAPI), wherein RoPE is selectively applied to temporally structured modalities to ensure precise and robust cross-modal alignment. By unfreezing all modalities and masking missing inputs, AudioGen-Omni mitigates the semantic constraints of text-frozen paradigms, enabling effective cross-modal conditioning. This joint training approach enhances audio quality, semantic alignment, and lip-sync accuracy, while also achieving state-of-the-art results on Text-to-Audio/Speech/Song tasks. With an inference time of 1.91 seconds for 8 seconds of audio, it offers substantial improvements in both efficiency and generality.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.00728v1" target="_blank">YOLO-Count: Differentiable Object Counting for Text-to-Image Generation</a></h3>
                    <p><strong>Authors:</strong> Guanning Zeng, Xiang Zhang, Zirui Wang, Haiyang Xu, Zeyuan Chen, Bingnan Li, Zhuowen Tu</p>
                    <p><strong>Published:</strong> 8/1/2025</p>
                    <p><strong>Categories:</strong> cs.CV</p>
                    <p><strong>Summary:</strong> We propose YOLO-Count, a differentiable open-vocabulary object counting model that tackles both general counting challenges and enables precise quantity control for text-to-image (T2I) generation. A core contribution is the cardinality map, a novel regression target that accounts for variations in object size and spatial distribution. Leveraging representation alignment and a hybrid strong-weak supervision scheme, YOLO-Count bridges the gap between open-vocabulary counting and T2I generation control. Its fully differentiable architecture facilitates gradient-based optimization, enabling accurate object count estimation and fine-grained guidance for generative models. Extensive experiments demonstrate that YOLO-Count achieves state-of-the-art counting accuracy while providing robust and effective quantity control for T2I systems.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.00723v1" target="_blank">Why Do Decision Makers (Not) Use AI? A Cross-Domain Analysis of Factors Impacting AI Adoption</a></h3>
                    <p><strong>Authors:</strong> Rebecca Yu, Valerie Chen, Ameet Talwalkar, Hoda Heidari</p>
                    <p><strong>Published:</strong> 8/1/2025</p>
                    <p><strong>Categories:</strong> cs.HC</p>
                    <p><strong>Summary:</strong> Growing excitement around deploying AI across various domains calls for a careful assessment of how human decision-makers interact with AI-powered systems. In particular, it is essential to understand when decision-makers voluntarily choose to consult AI tools, which we term decision-maker adoption. We interviewed experts across four domains -- medicine, law, journalism, and the public sector -- to explore current AI use cases and perceptions of adoption. From these interviews, we identify key factors that shape decision-maker adoption of AI tools: the decision-makers background, perceptions of the AI, consequences for the decision-maker, and perceived implications for other stakeholders. We translate these factors into an AI adoption sheet to analyze how decision-makers approach adoption choices through comparative, cross-domain case studies, highlighting how our factors help explain inter-domain differences in adoption. Our findings offer practical guidance for supporting the responsible and context-aware deployment of AI by better accounting for the decision-makers perspective.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.00718v1" target="_blank">Democratizing Tabular Data Access with an Open$\unicode{x2013}$Source Synthetic$\unicode{x2013}$Data SDK</a></h3>
                    <p><strong>Authors:</strong> Ivona Krchova, Mariana Vargas Vieyra, Mario Scriminaci, Andrey Sidorenko</p>
                    <p><strong>Published:</strong> 8/1/2025</p>
                    <p><strong>Categories:</strong> cs.LG</p>
                    <p><strong>Summary:</strong> Machine learning development critically depends on access to high-quality data. However, increasing restrictions due to privacy, proprietary interests, and ethical concerns have created significant barriers to data accessibility. Synthetic data offers a viable solution by enabling safe, broad data usage without compromising sensitive information. This paper presents the MOSTLY AI Synthetic Data Software Development Kit (SDK), an open-source toolkit designed specifically for synthesizing high-quality tabular data. The SDK integrates robust features such as differential privacy guarantees, fairness-aware data generation, and automated quality assurance into a flexible and accessible Python interface. Leveraging the TabularARGN autoregressive framework, the SDK supports diverse data types and complex multi-table and sequential datasets, delivering competitive performance with notable improvements in speed and usability. Currently deployed both as a cloud service and locally installable software, the SDK has seen rapid adoption, highlighting its practicality in addressing real-world data bottlenecks and promoting widespread data democratization.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.00717v1" target="_blank">Generative AI in Higher Education: Evidence from an Elite College</a></h3>
                    <p><strong>Authors:</strong> Zara Contractor, Germán Reyes</p>
                    <p><strong>Published:</strong> 8/1/2025</p>
                    <p><strong>Categories:</strong> econ.GN, q-fin.EC</p>
                    <p><strong>Summary:</strong> Generative AI is transforming higher education, yet systematic evidence on student adoption remains limited. Using novel survey data from a selective U.S. college, we document over 80 percent of students using AI academically within two years of ChatGPTs release. Adoption varies across disciplines, demographics, and achievement levels, highlighting AIs potential to reshape educational inequalities. Students predominantly use AI for augmenting learning (e.g., explanations, feedback), but also to automate tasks (e.g., essay generation). Positive perceptions of AIs educational benefits strongly predict adoption. Institutional policies can influence usage patterns but risk creating unintended disparate impacts across student groups due to uneven compliance.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.00716v1" target="_blank">Nested Graph Pseudo-Label Refinement for Noisy Label Domain Adaptation Learning</a></h3>
                    <p><strong>Authors:</strong> Yingxu Wang, Mengzhu Wang, Zhichao Huang, Suyu Liu</p>
                    <p><strong>Published:</strong> 8/1/2025</p>
                    <p><strong>Categories:</strong> cs.LG, cs.AI</p>
                    <p><strong>Summary:</strong> Graph Domain Adaptation (GDA) facilitates knowledge transfer from labeled source graphs to unlabeled target graphs by learning domain-invariant representations, which is essential in applications such as molecular property prediction and social network analysis. However, most existing GDA methods rely on the assumption of clean source labels, which rarely holds in real-world scenarios where annotation noise is pervasive. This label noise severely impairs feature alignment and degrades adaptation performance under domain shifts. To address this challenge, we propose Nested Graph Pseudo-Label Refinement (NeGPR), a novel framework tailored for graph-level domain adaptation with noisy labels. NeGPR first pretrains dual branches, i.e., semantic and topology branches, by enforcing neighborhood consistency in the feature space, thereby reducing the influence of noisy supervision. To bridge domain gaps, NeGPR employs a nested refinement mechanism in which one branch selects high-confidence target samples to guide the adaptation of the other, enabling progressive cross-domain learning. Furthermore, since pseudo-labels may still contain noise and the pre-trained branches are already overfitted to the noisy labels in the source domain, NeGPR incorporates a noise-aware regularization strategy. This regularization is theoretically proven to mitigate the adverse effects of pseudo-label noise, even under the presence of source overfitting, thus enhancing the robustness of the adaptation process. Extensive experiments on benchmark datasets demonstrate that NeGPR consistently outperforms state-of-the-art methods under severe label noise, achieving gains of up to 12.7% in accuracy.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.00713v1" target="_blank">Controllability of diffusive Lotka-Volterra strongly competitive systems under boundary constrained controls</a></h3>
                    <p><strong>Authors:</strong> Elisa Affili, Enrique Zuazua</p>
                    <p><strong>Published:</strong> 8/1/2025</p>
                    <p><strong>Categories:</strong> math.AP, math.OC, 35K57, 93B05, 35G60, 92D40</p>
                    <p><strong>Summary:</strong> We investigate the controllability of the competition-diffusion Lotka-Volterra system. Our primary focus is on the one-dimensional setting with Dirichlet boundary controls, interpreted as ecological management policies regulating the density of species at the habitat boundaries and satisfying bilateral constraints. We show that the system can be steered from any initial state to a constant steady state representing the extinction of the less competitive species. In contrast, we prove that controllability toward a steady state where the more competitive species vanishes is generally not achievable when the inter-species competition rates are too unbalanced. This obstruction is due to the existence of barrier solutions, which we explicitly construct based on the spectral properties of the associated reaction-diffusion operators. Our theoretical results are illustrated through numerical simulations and are accompanied by a discussion of open problems and potential directions for future research.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.00709v1" target="_blank">NyayaRAG: Realistic Legal Judgment Prediction with RAG under the Indian Common Law System</a></h3>
                    <p><strong>Authors:</strong> Shubham Kumar Nigam, Balaramamahanthi Deepak Patnaik, Shivam Mishra, Ajay Varghese Thomas, Noel Shallum, Kripabandhu Ghosh, Arnab Bhattacharya</p>
                    <p><strong>Published:</strong> 8/1/2025</p>
                    <p><strong>Categories:</strong> cs.CL, cs.AI, cs.IR, cs.LG</p>
                    <p><strong>Summary:</strong> Legal Judgment Prediction (LJP) has emerged as a key area in AI for law, aiming to automate judicial outcome forecasting and enhance interpretability in legal reasoning. While previous approaches in the Indian context have relied on internal case content such as facts, issues, and reasoning, they often overlook a core element of common law systems, which is reliance on statutory provisions and judicial precedents. In this work, we propose NyayaRAG, a Retrieval-Augmented Generation (RAG) framework that simulates realistic courtroom scenarios by providing models with factual case descriptions, relevant legal statutes, and semantically retrieved prior cases. NyayaRAG evaluates the effectiveness of these combined inputs in predicting court decisions and generating legal explanations using a domain-specific pipeline tailored to the Indian legal system. We assess performance across various input configurations using both standard lexical and semantic metrics as well as LLM-based evaluators such as G-Eval. Our results show that augmenting factual inputs with structured legal knowledge significantly improves both predictive accuracy and explanation quality.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.00704v1" target="_blank">XANES absorption spectra of penta-graphene and penta-SiC2 with different terminations: a computational study</a></h3>
                    <p><strong>Authors:</strong> Andrea Pedrielli, Tommaso Morresi, Simone Taioli</p>
                    <p><strong>Published:</strong> 8/1/2025</p>
                    <p><strong>Categories:</strong> cond-mat.mtrl-sci, cond-mat.mes-hall</p>
                    <p><strong>Summary:</strong> In recent research, penta-graphene and penta-SiC2 have emerged as innovative 2D materials consisting exclusively of pentagons. However, there is still a significant gap in the theoretical characterization of these materials, which hinders progress in their synthesis and potential technological applications. This study aims to close this gap by investigating the X-ray absorption near-edge spectroscopy (XANES) of these materials through ab initio calculations. In particular, we analyze the XANES spectra of penta-graphene in its pristine, hydrogenated, and hydroxylated states, and we investigate the effects of substitution by a single silicon in both penta-graphene and pentagraphane. In addition, we calculate the XANES spectra for pristine and hydrogenated penta-SiC2. This work sets the stage for the possible identification of penta-graphene and penta-SiC2 phases by X-ray spectroscopy at the experimental level and lays the foundation for the future engineering of the absorption properties of these materials in optical devices.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.00701v1" target="_blank">D3: Training-Free AI-Generated Video Detection Using Second-Order Features</a></h3>
                    <p><strong>Authors:</strong> Chende Zheng, Ruiqi suo, Chenhao Lin, Zhengyu Zhao, Le Yang, Shuai Liu, Minghui Yang, Cong Wang, Chao Shen</p>
                    <p><strong>Published:</strong> 8/1/2025</p>
                    <p><strong>Categories:</strong> cs.CV, cs.AI</p>
                    <p><strong>Summary:</strong> The evolution of video generation techniques, such as Sora, has made it increasingly easy to produce high-fidelity AI-generated videos, raising public concern over the dissemination of synthetic content. However, existing detection methodologies remain limited by their insufficient exploration of temporal artifacts in synthetic videos. To bridge this gap, we establish a theoretical framework through second-order dynamical analysis under Newtonian mechanics, subsequently extending the Second-order Central Difference features tailored for temporal artifact detection. Building on this theoretical foundation, we reveal a fundamental divergence in second-order feature distributions between real and AI-generated videos. Concretely, we propose Detection by Difference of Differences (D3), a novel training-free detection method that leverages the above second-order temporal discrepancies. We validate the superiority of our D3 on 4 open-source datasets (Gen-Video, VideoPhy, EvalCrafter, VidProM), 40 subsets in total. For example, on GenVideo, D3 outperforms the previous best method by 10.39% (absolute) mean Average Precision. Additional experiments on time cost and post-processing operations demonstrate D3s exceptional computational efficiency and strong robust performance. Our code is available at https://github.com/Zig-HS/D3.</p>
                
            
                
                    <h3><a href="http://dx.doi.org/10.7717/peerj-cs.3045" target="_blank">Classification of Psychiatry Clinical Notes by Diagnosis: A Deep Learning and Machine Learning Approach</a></h3>
                    <p><strong>Authors:</strong> Sergio Rubio-Martín, María Teresa García-Ordás, Antonio Serrano-García, Clara Margarita Franch-Pato, Arturo Crespo-Álvaro, José Alberto Benítez-Andrades</p>
                    <p><strong>Published:</strong> 8/1/2025</p>
                    <p><strong>Categories:</strong> cs.LG, cs.CL</p>
                    <p><strong>Summary:</strong> The classification of clinical notes into specific diagnostic categories is critical in healthcare, especially for mental health conditions like Anxiety and Adjustment Disorder. In this study, we compare the performance of various Artificial Intelligence models, including both traditional Machine Learning approaches (Random Forest, Support Vector Machine, K-nearest neighbors, Decision Tree, and eXtreme Gradient Boost) and Deep Learning models (DistilBERT and SciBERT), to classify clinical notes into these two diagnoses. Additionally, we implemented three oversampling strategies: No Oversampling, Random Oversampling, and Synthetic Minority Oversampling Technique (SMOTE), to assess their impact on model performance. Hyperparameter tuning was also applied to optimize model accuracy. Our results indicate that oversampling techniques had minimal impact on model performance overall. The only exception was SMOTE, which showed a positive effect specifically with BERT-based models. However, hyperparameter optimization significantly improved accuracy across the models, enhancing their ability to generalize and perform on the dataset. The Decision Tree and eXtreme Gradient Boost models achieved the highest accuracy among machine learning approaches, both reaching 96%, while the DistilBERT and SciBERT models also attained 96% accuracy in the deep learning category. These findings underscore the importance of hyperparameter tuning in maximizing model performance. This study contributes to the ongoing research on AI-assisted diagnostic tools in mental health by providing insights into the efficacy of different model architectures and data balancing methods.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.00679v1" target="_blank">Segment First, Retrieve Better: Realistic Legal Search via Rhetorical Role-Based Queries</a></h3>
                    <p><strong>Authors:</strong> Shubham Kumar Nigam, Tanmay Dubey, Noel Shallum, Arnab Bhattacharya</p>
                    <p><strong>Published:</strong> 8/1/2025</p>
                    <p><strong>Categories:</strong> cs.CL, cs.AI, cs.IR, cs.LG</p>
                    <p><strong>Summary:</strong> Legal precedent retrieval is a cornerstone of the common law system, governed by the principle of stare decisis, which demands consistency in judicial decisions. However, the growing complexity and volume of legal documents challenge traditional retrieval methods. TraceRetriever mirrors real-world legal search by operating with limited case information, extracting only rhetorically significant segments instead of requiring complete documents. Our pipeline integrates BM25, Vector Database, and Cross-Encoder models, combining initial results through Reciprocal Rank Fusion before final re-ranking. Rhetorical annotations are generated using a Hierarchical BiLSTM CRF classifier trained on Indian judgments. Evaluated on IL-PCR and COLIEE 2025 datasets, TraceRetriever addresses growing document volume challenges while aligning with practical search constraints, reliable and scalable foundation for precedent retrieval enhancing legal research when only partial case knowledge is available.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.00674v1" target="_blank">Context-Aware Visualization for Explainable AI Recommendations in Social Media: A Vision for User-Aligned Explanations</a></h3>
                    <p><strong>Authors:</strong> Banan Alkhateeb, Ellis Solaiman</p>
                    <p><strong>Published:</strong> 8/1/2025</p>
                    <p><strong>Categories:</strong> cs.AI, cs.HC, cs.LG</p>
                    <p><strong>Summary:</strong> Social media platforms today strive to improve user experience through AI recommendations, yet the value of such recommendations vanishes as users do not understand the reasons behind them. This issue arises because explainability in social media is general and lacks alignment with user-specific needs. In this vision paper, we outline a user-segmented and context-aware explanation layer by proposing a visual explanation system with diverse explanation methods. The proposed system is framed by the variety of user needs and contexts, showing explanations in different visualized forms, including a technically detailed version for AI experts and a simplified one for lay users. Our framework is the first to jointly adapt explanation style (visual vs. numeric) and granularity (expert vs. lay) inside a single pipeline. A public pilot with 30 X users will validate its impact on decision-making and trust.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.00673v1" target="_blank">MELAC: Massive Evaluation of Large Language Models with Alignment of Culture in Persian Language</a></h3>
                    <p><strong>Authors:</strong> Farhan Farsi, Farnaz Aghababaloo, Shahriar Shariati Motlagh, Parsa Ghofrani, MohammadAli SadraeiJavaheri, Shayan Bali, Amirhossein Shabani, Farbod Bijary, Ghazal Zamaninejad, AmirMohammad Salehoof, Saeedeh Momtazi</p>
                    <p><strong>Published:</strong> 8/1/2025</p>
                    <p><strong>Categories:</strong> cs.CL</p>
                    <p><strong>Summary:</strong> As large language models (LLMs) become increasingly embedded in our daily lives, evaluating their quality and reliability across diverse contexts has become essential. While comprehensive benchmarks exist for assessing LLM performance in English, there remains a significant gap in evaluation resources for other languages. Moreover, because most LLMs are trained primarily on data rooted in European and American cultures, they often lack familiarity with non-Western cultural contexts. To address this limitation, our study focuses on the Persian language and Iranian culture. We introduce 19 new evaluation datasets specifically designed to assess LLMs on topics such as Iranian law, Persian grammar, Persian idioms, and university entrance exams. Using these datasets, we benchmarked 41 prominent LLMs, aiming to bridge the existing cultural and linguistic evaluation gap in the field.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.00669v1" target="_blank">Medical Reasoning in the Era of LLMs: A Systematic Review of Enhancement Techniques and Applications</a></h3>
                    <p><strong>Authors:</strong> Wenxuan Wang, Zizhan Ma, Meidan Ding, Shiyi Zheng, Shengyuan Liu, Jie Liu, Jiaming Ji, Wenting Chen, Xiang Li, Linlin Shen, Yixuan Yuan</p>
                    <p><strong>Published:</strong> 8/1/2025</p>
                    <p><strong>Categories:</strong> cs.CL, cs.AI, cs.CV, cs.LG</p>
                    <p><strong>Summary:</strong> The proliferation of Large Language Models (LLMs) in medicine has enabled impressive capabilities, yet a critical gap remains in their ability to perform systematic, transparent, and verifiable reasoning, a cornerstone of clinical practice. This has catalyzed a shift from single-step answer generation to the development of LLMs explicitly designed for medical reasoning. This paper provides the first systematic review of this emerging field. We propose a taxonomy of reasoning enhancement techniques, categorized into training-time strategies (e.g., supervised fine-tuning, reinforcement learning) and test-time mechanisms (e.g., prompt engineering, multi-agent systems). We analyze how these techniques are applied across different data modalities (text, image, code) and in key clinical applications such as diagnosis, education, and treatment planning. Furthermore, we survey the evolution of evaluation benchmarks from simple accuracy metrics to sophisticated assessments of reasoning quality and visual interpretability. Based on an analysis of 60 seminal studies from 2022-2025, we conclude by identifying critical challenges, including the faithfulness-plausibility gap and the need for native multimodal reasoning, and outlining future directions toward building efficient, robust, and sociotechnically responsible medical AI.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.00668v1" target="_blank">Advancing Quantum Information Science Pre-College Education: The Case for Learning Sciences Collaboration</a></h3>
                    <p><strong>Authors:</strong> Raquel Coelho, Roy Pea, Christian Schunn, Jinglei Cheng, Junyu Liu</p>
                    <p><strong>Published:</strong> 8/1/2025</p>
                    <p><strong>Categories:</strong> physics.ed-ph, cs.AI, cs.CY, quant-ph</p>
                    <p><strong>Summary:</strong> As quantum information science advances and the need for pre-college engagement grows, a critical question remains: How can young learners be prepared to participate in a field so radically different from what they have encountered before? This paper argues that meeting this challenge will require strong interdisciplinary collaboration with the Learning Sciences (LS), a field dedicated to understanding how people learn and designing theory-guided environments to support learning. Drawing on lessons from previous STEM education efforts, we discuss two key contributions of the learning sciences to quantum information science (QIS) education. The first is design-based research, the signature methodology of learning sciences, which can inform the development, refinement, and scaling of effective QIS learning experiences. The second is a framework for reshaping how learners reason about, learn and participate in QIS practices through shifts in knowledge representations that provide new forms of engagement and associated learning. We call for a two-way partnership between quantum information science and the learning sciences, one that not only supports learning in quantum concepts and practices but also improves our understanding of how to teach and support learning in highly complex domains. We also consider potential questions involved in bridging these disciplinary communities and argue that the theoretical and practical benefits justify the effort.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.00665v1" target="_blank">Transparent Adaptive Learning via Data-Centric Multimodal Explainable AI</a></h3>
                    <p><strong>Authors:</strong> Maryam Mosleh, Marie Devlin, Ellis Solaiman</p>
                    <p><strong>Published:</strong> 8/1/2025</p>
                    <p><strong>Categories:</strong> cs.AI, cs.HC, cs.LG</p>
                    <p><strong>Summary:</strong> Artificial intelligence-driven adaptive learning systems are reshaping education through data-driven adaptation of learning experiences. Yet many of these systems lack transparency, offering limited insight into how decisions are made. Most explainable AI (XAI) techniques focus on technical outputs but neglect user roles and comprehension. This paper proposes a hybrid framework that integrates traditional XAI techniques with generative AI models and user personalisation to generate multimodal, personalised explanations tailored to user needs. We redefine explainability as a dynamic communication process tailored to user roles and learning goals. We outline the frameworks design, key XAI limitations in education, and research directions on accuracy, fairness, and personalisation. Our aim is to move towards explainable AI that enhances transparency while supporting user-centred experiences.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.00657v1" target="_blank">TrajSurv: Learning Continuous Latent Trajectories from Electronic Health Records for Trustworthy Survival Prediction</a></h3>
                    <p><strong>Authors:</strong> Sihang Zeng, Lucas Jing Liu, Jun Wen, Meliha Yetisgen, Ruth Etzioni, Gang Luo</p>
                    <p><strong>Published:</strong> 8/1/2025</p>
                    <p><strong>Categories:</strong> cs.LG</p>
                    <p><strong>Summary:</strong> Trustworthy survival prediction is essential for clinical decision making. Longitudinal electronic health records (EHRs) provide a uniquely powerful opportunity for the prediction. However, it is challenging to accurately model the continuous clinical progression of patients underlying the irregularly sampled clinical features and to transparently link the progression to survival outcomes. To address these challenges, we develop TrajSurv, a model that learns continuous latent trajectories from longitudinal EHR data for trustworthy survival prediction. TrajSurv employs a neural controlled differential equation (NCDE) to extract continuous-time latent states from the irregularly sampled data, forming continuous latent trajectories. To ensure the latent trajectories reflect the clinical progression, TrajSurv aligns the latent state space with patient state space through a time-aware contrastive learning approach. To transparently link clinical progression to the survival outcome, TrajSurv uses latent trajectories in a two-step divide-and-conquer interpretation process. First, it explains how the changes in clinical features translate into the latent trajectorys evolution using a learned vector field. Second, it clusters these latent trajectories to identify key clinical progression patterns associated with different survival outcomes. Evaluations on two real-world medical datasets, MIMIC-III and eICU, show TrajSurvs competitive accuracy and superior transparency over existing deep learning methods.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.00654v1" target="_blank">LEO: An Open-Source Platform for Linking OMERO with Lab Notebooks and Heterogeneous Metadata Sources</a></h3>
                    <p><strong>Authors:</strong> Rodrigo Escobar Díaz Guerrero, Jamile Mohammad Jafari, Tobias Meyer-Zedler, Michael Schmitt, Juergen Popp, Thomas Bocklitz</p>
                    <p><strong>Published:</strong> 8/1/2025</p>
                    <p><strong>Categories:</strong> cs.CE, cs.SE</p>
                    <p><strong>Summary:</strong> In the interdisciplinary field of microscopy research, managing and integrating large volumes of data stored across disparate platforms remains a major challenge. Data types such as bioimages, experimental records, and spectral information are often maintained in separate repositories, each following different management standards. However, linking these data sources across the research lifecycle is essential to align with the FAIR principles of data management: Findability, Accessibility, Interoperability, and Reusability. Despite this need, there is a notable lack of tools capable of effectively integrating and linking data from heterogeneous sources. To address this gap, we present LEO (Linking Electronic Lab Notebooks with OMERO), a web-based platform designed to create and manage links between distributed data systems. LEO was initially developed to link objects between Electronic Lab Notebooks (ELNs) and OMERO, but its functionality has since been extended through a plugin-based architecture, allowing the integration of additional data sources. This extensibility makes LEO a scalable and flexible solution for a wide range of microscopy research workflows.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.00652v1" target="_blank">The Manipulative Power of Voice Characteristics: Investigating Deceptive Patterns in Mandarin Chinese Female Synthetic Speech</a></h3>
                    <p><strong>Authors:</strong> Shuning Zhang, Han Chen, Yabo Wang, Yiqun Xu, Jiaqi Bai, Yuanyuan Wu, Shixuan Li, Xin Yi, Chunhui Wang, Hewu Li</p>
                    <p><strong>Published:</strong> 8/1/2025</p>
                    <p><strong>Categories:</strong> cs.HC</p>
                    <p><strong>Summary:</strong> Pervasive voice interaction enables deceptive patterns through subtle voice characteristics, yet empirical investigation into this manipulation lags behind, especially within major non-English language contexts. Addressing this gap, our study presents the first systematic investigation into voice characteristic-based dark patterns employing female synthetic voices in Mandarin Chinese. This focus is crucial given the prevalence of female personas in commercial assistants and the prosodic significance in the Chinese language. Guided by the conceptual framework identifying key influencing factors, we systematically evaluate effectiveness variations by manipulating voice characteristics (five characteristics, three intensities) across different scenarios (shopping vs. question-answering) with different commercial aims. A preliminary study (N=24) validated the experimental materials and the main study (N=36) revealed significant behavioral manipulation (up to +2027.6%). Crucially, the analysis showed that effectiveness varied significantly with voice characteristics and scenario, mediated by user perception (of tone, intonation, timbre) and user demographics (individual preferences, though limited demographic impact). These interconnected findings offer evidence-based insights for ethical design.</p>
                
            
                
                    <h3><a href="http://dx.doi.org/10.1145/3743049.3748581" target="_blank">Pull Requests From The Classroom: Co-Developing Curriculum And Code</a></h3>
                    <p><strong>Authors:</strong> Dennis Zyska, Ilia Kuznetsov, Florian Müller, Iryna Gurevych</p>
                    <p><strong>Published:</strong> 8/1/2025</p>
                    <p><strong>Categories:</strong> cs.HC</p>
                    <p><strong>Summary:</strong> Educational technologies often misalign with instructors pedagogical goals, forcing adaptations that compromise teaching efficacy. In this paper, we present a case study on the co-development of curriculum and technology in the context of a university course on scientific writing. Specifically, we examine how a custom-built peer feedback system was iteratively developed alongside the course to support annotation, feedback exchange, and revision. Results show that while co-development fostered stronger alignment between software features and course goals, it also exposed usability limitations and infrastructure-related frustrations, emphasizing the need for closer coordination between teaching and technical teams.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.00645v1" target="_blank">SmartFlow: A CFD-solver-agnostic deep reinforcement learning framework for computational fluid dynamics on HPC platforms</a></h3>
                    <p><strong>Authors:</strong> Maochao Xiao, Yuning Wang, Felix Rodach, Bernat Font, Marius Kurz, Pol Suárez, Di Zhou, Francisco Alcántara-Ávila, Ting Zhu, Junle Liu, Ricard Montalà, Jiawei Chen, Jean Rabault, Oriol Lehmkuhl, Andrea Beck, Johan Larsson, Ricardo Vinuesa, Sergio Pirozzoli</p>
                    <p><strong>Published:</strong> 8/1/2025</p>
                    <p><strong>Categories:</strong> physics.flu-dyn, physics.comp-ph</p>
                    <p><strong>Summary:</strong> Deep reinforcement learning (DRL) is emerging as a powerful tool for fluid-dynamics research, encompassing active flow control, autonomous navigation, turbulence modeling and discovery of novel numerical schemes. We introduce SmartFlow, a CFD-solver-agnostic framework for both single- and multi-agent DRL algorithms that can easily integrate with MPI-parallel CPU and GPU-accelerated solvers. Built on Relexi and SmartSOD2D, SmartFlow uses the SmartSim infrastructure library and our newly developed SmartRedis-MPI library to enable asynchronous, low-latency, in-memory communication between CFD solvers and Python-based DRL algorithms. SmartFlow leverages PyTorchs Stable-Baselines3 for training, which provides a modular, Gym-like environment API. We demonstrate its versatility via three case studies: single-agent synthetic-jet control for drag reduction in a cylinder flow simulated by the high-order FLEXI solver, multi-agent cylinder wake control using the GPU-accelerated spectral-element code SOD2D, and multi-agent wall-model learning for large-eddy simulation with the finite-difference solver CaLES. SmartFlows CFD-solver-agnostic design and seamless HPC integration is promising to accelerate RL-driven fluid-mechanics studies.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.00639v1" target="_blank">Minimum Data, Maximum Impact: 20 annotated samples for explainable lung nodule classification</a></h3>
                    <p><strong>Authors:</strong> Luisa Gallée, Catharina Silvia Lisson, Christoph Gerhard Lisson, Daniela Drees, Felix Weig, Daniel Vogele, Meinrad Beer, Michael Götz</p>
                    <p><strong>Published:</strong> 8/1/2025</p>
                    <p><strong>Categories:</strong> cs.CV</p>
                    <p><strong>Summary:</strong> Classification models that provide human-interpretable explanations enhance clinicians trust and usability in medical image diagnosis. One research focus is the integration and prediction of pathology-related visual attributes used by radiologists alongside the diagnosis, aligning AI decision-making with clinical reasoning. Radiologists use attributes like shape and texture as established diagnostic criteria and mirroring these in AI decision-making both enhances transparency and enables explicit validation of model outputs. However, the adoption of such models is limited by the scarcity of large-scale medical image datasets annotated with these attributes. To address this challenge, we propose synthesizing attribute-annotated data using a generative model. We enhance the Diffusion Model with attribute conditioning and train it using only 20 attribute-labeled lung nodule samples from the LIDC-IDRI dataset. Incorporating its generated images into the training of an explainable model boosts performance, increasing attribute prediction accuracy by 13.4% and target prediction accuracy by 1.8% compared to training with only the small real attribute-annotated dataset. This work highlights the potential of synthetic data to overcome dataset limitations, enhancing the applicability of explainable models in medical image analysis.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.00637v1" target="_blank">Cyber-Physical Co-Simulation of Load Frequency Control under Load-Altering Attacks</a></h3>
                    <p><strong>Authors:</strong> Michał Forystek, Andrew D. Syrmakesis, Alkistis Kontou, Panos Kotsampopoulos, Nikos D. Hatziargyriou, Charalambos Konstantinou</p>
                    <p><strong>Published:</strong> 8/1/2025</p>
                    <p><strong>Categories:</strong> eess.SY, cs.CR, cs.SY</p>
                    <p><strong>Summary:</strong> Integrating Information and Communications Technology (ICT) devices into the power grid brings many benefits. However, it also exposes the grid to new potential cyber threats. Many control and protection mechanisms, such as Load Frequency Control (LFC), responsible for maintaining nominal frequency during load fluctuations and Under Frequency Load Shedding (UFLS) disconnecting portion of the load during an emergency, are dependent on information exchange through the communication network. The recently emerging Load Altering Attacks (LAAs) utilize a botnet of high-wattage devices to introduce load fluctuation. In their dynamic form (DLAAs), they manipulate the load in response to live grid frequency measurements for increased efficiency, posing a notable threat to grid stability. Recognizing the importance of communication networks in power grid cyber security research, this paper presents an open-source co-simulation environment that models the power grid with the corresponding communication network, implementing grid protective mechanisms. This setup allows the comprehensive analysis of the attacks in concrete LFC and UFLS scenarios.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.00635v1" target="_blank">KFS: KAN based adaptive Frequency Selection learning architecture for long term time series forecasting</a></h3>
                    <p><strong>Authors:</strong> Changning Wu, Gao Wu, Rongyao Cai, Yong Liu, Kexin Zhang</p>
                    <p><strong>Published:</strong> 8/1/2025</p>
                    <p><strong>Categories:</strong> cs.LG</p>
                    <p><strong>Summary:</strong> Multi-scale decomposition architectures have emerged as predominant methodologies in time series forecasting. However, real-world time series exhibit noise interference across different scales, while heterogeneous information distribution among frequency components at varying scales leads to suboptimal multi-scale representation. Inspired by Kolmogorov-Arnold Networks (KAN) and Parsevals theorem, we propose a KAN based adaptive Frequency Selection learning architecture (KFS) to address these challenges. This framework tackles prediction challenges stemming from cross-scale noise interference and complex pattern modeling through its FreK module, which performs energy-distribution-based dominant frequency selection in the spectral domain. Simultaneously, KAN enables sophisticated pattern representation while timestamp embedding alignment synchronizes temporal representations across scales. The feature mixing module then fuses scale-specific patterns with aligned temporal features. Extensive experiments across multiple real-world time series datasets demonstrate that KT achieves state-of-the-art performance as a simple yet effective architecture.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.00632v1" target="_blank">Multi-Agent Game Generation and Evaluation via Audio-Visual Recordings</a></h3>
                    <p><strong>Authors:</strong> Alexia Jolicoeur-Martineau</p>
                    <p><strong>Published:</strong> 8/1/2025</p>
                    <p><strong>Categories:</strong> cs.AI, cs.MA, cs.MM</p>
                    <p><strong>Summary:</strong> While AI excels at generating text, audio, images, and videos, creating interactive audio-visual content such as video games remains challenging. Current LLMs can generate JavaScript games and animations, but lack automated evaluation metrics and struggle with complex content that normally requires teams of humans working for many months (multi-shot, multi-agents) using assets made by artists. To tackle these issues, we built a new metric and a multi-agent system. We propose AVR-Eval, a relative metric for multimedia content quality using Audio-Visual Recordings (AVRs). An omni-modal model (processing text, video, and audio) compares the AVRs of two contents, with a text model reviewing evaluations to determine superiority. We show that AVR-Eval properly identifies good from broken or mismatched content. We built AVR-Agent, a multi-agent system generating JavaScript code from a bank of multimedia assets (audio, images, 3D models). The coding agent selects relevant assets, generates multiple initial codes, uses AVR-Eval to identify the best version, and iteratively improves it through omni-modal agent feedback from the AVR. We run experiments on games and animations with AVR-Eval (win rate of content A against B). We find that content generated by AVR-Agent has a significantly higher win rate against content made through one-shot generation. However, models struggle to leverage custom assets and AVR feedback effectively, showing no higher win rate. This reveals a critical gap: while humans benefit from high-quality assets and audio-visual feedback, current coding models do not seem to utilize these resources as effectively, highlighting fundamental differences between human and machine content creation approaches.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.00630v1" target="_blank">MCeT: Behavioral Model Correctness Evaluation using Large Language Models</a></h3>
                    <p><strong>Authors:</strong> Khaled Ahmed, Jialing Song, Boqi Chen, Ou Wei, Bingzhou Zheng</p>
                    <p><strong>Published:</strong> 8/1/2025</p>
                    <p><strong>Categories:</strong> cs.SE</p>
                    <p><strong>Summary:</strong> Behavioral model diagrams, e.g., sequence diagrams, are an essential form of documentation that are typically designed by system engineers from requirements documentation, either fully manually or assisted by design tools. With the growing use of Large Language Models (LLM) as AI modeling assistants, more automation will be involved in generating diagrams. This necessitates the advancement of automatic model correctness evaluation tools. Such a tool can be used to evaluate both manually and AI automatically generated models; to provide feedback to system engineers, and enable AI assistants to self-evaluate and self-enhance their generated models. In this paper, we propose MCeT, the first fully automated tool to evaluate the correctness of a behavioral model, sequence diagrams in particular, against its corresponding requirements text and produce a list of issues that the model has. We utilize LLMs for the correctness evaluation tasks as they have shown outstanding natural language understanding ability. However, we show that directly asking an LLM to compare a diagram to requirements finds less than 35% of issues that experienced engineers can find. We propose to supplement the direct check with a fine-grained, multi-perspective approach; we split the diagram into atomic, non-divisible interactions, and split the requirements text into atomic, self-contained items. We compare the diagram with atomic requirements and each diagram-atom with the requirements. We also propose a self-consistency checking approach that combines perspectives to mitigate LLM hallucinated issues. Our combined approach improves upon the precision of the direct approach from 0.58 to 0.81 in a dataset of real requirements. Moreover, the approach finds 90% more issues that the experienced engineers found than the direct approach, and reports an average of 6 new issues per diagram.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.00627v1" target="_blank">IAMAP: Unlocking Deep Learning in QGIS for non-coders and limited computing resources</a></h3>
                    <p><strong>Authors:</strong> Paul Tresson, Pierre Le Coz, Hadrien Tulet, Anthony Malkassian, Maxime Réjou Méchain</p>
                    <p><strong>Published:</strong> 8/1/2025</p>
                    <p><strong>Categories:</strong> cs.LG, I.4.9; I.4.6</p>
                    <p><strong>Summary:</strong> Remote sensing has entered a new era with the rapid development of artificial intelligence approaches. However, the implementation of deep learning has largely remained restricted to specialists and has been impractical because it often requires (i) large reference datasets for model training and validation; (ii) substantial computing resources; and (iii) strong coding skills. Here, we introduce IAMAP, a user-friendly QGIS plugin that addresses these three challenges in an easy yet flexible way. IAMAP builds on recent advancements in self-supervised learning strategies, which now provide robust feature extractors, often referred to as foundation models. These generalist models can often be reliably used in few-shot or zero-shot scenarios (i.e., with little to no fine-tuning). IAMAPs interface allows users to streamline several key steps in remote sensing image analysis: (i) extracting image features using a wide range of deep learning architectures; (ii) reducing dimensionality with built-in algorithms; (iii) performing clustering on features or their reduced representations; (iv) generating feature similarity maps; and (v) calibrating and validating supervised machine learning models for prediction. By enabling non-AI specialists to leverage the high-quality features provided by recent deep learning approaches without requiring GPU capacity or extensive reference datasets, IAMAP contributes to the democratization of computationally efficient and energy-conscious deep learning methods.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.00625v1" target="_blank">OpenScout v1.1 mobile robot: a case study on open hardware continuation</a></h3>
                    <p><strong>Authors:</strong> Bartosz Krawczyk, Ahmed Elbary, Robbie Cato, Jagdish Patil, Kaung Myat, Anyeh Ndi-Tah, Nivetha Sakthivel, Mark Crampton, Gautham Das, Charles Fox</p>
                    <p><strong>Published:</strong> 8/1/2025</p>
                    <p><strong>Categories:</strong> cs.RO</p>
                    <p><strong>Summary:</strong> OpenScout is an Open Source Hardware (OSH) mobile robot for research and industry. It is extended to v1.1 which includes simplified, cheaper and more powerful onboard compute hardware; a simulated ROS2 interface; and a Gazebo simulation. Changes, their rationale, project methodology, and results are reported as an OSH case study.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.00620v1" target="_blank">Backdoor Attacks on Deep Learning Face Detection</a></h3>
                    <p><strong>Authors:</strong> Quentin Le Roux, Yannick Teglia, Teddy Furon, Philippe Loubet-Moundi</p>
                    <p><strong>Published:</strong> 8/1/2025</p>
                    <p><strong>Categories:</strong> cs.CV, cs.AI, cs.CR, cs.LG</p>
                    <p><strong>Summary:</strong> Face Recognition Systems that operate in unconstrained environments capture images under varying conditions,such as inconsistent lighting, or diverse face poses. These challenges require including a Face Detection module that regresses bounding boxes and landmark coordinates for proper Face Alignment. This paper shows the effectiveness of Object Generation Attacks on Face Detection, dubbed Face Generation Attacks, and demonstrates for the first time a Landmark Shift Attack that backdoors the coordinate regression task performed by face detectors. We then offer mitigations against these vulnerabilities.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.00823v1" target="_blank">IGL-Nav: Incremental 3D Gaussian Localization for Image-goal Navigation</a></h3>
                    <p><strong>Authors:</strong> Wenxuan Guo, Xiuwei Xu, Hang Yin, Ziwei Wang, Jianjiang Feng, Jie Zhou, Jiwen Lu</p>
                    <p><strong>Published:</strong> 8/1/2025</p>
                    <p><strong>Categories:</strong> cs.CV, cs.RO</p>
                    <p><strong>Summary:</strong> Visual navigation with an image as goal is a fundamental and challenging problem. Conventional methods either rely on end-to-end RL learning or modular-based policy with topological graph or BEV map as memory, which cannot fully model the geometric relationship between the explored 3D environment and the goal image. In order to efficiently and accurately localize the goal image in 3D space, we build our navigation system upon the renderable 3D gaussian (3DGS) representation. However, due to the computational intensity of 3DGS optimization and the large search space of 6-DoF camera pose, directly leveraging 3DGS for image localization during agent exploration process is prohibitively inefficient. To this end, we propose IGL-Nav, an Incremental 3D Gaussian Localization framework for efficient and 3D-aware image-goal navigation. Specifically, we incrementally update the scene representation as new images arrive with feed-forward monocular prediction. Then we coarsely localize the goal by leveraging the geometric information for discrete space matching, which can be equivalent to efficient 3D convolution. When the agent is close to the goal, we finally solve the fine target pose with optimization via differentiable rendering. The proposed IGL-Nav outperforms existing state-of-the-art methods by a large margin across diverse experimental configurations. It can also handle the more challenging free-view image-goal setting and be deployed on real-world robotic platform using a cellphone to capture goal image at arbitrary pose. Project page: https://gwxuan.github.io/IGL-Nav/.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.00822v1" target="_blank">Cross-Dataset Semantic Segmentation Performance Analysis: Unifying NIST Point Cloud City Datasets for 3D Deep Learning</a></h3>
                    <p><strong>Authors:</strong> Alexander Nikitas Dimopoulos, Joseph Grasso</p>
                    <p><strong>Published:</strong> 8/1/2025</p>
                    <p><strong>Categories:</strong> cs.CV</p>
                    <p><strong>Summary:</strong> This study analyzes semantic segmentation performance across heterogeneously labeled point-cloud datasets relevant to public safety applications, including pre-incident planning systems derived from lidar scans. Using NISTs Point Cloud City dataset (Enfield and Memphis collections), we investigate challenges in unifying differently labeled 3D data. Our methodology employs a graded schema with the KPConv architecture, evaluating performance through IoU metrics on safety-relevant features. Results indicate performance variability: geometrically large objects (e.g. stairs, windows) achieve higher segmentation performance, suggesting potential for navigational context, while smaller safety-critical features exhibit lower recognition rates. Performance is impacted by class imbalance and the limited geometric distinction of smaller objects in typical lidar scans, indicating limitations in detecting certain safety-relevant features using current point-cloud methods. Key identified challenges include insufficient labeled data, difficulties in unifying class labels across datasets, and the need for standardization. Potential directions include automated labeling and multi-dataset learning strategies. We conclude that reliable point-cloud semantic segmentation for public safety necessitates standardized annotation protocols and improved labeling techniques to address data heterogeneity and the detection of small, safety-critical elements.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.00819v1" target="_blank">Beyond Fixed: Variable-Length Denoising for Diffusion Large Language Models</a></h3>
                    <p><strong>Authors:</strong> Jinsong Li, Xiaoyi Dong, Yuhang Zang, Yuhang Cao, Jiaqi Wang, Dahua Lin</p>
                    <p><strong>Published:</strong> 8/1/2025</p>
                    <p><strong>Categories:</strong> cs.CL</p>
                    <p><strong>Summary:</strong> Diffusion Large Language Models (DLLMs) are emerging as a powerful alternative to the dominant Autoregressive Large Language Models, offering efficient parallel generation and capable global context modeling. However, the practical application of DLLMs is hindered by a critical architectural constraint: the need for a statically predefined generation length. This static length allocation leads to a problematic trade-off: insufficient lengths cripple performance on complex tasks, while excessive lengths incur significant computational overhead and sometimes result in performance degradation. While the inference framework is rigid, we observe that the model itself possesses internal signals that correlate with the optimal response length for a given task. To bridge this gap, we leverage these latent signals and introduce DAEDAL, a novel training-free denoising strategy that enables Dynamic Adaptive Length Expansion for Diffusion Large Language Models. DAEDAL operates in two phases: 1) Before the denoising process, DAEDAL starts from a short initial length and iteratively expands it to a coarse task-appropriate length, guided by a sequence completion metric. 2) During the denoising process, DAEDAL dynamically intervenes by pinpointing and expanding insufficient generation regions through mask token insertion, ensuring the final output is fully developed. Extensive experiments on DLLMs demonstrate that DAEDAL achieves performance comparable, and in some cases superior, to meticulously tuned fixed-length baselines, while simultaneously enhancing computational efficiency by achieving a higher effective token ratio. By resolving the static length constraint, DAEDAL unlocks new potential for DLLMs, bridging a critical gap with their Autoregressive counterparts and paving the way for more efficient and capable generation.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.00815v1" target="_blank">Search for Dark Matter Scattering from Optically Levitated Nanoparticles</a></h3>
                    <p><strong>Authors:</strong> Yu-Han Tseng, T. W. Penny, Benjamin Siegel, Jiaxiang Wang, David C. Moore</p>
                    <p><strong>Published:</strong> 8/1/2025</p>
                    <p><strong>Categories:</strong> hep-ex, hep-ph, quant-ph</p>
                    <p><strong>Summary:</strong> The development of levitated optomechanics has enabled precise force sensors that operate in the quantum measurement regime, opening up unique opportunities to search for new physics whose weak interactions may have evaded existing sensors. We demonstrate the detection of impulsive forces acting on optically levitated nanoparticles, where the dominant noise source is provided by measurement backaction. Using these sensors, we search for momentum transfers that may originate from scattering of passing particle-like dark matter. For dark matter that couples to Standard Model neutrons via a generic long-range interaction, this search constrains a range of models in the mass range 1-$10^7~\mathrm{GeV/}c^2$, placing upper limits on single neutron coupling strength as low as $\leq 1 \times 10^{-7}$ at the 95% confidence level. We also demonstrate the ability of using the inherent directional sensitivity of these sensors to separate possible dark matter signals from backgrounds. Future extensions of the techniques developed here can enable searches for light dark matter and massive neutrinos that can reach sensitivity several orders of magnitude beyond existing searches.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.00813v1" target="_blank">Entanglement swapping for partially entangled qudits and the role of quantum complementarity</a></h3>
                    <p><strong>Authors:</strong> Diego S. Starke, Marcos L. W. Basso, Lucas C. Céleri, Jonas Maziero</p>
                    <p><strong>Published:</strong> 8/1/2025</p>
                    <p><strong>Categories:</strong> quant-ph</p>
                    <p><strong>Summary:</strong> We extend the entanglement swapping protocol (ESP) to partially entangled qudit states and analyze the process within the framework of complete complementarity relations (CCRs). Building on previous results for qubits, we show that the average distributed entanglement between two parties via ESP is bounded above by the initial entanglement of one of the input pairs, and also by the product of the initial entanglements. Notably, we find that using initial states with vanishing local quantum coherence is sufficient to capture the essential features of the protocol, simplifying the analysis. By exploring the cases of qubits and qutrits, we observe that the upper bound on the average distributed entanglement -- expressed in terms of the product of the initial entanglements -- can be improved, and we conjecture what this tighter bound might be. Finally, we discuss the role of quantum complementarity in the ESP and show how local predictability constrains the entanglement that can be operationally distributed via ESP.</p>
                
            
        
        
            <p><em>Generated by AI News Agent</em></p>
        
    

