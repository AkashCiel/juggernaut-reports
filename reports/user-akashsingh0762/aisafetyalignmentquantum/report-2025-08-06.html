
    
        <h1>ðŸ¤– AI Research Report</h1>
        
            <strong>Date:</strong> 2025-08-06<br>
            <strong>Topics:</strong> ai safety research, ai alignment research, quantum computing<br>
            <strong>Papers Found:</strong> 150
        
        
        
            
                <h2>ðŸ¤– AI Summary</h2>
                <p>## ai safety research

The selected research papers present several advancements and emerging trends in AI safety research, focusing on ensuring the reliability, transparency, and alignment of AI systems with human values. Key contributions include the introduction of PAC-EIG in Bayesian Active Inverse Reinforcement Learning, which aims to align AI decision-making with human preferences by providing probably-approximately-correct guarantees even with noisy demonstrations. This approach enhances the safety of autonomous systems in critical domains like robotics and autonomous driving by efficiently identifying scenarios that require further human input to minimize errors.

Another significant trend is the development of frameworks and tools to enhance explainability and trust in AI systems. The CompassVerifier offers a unified approach to evaluate and guide the optimization of large language models (LLMs), addressing limitations in existing verification frameworks. This is complemented by DeepFaith, which proposes a domain-free, model-agnostic framework for generating highly faithful explanations, crucial for building trust in AI decision-making. Addressing the problem of hallucinations in AI outputs, MultiRAG introduces a knowledge-guided framework for retrieval-augmented generation, mitigating hallucinations by efficiently aggregating logical relationships across multiple data sources. These efforts reflect a growing emphasis on enhancing the interpretability and reliability of AI systems, crucial for their safe deployment in real-world applications. Additionally, initiatives like the proposed proto-framework for assessing the societal impact of AI systems highlight the importance of considering broader societal implications, beyond immediate risks, to ensure responsible AI development.

*Based on 50 research papers*</p>
            
        
        
        <h2>ðŸ“š Research Papers</h2>
        
                
                    <h3><a href="http://arxiv.org/abs/2508.03693v1" target="_blank">PAC Apprenticeship Learning with Bayesian Active Inverse Reinforcement Learning</a></h3>
                    <p><strong>Authors:</strong> Ondrej Bajgar, Dewi S. W. Gould, Jonathon Liu, Alessandro Abate, Konstantinos Gatsis, Michael A. Osborne</p>
                    <p><strong>Published:</strong> 8/5/2025</p>
                    <p><strong>Categories:</strong> cs.LG</p>
                    <p><strong>Summary:</strong> As AI systems become increasingly autonomous, reliably aligning their decision-making to human preferences is essential. Inverse reinforcement learning (IRL) offers a promising approach to infer preferences from demonstrations. These preferences can then be used to produce an apprentice policy that performs well on the demonstrated task. However, in domains like autonomous driving or robotics, where errors can have serious consequences, we need not just good average performance but reliable policies with formal guarantees -- yet obtaining sufficient human demonstrations for reliability guarantees can be costly. Active IRL addresses this challenge by strategically selecting the most informative scenarios for human demonstration. We introduce PAC-EIG, an information-theoretic acquisition function that directly targets probably-approximately-correct (PAC) guarantees for the learned policy -- providing the first such theoretical guarantee for active IRL with noisy expert demonstrations. Our method maximises information gain about the regret of the apprentice policy, efficiently identifying states requiring further demonstration. We also present Reward-EIG as an alternative when learning the reward itself is the primary objective. Focusing on finite state-action spaces, we prove convergence bounds, illustrate failure modes of prior heuristic methods, and demonstrate our methods advantages experimentally.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.03691v1" target="_blank">La La LiDAR: Large-Scale Layout Generation from LiDAR Data</a></h3>
                    <p><strong>Authors:</strong> Youquan Liu, Lingdong Kong, Weidong Yang, Xin Li, Ao Liang, Runnan Chen, Ben Fei, Tongliang Liu</p>
                    <p><strong>Published:</strong> 8/5/2025</p>
                    <p><strong>Categories:</strong> cs.CV, cs.RO</p>
                    <p><strong>Summary:</strong> Controllable generation of realistic LiDAR scenes is crucial for applications such as autonomous driving and robotics. While recent diffusion-based models achieve high-fidelity LiDAR generation, they lack explicit control over foreground objects and spatial relationships, limiting their usefulness for scenario simulation and safety validation. To address these limitations, we propose Large-scale Layout-guided LiDAR generation model (La La LiDAR), a novel layout-guided generative framework that introduces semantic-enhanced scene graph diffusion with relation-aware contextual conditioning for structured LiDAR layout generation, followed by foreground-aware control injection for complete scene generation. This enables customizable control over object placement while ensuring spatial and semantic consistency. To support our structured LiDAR generation, we introduce Waymo-SG and nuScenes-SG, two large-scale LiDAR scene graph datasets, along with new evaluation metrics for layout synthesis. Extensive experiments demonstrate that La La LiDAR achieves state-of-the-art performance in both LiDAR generation and downstream perception tasks, establishing a new benchmark for controllable 3D scene generation.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.03686v1" target="_blank">CompassVerifier: A Unified and Robust Verifier for LLMs Evaluation and Outcome Reward</a></h3>
                    <p><strong>Authors:</strong> Shudong Liu, Hongwei Liu, Junnan Liu, Linchen Xiao, Songyang Gao, Chengqi Lyu, Yuzhe Gu, Wenwei Zhang, Derek F. Wong, Songyang Zhang, Kai Chen</p>
                    <p><strong>Published:</strong> 8/5/2025</p>
                    <p><strong>Categories:</strong> cs.CL, cs.AI</p>
                    <p><strong>Summary:</strong> Answer verification is crucial not only for evaluating large language models (LLMs) by matching their unstructured outputs against standard answers, but also serves as the reward model to guide LLM optimization. Most evaluation frameworks rely on regularized matching or employ general LLMs for answer verification, which demands extensive, repetitive customization for regex rules or evaluation prompts. Two fundamental limitations persist in current methodologies: 1) the absence of comprehensive benchmarks that systematically evaluate verification capabilities across different LLMs; and 2) the nascent stage of verifier development, where existing approaches lack both the robustness to handle complex edge cases and the generalizability across different domains. In this work, we develop CompassVerifier, an accurate and robust lightweight verifier model for evaluation and outcome reward. It demonstrates multi-domain competency spanning math, knowledge, and diverse reasoning tasks, with the capability to process various answer types, including multi-subproblems, formulas, and sequence answers, while effectively identifying abnormal/invalid responses. We introduce VerifierBench benchmark comprising model outputs collected from multiple data sources, augmented through manual analysis of metaerror patterns to enhance CompassVerifier. We anticipate that CompassVerifier and VerifierBench will facilitate answer verification, evaluation protocols, and reinforcement learning research. Code and dataset are available at https://github.com/open-compass/CompassVerifier.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.03680v1" target="_blank">Agent Lightning: Train ANY AI Agents with Reinforcement Learning</a></h3>
                    <p><strong>Authors:</strong> Xufang Luo, Yuge Zhang, Zhiyuan He, Zilong Wang, Siyun Zhao, Dongsheng Li, Luna K. Qiu, Yuqing Yang</p>
                    <p><strong>Published:</strong> 8/5/2025</p>
                    <p><strong>Categories:</strong> cs.AI, cs.LG</p>
                    <p><strong>Summary:</strong> We present Agent Lightning, a flexible and extensible framework that enables Reinforcement Learning (RL)-based training of Large Language Models (LLMs) for any AI agent. Unlike existing methods that tightly couple RL training with agent or rely on sequence concatenation with masking, Agent Lightning achieves complete decoupling between agent execution and training, allowing seamless integration with existing agents developed via diverse ways (e.g., using frameworks like LangChain, OpenAI Agents SDK, AutoGen, and building from scratch) with almost ZERO code modifications. By formulating agent execution as Markov decision process, we define an unified data interface and propose a hierarchical RL algorithm, LightningRL, which contains a credit assignment module, allowing us to decompose trajectories generated by ANY agents into training transition. This enables RL to handle complex interaction logic, such as multi-agent scenarios and dynamic workflows. For the system design, we introduce a Training-Agent Disaggregation architecture, and brings agent observability frameworks into agent runtime, providing a standardized agent finetuning interface. Experiments across text-to-SQL, retrieval-augmented generation, and math tool-use tasks demonstrate stable, continuous improvements, showcasing the frameworks potential for real-world agent training and deployment.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.03679v1" target="_blank">Streaming Generated Gaussian Process Experts for Online Learning and Control</a></h3>
                    <p><strong>Authors:</strong> Zewen Yang, Dongfa Zhang, Xiaobing Dai, Fengyi Yu, Chi Zhang, Bingkun Huang, Hamid Sadeghian, Sami Haddadin</p>
                    <p><strong>Published:</strong> 8/5/2025</p>
                    <p><strong>Categories:</strong> cs.LG, cs.SY, eess.SY, stat.ML</p>
                    <p><strong>Summary:</strong> Gaussian Processes (GPs), as a nonparametric learning method, offer flexible modeling capabilities and calibrated uncertainty quantification for function approximations. Additionally, GPs support online learning by efficiently incorporating new data with polynomial-time computation, making them well-suited for safety-critical dynamical systems that require rapid adaptation. However, the inference and online updates of exact GPs, when processing streaming data, incur cubic computation time and quadratic storage memory complexity, limiting their scalability to large datasets in real-time settings. In this paper, we propose a \underline{s}treaming \underline{k}ernel-induced progressivel\underline{y} generated expert framework of \underline{G}aussian \underline{p}rocesses (SkyGP) that addresses both computational and memory constraints by maintaining a bounded set of experts, while inheriting the learning performance guarantees from exact Gaussian processes. Furthermore, two SkyGP variants are introduced, each tailored to a specific objective, either maximizing prediction accuracy (SkyGP-Dense) or improving computational efficiency (SkyGP-Fast). The effectiveness of SkyGP is validated through extensive benchmarks and real-time control experiments demonstrating its superior performance compared to state-of-the-art approaches.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.03676v1" target="_blank">MaLV-OS: Rethinking the Operating System Architecture for Machine Learning in Virtualized Clouds</a></h3>
                    <p><strong>Authors:</strong> Stella Bitchebe, Oana Balmau</p>
                    <p><strong>Published:</strong> 8/5/2025</p>
                    <p><strong>Categories:</strong> cs.OS, cs.LG</p>
                    <p><strong>Summary:</strong> A large body of research has employed Machine Learning (ML) models to develop learned operating systems (OSes) and kernels. The latter dynamically adapts to the job load and dynamically adjusts resources (CPU, IO, memory, network bandwidth) allocation to respond to the actual user demand. What this work has in common is that it utilizes ML to improve kernel decisions. To this day, and to the best of our knowledge, no work has taken the opposite direction, i.e., using OS to improve ML. While some work proposes applying system-level optimizations to ML algorithms, they do not tailor the OS to adapt to the ML context. To address this limitation, we take an orthogonal approach in this paper by leveraging the OS to enhance the performance of ML models and algorithms. We explore the path towards an ML-specialized OS, MaLV-OS. MaLV-OS rethinks the OS architecture to make it specifically tailored to ML workloads, especially in virtualized clouds, which are now widely used to run ML applications. MaLV-OS envisioned architecture includes (1) a micro-kernel, Micro-LAKE, which allows kernel space applications to use the GPU, and (2) an MLaaS (ML as a Service) subsystem that gathers ML models to help Micro-LAKE with memory management and CPU scheduling. MaLV-OS architecture also offloads system-sensitive parts of the models to the OS, to lighten the model complexity and programming, and speed up its execution. Finally, MaLV-OS integrates an open-source GPU virtualization software, merged directly into the hypervisor. For more flexibility, MaLV-OS vision is to enable the virtual machine to dynamically select MLaaS policies that can improve the performance of the model the user is running. Because MLaaS is designed as loadable kernel modules, the MaLV-OS architecture enables the dynamic addition of new capabilities to the MLaaS subsystem.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.03666v1" target="_blank">Beyond risk: A proto-framework for assessing the societal impact of AI systems</a></h3>
                    <p><strong>Authors:</strong> Willem Fourie</p>
                    <p><strong>Published:</strong> 8/5/2025</p>
                    <p><strong>Categories:</strong> cs.CY, cs.AI, cs.ET</p>
                    <p><strong>Summary:</strong> In the discourse on AI regulation, responsible AI is the dominant paradigm, with the focus on mitigating the risks related to AI systems. While this focus is important and necessary, it has limited use for a systematic consideration of AIs societal impact. This paper proposes a proto-framework for assessing the societal impact of AI systems by operationalising the concept of freedom. This proto-framework is intended as a step towards a fully operationalised framework to be used in policymaking contexts. By drawing on Kantian philosophy and related contemporary interpretations, freedom is developed as the counterpart to the concept of responsibility. Two dimensions of freedom are developed in further detail: freedom as capability and freedom as opportunity. These two dimensions of freedom are then applied in a proto-framework that systematically considers AIs impact on society using the Sustainable Development Goals. This proto-framework aims to complement current risk-based approaches and thereby offers a first step towards operationalising the concept of freedom in AI regulation.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.03663v1" target="_blank">Forest vs Tree: The $(N, K)$ Trade-off in Reproducible ML Evaluation</a></h3>
                    <p><strong>Authors:</strong> Deepak Pandita, Flip Korn, Chris Welty, Christopher M. Homan</p>
                    <p><strong>Published:</strong> 8/5/2025</p>
                    <p><strong>Categories:</strong> cs.LG, cs.AI, cs.CL</p>
                    <p><strong>Summary:</strong> Reproducibility is a cornerstone of scientific validation and of the authority it confers on its results. Reproducibility in machine learning evaluations leads to greater trust, confidence, and value. However, the ground truth responses used in machine learning often necessarily come from humans, among whom disagreement is prevalent, and surprisingly little research has studied the impact of effectively ignoring disagreement in these responses, as is typically the case. One reason for the lack of research is that budgets for collecting human-annotated evaluation data are limited, and obtaining more samples from multiple annotators for each example greatly increases the per-item annotation costs. We investigate the trade-off between the number of items ($N$) and the number of responses per item ($K$) needed for reliable machine learning evaluation. We analyze a diverse collection of categorical datasets for which multiple annotations per item exist, and simulated distributions fit to these datasets, to determine the optimal $(N, K)$ configuration, given a fixed budget ($N \times K$), for collecting evaluation data and reliably comparing the performance of machine learning models. Our findings show, first, that accounting for human disagreement may come with $N \times K$ at no more than 1000 (and often much lower) for every dataset tested on at least one metric. Moreover, this minimal $N \times K$ almost always occurred for $K  10$. Furthermore, the nature of the tradeoff between $K$ and $N$ -- or if one even existed -- depends on the evaluation metric, with metrics that are more sensitive to the full distribution of responses performing better at higher levels of $K$. Our methods can be used to help ML practitioners get more effective test data by finding the optimal metrics and number of items and annotations per item to collect to get the most reliability for their budget.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.03657v1" target="_blank">High-Resolution Dynamic Full-Field Optical Coherence Microscopy: Illuminating Intracellular Activity in Deep Tissue</a></h3>
                    <p><strong>Authors:</strong> Erikas Tarvydas, Austeja Treciokaite, Egidijus Auksorius</p>
                    <p><strong>Published:</strong> 8/5/2025</p>
                    <p><strong>Categories:</strong> physics.optics, physics.med-ph</p>
                    <p><strong>Summary:</strong> Dynamic full-field optical coherence microscopy (d-FF-OCM) is a label-free imaging technique that captures intrinsic subcellular motions to generate functional contrast. This dynamic approach yields images with fluorescence-like contrast, highlighting active structures without the need for fluorescent labels. However, current d-FF-OCM implementations struggle to image deep within highly scattering tissues at high resolution. Here, we present a new high-resolution d-FF-OCM system that overcomes these limitations, enabling much deeper high-resolution imaging in such tissues. The setup uses 100x oil-immersion objectives (NA = 1.25) and a high-brightness, laser-pumped incoherent white light source to achieve nanometer-scale resolution at depths up to approximately 100 micrometers in highly scattering samples. We also incorporate real-time reference arm adjustment to maintain signal strength and contrast as the focus moves deeper into the sample. Using this system, we imaged fresh ex vivo mouse liver and small intestine with unprecedented depth and detail. In these tissues, the dynamic contrast clearly revealed fine structures not visible with conventional OCT-for example, the sinusoidal microvasculature and organized cell layers in the liver, as well as neural plexuses and crypts in the intestine-all visualized label-free. By bridging the gap between high-resolution and deep imaging in highly scattering tissue, this advance provides a powerful new tool for biological microscopy, with potential applications from fundamental research to rapid intraoperative pathology.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.03651v1" target="_blank">Probing the Gaps in ChatGPT Live Video Chat for Real-World Assistance for People who are Blind or Visually Impaired</a></h3>
                    <p><strong>Authors:</strong> Ruei-Che Chang, Rosiana Natalie, Wenqian Xu, Jovan Zheng Feng Yap, Anhong Guo</p>
                    <p><strong>Published:</strong> 8/5/2025</p>
                    <p><strong>Categories:</strong> cs.HC, cs.AI</p>
                    <p><strong>Summary:</strong> Recent advancements in large multimodal models have provided blind or visually impaired (BVI) individuals with new capabilities to interpret and engage with the real world through interactive systems that utilize live video feeds. However, the potential benefits and challenges of such capabilities to support diverse real-world assistive tasks remain unclear. In this paper, we present findings from an exploratory study with eight BVI participants. Participants used ChatGPTs Advanced Voice with Video, a state-of-the-art live video AI released in late 2024, in various real-world scenarios, from locating objects to recognizing visual landmarks, across unfamiliar indoor and outdoor environments. Our findings indicate that current live video AI effectively provides guidance and answers for static visual scenes but falls short in delivering essential live descriptions required in dynamic situations. Despite inaccuracies in spatial and distance information, participants leveraged the provided visual information to supplement their mobility strategies. Although the system was perceived as human-like due to high-quality voice interactions, assumptions about users visual abilities, hallucinations, generic responses, and a tendency towards sycophancy led to confusion, distrust, and potential risks for BVI users. Based on the results, we discuss implications for assistive video AI agents, including incorporating additional sensing capabilities for real-world use, determining appropriate intervention timing beyond turn-taking interactions, and addressing ecological and safety concerns.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.03644v1" target="_blank">Are We on the Right Way for Assessing Document Retrieval-Augmented Generation?</a></h3>
                    <p><strong>Authors:</strong> Wenxuan Shen, Mingjia Wang, Yaochen Wang, Dongping Chen, Junjie Yang, Yao Wan, Weiwei Lin</p>
                    <p><strong>Published:</strong> 8/5/2025</p>
                    <p><strong>Categories:</strong> cs.CL, cs.CV, cs.IR</p>
                    <p><strong>Summary:</strong> Retrieval-Augmented Generation (RAG) systems using Multimodal Large Language Models (MLLMs) show great promise for complex document understanding, yet their development is critically hampered by inadequate evaluation. Current benchmarks often focus on specific part of document RAG system and use synthetic data with incomplete ground truth and evidence labels, therefore failing to reflect real-world bottlenecks and challenges. To overcome these limitations, we introduce Double-Bench: a new large-scale, multilingual, and multimodal evaluation system that is able to produce fine-grained assessment to each component within document RAG systems. It comprises 3,276 documents (72,880 pages) and 5,168 single- and multi-hop queries across 6 languages and 4 document types with streamlined dynamic update support for potential data contamination issues. Queries are grounded in exhaustively scanned evidence pages and verified by human experts to ensure maximum quality and completeness. Our comprehensive experiments across 9 state-of-the-art embedding models, 4 MLLMs and 4 end-to-end document RAG frameworks demonstrate the gap between text and visual embedding models is narrowing, highlighting the need in building stronger document retrieval models. Our findings also reveal the over-confidence dilemma within current document RAG frameworks that tend to provide answer even without evidence support. We hope our fully open-source Double-Bench provide a rigorous foundation for future research in advanced document RAG systems. We plan to retrieve timely corpus and release new benchmarks on an annual basis.</p>
                
            
                
                    <h3><a href="http://dx.doi.org/10.1145/3746059.3747736" target="_blank">SlideAudit: A Dataset and Taxonomy for Automated Evaluation of Presentation Slides</a></h3>
                    <p><strong>Authors:</strong> Zhuohao Jerry Zhang, Ruiqi Chen, Mingyuan Zhong, Jacob O. Wobbrock</p>
                    <p><strong>Published:</strong> 8/5/2025</p>
                    <p><strong>Categories:</strong> cs.HC</p>
                    <p><strong>Summary:</strong> Automated evaluation of specific graphic designs like presentation slides is an open problem. We present SlideAudit, a dataset for automated slide evaluation. We collaborated with design experts to develop a thorough taxonomy of slide design flaws. Our dataset comprises 2400 slides collected and synthesized from multiple sources, including a subset intentionally modified with specific design problems. We then fully annotated them using our taxonomy through strictly trained crowdsourcing from Prolific. To evaluate whether AI is capable of identifying design flaws, we compared multiple large language models under different prompting strategies, and with an existing design critique pipeline. We show that AI models struggle to accurately identify slide design flaws, with F1 scores ranging from 0.331 to 0.655. Notably, prompting techniques leveraging our taxonomy achieved the highest performance. We further conducted a remediation study to assess AIs potential for improving slides. Among 82.0% of slides that showed significant improvement, 87.8% of them were improved more with our taxonomy, further demonstrating its utility.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.03606v1" target="_blank">Demystifying Sequential Recommendations: Counterfactual Explanations via Genetic Algorithms</a></h3>
                    <p><strong>Authors:</strong> Domiziano Scarcelli, Filippo Betello, Giuseppe Perelli, Fabrizio Silvestri, Gabriele Tolomei</p>
                    <p><strong>Published:</strong> 8/5/2025</p>
                    <p><strong>Categories:</strong> cs.IR</p>
                    <p><strong>Summary:</strong> Sequential Recommender Systems (SRSs) have demonstrated remarkable effectiveness in capturing users evolving preferences. However, their inherent complexity as black box models poses significant challenges for explainability. This work presents the first counterfactual explanation technique specifically developed for SRSs, introducing a novel approach in this space, addressing the key question: What minimal changes in a users interaction history would lead to different recommendations? To achieve this, we introduce a specialized genetic algorithm tailored for discrete sequences and show that generating counterfactual explanations for sequential data is an NP-Complete problem. We evaluate these approaches across four experimental settings, varying between targeted-untargeted and categorized-uncategorized scenarios, to comprehensively assess their capability in generating meaningful explanations. Using three different datasets and three models, we are able to demonstrate that our methods successfully generate interpretable counterfactual explanation while maintaining model fidelity close to one. Our findings contribute to the growing field of Explainable AI by providing a framework for understanding sequential recommendation decisions through the lens of what-if scenarios, ultimately enhancing user trust and system transparency.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.03605v1" target="_blank">$Î²$-Ga$_2$O$_3$--Based Radiation Detector for Proton Therapy</a></h3>
                    <p><strong>Authors:</strong> Hunter D. Ellis, Imteaz Rahaman, Apostoli Hillas, Botong Li, Vikren Sarkar, Kai Fu</p>
                    <p><strong>Published:</strong> 8/5/2025</p>
                    <p><strong>Categories:</strong> physics.med-ph, cond-mat.mtrl-sci</p>
                    <p><strong>Summary:</strong> Intensity modulated proton therapy (IMPT) is an advanced cancer treatment modality that offers significant advantages over conventional X-ray therapies, particularly in its ability to minimize radiation dose beyond the tumor target. This reduction in unnecessary irradiation exposure significantly lowers the risk to surrounding healthy tissue and reduces side effects compared to conventional X-ray treatments. However, due to the high complexity of IMPT plans, each plan must be independently validated to ensure the safety and efficacy of the radiation exposure to the patient. While ion chambers are currently used for this purpose, their limitations-particularly in angled-beam measurements and multi-depth assessments-hinder their effectiveness. Silicon-based detectors, commonly used in X-ray therapy, are unsuitable for IMPT due to their rapid degradation under proton irradiation. In this study, a $\beta$-Ga$_2$O$_3$-based metal-semiconductor-metal (MSM) detector was evaluated and compared with a commercial ion chamber using a MEVION S250i proton accelerator. The $\beta$-Ga$_2$O$_3$ detector demonstrated reliable detection of single-pulse proton doses as low as 0.26 MU and exhibited a linear charge-to-dose relationship across a wide range of irradiation conditions. Furthermore, its measurement variability was comparable to that of the ion chamber, with improved sensitivity observed at higher bias voltages. These results highlight the strong potential of $\beta$-Ga$_2$O$_3$ as a radiation-hard detector material for accurate dose verification in IMPT.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.03599v1" target="_blank">OSINT or BULLSHINT? Exploring Open-Source Intelligence tweets about the Russo-Ukrainian War</a></h3>
                    <p><strong>Authors:</strong> Johannes Niu, Mila Stillman, Anna Kruspe</p>
                    <p><strong>Published:</strong> 8/5/2025</p>
                    <p><strong>Categories:</strong> cs.SI, cs.CL</p>
                    <p><strong>Summary:</strong> This paper examines the role of Open Source Intelligence (OSINT) on Twitter regarding the Russo-Ukrainian war, distinguishing between genuine OSINT and deceptive misinformation efforts, termed BULLSHINT. Utilizing a dataset spanning from January 2022 to July 2023, we analyze nearly 2 million tweets from approximately 1,040 users involved in discussing real-time military engagements, strategic analyses, and misinformation related to the conflict. Using sentiment analysis, partisanship detection, misinformation identification, and Named Entity Recognition (NER), we uncover communicative patterns and dissemination strategies within the OSINT community. Significant findings reveal a predominant negative sentiment influenced by war events, a nuanced distribution of pro-Ukrainian and pro-Russian partisanship, and the potential strategic manipulation of information. Additionally, we apply community detection techniques, which are able to identify distinct clusters partisanship, topics, and misinformation, highlighting the complex dynamics of information spread on social media. This research contributes to the understanding of digital warfare and misinformation dynamics, offering insights into the operationalization of OSINT in geopolitical conflicts.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.03596v1" target="_blank">MetaScope: Optics-Driven Neural Network for Ultra-Micro Metalens Endoscopy</a></h3>
                    <p><strong>Authors:</strong> Wuyang Li, Wentao Pan, Xiaoyuan Liu, Zhendong Luo, Chenxin Li, Hengyu Liu, Din Ping Tsai, Mu Ku Chen, Yixuan Yuan</p>
                    <p><strong>Published:</strong> 8/5/2025</p>
                    <p><strong>Categories:</strong> cs.CV, cs.AI</p>
                    <p><strong>Summary:</strong> Miniaturized endoscopy has advanced accurate visual perception within the human body. Prevailing research remains limited to conventional cameras employing convex lenses, where the physical constraints with millimetre-scale thickness impose serious impediments on the micro-level clinical. Recently, with the emergence of meta-optics, ultra-micro imaging based on metalenses (micron-scale) has garnered great attention, serving as a promising solution. However, due to the physical difference of metalens, there is a large gap in data acquisition and algorithm research. In light of this, we aim to bridge this unexplored gap, advancing the novel metalens endoscopy. First, we establish datasets for metalens endoscopy and conduct preliminary optical simulation, identifying two derived optical issues that physically adhere to strong optical priors. Second, we propose MetaScope, a novel optics-driven neural network tailored for metalens endoscopy driven by physical optics. MetaScope comprises two novel designs: Optics-informed Intensity Adjustment (OIA), rectifying intensity decay by learning optical embeddings, and Optics-informed Chromatic Correction (OCC), mitigating chromatic aberration by learning spatial deformations informed by learned Point Spread Function (PSF) distributions. To enhance joint learning, we further deploy a gradient-guided distillation to transfer knowledge from the foundational model adaptively. Extensive experiments demonstrate that MetaScope not only outperforms state-of-the-art methods in both metalens segmentation and restoration but also achieves impressive generalized ability in real biomedical scenes.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.03593v1" target="_blank">On the (In)Significance of Feature Selection in High-Dimensional Datasets</a></h3>
                    <p><strong>Authors:</strong> Bhavesh Neekhra, Debayan Gupta, Partha Pratim Chakravarti</p>
                    <p><strong>Published:</strong> 8/5/2025</p>
                    <p><strong>Categories:</strong> cs.LG, q-bio.GN, stat.ML</p>
                    <p><strong>Summary:</strong> Extensive research has been done on feature selection (FS) algorithms for high-dimensional datasets aiming to improve model performance, reduce computational cost and identify features of interest. We test the null hypothesis of using randomly selected features to compare against features selected by FS algorithms to validate the performance of the latter. Our results show that FS on high-dimensional datasets (in particular gene expression) in classification tasks is not useful. We find that (1) models trained on small subsets (0.02%-1% of all features) of randomly selected features almost always perform comparably to those trained on all features, and (2) a typical- sized random subset provides comparable or superior performance to that of top-k features selected in various published studies. Thus, our work challenges many feature selection results on high dimensional datasets, particularly in computational genomics. It raises serious concerns about studies that propose drug design or targeted interventions based on computationally selected genes, without further validation in a wet lab.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.03590v1" target="_blank">SolarSeer: Ultrafast and accurate 24-hour solar irradiance forecasts outperforming numerical weather prediction across the USA</a></h3>
                    <p><strong>Authors:</strong> Mingliang Bai, Zuliang Fang, Shengyu Tao, Siqi Xiang, Jiang Bian, Yanfei Xiang, Pengcheng Zhao, Weixin Jin, Jonathan A. Weyn, Haiyu Dong, Bin Zhang, Hongyu Sun, Kit Thambiratnam, Qi Zhang, Hongbin Sun, Xuan Zhang, Qiuwei Wu</p>
                    <p><strong>Published:</strong> 8/5/2025</p>
                    <p><strong>Categories:</strong> cs.LG, cs.CE</p>
                    <p><strong>Summary:</strong> Accurate 24-hour solar irradiance forecasting is essential for the safe and economic operation of solar photovoltaic systems. Traditional numerical weather prediction (NWP) models represent the state-of-the-art in forecasting performance but rely on computationally costly data assimilation and solving complicated partial differential equations (PDEs) that simulate atmospheric physics. Here, we introduce SolarSeer, an end-to-end large artificial intelligence (AI) model for solar irradiance forecasting across the Contiguous United States (CONUS). SolarSeer is designed to directly map the historical satellite observations to future forecasts, eliminating the computational overhead of data assimilation and PDEs solving. This efficiency allows SolarSeer to operate over 1,500 times faster than traditional NWP, generating 24-hour cloud cover and solar irradiance forecasts for the CONUS at 5-kilometer resolution in under 3 seconds. Compared with the state-of-the-art NWP in the CONUS, i.e., High-Resolution Rapid Refresh (HRRR), SolarSeer significantly reduces the root mean squared error of solar irradiance forecasting by 27.28% in reanalysis data and 15.35% across 1,800 stations. SolarSeer also effectively captures solar irradiance fluctuations and significantly enhances the first-order irradiance difference forecasting accuracy. SolarSeers ultrafast, accurate 24-hour solar irradiance forecasts provide strong support for the transition to sustainable, net-zero energy systems.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.03589v1" target="_blank">VITA: Variational Pretraining of Transformers for Climate-Robust Crop Yield Forecasting</a></h3>
                    <p><strong>Authors:</strong> Adib Hasan, Mardavij Roozbehani, Munther Dahleh</p>
                    <p><strong>Published:</strong> 8/5/2025</p>
                    <p><strong>Categories:</strong> cs.LG</p>
                    <p><strong>Summary:</strong> Accurate crop yield forecasting is essential for global food security. However, current AI models systematically underperform when yields deviate from historical trends. This issue arises from key data challenges, including a major asymmetry between rich pretraining weather datasets and the limited data available for fine-tuning. We introduce VITA (Variational Inference Transformer for Asymmetric data), a variational pretraining framework that addresses this asymmetry. Instead of relying on input reconstruction, VITA uses detailed weather variables as proxy targets during pretraining and learns to predict rich atmospheric states through self-supervised feature masking. This allows the model to be fine-tuned using only basic weather statistics during deployment. Applied to 763 counties in the U.S. Corn Belt, VITA achieves state-of-the-art performance in predicting corn and soybean yields across all evaluation scenarios. While it consistently delivers superior performance under normal conditions, its advantages are particularly pronounced during extreme weather years, with statistically significant improvements (paired t-test, $p \approx 0.01$). Importantly, VITA outperforms prior frameworks like GNN-RNN using less data, making it more practical for real-world use--particularly in data-scarce regions. This work highlights how domain-aware AI design can overcome data limitations and support resilient agricultural forecasting in a changing climate.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.03586v1" target="_blank">DeepFaith: A Domain-Free and Model-Agnostic Unified Framework for Highly Faithful Explanations</a></h3>
                    <p><strong>Authors:</strong> Yuhan Guo, Lizhong Ding, Shihan Jia, Yanyu Ren, Pengqi Li, Jiarun Fu, Changsheng Li, Ye yuan, Guoren Wang</p>
                    <p><strong>Published:</strong> 8/5/2025</p>
                    <p><strong>Categories:</strong> cs.LG, cs.AI</p>
                    <p><strong>Summary:</strong> Explainable AI (XAI) builds trust in complex systems through model attribution methods that reveal the decision rationale. However, due to the absence of a unified optimal explanation, existing XAI methods lack a ground truth for objective evaluation and optimization. To address this issue, we propose Deep architecture-based Faith explainer (DeepFaith), a domain-free and model-agnostic unified explanation framework under the lens of faithfulness. By establishing a unified formulation for multiple widely used and well-validated faithfulness metrics, we derive an optimal explanation objective whose solution simultaneously achieves optimal faithfulness across these metrics, thereby providing a ground truth from a theoretical perspective. We design an explainer learning framework that leverages multiple existing explanation methods, applies deduplicating and filtering to construct high-quality supervised explanation signals, and optimizes both pattern consistency loss and local correlation to train a faithful explainer. Once trained, DeepFaith can generate highly faithful explanations through a single forward pass without accessing the model being explained. On 12 diverse explanation tasks spanning 6 models and 6 datasets, DeepFaith achieves the highest overall faithfulness across 10 metrics compared to all baseline methods, highlighting its effectiveness and cross-domain generalizability.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.03584v1" target="_blank">Decoding and Engineering the Phytobiome Communication for Smart Agriculture</a></h3>
                    <p><strong>Authors:</strong> Fatih Gulec, Hamdan Awan, Nigel Wallbridge, Andrew W. Eckford</p>
                    <p><strong>Published:</strong> 8/5/2025</p>
                    <p><strong>Categories:</strong> eess.SP, cs.AI, cs.ET, cs.NI, q-bio.MN</p>
                    <p><strong>Summary:</strong> Smart agriculture applications, integrating technologies like the Internet of Things and machine learning/artificial intelligence (ML/AI) into agriculture, hold promise to address modern challenges of rising food demand, environmental pollution, and water scarcity. Alongside the concept of the phytobiome, which defines the area including the plant, its environment, and associated organisms, and the recent emergence of molecular communication (MC), there exists an important opportunity to advance agricultural science and practice using communication theory. In this article, we motivate to use the communication engineering perspective for developing a holistic understanding of the phytobiome communication and bridge the gap between the phytobiome communication and smart agriculture. Firstly, an overview of phytobiome communication via molecular and electrophysiological signals is presented and a multi-scale framework modeling the phytobiome as a communication network is conceptualized. Then, how this framework is used to model electrophysiological signals is demonstrated with plant experiments. Furthermore, possible smart agriculture applications, such as smart irrigation and targeted delivery of agrochemicals, through engineering the phytobiome communication are proposed. These applications merge ML/AI methods with the Internet of Bio-Nano-Things enabled by MC and pave the way towards more efficient, sustainable, and eco-friendly agricultural production. Finally, the implementation challenges, open research issues, and industrial outlook for these applications are discussed.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.03583v1" target="_blank">OpenLifelogQA: An Open-Ended Multi-Modal Lifelog Question-Answering Dataset</a></h3>
                    <p><strong>Authors:</strong> Quang-Linh Tran, Binh Nguyen, Gareth J. F. Jones, Cathal Gurrin</p>
                    <p><strong>Published:</strong> 8/5/2025</p>
                    <p><strong>Categories:</strong> cs.MM, cs.IR</p>
                    <p><strong>Summary:</strong> Lifelogging refers to the process of passively collecting, storing, and analysing personal daily life data using wearable devices. This data can support applications in memory preservation and enhancement. For example, using an ask-and-answer strategy, question-answering (QA) on lifelog data opens an interactive and interesting way to explore memorable events and insights into daily life. However, research resources for QA on lifelog data are limited to small-sized or synthetic QA datasets. In this paper, we present a novel lifelog QA dataset called OpenLifelogQA, building upon an 18-month lifelog dataset. Our dataset focuses on an open-ended and practical QA with real-world application in daily lifelog usage. We construct 14,187 pairs of QA with diverse types and difficulty levels. A baseline experiment is reported for this dataset with competitive average performance of 89.7% BERT Score, 25.87% ROUGE-L and 3.9665 LLM Score from LLaVA-NeXT-Interleave 7B model. We release this QA dataset to the research community to support new research into lifelog technologies, such as enabling personal chat-based assistants for lifelog data to become a reality.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.03565v1" target="_blank">[Technical Report] ArceKV: Towards Workload-driven LSM-compactions for Key-Value Store Under Dynamic Workloads</a></h3>
                    <p><strong>Authors:</strong> Junfeng Liu, Haoxuan Xie, Siqiang Luo</p>
                    <p><strong>Published:</strong> 8/5/2025</p>
                    <p><strong>Categories:</strong> cs.DB, H.2.0</p>
                    <p><strong>Summary:</strong> Key-value stores underpin a wide range of applications due to their simplicity and efficiency. Log-Structured Merge Trees (LSM-trees) dominate as their underlying structure, excelling at handling rapidly growing data. Recent research has focused on optimizing LSM-tree performance under static workloads with fixed read-write ratios. However, real-world workloads are highly dynamic, and existing workload-aware approaches often struggle to sustain optimal performance or incur substantial transition overhead when workload patterns shift. To address this, we propose ElasticLSM, which removes traditional LSM-tree structural constraints to allow more flexible management actions (i.e., compactions and write stalls) creating greater opportunities for continuous performance optimization. We further design Arce, a lightweight compaction decision engine that guides ElasticLSM in selecting the optimal action from its expanded action space. Building on these components, we implement ArceKV, a full-fledged key-value store atop RocksDB. Extensive evaluations demonstrate that ArceKV outperforms state-of-the-art compaction strategies across diverse workloads, delivering around 3x faster performance in dynamic scenarios.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.03564v1" target="_blank">A Scalable Machine Learning Pipeline for Building Footprint Detection in Historical Maps</a></h3>
                    <p><strong>Authors:</strong> Annemarie McCarthy</p>
                    <p><strong>Published:</strong> 8/5/2025</p>
                    <p><strong>Categories:</strong> cs.CV, I.4</p>
                    <p><strong>Summary:</strong> Historical maps offer a valuable lens through which to study past landscapes and settlement patterns. While prior research has leveraged machine learning based techniques to extract building footprints from historical maps, such approaches have largely focused on urban areas and tend to be computationally intensive. This presents a challenge for research questions requiring analysis across extensive rural regions, such as verifying historical census data or locating abandoned settlements. In this paper, this limitation is addressed by proposing a scalable and efficient pipeline tailored to rural maps with sparse building distributions. The method described employs a hierarchical machine learning based approach: convolutional neural network (CNN) classifiers are first used to progressively filter out map sections unlikely to contain buildings, significantly reducing the area requiring detailed analysis. The remaining high probability sections are then processed using CNN segmentation algorithms to extract building features. The pipeline is validated using test sections from the Ordnance Survey Ireland historical 25 inch map series and 6 inch map series, demonstrating both high performance and improved efficiency compared to conventional segmentation-only approaches. Application of the technique to both map series, covering the same geographic region, highlights its potential for historical and archaeological discovery. Notably, the pipeline identified a settlement of approximately 22 buildings in Tully, Co. Galway, present in the 6 inch map, produced in 1839, but absent from the 25 inch map, produced in 1899, suggesting it may have been abandoned during the Great Famine period.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.03562v1" target="_blank">Beyond Meme Templates: Limitations of Visual Similarity Measures in Meme Matching</a></h3>
                    <p><strong>Authors:</strong> Muzhaffar Hazman, Susan McKeever, Josephine Griffith</p>
                    <p><strong>Published:</strong> 8/5/2025</p>
                    <p><strong>Categories:</strong> cs.CV, cs.CL</p>
                    <p><strong>Summary:</strong> Internet memes, now a staple of digital communication, play a pivotal role in how users engage within online communities and allow researchers to gain insight into contemporary digital culture. These engaging user-generated content are characterised by their reuse of visual elements also found in other memes. Matching instances of memes via these shared visual elements, called Meme Matching, is the basis of a wealth of meme analysis approaches. However, most existing methods assume that every meme consists of a shared visual background, called a Template, with some overlaid text, thereby limiting meme matching to comparing the background image alone. Current approaches exclude the many memes that are not template-based and limit the effectiveness of automated meme analysis and would not be effective at linking memes to contemporary web-based meme dictionaries. In this work, we introduce a broader formulation of meme matching that extends beyond template matching. We show that conventional similarity measures, including a novel segment-wise computation of the similarity measures, excel at matching template-based memes but fall short when applied to non-template-based meme formats. However, the segment-wise approach was found to consistently outperform the whole-image measures on matching non-template-based memes. Finally, we explore a prompting-based approach using a pretrained Multimodal Large Language Model for meme matching. Our results highlight that accurately matching memes via shared visual elements, not just background templates, remains an open challenge that requires more sophisticated matching techniques.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.03561v1" target="_blank">A Simple Model of Current Ramp-Up and Ramp-Down in Tokamaks</a></h3>
                    <p><strong>Authors:</strong> R. Fitzpatrick</p>
                    <p><strong>Published:</strong> 8/5/2025</p>
                    <p><strong>Categories:</strong> physics.plasm-ph</p>
                    <p><strong>Summary:</strong> A simple model of the ramp-up and ramp-down of the toroidal current in a tokamak plasma is developed. Faradays law of electric induction is found to limit how rapidly the current can be safety ramped up or down. It is estimated that the minimum safe ramp-up/down times for the JET, SPARC, and ITER tokamaks are 4.2, 2.0, and 14.7 seconds, respectively. The JET ramp rate is in accordance with operational experience. The SPARC and ITER minimum safe ramp rates are less than the ramp rates in the respective designs. Hence, there is no indication that the design ramp rates are infeasible, as was recently suggested in arXiv:2507.05456 (2025). The typical ratios of the inductive electric field to the Connor-Hastie field in SPARC and ITER are found to be less than those in JET. Thus, the fact that the JET tokamak was able to operate successfully without encountering runaway electron problems during current ramps suggests that the future SPARC and ITER tokamaks should also be able to avoid such problems.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.03558v1" target="_blank">SAGE-HLS: Syntax-Aware AST-Guided LLM for High-Level Synthesis Code Generation</a></h3>
                    <p><strong>Authors:</strong> M Zafir Sadik Khan, Nowfel Mashnoor, Mohammad Akyash, Kimia Azar, Hadi Kamali</p>
                    <p><strong>Published:</strong> 8/5/2025</p>
                    <p><strong>Categories:</strong> cs.PL</p>
                    <p><strong>Summary:</strong> In todays rapidly evolving field of electronic design automation (EDA), the complexity of hardware designs is increasing, necessitating more sophisticated automation solutions. High-level synthesis (HLS), as a pivotal solution, automates hardware designs from high-level abstractions (e.g., C/C++). However, it faces significant challenges, particularly in design space exploration and optimization. While large language models (LLMs) have shown notable capabilities in code generation, their application to HLS has been limited due to the scarcity of (publicly) available HLS code datasets. Hence, research in this domain has primarily focused on techniques such as prompt engineering and retrieval-augmented generation (RAG). To overcome this limitation, this paper introduces SAGE-HLS, the first-of-its-kind fine-tuned LLM specifically for HLS code generation. Our method includes three key advancements: (i) We implement Verilog-to-C/C++ porting, converting verified and synthesizable Verilog codes into corresponding C, creating a dataset of 16.7K HLS codes; (ii) We implement a fine-tuning strategy, which is based on instruction prompting to code generation guided by abstract syntax tree (AST); (iii) We develop a semi-automated evaluation framework using VerilogEval to assess the functionality of the generated HLS code. Our experiments show that SAGE-HLS, fined-tuned on the QwenCoder (2.5) 7B model, achieves a near 100% success rate in code synthesizability and a 75% success rate in functional correctness.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.03555v1" target="_blank">PyLate: Flexible Training and Retrieval for Late Interaction Models</a></h3>
                    <p><strong>Authors:</strong> Antoine Chaffin, RaphaÃ«l Sourty</p>
                    <p><strong>Published:</strong> 8/5/2025</p>
                    <p><strong>Categories:</strong> cs.IR, cs.CL</p>
                    <p><strong>Summary:</strong> Neural ranking has become a cornerstone of modern information retrieval. While single vector search remains the dominant paradigm, it suffers from the shortcoming of compressing all the information into a single vector. This compression leads to notable performance degradation in out-of-domain, long-context, and reasoning-intensive retrieval tasks. Multi-vector approaches pioneered by ColBERT aim to address these limitations by preserving individual token embeddings and computing similarity via the MaxSim operator. This architecture has demonstrated superior empirical advantages, including enhanced out-of-domain generalization, long-context handling, and performance in complex retrieval scenarios. Despite these compelling empirical results and clear theoretical advantages, the practical adoption and public availability of late interaction models remain low compared to their single-vector counterparts, primarily due to a lack of accessible and modular tools for training and experimenting with such models. To bridge this gap, we introduce PyLate, a streamlined library built on top of Sentence Transformers to support multi-vector architectures natively, inheriting its efficient training, advanced logging, and automated model card generation while requiring minimal code changes to code templates users are already familiar with. By offering multi-vector-specific features such as efficient indexes, PyLate aims to accelerate research and real-world application of late interaction models, thereby unlocking their full potential in modern IR systems. Finally, PyLate has already enabled the development of state-of-the-art models, including GTE-ModernColBERT and Reason-ModernColBERT, demonstrating its practical utility for both research and production environments.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.03553v1" target="_blank">MultiRAG: A Knowledge-guided Framework for Mitigating Hallucination in Multi-source Retrieval Augmented Generation</a></h3>
                    <p><strong>Authors:</strong> Wenlong Wu, Haofen Wang, Bohan Li, Peixuan Huang, Xinzhe Zhao, Lei Liang</p>
                    <p><strong>Published:</strong> 8/5/2025</p>
                    <p><strong>Categories:</strong> cs.IR, cs.CL</p>
                    <p><strong>Summary:</strong> Retrieval Augmented Generation (RAG) has emerged as a promising solution to address hallucination issues in Large Language Models (LLMs). However, the integration of multiple retrieval sources, while potentially more informative, introduces new challenges that can paradoxically exacerbate hallucination problems. These challenges manifest primarily in two aspects: the sparse distribution of multi-source data that hinders the capture of logical relationships and the inherent inconsistencies among different sources that lead to information conflicts. To address these challenges, we propose MultiRAG, a novel framework designed to mitigate hallucination in multi-source retrieval-augmented generation through knowledge-guided approaches. Our framework introduces two key innovations: (1) a knowledge construction module that employs multi-source line graphs to efficiently aggregate logical relationships across different knowledge sources, effectively addressing the sparse data distribution issue; and (2) a sophisticated retrieval module that implements a multi-level confidence calculation mechanism, performing both graph-level and node-level assessments to identify and eliminate unreliable information nodes, thereby reducing hallucinations caused by inter-source inconsistencies. Extensive experiments on four multi-domain query datasets and two multi-hop QA datasets demonstrate that MultiRAG significantly enhances the reliability and efficiency of knowledge retrieval in complex multi-source scenarios. \textcolor{blue}{Our code is available in https://github.com/wuwenlong123/MultiRAG.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.03542v1" target="_blank">Speech-to-LaTeX: New Models and Datasets for Converting Spoken Equations and Sentences</a></h3>
                    <p><strong>Authors:</strong> Dmitrii Korzh, Dmitrii Tarasov, Artyom Iudin, Elvir Karimov, Matvey Skripkin, Nikita Kuzmin, Andrey Kuznetsov, Oleg Y. Rogov, Ivan Oseledets</p>
                    <p><strong>Published:</strong> 8/5/2025</p>
                    <p><strong>Categories:</strong> cs.CV</p>
                    <p><strong>Summary:</strong> Conversion of spoken mathematical expressions is a challenging task that involves transcribing speech into a strictly structured symbolic representation while addressing the ambiguity inherent in the pronunciation of equations. Although significant progress has been achieved in automatic speech recognition (ASR) and language models (LM), the problem of converting spoken mathematics into LaTeX remains underexplored. This task directly applies to educational and research domains, such as lecture transcription or note creation. Based on ASR post-correction, prior work requires 2 transcriptions, focuses only on isolated equations, has a limited test set, and provides neither training data nor multilingual coverage. To address these issues, we present the first fully open-source large-scale dataset, comprising over 66,000 human-annotated audio samples of mathematical equations and sentences in both English and Russian, drawn from diverse scientific domains. In addition to the ASR post-correction models and few-shot prompting, we apply audio language models, demonstrating comparable character error rate (CER) results on the MathSpeech benchmark (28% vs. 30%) for the equations conversion. In contrast, on the proposed S2L-equations benchmark, our models outperform the MathSpeech model by a substantial margin of more than 40 percentage points, even after accounting for LaTeX formatting artifacts (27% vs. 64%). We establish the first benchmark for mathematical sentence recognition (S2L-sentences) and achieve an equation CER of 40%. This work lays the groundwork for future advances in multimodal AI, with a particular focus on mathematical content recognition.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.03533v1" target="_blank">EmbedGrad: Gradient-Based Prompt Optimization in Embedding Space for Large Language Models</a></h3>
                    <p><strong>Authors:</strong> Xiaoming Hou, Jiquan Zhang, Zibin Lin, DaCheng Tao, Shengli Zhang</p>
                    <p><strong>Published:</strong> 8/5/2025</p>
                    <p><strong>Categories:</strong> cs.CL</p>
                    <p><strong>Summary:</strong> Effectively adapting powerful pretrained foundation models to diverse tasks remains a key challenge in AI deployment. Current approaches primarily follow two paradigms:discrete optimization of text prompts through prompt engineering, or continuous adaptation via additional trainable parameters. Both exhibit limitations-discrete methods lack refinement precision while parameter-based techniques increase complexity and reduce interpretability. To address these constraints, we propose EmbedGrad, a novel framework that optimizes text prompt embeddings through gradient-based refinement. Our approach uniquely decouples training from deployment:during optimization,labeled examples guide precise embedding adjustments while preserving semantic meaning; during inference, only optimized embeddings integrate with user queries. This enables fine-grained calibration impossible in text space, such as enhancing the reasoning capability of prompts like please reason step by step. Comprehensive evaluations across mathematical reasoning, sentiment analysis, and causal judgment tasks demonstrate EmbedGrads effectiveness:optimizing this reasoning prompt for Qwen2.5-Math-1.5B increased accuracy from 14.74\% to 58.96\% on mathematical problems. Consistent improvements were observed across model scales (0.5B-14B) and all tasks, with particularly significant gains for smaller models on complex problems like causal judgment. By bridging prompt engineering and parameter efficiency without architectural changes, our work establishes embedding refinement as a powerful new paradigm for task adaptation.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.03529v1" target="_blank">Marito: Structuring and Building Open Multilingual Terminologies for South African NLP</a></h3>
                    <p><strong>Authors:</strong> Vukosi Marivate, Isheanesu Dzingirai, Fiskani Banda, Richard Lastrucci, Thapelo Sindane, Keabetswe Madumo, Kayode Olaleye, Abiodun Modupe, Unarine Netshifhefhe, Herkulaas Combrink, Mohlatlego Nakeng, Matome Ledwaba</p>
                    <p><strong>Published:</strong> 8/5/2025</p>
                    <p><strong>Categories:</strong> cs.CL</p>
                    <p><strong>Summary:</strong> The critical lack of structured terminological data for South Africas official languages hampers progress in multilingual NLP, despite the existence of numerous government and academic terminology lists. These valuable assets remain fragmented and locked in non-machine-readable formats, rendering them unusable for computational research and development. \emph{Marito} addresses this challenge by systematically aggregating, cleaning, and standardising these scattered resources into open, interoperable datasets. We introduce the foundational \emph{Marito} dataset, released under the equitable, Africa-centered NOODL framework. To demonstrate its immediate utility, we integrate the terminology into a Retrieval-Augmented Generation (RAG) pipeline. Experiments show substantial improvements in the accuracy and domain-specific consistency of English-to-Tshivenda machine translation for large language models. \emph{Marito} provides a scalable foundation for developing robust and equitable NLP technologies, ensuring South Africas rich linguistic diversity is represented in the digital age.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.03526v1" target="_blank">CollaBot: Vision-Language Guided Simultaneous Collaborative Manipulation</a></h3>
                    <p><strong>Authors:</strong> Kun Song, Shentao Ma, Gaoming Chen, Ninglong Jin, Guangbao Zhao, Mingyu Ding, Zhenhua Xiong, Jia Pan</p>
                    <p><strong>Published:</strong> 8/5/2025</p>
                    <p><strong>Categories:</strong> cs.RO</p>
                    <p><strong>Summary:</strong> A central research topic in robotics is how to use this system to interact with the physical world. Traditional manipulation tasks primarily focus on small objects. However, in factory or home environments, there is often a need for the movement of large objects, such as moving tables. These tasks typically require multi-robot systems to work collaboratively. Previous research lacks a framework that can scale to arbitrary sizes of robots and generalize to various kinds of tasks. In this work, we propose CollaBot, a generalist framework for simultaneous collaborative manipulation. First, we use SEEM for scene segmentation and point cloud extraction of the target object. Then, we propose a collaborative grasping framework, which decomposes the task into local grasp pose generation and global collaboration. Finally, we design a 2-stage planning module that can generate collision-free trajectories to achieve this task. Experiments show a success rate of 52% across different numbers of robots, objects, and tasks, indicating the effectiveness of the proposed framework.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.03523v1" target="_blank">FilBench: Can LLMs Understand and Generate Filipino?</a></h3>
                    <p><strong>Authors:</strong> Lester James V. Miranda, Elyanah Aco, Conner Manuel, Jan Christian Blaise Cruz, Joseph Marvin Imperial</p>
                    <p><strong>Published:</strong> 8/5/2025</p>
                    <p><strong>Categories:</strong> cs.CL</p>
                    <p><strong>Summary:</strong> Despite the impressive performance of LLMs on English-based tasks, little is known about their capabilities in specific languages such as Filipino. In this work, we address this gap by introducing FilBench, a Filipino-centric benchmark designed to evaluate LLMs across a diverse set of tasks and capabilities in Filipino, Tagalog, and Cebuano. We carefully curate the tasks in FilBench to reflect the priorities and trends of NLP research in the Philippines such as Cultural Knowledge, Classical NLP, Reading Comprehension, and Generation. By evaluating 27 state-of-the-art LLMs on FilBench, we find that several LLMs suffer from reading comprehension and translation capabilities. Our results indicate that FilBench is challenging, with the best model, GPT-4o, achieving only a score of 72.23%. Moreover, we also find that models trained specifically for Southeast Asian languages tend to underperform on FilBench, with the highest-performing model, SEA-LION v3 70B, achieving only a score of 61.07%. Our work demonstrates the value of curating language-specific LLM benchmarks to aid in driving progress on Filipino NLP and increasing the inclusion of Philippine languages in LLM development.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.03521v1" target="_blank">Understanding Demand for Shared Autonomous Micro-Mobility</a></h3>
                    <p><strong>Authors:</strong> Naroa Coretti Sanchez, Kent Larson</p>
                    <p><strong>Published:</strong> 8/5/2025</p>
                    <p><strong>Categories:</strong> cs.ET, cs.CY</p>
                    <p><strong>Summary:</strong> This study examines the behavioral and environmental implications of shared autonomous micro-mobility systems, focusing on autonomous bicycles and their integration with transit in the U.S. While prior research has addressed operational and lifecycle aspects, a critical gap remains in understanding which modes these services are likely to substitute, who is most inclined to adopt them, and how service attributes influence user decisions. We design a context-aware stated preference survey grounded in real-world trips and estimate discrete choice models, including a hybrid model incorporating latent attitudes. Findings indicate that adoption, mode shift, and environmental impacts are highly sensitive to service design. Scenarios with minimal wait and cost yield high adoption but increase emissions, while moderate waits are more likely to reduce impacts. Adoption likelihood varies with demographic characteristics, and outcomes depend on city type, context, and infrastructure assumptions. These insights can inform the development of more sustainable and equitable mobility systems.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.03515v1" target="_blank">Oxide Interface-Based Polymorphic Electronic Devices for Neuromorphic Computing</a></h3>
                    <p><strong>Authors:</strong> Soumen Pradhan, Kirill Miller, Fabian Hartmann, Merit Spring, Judith Gabel, Berengar Leikert, Silke Kuhn, Martin Kamp, Victor Lopez-Richard, Michael Sing, Ralph Claessen, Sven HÃ¶fling</p>
                    <p><strong>Published:</strong> 8/5/2025</p>
                    <p><strong>Categories:</strong> cond-mat.dis-nn, cond-mat.mtrl-sci</p>
                    <p><strong>Summary:</strong> Aside from recent advances in artificial intelligence (AI) models, specialized AI hardware is crucial to address large volumes of unstructured and dynamic data. Hardware-based AI, built on conventional complementary metal-oxidesemiconductor (CMOS)-technology, faces several critical challenges including scaling limitation of devices [1, 2], separation of computation and memory units [3] and most importantly, overall system energy efficiency [4]. While numerous materials with emergent functionalities have been proposed to overcome these limitations, scalability, reproducibility, and compatibility remain critical obstacles [5, 6]. Here, we demonstrate oxide-interface based polymorphic electronic devices with programmable transistor, memristor, and memcapacitor functionalities by manipulating the quasi-two-dimensional electron gas in LaAlO3/SrTiO3 heterostructures [7, 8] using lateral gates. A circuit utilizing two polymorphic functionalities of transistor and memcapacitor exhibits nonlinearity and short-term memory, enabling implementation in physical reservoir computing. An integrated circuit incorporating transistor and memristor functionalities is utilized for the transition from short- to long-term synaptic plasticity and for logic operations, along with in-situ logic output storage. The same circuit with advanced reconfigurable synaptic logic operations presents high-level multi-input decision-making tasks, such as patient-monitoring in healthcare applications. Our findings pave the way for oxide-based monolithic integrated circuits in a scalable, silicon compatible, energy efficient single platform, advancing both the polymorphic and neuromorphic computings.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.03514v1" target="_blank">Theatre in the Loop: A Rehearsal-Based, Collaborative Workflow for Expressive Robotic Behaviours</a></h3>
                    <p><strong>Authors:</strong> Pavlos Panagiotidis, Victor Zhi Heung Ngo, Sean Myatt, Roma Patel, Rachel Ramchurn, Alan Chamberlain, Ayse Kucukyilmaz</p>
                    <p><strong>Published:</strong> 8/5/2025</p>
                    <p><strong>Categories:</strong> cs.RO, cs.HC</p>
                    <p><strong>Summary:</strong> In this paper, we propose theatre-in-the-loop, a framework for developing expressive robot behaviours tailored to artistic performance through a director-guided puppeteering workflow. Leveraging theatrical methods, we use narrative objectives to direct a puppeteer in generating improvised robotic gestures that convey specific emotions. These improvisations are captured and curated to build a dataset of reusable movement templates for standalone playback in future autonomous performances. Initial trials demonstrate the feasibility of this approach, illustrating how the workflow enables precise sculpting of robotic gestures into coherent emotional arcs while revealing challenges posed by the robots mechanical constraints. We argue that this practice-led framework provides a model for interdisciplinary teams creating socially expressive robot behaviours, contributing to (1) theatre as an interactive training ground for human-robot interaction and (2) co-creation methodologies between humans and machines.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.03501v1" target="_blank">Training Long-Context, Multi-Turn Software Engineering Agents with Reinforcement Learning</a></h3>
                    <p><strong>Authors:</strong> Alexander Golubev, Maria Trofimova, Sergei Polezhaev, Ibragim Badertdinov, Maksim Nekrashevich, Anton Shevtsov, Simon Karasik, Sergey Abramov, Andrei Andriushchenko, Filipp Fisin, Sergei Skvortsov, Boris Yangel</p>
                    <p><strong>Published:</strong> 8/5/2025</p>
                    <p><strong>Categories:</strong> cs.LG, cs.CL, cs.SE</p>
                    <p><strong>Summary:</strong> Research on applications of Reinforcement Learning (RL) to Large Language Models (LLMs) has mostly been focused on single-turn problems, such as mathematical reasoning or single-shot code generation. While these problems can be viewed as token-level multi-turn MDPs, this view corresponds to a degenerate case of multi-turn interaction where the environment provides no feedback. This contrasts with many real-world domains, such as software engineering (SWE), which require rich multi-turn interactions with a stateful environment that responds to each action with a non-trivial observation. To bridge this gap, we demonstrate the successful application of RL to this general regime. Using a modified Decoupled Advantage Policy Optimization (DAPO) algorithm, we train an agent based on Qwen2.5-72B-Instruct to solve real-world software engineering tasks. Our approach increases the agents success rate on the SWE-bench Verified benchmark from a 20% rejection fine-tuned baseline to 39%, without relying on any teacher models. On SWE-rebench, our agent matches or outperforms leading open-weight models such as DeepSeek-V3-0324 and Qwen3-235B-A22B using an identical scaffolding, offering a viable path toward building more capable autonomous agents for complex real-world problems based on open models.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.03498v1" target="_blank">Mapping Innovation Networks: A Network-Based Approach to Actor Heterogeneity in National Innovation Systems</a></h3>
                    <p><strong>Authors:</strong> Dawoon Jeong, Taewon Kang, Saerom Si, Sangnam Lee, Wonsub Eum</p>
                    <p><strong>Published:</strong> 8/5/2025</p>
                    <p><strong>Categories:</strong> physics.soc-ph</p>
                    <p><strong>Summary:</strong> The Triple Helix model has provided a foundational framework for analyzing National Innovation Systems by highlighting the roles of universities, industries, and government research institutes. However, increasing heterogeneity within these actor groups limits the explanatory power of typological approaches. This study introduces a capability-based network methodology that maps the structural relationships among innovation actors based on the similarity of their research and development (RD) capabilities. Drawing on Economic Complexity Theory, we measure each actors revealed comparative advantage (RCA) across scientific and technological fields and construct an RD Actor Space - a proximity-based network that reflects the relational configuration of innovation capacities. Applying this method to Korean RD data, we uncover a stratified system in which central, highly diversified universities coexist with more specialized firms and government institutes. Network analysis reveals assortative and unequal structures, and hierarchical clustering further highlights layered subgroupings. By moving beyond categorical classification, this capability-based network approach provides a scalable and generalizable tool for analyzing structural complexity within national innovation systems.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.03492v1" target="_blank">Quality Versus Sparsity in Image Recovery by Dictionary Learning Using Iterative Shrinkage</a></h3>
                    <p><strong>Authors:</strong> Mohammadsadegh Khoshghiaferezaee, Moritz Krauth, Shima Shabani, Michael BreuÃŸ</p>
                    <p><strong>Published:</strong> 8/5/2025</p>
                    <p><strong>Categories:</strong> cs.CV, 65K05, 68T30, I.4.5; I.2.6</p>
                    <p><strong>Summary:</strong> Sparse dictionary learning (SDL) is a fundamental technique that is useful for many image processing tasks. As an example we consider here image recovery, where SDL can be cast as a nonsmooth optimization problem. For this kind of problems, iterative shrinkage methods represent a powerful class of algorithms that are subject of ongoing research. Sparsity is an important property of the learned solutions, as exactly the sparsity enables efficient further processing or storage. The sparsity implies that a recovered image is determined as a combination of a number of dictionary elements that is as low as possible. Therefore, the question arises, to which degree sparsity should be enforced in SDL in order to not compromise recovery quality. In this paper we focus on the sparsity of solutions that can be obtained using a variety of optimization methods. It turns out that there are different sparsity regimes depending on the method in use. Furthermore, we illustrate that high sparsity does in general not compromise recovery quality, even if the recovered image is quite different from the learning database.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.03488v1" target="_blank">VQA support to Arabic Language Learning Educational Tool</a></h3>
                    <p><strong>Authors:</strong> Khaled Bachir Delassi, Lakhdar Zeggane, Hadda Cherroun, Abdelhamid Haouhat, Kaoutar Bouzouad</p>
                    <p><strong>Published:</strong> 8/5/2025</p>
                    <p><strong>Categories:</strong> cs.AI, cs.SE</p>
                    <p><strong>Summary:</strong> We address the problem of scarcity of educational Arabic Language Learning tools that advocate modern pedagogical models such as active learning which ensures language proficiency. In fact, we investigate the design and evaluation of an AI-powered educational tool designed to enhance Arabic language learning for non-native speakers with beginner-to-intermediate proficiency level. The tool leverages advanced AI models to generate interactive visual quizzes, deploying Visual Question Answering as the primary activity. Adopting a constructivist learning approach, the system encourages active learning through real-life visual quizzes, and image-based questions that focus on improving vocabulary, grammar, and comprehension. The system integrates Vision-Language Pretraining models to generate contextually relevant image description from which Large Language Model generate assignments based on customized Arabic language Learning quizzes thanks to prompting. The effectiveness of the tool is evaluated through a manual annotated benchmark consisting of 1266 real-life visual quizzes, with human participants providing feedback. The results show a suitable accuracy rates, validating the tools potential to bridge the gap in Arabic language education and highlighting the tools promise as a reliable, AI-powered resource for Arabic learners, offering personalized and interactive learning experiences.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.03483v1" target="_blank">When Cars Have Stereotypes: Auditing Demographic Bias in Objects from Text-to-Image Models</a></h3>
                    <p><strong>Authors:</strong> Dasol Choi Jihwan Lee, Minjae Lee, Minsuk Kahng</p>
                    <p><strong>Published:</strong> 8/5/2025</p>
                    <p><strong>Categories:</strong> cs.CV, cs.AI</p>
                    <p><strong>Summary:</strong> While prior research on text-to-image generation has predominantly focused on biases in human depictions, we investigate a more subtle yet pervasive phenomenon: demographic bias in generated objects (e.g., cars). We introduce SODA (Stereotyped Object Diagnostic Audit), a novel framework for systematically measuring such biases. Our approach compares visual attributes of objects generated with demographic cues (e.g., for young people) to those from neutral prompts, across 2,700 images produced by three state-of-the-art models (GPT Image-1, Imagen 4, and Stable Diffusion) in five object categories. Through a comprehensive analysis, we uncover strong associations between specific demographic groups and visual attributes, such as recurring color patterns prompted by gender or ethnicity cues. These patterns reflect and reinforce not only well-known stereotypes but also more subtle and unintuitive biases. We also observe that some models generate less diverse outputs, which in turn amplifies the visual disparities compared to neutral prompts. Our proposed auditing framework offers a practical approach for testing, revealing how stereotypes still remain embedded in todays generative models. We see this as an essential step toward more systematic and responsible AI development.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.03480v1" target="_blank">VideoGuard: Protecting Video Content from Unauthorized Editing</a></h3>
                    <p><strong>Authors:</strong> Junjie Cao, Kaizhou Li, Xinchun Yu, Hongxiang Li, Xiaoping Zhang</p>
                    <p><strong>Published:</strong> 8/5/2025</p>
                    <p><strong>Categories:</strong> cs.CV, cs.AI</p>
                    <p><strong>Summary:</strong> With the rapid development of generative technology, current generative models can generate high-fidelity digital content and edit it in a controlled manner. However, there is a risk that malicious individuals might misuse these capabilities for misleading activities. Although existing research has attempted to shield photographic images from being manipulated by generative models, there remains a significant disparity in the protection offered to video content editing. To bridge the gap, we propose a protection method named VideoGuard, which can effectively protect videos from unauthorized malicious editing. This protection is achieved through the subtle introduction of nearly unnoticeable perturbations that interfere with the functioning of the intended generative diffusion models. Due to the redundancy between video frames, and inter-frame attention mechanism in video diffusion models, simply applying image-based protection methods separately to every video frame can not shield video from unauthorized editing. To tackle the above challenge, we adopt joint frame optimization, treating all video frames as an optimization entity. Furthermore, we extract video motion information and fuse it into optimization objectives. Thus, these alterations can effectively force the models to produce outputs that are implausible and inconsistent. We provide a pipeline to optimize this perturbation. Finally, we use both objective metrics and subjective metrics to demonstrate the efficacy of our method, and the results show that the protection performance of VideoGuard is superior to all the baseline methods.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.03475v1" target="_blank">fact check AI at SemEval-2025 Task 7: Multilingual and Crosslingual Fact-checked Claim Retrieval</a></h3>
                    <p><strong>Authors:</strong> Pranshu Rastogi</p>
                    <p><strong>Published:</strong> 8/5/2025</p>
                    <p><strong>Categories:</strong> cs.CL, cs.AI, cs.IR</p>
                    <p><strong>Summary:</strong> SemEval-2025 Task 7: Multilingual and Crosslingual Fact-Checked Claim Retrieval is approached as a Learning-to-Rank task using a bi-encoder model fine-tuned from a pre-trained transformer optimized for sentence similarity. Training used both the source languages and their English translations for multilingual retrieval and only English translations for cross-lingual retrieval. Using lightweight models with fewer than 500M parameters and training on Kaggle T4 GPUs, the method achieved 92% Success@10 in multilingual and 80% Success@10 in 5th in crosslingual and 10th in multilingual tracks.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.03471v1" target="_blank">Learned Adaptive Indexing</a></h3>
                    <p><strong>Authors:</strong> Suvam Kumar Das, Suprio Ray</p>
                    <p><strong>Published:</strong> 8/5/2025</p>
                    <p><strong>Categories:</strong> cs.DB</p>
                    <p><strong>Summary:</strong> Indexes can significantly improve search performance in relational databases. However, if the query workload changes frequently or new data updates occur continuously, it may not be worthwhile to build a conventional index upfront for query processing. Adaptive indexing is a technique in which an index gets built on the fly as a byproduct of query processing. In recent years, research in database indexing has taken a new direction where machine learning models are employed for the purpose of indexing. These indexes, known as learned indexes, can be more efficient compared to traditional indexes such as B+-tree in terms of memory footprints and query performance. However, a learned index has to be constructed upfront and requires training the model in advance, which becomes a challenge in dynamic situations when workload changes frequently. To the best of our knowledge, no learned indexes exist yet for adaptive indexing. We propose a novel learned approach for adaptive indexing. It is built on the fly as queries are submitted and utilizes learned models for indexing data. To enhance query performance, we employ a query workload prediction technique that makes future workload projection based on past workload data. We have evaluated our learned adaptive indexing approach against existing adaptive indexes for various query workloads. Our results show that our approach performs better than others in most cases, offering 1.2x - 5.6x improvement in query performance.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.03469v1" target="_blank">IKOD: Mitigating Visual Attention Degradation in Large Vision-Language Models</a></h3>
                    <p><strong>Authors:</strong> Jiabing Yang, Chenhang Cui, Yiyang Zhou, Yixiang Chen, Peng Xia, Ying Wei, Tao Yu, Yan Huang, Liang Wang</p>
                    <p><strong>Published:</strong> 8/5/2025</p>
                    <p><strong>Categories:</strong> cs.CV</p>
                    <p><strong>Summary:</strong> Recent advancements in Large Vision-Language Models (LVLMs) have demonstrated significant progress across multiple domains. However, these models still face the inherent challenge of integrating vision and language for collaborative inference, which often leads to hallucinations, outputs that are not grounded in the corresponding images. Many efforts have been made to address these issues, but each comes with its own limitations, such as high computational cost or expensive dataset annotation. Recent research shows that LVLMs exhibit a long-term bias where hallucinations increase as the sequence length grows, yet the underlying cause remains poorly understood. Building on extensive research into attention mechanisms in LVLMs, we analyze the relationship between this long-term bias and visual attention. In our research, we identify a consistent phenomenon in current LVLMs: the models attention to visual input diminishes as the generated sequence grows, which we hypothesize to be a key factor contributing to observed increasing hallucinations. Based on these insights, we propose Image attention-guided Key-value merging cOllaborative Decoding (IKOD), a collaborative decoding strategy generating more image-focused sequences. This method derives logits from shorter sequences with higher image attention through key-value merging and combines them with those from the original decoding, effectively mitigating attention degradation and suppressing hallucinations while not incurring too much inference cost. Extensive experiments on both hallucination and comprehensive benchmarks demonstrate IKODs superior effectiveness in mitigating hallucinations and improving comprehensive capacities for LVLMs. Importantly, IKOD requires no additional training or external tools, making it a lightweight and efficient framework applicable to various models.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.03464v1" target="_blank">Learning to Incentivize: LLM-Empowered Contract for AIGC Offloading in Teleoperation</a></h3>
                    <p><strong>Authors:</strong> Zijun Zhan, Yaxian Dong, Daniel Mawunyo Doe, Yuqing Hu, Shuai Li, Shaohua Cao, Zhu Han</p>
                    <p><strong>Published:</strong> 8/5/2025</p>
                    <p><strong>Categories:</strong> cs.CE</p>
                    <p><strong>Summary:</strong> With the rapid growth in demand for AI-generated content (AIGC), edge AIGC service providers (ASPs) have become indispensable. However, designing incentive mechanisms that motivate ASPs to deliver high-quality AIGC services remains a challenge, especially in the presence of information asymmetry. In this paper, we address bonus design between a teleoperator and an edge ASP when the teleoperator cannot observe the ASPs private settings and chosen actions (diffusion steps). We formulate this as an online learning contract design problem and decompose it into two subproblems: ASPs settings inference and contract derivation. To tackle the NP-hard setting-inference subproblem with unknown variable sizes, we introduce a large language model (LLM)-empowered framework that iteratively refines a naive seed solver using the LLMs domain expertise. Upon obtaining the solution from the LLM-evolved solver, we directly address the contract derivation problem using convex optimization techniques and obtain a near-optimal contract. Simulation results on our Unity-based teleoperation platform show that our method boosts the teleoperators utility by $5 \sim 40\%$ compared to benchmarks, while preserving positive incentives for the ASP. The code is available at https://github.com/Zijun0819/llm4contract.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.03456v1" target="_blank">Decoding Polyphenol-Protein Interactions with Deep Learning: From Molecular Mechanisms to Food Applications</a></h3>
                    <p><strong>Authors:</strong> Qiang Liu, Tiantian Wang, Binbin Nian, Feiyang Ma, Siqi Zhao, AndrÃ©s F. VÃ¡squez, Liping Guo, Chao Ding, Mehdi D. Davari</p>
                    <p><strong>Published:</strong> 8/5/2025</p>
                    <p><strong>Categories:</strong> q-bio.BM</p>
                    <p><strong>Summary:</strong> Polyphenols and proteins are essential biomolecules that influence food functionality and, by extension, human health. Their interactions -- hereafter referred to as PhPIs (polyphenol-protein interactions) -- affect key processes such as nutrient bioavailability, antioxidant activity, and therapeutic efficacy. However, these interactions remain challenging due to the structural diversity of polyphenols and the dynamic nature of protein binding. Traditional experimental techniques like nuclear magnetic resonance (NMR) and mass spectrometry (MS), along with computational tools such as molecular docking and molecular dynamics (MD), have offered important insights but face constraints in scalability, throughput, and reproducibility. This review explores how deep learning (DL) is reshaping the study of PhPIs by enabling efficient prediction of binding sites, interaction affinities, and MD using high-dimensional bio- and chem-informatics data. While DL enhances prediction accuracy and reduces experimental redundancy, its effectiveness remains limited by data availability, quality, and representativeness, particularly in the context of natural products. We critically assess current DL frameworks for PhPIs analysis and outline future directions, including multimodal data integration, improved model generalizability, and development of domain-specific benchmark datasets. This synthesis offers guidance for researchers aiming to apply DL in unraveling structure-function relationships of polyphenols, accelerating discovery in nutritional science and therapeutic development.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.03446v1" target="_blank">Quantum Neural Network applications to Protein Binding Affinity Predictions</a></h3>
                    <p><strong>Authors:</strong> Erico Souza Teixeira, Lucas Barros Fernandes, Yara Rodrigues InÃ¡cio</p>
                    <p><strong>Published:</strong> 8/5/2025</p>
                    <p><strong>Categories:</strong> quant-ph, cs.LG</p>
                    <p><strong>Summary:</strong> Binding energy is a fundamental thermodynamic property that governs molecular interactions, playing a crucial role in fields such as healthcare and the natural sciences. It is particularly relevant in drug development, vaccine design, and other biomedical applications. Over the years, various methods have been developed to estimate protein binding energy, ranging from experimental techniques to computational approaches, with machine learning making significant contributions to this field. Although classical computing has demonstrated strong results in constructing predictive models, the variation of quantum computing for machine learning has emerged as a promising alternative. Quantum neural networks (QNNs) have gained traction as a research focus, raising the question of their potential advantages in predicting binding energies. To investigate this potential, this study explored the feasibility of QNNs for this task by proposing thirty variations of multilayer perceptron-based quantum neural networks. These variations span three distinct architectures, each incorporating ten different quantum circuits to configure their quantum layers. The performance of these quantum models was compared with that of a state-of-the-art classical multilayer perceptron-based artificial neural network, evaluating both accuracy and training time. A primary dataset was used for training, while two additional datasets containing entirely unseen samples were employed for testing. Results indicate that the quantum models achieved approximately 20% higher accuracy on one unseen dataset, although their accuracy was lower on the other datasets. Notably, quantum models exhibited training times several orders of magnitude shorter than their classical counterparts, highlighting their potential for efficient protein binding energy prediction.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.03444v1" target="_blank">An Auditable Agent Platform For Automated Molecular Optimisation</a></h3>
                    <p><strong>Authors:</strong> Atabey ÃœnlÃ¼, Phil Rohr, Ahmet Celebi</p>
                    <p><strong>Published:</strong> 8/5/2025</p>
                    <p><strong>Categories:</strong> cs.LG</p>
                    <p><strong>Summary:</strong> Drug discovery frequently loses momentum when data, expertise, and tools are scattered, slowing design cycles. To shorten this loop we built a hierarchical, tool using agent framework that automates molecular optimisation. A Principal Researcher defines each objective, a Database agent retrieves target information, an AI Expert generates de novo scaffolds with a sequence to molecule deep learning model, a Medicinal Chemist edits them while invoking a docking tool, a Ranking agent scores the candidates, and a Scientific Critic polices the logic. Each tool call is summarised and stored causing the full reasoning path to remain inspectable. The agents communicate through concise provenance records that capture molecular lineage, to build auditable, molecule centered reasoning trajectories and reuse successful transformations via in context learning. Three cycle research loops were run against AKT1 protein using five large language models. After ranking the models by mean docking score, we ran 20 independent scale ups on the two top performers. We then compared the leading LLMs binding affinity results across three configurations, LLM only, single agent, and multi agent. Our results reveal an architectural trade off, the multi agent setting excelled at focused binding optimization, improving average predicted binding affinity by 31%. In contrast, single agent runs generated molecules with superior drug like properties at the cost of less potent binding scores. Unguided LLM runs finished fastest, yet their lack of transparent tool signals left the validity of their reasoning paths unverified. These results show that test time scaling, focused feedback loops and provenance convert general purpose LLMs into auditable systems for molecular design, and suggest that extending the toolset to ADMET and selectivity predictors could push research workflows further along the discovery pipeline.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.03694v1" target="_blank">LongVie: Multimodal-Guided Controllable Ultra-Long Video Generation</a></h3>
                    <p><strong>Authors:</strong> Jianxiong Gao, Zhaoxi Chen, Xian Liu, Jianfeng Feng, Chenyang Si, Yanwei Fu, Yu Qiao, Ziwei Liu</p>
                    <p><strong>Published:</strong> 8/5/2025</p>
                    <p><strong>Categories:</strong> cs.CV</p>
                    <p><strong>Summary:</strong> Controllable ultra-long video generation is a fundamental yet challenging task. Although existing methods are effective for short clips, they struggle to scale due to issues such as temporal inconsistency and visual degradation. In this paper, we initially investigate and identify three key factors: separate noise initialization, independent control signal normalization, and the limitations of single-modality guidance. To address these issues, we propose LongVie, an end-to-end autoregressive framework for controllable long video generation. LongVie introduces two core designs to ensure temporal consistency: 1) a unified noise initialization strategy that maintains consistent generation across clips, and 2) global control signal normalization that enforces alignment in the control space throughout the entire video. To mitigate visual degradation, LongVie employs 3) a multi-modal control framework that integrates both dense (e.g., depth maps) and sparse (e.g., keypoints) control signals, complemented by 4) a degradation-aware training strategy that adaptively balances modality contributions over time to preserve visual quality. We also introduce LongVGenBench, a comprehensive benchmark consisting of 100 high-resolution videos spanning diverse real-world and synthetic environments, each lasting over one minute. Extensive experiments show that LongVie achieves state-of-the-art performance in long-range controllability, consistency, and quality.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.03693v1" target="_blank">PAC Apprenticeship Learning with Bayesian Active Inverse Reinforcement Learning</a></h3>
                    <p><strong>Authors:</strong> Ondrej Bajgar, Dewi S. W. Gould, Jonathon Liu, Alessandro Abate, Konstantinos Gatsis, Michael A. Osborne</p>
                    <p><strong>Published:</strong> 8/5/2025</p>
                    <p><strong>Categories:</strong> cs.LG</p>
                    <p><strong>Summary:</strong> As AI systems become increasingly autonomous, reliably aligning their decision-making to human preferences is essential. Inverse reinforcement learning (IRL) offers a promising approach to infer preferences from demonstrations. These preferences can then be used to produce an apprentice policy that performs well on the demonstrated task. However, in domains like autonomous driving or robotics, where errors can have serious consequences, we need not just good average performance but reliable policies with formal guarantees -- yet obtaining sufficient human demonstrations for reliability guarantees can be costly. Active IRL addresses this challenge by strategically selecting the most informative scenarios for human demonstration. We introduce PAC-EIG, an information-theoretic acquisition function that directly targets probably-approximately-correct (PAC) guarantees for the learned policy -- providing the first such theoretical guarantee for active IRL with noisy expert demonstrations. Our method maximises information gain about the regret of the apprentice policy, efficiently identifying states requiring further demonstration. We also present Reward-EIG as an alternative when learning the reward itself is the primary objective. Focusing on finite state-action spaces, we prove convergence bounds, illustrate failure modes of prior heuristic methods, and demonstrate our methods advantages experimentally.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.03690v1" target="_blank">Veila: Panoramic LiDAR Generation from a Monocular RGB Image</a></h3>
                    <p><strong>Authors:</strong> Youquan Liu, Lingdong Kong, Weidong Yang, Ao Liang, Jianxiong Gao, Yang Wu, Xiang Xu, Xin Li, Linfeng Li, Runnan Chen, Ben Fei</p>
                    <p><strong>Published:</strong> 8/5/2025</p>
                    <p><strong>Categories:</strong> cs.CV, cs.RO</p>
                    <p><strong>Summary:</strong> Realistic and controllable panoramic LiDAR data generation is critical for scalable 3D perception in autonomous driving and robotics. Existing methods either perform unconditional generation with poor controllability or adopt text-guided synthesis, which lacks fine-grained spatial control. Leveraging a monocular RGB image as a spatial control signal offers a scalable and low-cost alternative, which remains an open problem. However, it faces three core challenges: (i) semantic and depth cues from RGB are vary spatially, complicating reliable conditioning generation; (ii) modality gaps between RGB appearance and LiDAR geometry amplify alignment errors under noisy diffusion; and (iii) maintaining structural coherence between monocular RGB and panoramic LiDAR is challenging, particularly in non-overlap regions between images and LiDAR. To address these challenges, we propose Veila, a novel conditional diffusion framework that integrates: a Confidence-Aware Conditioning Mechanism (CACM) that strengthens RGB conditioning by adaptively balancing semantic and depth cues according to their local reliability; a Geometric Cross-Modal Alignment (GCMA) for robust RGB-LiDAR alignment under noisy diffusion; and a Panoramic Feature Coherence (PFC) for enforcing global structural consistency across monocular RGB and panoramic LiDAR. Additionally, we introduce two metrics, Cross-Modal Semantic Consistency and Cross-Modal Depth Consistency, to evaluate alignment quality across modalities. Experiments on nuScenes, SemanticKITTI, and our proposed KITTI-Weather benchmark demonstrate that Veila achieves state-of-the-art generation fidelity and cross-modal consistency, while enabling generative data augmentation that improves downstream LiDAR semantic segmentation.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.03686v1" target="_blank">CompassVerifier: A Unified and Robust Verifier for LLMs Evaluation and Outcome Reward</a></h3>
                    <p><strong>Authors:</strong> Shudong Liu, Hongwei Liu, Junnan Liu, Linchen Xiao, Songyang Gao, Chengqi Lyu, Yuzhe Gu, Wenwei Zhang, Derek F. Wong, Songyang Zhang, Kai Chen</p>
                    <p><strong>Published:</strong> 8/5/2025</p>
                    <p><strong>Categories:</strong> cs.CL, cs.AI</p>
                    <p><strong>Summary:</strong> Answer verification is crucial not only for evaluating large language models (LLMs) by matching their unstructured outputs against standard answers, but also serves as the reward model to guide LLM optimization. Most evaluation frameworks rely on regularized matching or employ general LLMs for answer verification, which demands extensive, repetitive customization for regex rules or evaluation prompts. Two fundamental limitations persist in current methodologies: 1) the absence of comprehensive benchmarks that systematically evaluate verification capabilities across different LLMs; and 2) the nascent stage of verifier development, where existing approaches lack both the robustness to handle complex edge cases and the generalizability across different domains. In this work, we develop CompassVerifier, an accurate and robust lightweight verifier model for evaluation and outcome reward. It demonstrates multi-domain competency spanning math, knowledge, and diverse reasoning tasks, with the capability to process various answer types, including multi-subproblems, formulas, and sequence answers, while effectively identifying abnormal/invalid responses. We introduce VerifierBench benchmark comprising model outputs collected from multiple data sources, augmented through manual analysis of metaerror patterns to enhance CompassVerifier. We anticipate that CompassVerifier and VerifierBench will facilitate answer verification, evaluation protocols, and reinforcement learning research. Code and dataset are available at https://github.com/open-compass/CompassVerifier.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.03680v1" target="_blank">Agent Lightning: Train ANY AI Agents with Reinforcement Learning</a></h3>
                    <p><strong>Authors:</strong> Xufang Luo, Yuge Zhang, Zhiyuan He, Zilong Wang, Siyun Zhao, Dongsheng Li, Luna K. Qiu, Yuqing Yang</p>
                    <p><strong>Published:</strong> 8/5/2025</p>
                    <p><strong>Categories:</strong> cs.AI, cs.LG</p>
                    <p><strong>Summary:</strong> We present Agent Lightning, a flexible and extensible framework that enables Reinforcement Learning (RL)-based training of Large Language Models (LLMs) for any AI agent. Unlike existing methods that tightly couple RL training with agent or rely on sequence concatenation with masking, Agent Lightning achieves complete decoupling between agent execution and training, allowing seamless integration with existing agents developed via diverse ways (e.g., using frameworks like LangChain, OpenAI Agents SDK, AutoGen, and building from scratch) with almost ZERO code modifications. By formulating agent execution as Markov decision process, we define an unified data interface and propose a hierarchical RL algorithm, LightningRL, which contains a credit assignment module, allowing us to decompose trajectories generated by ANY agents into training transition. This enables RL to handle complex interaction logic, such as multi-agent scenarios and dynamic workflows. For the system design, we introduce a Training-Agent Disaggregation architecture, and brings agent observability frameworks into agent runtime, providing a standardized agent finetuning interface. Experiments across text-to-SQL, retrieval-augmented generation, and math tool-use tasks demonstrate stable, continuous improvements, showcasing the frameworks potential for real-world agent training and deployment.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.03676v1" target="_blank">MaLV-OS: Rethinking the Operating System Architecture for Machine Learning in Virtualized Clouds</a></h3>
                    <p><strong>Authors:</strong> Stella Bitchebe, Oana Balmau</p>
                    <p><strong>Published:</strong> 8/5/2025</p>
                    <p><strong>Categories:</strong> cs.OS, cs.LG</p>
                    <p><strong>Summary:</strong> A large body of research has employed Machine Learning (ML) models to develop learned operating systems (OSes) and kernels. The latter dynamically adapts to the job load and dynamically adjusts resources (CPU, IO, memory, network bandwidth) allocation to respond to the actual user demand. What this work has in common is that it utilizes ML to improve kernel decisions. To this day, and to the best of our knowledge, no work has taken the opposite direction, i.e., using OS to improve ML. While some work proposes applying system-level optimizations to ML algorithms, they do not tailor the OS to adapt to the ML context. To address this limitation, we take an orthogonal approach in this paper by leveraging the OS to enhance the performance of ML models and algorithms. We explore the path towards an ML-specialized OS, MaLV-OS. MaLV-OS rethinks the OS architecture to make it specifically tailored to ML workloads, especially in virtualized clouds, which are now widely used to run ML applications. MaLV-OS envisioned architecture includes (1) a micro-kernel, Micro-LAKE, which allows kernel space applications to use the GPU, and (2) an MLaaS (ML as a Service) subsystem that gathers ML models to help Micro-LAKE with memory management and CPU scheduling. MaLV-OS architecture also offloads system-sensitive parts of the models to the OS, to lighten the model complexity and programming, and speed up its execution. Finally, MaLV-OS integrates an open-source GPU virtualization software, merged directly into the hypervisor. For more flexibility, MaLV-OS vision is to enable the virtual machine to dynamically select MLaaS policies that can improve the performance of the model the user is running. Because MLaaS is designed as loadable kernel modules, the MaLV-OS architecture enables the dynamic addition of new capabilities to the MLaaS subsystem.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.03672v1" target="_blank">Inland-LOAM: Voxel-Based Structural Semantic Mapping for Inland Waterways</a></h3>
                    <p><strong>Authors:</strong> Zhongbi Luo, Yunjia Wang, Jan Swevers, Peter Slaets, Herman Bruyninckx</p>
                    <p><strong>Published:</strong> 8/5/2025</p>
                    <p><strong>Categories:</strong> cs.RO</p>
                    <p><strong>Summary:</strong> Accurate geospatial information is crucial for safe, autonomous Inland Waterway Transport (IWT), as existing charts (IENC) lack real-time detail and conventional LiDAR SLAM fails in waterway environments. These challenges lead to vertical drift and non-semantic maps, hindering autonomous navigation. This paper introduces Inland-LOAM, a LiDAR SLAM framework for waterways. It uses an improved feature extraction and a water surface planar constraint to mitigate vertical drift. A novel pipeline transforms 3D point clouds into structured 2D semantic maps using voxel-based geometric analysis, enabling real-time computation of navigational parameters like bridge clearances. An automated module extracts shorelines and exports them into a lightweight, IENC-compatible format. Evaluations on a real-world dataset show Inland-LOAM achieves superior localization accuracy over state-of-the-art methods. The generated semantic maps and shorelines align with real-world conditions, providing reliable data for enhanced situational awareness. The code and dataset will be publicly available</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.03666v1" target="_blank">Beyond risk: A proto-framework for assessing the societal impact of AI systems</a></h3>
                    <p><strong>Authors:</strong> Willem Fourie</p>
                    <p><strong>Published:</strong> 8/5/2025</p>
                    <p><strong>Categories:</strong> cs.CY, cs.AI, cs.ET</p>
                    <p><strong>Summary:</strong> In the discourse on AI regulation, responsible AI is the dominant paradigm, with the focus on mitigating the risks related to AI systems. While this focus is important and necessary, it has limited use for a systematic consideration of AIs societal impact. This paper proposes a proto-framework for assessing the societal impact of AI systems by operationalising the concept of freedom. This proto-framework is intended as a step towards a fully operationalised framework to be used in policymaking contexts. By drawing on Kantian philosophy and related contemporary interpretations, freedom is developed as the counterpart to the concept of responsibility. Two dimensions of freedom are developed in further detail: freedom as capability and freedom as opportunity. These two dimensions of freedom are then applied in a proto-framework that systematically considers AIs impact on society using the Sustainable Development Goals. This proto-framework aims to complement current risk-based approaches and thereby offers a first step towards operationalising the concept of freedom in AI regulation.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.03663v1" target="_blank">Forest vs Tree: The $(N, K)$ Trade-off in Reproducible ML Evaluation</a></h3>
                    <p><strong>Authors:</strong> Deepak Pandita, Flip Korn, Chris Welty, Christopher M. Homan</p>
                    <p><strong>Published:</strong> 8/5/2025</p>
                    <p><strong>Categories:</strong> cs.LG, cs.AI, cs.CL</p>
                    <p><strong>Summary:</strong> Reproducibility is a cornerstone of scientific validation and of the authority it confers on its results. Reproducibility in machine learning evaluations leads to greater trust, confidence, and value. However, the ground truth responses used in machine learning often necessarily come from humans, among whom disagreement is prevalent, and surprisingly little research has studied the impact of effectively ignoring disagreement in these responses, as is typically the case. One reason for the lack of research is that budgets for collecting human-annotated evaluation data are limited, and obtaining more samples from multiple annotators for each example greatly increases the per-item annotation costs. We investigate the trade-off between the number of items ($N$) and the number of responses per item ($K$) needed for reliable machine learning evaluation. We analyze a diverse collection of categorical datasets for which multiple annotations per item exist, and simulated distributions fit to these datasets, to determine the optimal $(N, K)$ configuration, given a fixed budget ($N \times K$), for collecting evaluation data and reliably comparing the performance of machine learning models. Our findings show, first, that accounting for human disagreement may come with $N \times K$ at no more than 1000 (and often much lower) for every dataset tested on at least one metric. Moreover, this minimal $N \times K$ almost always occurred for $K  10$. Furthermore, the nature of the tradeoff between $K$ and $N$ -- or if one even existed -- depends on the evaluation metric, with metrics that are more sensitive to the full distribution of responses performing better at higher levels of $K$. Our methods can be used to help ML practitioners get more effective test data by finding the optimal metrics and number of items and annotations per item to collect to get the most reliability for their budget.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.03657v1" target="_blank">High-Resolution Dynamic Full-Field Optical Coherence Microscopy: Illuminating Intracellular Activity in Deep Tissue</a></h3>
                    <p><strong>Authors:</strong> Erikas Tarvydas, Austeja Treciokaite, Egidijus Auksorius</p>
                    <p><strong>Published:</strong> 8/5/2025</p>
                    <p><strong>Categories:</strong> physics.optics, physics.med-ph</p>
                    <p><strong>Summary:</strong> Dynamic full-field optical coherence microscopy (d-FF-OCM) is a label-free imaging technique that captures intrinsic subcellular motions to generate functional contrast. This dynamic approach yields images with fluorescence-like contrast, highlighting active structures without the need for fluorescent labels. However, current d-FF-OCM implementations struggle to image deep within highly scattering tissues at high resolution. Here, we present a new high-resolution d-FF-OCM system that overcomes these limitations, enabling much deeper high-resolution imaging in such tissues. The setup uses 100x oil-immersion objectives (NA = 1.25) and a high-brightness, laser-pumped incoherent white light source to achieve nanometer-scale resolution at depths up to approximately 100 micrometers in highly scattering samples. We also incorporate real-time reference arm adjustment to maintain signal strength and contrast as the focus moves deeper into the sample. Using this system, we imaged fresh ex vivo mouse liver and small intestine with unprecedented depth and detail. In these tissues, the dynamic contrast clearly revealed fine structures not visible with conventional OCT-for example, the sinusoidal microvasculature and organized cell layers in the liver, as well as neural plexuses and crypts in the intestine-all visualized label-free. By bridging the gap between high-resolution and deep imaging in highly scattering tissue, this advance provides a powerful new tool for biological microscopy, with potential applications from fundamental research to rapid intraoperative pathology.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.03651v1" target="_blank">Probing the Gaps in ChatGPT Live Video Chat for Real-World Assistance for People who are Blind or Visually Impaired</a></h3>
                    <p><strong>Authors:</strong> Ruei-Che Chang, Rosiana Natalie, Wenqian Xu, Jovan Zheng Feng Yap, Anhong Guo</p>
                    <p><strong>Published:</strong> 8/5/2025</p>
                    <p><strong>Categories:</strong> cs.HC, cs.AI</p>
                    <p><strong>Summary:</strong> Recent advancements in large multimodal models have provided blind or visually impaired (BVI) individuals with new capabilities to interpret and engage with the real world through interactive systems that utilize live video feeds. However, the potential benefits and challenges of such capabilities to support diverse real-world assistive tasks remain unclear. In this paper, we present findings from an exploratory study with eight BVI participants. Participants used ChatGPTs Advanced Voice with Video, a state-of-the-art live video AI released in late 2024, in various real-world scenarios, from locating objects to recognizing visual landmarks, across unfamiliar indoor and outdoor environments. Our findings indicate that current live video AI effectively provides guidance and answers for static visual scenes but falls short in delivering essential live descriptions required in dynamic situations. Despite inaccuracies in spatial and distance information, participants leveraged the provided visual information to supplement their mobility strategies. Although the system was perceived as human-like due to high-quality voice interactions, assumptions about users visual abilities, hallucinations, generic responses, and a tendency towards sycophancy led to confusion, distrust, and potential risks for BVI users. Based on the results, we discuss implications for assistive video AI agents, including incorporating additional sensing capabilities for real-world use, determining appropriate intervention timing beyond turn-taking interactions, and addressing ecological and safety concerns.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.03649v1" target="_blank">Cross-Model Semantics in Representation Learning</a></h3>
                    <p><strong>Authors:</strong> Saleh Nikooroo, Thomas Engel</p>
                    <p><strong>Published:</strong> 8/5/2025</p>
                    <p><strong>Categories:</strong> cs.LG, cs.AI</p>
                    <p><strong>Summary:</strong> The internal representations learned by deep networks are often sensitive to architecture-specific choices, raising questions about the stability, alignment, and transferability of learned structure across models. In this paper, we investigate how structural constraints--such as linear shaping operators and corrective paths--affect the compatibility of internal representations across different architectures. Building on the insights from prior studies on structured transformations and convergence, we develop a framework for measuring and analyzing representational alignment across networks with distinct but related architectural priors. Through a combination of theoretical insights, empirical probes, and controlled transfer experiments, we demonstrate that structural regularities induce representational geometry that is more stable under architectural variation. This suggests that certain forms of inductive bias not only support generalization within a model, but also improve the interoperability of learned features across models. We conclude with a discussion on the implications of representational transferability for model distillation, modular learning, and the principled design of robust learning systems.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.03644v1" target="_blank">Are We on the Right Way for Assessing Document Retrieval-Augmented Generation?</a></h3>
                    <p><strong>Authors:</strong> Wenxuan Shen, Mingjia Wang, Yaochen Wang, Dongping Chen, Junjie Yang, Yao Wan, Weiwei Lin</p>
                    <p><strong>Published:</strong> 8/5/2025</p>
                    <p><strong>Categories:</strong> cs.CL, cs.CV, cs.IR</p>
                    <p><strong>Summary:</strong> Retrieval-Augmented Generation (RAG) systems using Multimodal Large Language Models (MLLMs) show great promise for complex document understanding, yet their development is critically hampered by inadequate evaluation. Current benchmarks often focus on specific part of document RAG system and use synthetic data with incomplete ground truth and evidence labels, therefore failing to reflect real-world bottlenecks and challenges. To overcome these limitations, we introduce Double-Bench: a new large-scale, multilingual, and multimodal evaluation system that is able to produce fine-grained assessment to each component within document RAG systems. It comprises 3,276 documents (72,880 pages) and 5,168 single- and multi-hop queries across 6 languages and 4 document types with streamlined dynamic update support for potential data contamination issues. Queries are grounded in exhaustively scanned evidence pages and verified by human experts to ensure maximum quality and completeness. Our comprehensive experiments across 9 state-of-the-art embedding models, 4 MLLMs and 4 end-to-end document RAG frameworks demonstrate the gap between text and visual embedding models is narrowing, highlighting the need in building stronger document retrieval models. Our findings also reveal the over-confidence dilemma within current document RAG frameworks that tend to provide answer even without evidence support. We hope our fully open-source Double-Bench provide a rigorous foundation for future research in advanced document RAG systems. We plan to retrieve timely corpus and release new benchmarks on an annual basis.</p>
                
            
                
                    <h3><a href="http://dx.doi.org/10.1145/3746059.3747736" target="_blank">SlideAudit: A Dataset and Taxonomy for Automated Evaluation of Presentation Slides</a></h3>
                    <p><strong>Authors:</strong> Zhuohao Jerry Zhang, Ruiqi Chen, Mingyuan Zhong, Jacob O. Wobbrock</p>
                    <p><strong>Published:</strong> 8/5/2025</p>
                    <p><strong>Categories:</strong> cs.HC</p>
                    <p><strong>Summary:</strong> Automated evaluation of specific graphic designs like presentation slides is an open problem. We present SlideAudit, a dataset for automated slide evaluation. We collaborated with design experts to develop a thorough taxonomy of slide design flaws. Our dataset comprises 2400 slides collected and synthesized from multiple sources, including a subset intentionally modified with specific design problems. We then fully annotated them using our taxonomy through strictly trained crowdsourcing from Prolific. To evaluate whether AI is capable of identifying design flaws, we compared multiple large language models under different prompting strategies, and with an existing design critique pipeline. We show that AI models struggle to accurately identify slide design flaws, with F1 scores ranging from 0.331 to 0.655. Notably, prompting techniques leveraging our taxonomy achieved the highest performance. We further conducted a remediation study to assess AIs potential for improving slides. Among 82.0% of slides that showed significant improvement, 87.8% of them were improved more with our taxonomy, further demonstrating its utility.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.03628v1" target="_blank">LLMDistill4Ads: Using Cross-Encoders to Distill from LLM Signals for Advertiser Keyphrase Recommendations at eBay</a></h3>
                    <p><strong>Authors:</strong> Soumik Dey, Benjamin Braun, Naveen Ravipati, Hansi Wu, Binbin Li</p>
                    <p><strong>Published:</strong> 8/5/2025</p>
                    <p><strong>Categories:</strong> cs.IR, cs.AI, cs.LG</p>
                    <p><strong>Summary:</strong> Sellers at eBay are recommended keyphrases to bid on to enhance the performance of their advertising campaigns. The relevance of these keyphrases is crucial in avoiding the overcrowding of search systems with irrelevant items and maintaining a positive seller perception. It is essential that keyphrase recommendations align with both seller and Search judgments regarding auctions. Due to the difficulty in procuring negative human judgment at scale, employing LLM-as-a-judge to mimic seller judgment has been established as the norm in several studies. This study introduces a novel two-step LLM distillation process from a LLM-judge used to debias our Embedding Based Retrieval (EBR) model from the various biases that exist in click-data. We distill from an LLM teacher via a cross-encoder assistant into a bi-encoder student using a multi-task training approach, ultimately employing the student bi-encoder to retrieve relevant advertiser keyphrases. We show that integrating a knowledge distillation process from LLMs in a multi-task training setup enhances bi-encoder performance in retrieving relevant advertiser keyphrases at eBay.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.03624v1" target="_blank">Radiative Nonideal MHD Simulations of Inner Protoplanetary Disks: Temperature Structures, Asymmetric Winds, and Episodic Surface Accretion</a></h3>
                    <p><strong>Authors:</strong> Shoji Mori, Xue-Ning Bai, Kengo Tomida</p>
                    <p><strong>Published:</strong> 8/5/2025</p>
                    <p><strong>Categories:</strong> astro-ph.EP, astro-ph.SR</p>
                    <p><strong>Summary:</strong> We perform two-dimensional global magnetohydrodynamic (MHD) simulations including the full nonideal MHD effects (Ohmic diffusion, Hall effect, and ambipolar diffusion) and approximate radiation transport to understand the dynamics and thermal structure of the inner protoplanetary disks (PPDs). We have developed a simple radiative transfer model for PPDs that reasonably treats stellar non-thermal (XUV), stellar thermal (optical/infrared), and re-emitted radiations, reproducing the temperature structures from Monte Carlo radiative transfer. Our simulations show fast one-sided surface accretion ($\sim 10\%$ of Keplerian velocity) and asymmetric disk winds when the vertical magnetic field is aligned with the disk angular momentum. The asymmetry is due to the failure of the wind on the side with the accretion layer. On the accreting surface, clumps are repeatedly generated and accrete, driven by radiative feedback. For the anti-aligned fields, surface accretion becomes more moderate and time-variable, while the winds remain largely symmetric. For the thermal structure, accretion heating does not affect the disk temperature in any of our runs. This is because (1) the accretion energy dissipates via Joule heating at 2--3 gas scale heights, where low optical depth enables efficient radiative cooling, and (2) the winds remove $\gtrsim 10\%$ of the accretion energy. In contrast, the winds enhance radiative heating by elevating the irradiation front. These results highlight the importance of coupling between gas dynamics and radiation transport in PPDs, and provide observable magnetic activities such as fast episodic accretion, wind asymmetry, and molecular survival in XUV-irradiated winds.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.03606v1" target="_blank">Demystifying Sequential Recommendations: Counterfactual Explanations via Genetic Algorithms</a></h3>
                    <p><strong>Authors:</strong> Domiziano Scarcelli, Filippo Betello, Giuseppe Perelli, Fabrizio Silvestri, Gabriele Tolomei</p>
                    <p><strong>Published:</strong> 8/5/2025</p>
                    <p><strong>Categories:</strong> cs.IR</p>
                    <p><strong>Summary:</strong> Sequential Recommender Systems (SRSs) have demonstrated remarkable effectiveness in capturing users evolving preferences. However, their inherent complexity as black box models poses significant challenges for explainability. This work presents the first counterfactual explanation technique specifically developed for SRSs, introducing a novel approach in this space, addressing the key question: What minimal changes in a users interaction history would lead to different recommendations? To achieve this, we introduce a specialized genetic algorithm tailored for discrete sequences and show that generating counterfactual explanations for sequential data is an NP-Complete problem. We evaluate these approaches across four experimental settings, varying between targeted-untargeted and categorized-uncategorized scenarios, to comprehensively assess their capability in generating meaningful explanations. Using three different datasets and three models, we are able to demonstrate that our methods successfully generate interpretable counterfactual explanation while maintaining model fidelity close to one. Our findings contribute to the growing field of Explainable AI by providing a framework for understanding sequential recommendation decisions through the lens of what-if scenarios, ultimately enhancing user trust and system transparency.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.03599v1" target="_blank">OSINT or BULLSHINT? Exploring Open-Source Intelligence tweets about the Russo-Ukrainian War</a></h3>
                    <p><strong>Authors:</strong> Johannes Niu, Mila Stillman, Anna Kruspe</p>
                    <p><strong>Published:</strong> 8/5/2025</p>
                    <p><strong>Categories:</strong> cs.SI, cs.CL</p>
                    <p><strong>Summary:</strong> This paper examines the role of Open Source Intelligence (OSINT) on Twitter regarding the Russo-Ukrainian war, distinguishing between genuine OSINT and deceptive misinformation efforts, termed BULLSHINT. Utilizing a dataset spanning from January 2022 to July 2023, we analyze nearly 2 million tweets from approximately 1,040 users involved in discussing real-time military engagements, strategic analyses, and misinformation related to the conflict. Using sentiment analysis, partisanship detection, misinformation identification, and Named Entity Recognition (NER), we uncover communicative patterns and dissemination strategies within the OSINT community. Significant findings reveal a predominant negative sentiment influenced by war events, a nuanced distribution of pro-Ukrainian and pro-Russian partisanship, and the potential strategic manipulation of information. Additionally, we apply community detection techniques, which are able to identify distinct clusters partisanship, topics, and misinformation, highlighting the complex dynamics of information spread on social media. This research contributes to the understanding of digital warfare and misinformation dynamics, offering insights into the operationalization of OSINT in geopolitical conflicts.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.03596v1" target="_blank">MetaScope: Optics-Driven Neural Network for Ultra-Micro Metalens Endoscopy</a></h3>
                    <p><strong>Authors:</strong> Wuyang Li, Wentao Pan, Xiaoyuan Liu, Zhendong Luo, Chenxin Li, Hengyu Liu, Din Ping Tsai, Mu Ku Chen, Yixuan Yuan</p>
                    <p><strong>Published:</strong> 8/5/2025</p>
                    <p><strong>Categories:</strong> cs.CV, cs.AI</p>
                    <p><strong>Summary:</strong> Miniaturized endoscopy has advanced accurate visual perception within the human body. Prevailing research remains limited to conventional cameras employing convex lenses, where the physical constraints with millimetre-scale thickness impose serious impediments on the micro-level clinical. Recently, with the emergence of meta-optics, ultra-micro imaging based on metalenses (micron-scale) has garnered great attention, serving as a promising solution. However, due to the physical difference of metalens, there is a large gap in data acquisition and algorithm research. In light of this, we aim to bridge this unexplored gap, advancing the novel metalens endoscopy. First, we establish datasets for metalens endoscopy and conduct preliminary optical simulation, identifying two derived optical issues that physically adhere to strong optical priors. Second, we propose MetaScope, a novel optics-driven neural network tailored for metalens endoscopy driven by physical optics. MetaScope comprises two novel designs: Optics-informed Intensity Adjustment (OIA), rectifying intensity decay by learning optical embeddings, and Optics-informed Chromatic Correction (OCC), mitigating chromatic aberration by learning spatial deformations informed by learned Point Spread Function (PSF) distributions. To enhance joint learning, we further deploy a gradient-guided distillation to transfer knowledge from the foundational model adaptively. Extensive experiments demonstrate that MetaScope not only outperforms state-of-the-art methods in both metalens segmentation and restoration but also achieves impressive generalized ability in real biomedical scenes.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.03593v1" target="_blank">On the (In)Significance of Feature Selection in High-Dimensional Datasets</a></h3>
                    <p><strong>Authors:</strong> Bhavesh Neekhra, Debayan Gupta, Partha Pratim Chakravarti</p>
                    <p><strong>Published:</strong> 8/5/2025</p>
                    <p><strong>Categories:</strong> cs.LG, q-bio.GN, stat.ML</p>
                    <p><strong>Summary:</strong> Extensive research has been done on feature selection (FS) algorithms for high-dimensional datasets aiming to improve model performance, reduce computational cost and identify features of interest. We test the null hypothesis of using randomly selected features to compare against features selected by FS algorithms to validate the performance of the latter. Our results show that FS on high-dimensional datasets (in particular gene expression) in classification tasks is not useful. We find that (1) models trained on small subsets (0.02%-1% of all features) of randomly selected features almost always perform comparably to those trained on all features, and (2) a typical- sized random subset provides comparable or superior performance to that of top-k features selected in various published studies. Thus, our work challenges many feature selection results on high dimensional datasets, particularly in computational genomics. It raises serious concerns about studies that propose drug design or targeted interventions based on computationally selected genes, without further validation in a wet lab.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.03590v1" target="_blank">SolarSeer: Ultrafast and accurate 24-hour solar irradiance forecasts outperforming numerical weather prediction across the USA</a></h3>
                    <p><strong>Authors:</strong> Mingliang Bai, Zuliang Fang, Shengyu Tao, Siqi Xiang, Jiang Bian, Yanfei Xiang, Pengcheng Zhao, Weixin Jin, Jonathan A. Weyn, Haiyu Dong, Bin Zhang, Hongyu Sun, Kit Thambiratnam, Qi Zhang, Hongbin Sun, Xuan Zhang, Qiuwei Wu</p>
                    <p><strong>Published:</strong> 8/5/2025</p>
                    <p><strong>Categories:</strong> cs.LG, cs.CE</p>
                    <p><strong>Summary:</strong> Accurate 24-hour solar irradiance forecasting is essential for the safe and economic operation of solar photovoltaic systems. Traditional numerical weather prediction (NWP) models represent the state-of-the-art in forecasting performance but rely on computationally costly data assimilation and solving complicated partial differential equations (PDEs) that simulate atmospheric physics. Here, we introduce SolarSeer, an end-to-end large artificial intelligence (AI) model for solar irradiance forecasting across the Contiguous United States (CONUS). SolarSeer is designed to directly map the historical satellite observations to future forecasts, eliminating the computational overhead of data assimilation and PDEs solving. This efficiency allows SolarSeer to operate over 1,500 times faster than traditional NWP, generating 24-hour cloud cover and solar irradiance forecasts for the CONUS at 5-kilometer resolution in under 3 seconds. Compared with the state-of-the-art NWP in the CONUS, i.e., High-Resolution Rapid Refresh (HRRR), SolarSeer significantly reduces the root mean squared error of solar irradiance forecasting by 27.28% in reanalysis data and 15.35% across 1,800 stations. SolarSeer also effectively captures solar irradiance fluctuations and significantly enhances the first-order irradiance difference forecasting accuracy. SolarSeers ultrafast, accurate 24-hour solar irradiance forecasts provide strong support for the transition to sustainable, net-zero energy systems.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.03589v1" target="_blank">VITA: Variational Pretraining of Transformers for Climate-Robust Crop Yield Forecasting</a></h3>
                    <p><strong>Authors:</strong> Adib Hasan, Mardavij Roozbehani, Munther Dahleh</p>
                    <p><strong>Published:</strong> 8/5/2025</p>
                    <p><strong>Categories:</strong> cs.LG</p>
                    <p><strong>Summary:</strong> Accurate crop yield forecasting is essential for global food security. However, current AI models systematically underperform when yields deviate from historical trends. This issue arises from key data challenges, including a major asymmetry between rich pretraining weather datasets and the limited data available for fine-tuning. We introduce VITA (Variational Inference Transformer for Asymmetric data), a variational pretraining framework that addresses this asymmetry. Instead of relying on input reconstruction, VITA uses detailed weather variables as proxy targets during pretraining and learns to predict rich atmospheric states through self-supervised feature masking. This allows the model to be fine-tuned using only basic weather statistics during deployment. Applied to 763 counties in the U.S. Corn Belt, VITA achieves state-of-the-art performance in predicting corn and soybean yields across all evaluation scenarios. While it consistently delivers superior performance under normal conditions, its advantages are particularly pronounced during extreme weather years, with statistically significant improvements (paired t-test, $p \approx 0.01$). Importantly, VITA outperforms prior frameworks like GNN-RNN using less data, making it more practical for real-world use--particularly in data-scarce regions. This work highlights how domain-aware AI design can overcome data limitations and support resilient agricultural forecasting in a changing climate.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.03586v1" target="_blank">DeepFaith: A Domain-Free and Model-Agnostic Unified Framework for Highly Faithful Explanations</a></h3>
                    <p><strong>Authors:</strong> Yuhan Guo, Lizhong Ding, Shihan Jia, Yanyu Ren, Pengqi Li, Jiarun Fu, Changsheng Li, Ye yuan, Guoren Wang</p>
                    <p><strong>Published:</strong> 8/5/2025</p>
                    <p><strong>Categories:</strong> cs.LG, cs.AI</p>
                    <p><strong>Summary:</strong> Explainable AI (XAI) builds trust in complex systems through model attribution methods that reveal the decision rationale. However, due to the absence of a unified optimal explanation, existing XAI methods lack a ground truth for objective evaluation and optimization. To address this issue, we propose Deep architecture-based Faith explainer (DeepFaith), a domain-free and model-agnostic unified explanation framework under the lens of faithfulness. By establishing a unified formulation for multiple widely used and well-validated faithfulness metrics, we derive an optimal explanation objective whose solution simultaneously achieves optimal faithfulness across these metrics, thereby providing a ground truth from a theoretical perspective. We design an explainer learning framework that leverages multiple existing explanation methods, applies deduplicating and filtering to construct high-quality supervised explanation signals, and optimizes both pattern consistency loss and local correlation to train a faithful explainer. Once trained, DeepFaith can generate highly faithful explanations through a single forward pass without accessing the model being explained. On 12 diverse explanation tasks spanning 6 models and 6 datasets, DeepFaith achieves the highest overall faithfulness across 10 metrics compared to all baseline methods, highlighting its effectiveness and cross-domain generalizability.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.03584v1" target="_blank">Decoding and Engineering the Phytobiome Communication for Smart Agriculture</a></h3>
                    <p><strong>Authors:</strong> Fatih Gulec, Hamdan Awan, Nigel Wallbridge, Andrew W. Eckford</p>
                    <p><strong>Published:</strong> 8/5/2025</p>
                    <p><strong>Categories:</strong> eess.SP, cs.AI, cs.ET, cs.NI, q-bio.MN</p>
                    <p><strong>Summary:</strong> Smart agriculture applications, integrating technologies like the Internet of Things and machine learning/artificial intelligence (ML/AI) into agriculture, hold promise to address modern challenges of rising food demand, environmental pollution, and water scarcity. Alongside the concept of the phytobiome, which defines the area including the plant, its environment, and associated organisms, and the recent emergence of molecular communication (MC), there exists an important opportunity to advance agricultural science and practice using communication theory. In this article, we motivate to use the communication engineering perspective for developing a holistic understanding of the phytobiome communication and bridge the gap between the phytobiome communication and smart agriculture. Firstly, an overview of phytobiome communication via molecular and electrophysiological signals is presented and a multi-scale framework modeling the phytobiome as a communication network is conceptualized. Then, how this framework is used to model electrophysiological signals is demonstrated with plant experiments. Furthermore, possible smart agriculture applications, such as smart irrigation and targeted delivery of agrochemicals, through engineering the phytobiome communication are proposed. These applications merge ML/AI methods with the Internet of Bio-Nano-Things enabled by MC and pave the way towards more efficient, sustainable, and eco-friendly agricultural production. Finally, the implementation challenges, open research issues, and industrial outlook for these applications are discussed.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.03583v1" target="_blank">OpenLifelogQA: An Open-Ended Multi-Modal Lifelog Question-Answering Dataset</a></h3>
                    <p><strong>Authors:</strong> Quang-Linh Tran, Binh Nguyen, Gareth J. F. Jones, Cathal Gurrin</p>
                    <p><strong>Published:</strong> 8/5/2025</p>
                    <p><strong>Categories:</strong> cs.MM, cs.IR</p>
                    <p><strong>Summary:</strong> Lifelogging refers to the process of passively collecting, storing, and analysing personal daily life data using wearable devices. This data can support applications in memory preservation and enhancement. For example, using an ask-and-answer strategy, question-answering (QA) on lifelog data opens an interactive and interesting way to explore memorable events and insights into daily life. However, research resources for QA on lifelog data are limited to small-sized or synthetic QA datasets. In this paper, we present a novel lifelog QA dataset called OpenLifelogQA, building upon an 18-month lifelog dataset. Our dataset focuses on an open-ended and practical QA with real-world application in daily lifelog usage. We construct 14,187 pairs of QA with diverse types and difficulty levels. A baseline experiment is reported for this dataset with competitive average performance of 89.7% BERT Score, 25.87% ROUGE-L and 3.9665 LLM Score from LLaVA-NeXT-Interleave 7B model. We release this QA dataset to the research community to support new research into lifelog technologies, such as enabling personal chat-based assistants for lifelog data to become a reality.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.03578v1" target="_blank">RadProPoser: A Framework for Human Pose Estimation with Uncertainty Quantification from Raw Radar Data</a></h3>
                    <p><strong>Authors:</strong> Jonas Leo Mueller, Lukas Engel, Eva Dorschky, Daniel Krauss, Ingrid Ullmann, Martin Vossiek, Bjoern M. Eskofier</p>
                    <p><strong>Published:</strong> 8/5/2025</p>
                    <p><strong>Categories:</strong> cs.CV</p>
                    <p><strong>Summary:</strong> Radar-based human pose estimation (HPE) provides a privacy-preserving, illumination-invariant sensing modality but is challenged by noisy, multipath-affected measurements. We introduce RadProPoser, a probabilistic encoder-decoder architecture that processes complex-valued radar tensors from a compact 3-transmitter, 4-receiver MIMO radar. By incorporating variational inference into keypoint regression, RadProPoser jointly predicts 26 three-dimensional joint locations alongside heteroscedastic aleatoric uncertainties and can be recalibrated to predict total uncertainty. We explore different probabilistic formulations using both Gaussian and Laplace distributions for latent priors and likelihoods. On our newly released dataset with optical motion-capture ground truth, RadProPoser achieves an overall mean per-joint position error (MPJPE) of 6.425 cm, with 5.678 cm at the 45 degree aspect angle. The learned uncertainties exhibit strong alignment with actual pose errors and can be calibrated to produce reliable prediction intervals, with our best configuration achieving an expected calibration error of 0.021. As an additional demonstration, sampling from these latent distributions enables effective data augmentation for downstream activity classification, resulting in an F1 score of 0.870. To our knowledge, this is the first end-to-end radar tensor-based HPE system to explicitly model and quantify per-joint uncertainty from raw radar tensor data, establishing a foundation for explainable and reliable human motion analysis in radar applications.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.03565v1" target="_blank">[Technical Report] ArceKV: Towards Workload-driven LSM-compactions for Key-Value Store Under Dynamic Workloads</a></h3>
                    <p><strong>Authors:</strong> Junfeng Liu, Haoxuan Xie, Siqiang Luo</p>
                    <p><strong>Published:</strong> 8/5/2025</p>
                    <p><strong>Categories:</strong> cs.DB, H.2.0</p>
                    <p><strong>Summary:</strong> Key-value stores underpin a wide range of applications due to their simplicity and efficiency. Log-Structured Merge Trees (LSM-trees) dominate as their underlying structure, excelling at handling rapidly growing data. Recent research has focused on optimizing LSM-tree performance under static workloads with fixed read-write ratios. However, real-world workloads are highly dynamic, and existing workload-aware approaches often struggle to sustain optimal performance or incur substantial transition overhead when workload patterns shift. To address this, we propose ElasticLSM, which removes traditional LSM-tree structural constraints to allow more flexible management actions (i.e., compactions and write stalls) creating greater opportunities for continuous performance optimization. We further design Arce, a lightweight compaction decision engine that guides ElasticLSM in selecting the optimal action from its expanded action space. Building on these components, we implement ArceKV, a full-fledged key-value store atop RocksDB. Extensive evaluations demonstrate that ArceKV outperforms state-of-the-art compaction strategies across diverse workloads, delivering around 3x faster performance in dynamic scenarios.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.03564v1" target="_blank">A Scalable Machine Learning Pipeline for Building Footprint Detection in Historical Maps</a></h3>
                    <p><strong>Authors:</strong> Annemarie McCarthy</p>
                    <p><strong>Published:</strong> 8/5/2025</p>
                    <p><strong>Categories:</strong> cs.CV, I.4</p>
                    <p><strong>Summary:</strong> Historical maps offer a valuable lens through which to study past landscapes and settlement patterns. While prior research has leveraged machine learning based techniques to extract building footprints from historical maps, such approaches have largely focused on urban areas and tend to be computationally intensive. This presents a challenge for research questions requiring analysis across extensive rural regions, such as verifying historical census data or locating abandoned settlements. In this paper, this limitation is addressed by proposing a scalable and efficient pipeline tailored to rural maps with sparse building distributions. The method described employs a hierarchical machine learning based approach: convolutional neural network (CNN) classifiers are first used to progressively filter out map sections unlikely to contain buildings, significantly reducing the area requiring detailed analysis. The remaining high probability sections are then processed using CNN segmentation algorithms to extract building features. The pipeline is validated using test sections from the Ordnance Survey Ireland historical 25 inch map series and 6 inch map series, demonstrating both high performance and improved efficiency compared to conventional segmentation-only approaches. Application of the technique to both map series, covering the same geographic region, highlights its potential for historical and archaeological discovery. Notably, the pipeline identified a settlement of approximately 22 buildings in Tully, Co. Galway, present in the 6 inch map, produced in 1839, but absent from the 25 inch map, produced in 1899, suggesting it may have been abandoned during the Great Famine period.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.03562v1" target="_blank">Beyond Meme Templates: Limitations of Visual Similarity Measures in Meme Matching</a></h3>
                    <p><strong>Authors:</strong> Muzhaffar Hazman, Susan McKeever, Josephine Griffith</p>
                    <p><strong>Published:</strong> 8/5/2025</p>
                    <p><strong>Categories:</strong> cs.CV, cs.CL</p>
                    <p><strong>Summary:</strong> Internet memes, now a staple of digital communication, play a pivotal role in how users engage within online communities and allow researchers to gain insight into contemporary digital culture. These engaging user-generated content are characterised by their reuse of visual elements also found in other memes. Matching instances of memes via these shared visual elements, called Meme Matching, is the basis of a wealth of meme analysis approaches. However, most existing methods assume that every meme consists of a shared visual background, called a Template, with some overlaid text, thereby limiting meme matching to comparing the background image alone. Current approaches exclude the many memes that are not template-based and limit the effectiveness of automated meme analysis and would not be effective at linking memes to contemporary web-based meme dictionaries. In this work, we introduce a broader formulation of meme matching that extends beyond template matching. We show that conventional similarity measures, including a novel segment-wise computation of the similarity measures, excel at matching template-based memes but fall short when applied to non-template-based meme formats. However, the segment-wise approach was found to consistently outperform the whole-image measures on matching non-template-based memes. Finally, we explore a prompting-based approach using a pretrained Multimodal Large Language Model for meme matching. Our results highlight that accurately matching memes via shared visual elements, not just background templates, remains an open challenge that requires more sophisticated matching techniques.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.03558v1" target="_blank">SAGE-HLS: Syntax-Aware AST-Guided LLM for High-Level Synthesis Code Generation</a></h3>
                    <p><strong>Authors:</strong> M Zafir Sadik Khan, Nowfel Mashnoor, Mohammad Akyash, Kimia Azar, Hadi Kamali</p>
                    <p><strong>Published:</strong> 8/5/2025</p>
                    <p><strong>Categories:</strong> cs.PL</p>
                    <p><strong>Summary:</strong> In todays rapidly evolving field of electronic design automation (EDA), the complexity of hardware designs is increasing, necessitating more sophisticated automation solutions. High-level synthesis (HLS), as a pivotal solution, automates hardware designs from high-level abstractions (e.g., C/C++). However, it faces significant challenges, particularly in design space exploration and optimization. While large language models (LLMs) have shown notable capabilities in code generation, their application to HLS has been limited due to the scarcity of (publicly) available HLS code datasets. Hence, research in this domain has primarily focused on techniques such as prompt engineering and retrieval-augmented generation (RAG). To overcome this limitation, this paper introduces SAGE-HLS, the first-of-its-kind fine-tuned LLM specifically for HLS code generation. Our method includes three key advancements: (i) We implement Verilog-to-C/C++ porting, converting verified and synthesizable Verilog codes into corresponding C, creating a dataset of 16.7K HLS codes; (ii) We implement a fine-tuning strategy, which is based on instruction prompting to code generation guided by abstract syntax tree (AST); (iii) We develop a semi-automated evaluation framework using VerilogEval to assess the functionality of the generated HLS code. Our experiments show that SAGE-HLS, fined-tuned on the QwenCoder (2.5) 7B model, achieves a near 100% success rate in code synthesizability and a 75% success rate in functional correctness.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.03555v1" target="_blank">PyLate: Flexible Training and Retrieval for Late Interaction Models</a></h3>
                    <p><strong>Authors:</strong> Antoine Chaffin, RaphaÃ«l Sourty</p>
                    <p><strong>Published:</strong> 8/5/2025</p>
                    <p><strong>Categories:</strong> cs.IR, cs.CL</p>
                    <p><strong>Summary:</strong> Neural ranking has become a cornerstone of modern information retrieval. While single vector search remains the dominant paradigm, it suffers from the shortcoming of compressing all the information into a single vector. This compression leads to notable performance degradation in out-of-domain, long-context, and reasoning-intensive retrieval tasks. Multi-vector approaches pioneered by ColBERT aim to address these limitations by preserving individual token embeddings and computing similarity via the MaxSim operator. This architecture has demonstrated superior empirical advantages, including enhanced out-of-domain generalization, long-context handling, and performance in complex retrieval scenarios. Despite these compelling empirical results and clear theoretical advantages, the practical adoption and public availability of late interaction models remain low compared to their single-vector counterparts, primarily due to a lack of accessible and modular tools for training and experimenting with such models. To bridge this gap, we introduce PyLate, a streamlined library built on top of Sentence Transformers to support multi-vector architectures natively, inheriting its efficient training, advanced logging, and automated model card generation while requiring minimal code changes to code templates users are already familiar with. By offering multi-vector-specific features such as efficient indexes, PyLate aims to accelerate research and real-world application of late interaction models, thereby unlocking their full potential in modern IR systems. Finally, PyLate has already enabled the development of state-of-the-art models, including GTE-ModernColBERT and Reason-ModernColBERT, demonstrating its practical utility for both research and production environments.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.03553v1" target="_blank">MultiRAG: A Knowledge-guided Framework for Mitigating Hallucination in Multi-source Retrieval Augmented Generation</a></h3>
                    <p><strong>Authors:</strong> Wenlong Wu, Haofen Wang, Bohan Li, Peixuan Huang, Xinzhe Zhao, Lei Liang</p>
                    <p><strong>Published:</strong> 8/5/2025</p>
                    <p><strong>Categories:</strong> cs.IR, cs.CL</p>
                    <p><strong>Summary:</strong> Retrieval Augmented Generation (RAG) has emerged as a promising solution to address hallucination issues in Large Language Models (LLMs). However, the integration of multiple retrieval sources, while potentially more informative, introduces new challenges that can paradoxically exacerbate hallucination problems. These challenges manifest primarily in two aspects: the sparse distribution of multi-source data that hinders the capture of logical relationships and the inherent inconsistencies among different sources that lead to information conflicts. To address these challenges, we propose MultiRAG, a novel framework designed to mitigate hallucination in multi-source retrieval-augmented generation through knowledge-guided approaches. Our framework introduces two key innovations: (1) a knowledge construction module that employs multi-source line graphs to efficiently aggregate logical relationships across different knowledge sources, effectively addressing the sparse data distribution issue; and (2) a sophisticated retrieval module that implements a multi-level confidence calculation mechanism, performing both graph-level and node-level assessments to identify and eliminate unreliable information nodes, thereby reducing hallucinations caused by inter-source inconsistencies. Extensive experiments on four multi-domain query datasets and two multi-hop QA datasets demonstrate that MultiRAG significantly enhances the reliability and efficiency of knowledge retrieval in complex multi-source scenarios. \textcolor{blue}{Our code is available in https://github.com/wuwenlong123/MultiRAG.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.03550v1" target="_blank">Beyond the Surface: Enhancing LLM-as-a-Judge Alignment with Human via Internal Representations</a></h3>
                    <p><strong>Authors:</strong> Peng Lai, Jianjie Zheng, Sijie Cheng, Yun Chen, Peng Li, Yang Liu, Guanhua Chen</p>
                    <p><strong>Published:</strong> 8/5/2025</p>
                    <p><strong>Categories:</strong> cs.CL</p>
                    <p><strong>Summary:</strong> The growing scale of evaluation tasks has led to the widespread adoption of automated evaluation using large language models, a paradigm known as LLMas-a-judge. However, improving its alignment with human preferences without complex prompts or fine-tuning remains challenging. In this work, motivated by preliminary findings that middle-to-upper layers encode semantically and taskrelevant representations that are often more aligned with human judgments than the final layer, we propose LAGER, a lightweight and efficient framework for enhancing LLM-as-a-Judge alignment with human scoring, via internal representations. LAGER produces fine-grained judgment scores by aggregating cross-layer scoretoken logits and computing the expected score from a softmax-based distribution, with the LLM backbone kept frozen. LAGER fully leverages the complementary information across different layers, overcoming the limitations of relying solely on the final layer. We evaluate our method on the standard alignment benchmarks Flask, HelpSteer, and BIGGen using Spearman correlation, and find that LAGER achieves improvements of up to 7.5% over the best baseline across these benchmarks. Without reasoning steps, LAGER matches or outperforms reasoning-based methods. Experiments on downstream applications, such as data selection and emotional understanding, further show the effectiveness of our method.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.03542v1" target="_blank">Speech-to-LaTeX: New Models and Datasets for Converting Spoken Equations and Sentences</a></h3>
                    <p><strong>Authors:</strong> Dmitrii Korzh, Dmitrii Tarasov, Artyom Iudin, Elvir Karimov, Matvey Skripkin, Nikita Kuzmin, Andrey Kuznetsov, Oleg Y. Rogov, Ivan Oseledets</p>
                    <p><strong>Published:</strong> 8/5/2025</p>
                    <p><strong>Categories:</strong> cs.CV</p>
                    <p><strong>Summary:</strong> Conversion of spoken mathematical expressions is a challenging task that involves transcribing speech into a strictly structured symbolic representation while addressing the ambiguity inherent in the pronunciation of equations. Although significant progress has been achieved in automatic speech recognition (ASR) and language models (LM), the problem of converting spoken mathematics into LaTeX remains underexplored. This task directly applies to educational and research domains, such as lecture transcription or note creation. Based on ASR post-correction, prior work requires 2 transcriptions, focuses only on isolated equations, has a limited test set, and provides neither training data nor multilingual coverage. To address these issues, we present the first fully open-source large-scale dataset, comprising over 66,000 human-annotated audio samples of mathematical equations and sentences in both English and Russian, drawn from diverse scientific domains. In addition to the ASR post-correction models and few-shot prompting, we apply audio language models, demonstrating comparable character error rate (CER) results on the MathSpeech benchmark (28% vs. 30%) for the equations conversion. In contrast, on the proposed S2L-equations benchmark, our models outperform the MathSpeech model by a substantial margin of more than 40 percentage points, even after accounting for LaTeX formatting artifacts (27% vs. 64%). We establish the first benchmark for mathematical sentence recognition (S2L-sentences) and achieve an equation CER of 40%. This work lays the groundwork for future advances in multimodal AI, with a particular focus on mathematical content recognition.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.03533v1" target="_blank">EmbedGrad: Gradient-Based Prompt Optimization in Embedding Space for Large Language Models</a></h3>
                    <p><strong>Authors:</strong> Xiaoming Hou, Jiquan Zhang, Zibin Lin, DaCheng Tao, Shengli Zhang</p>
                    <p><strong>Published:</strong> 8/5/2025</p>
                    <p><strong>Categories:</strong> cs.CL</p>
                    <p><strong>Summary:</strong> Effectively adapting powerful pretrained foundation models to diverse tasks remains a key challenge in AI deployment. Current approaches primarily follow two paradigms:discrete optimization of text prompts through prompt engineering, or continuous adaptation via additional trainable parameters. Both exhibit limitations-discrete methods lack refinement precision while parameter-based techniques increase complexity and reduce interpretability. To address these constraints, we propose EmbedGrad, a novel framework that optimizes text prompt embeddings through gradient-based refinement. Our approach uniquely decouples training from deployment:during optimization,labeled examples guide precise embedding adjustments while preserving semantic meaning; during inference, only optimized embeddings integrate with user queries. This enables fine-grained calibration impossible in text space, such as enhancing the reasoning capability of prompts like please reason step by step. Comprehensive evaluations across mathematical reasoning, sentiment analysis, and causal judgment tasks demonstrate EmbedGrads effectiveness:optimizing this reasoning prompt for Qwen2.5-Math-1.5B increased accuracy from 14.74\% to 58.96\% on mathematical problems. Consistent improvements were observed across model scales (0.5B-14B) and all tasks, with particularly significant gains for smaller models on complex problems like causal judgment. By bridging prompt engineering and parameter efficiency without architectural changes, our work establishes embedding refinement as a powerful new paradigm for task adaptation.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.03529v1" target="_blank">Marito: Structuring and Building Open Multilingual Terminologies for South African NLP</a></h3>
                    <p><strong>Authors:</strong> Vukosi Marivate, Isheanesu Dzingirai, Fiskani Banda, Richard Lastrucci, Thapelo Sindane, Keabetswe Madumo, Kayode Olaleye, Abiodun Modupe, Unarine Netshifhefhe, Herkulaas Combrink, Mohlatlego Nakeng, Matome Ledwaba</p>
                    <p><strong>Published:</strong> 8/5/2025</p>
                    <p><strong>Categories:</strong> cs.CL</p>
                    <p><strong>Summary:</strong> The critical lack of structured terminological data for South Africas official languages hampers progress in multilingual NLP, despite the existence of numerous government and academic terminology lists. These valuable assets remain fragmented and locked in non-machine-readable formats, rendering them unusable for computational research and development. \emph{Marito} addresses this challenge by systematically aggregating, cleaning, and standardising these scattered resources into open, interoperable datasets. We introduce the foundational \emph{Marito} dataset, released under the equitable, Africa-centered NOODL framework. To demonstrate its immediate utility, we integrate the terminology into a Retrieval-Augmented Generation (RAG) pipeline. Experiments show substantial improvements in the accuracy and domain-specific consistency of English-to-Tshivenda machine translation for large language models. \emph{Marito} provides a scalable foundation for developing robust and equitable NLP technologies, ensuring South Africas rich linguistic diversity is represented in the digital age.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.03526v1" target="_blank">CollaBot: Vision-Language Guided Simultaneous Collaborative Manipulation</a></h3>
                    <p><strong>Authors:</strong> Kun Song, Shentao Ma, Gaoming Chen, Ninglong Jin, Guangbao Zhao, Mingyu Ding, Zhenhua Xiong, Jia Pan</p>
                    <p><strong>Published:</strong> 8/5/2025</p>
                    <p><strong>Categories:</strong> cs.RO</p>
                    <p><strong>Summary:</strong> A central research topic in robotics is how to use this system to interact with the physical world. Traditional manipulation tasks primarily focus on small objects. However, in factory or home environments, there is often a need for the movement of large objects, such as moving tables. These tasks typically require multi-robot systems to work collaboratively. Previous research lacks a framework that can scale to arbitrary sizes of robots and generalize to various kinds of tasks. In this work, we propose CollaBot, a generalist framework for simultaneous collaborative manipulation. First, we use SEEM for scene segmentation and point cloud extraction of the target object. Then, we propose a collaborative grasping framework, which decomposes the task into local grasp pose generation and global collaboration. Finally, we design a 2-stage planning module that can generate collision-free trajectories to achieve this task. Experiments show a success rate of 52% across different numbers of robots, objects, and tasks, indicating the effectiveness of the proposed framework.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.03523v1" target="_blank">FilBench: Can LLMs Understand and Generate Filipino?</a></h3>
                    <p><strong>Authors:</strong> Lester James V. Miranda, Elyanah Aco, Conner Manuel, Jan Christian Blaise Cruz, Joseph Marvin Imperial</p>
                    <p><strong>Published:</strong> 8/5/2025</p>
                    <p><strong>Categories:</strong> cs.CL</p>
                    <p><strong>Summary:</strong> Despite the impressive performance of LLMs on English-based tasks, little is known about their capabilities in specific languages such as Filipino. In this work, we address this gap by introducing FilBench, a Filipino-centric benchmark designed to evaluate LLMs across a diverse set of tasks and capabilities in Filipino, Tagalog, and Cebuano. We carefully curate the tasks in FilBench to reflect the priorities and trends of NLP research in the Philippines such as Cultural Knowledge, Classical NLP, Reading Comprehension, and Generation. By evaluating 27 state-of-the-art LLMs on FilBench, we find that several LLMs suffer from reading comprehension and translation capabilities. Our results indicate that FilBench is challenging, with the best model, GPT-4o, achieving only a score of 72.23%. Moreover, we also find that models trained specifically for Southeast Asian languages tend to underperform on FilBench, with the highest-performing model, SEA-LION v3 70B, achieving only a score of 61.07%. Our work demonstrates the value of curating language-specific LLM benchmarks to aid in driving progress on Filipino NLP and increasing the inclusion of Philippine languages in LLM development.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.03521v1" target="_blank">Understanding Demand for Shared Autonomous Micro-Mobility</a></h3>
                    <p><strong>Authors:</strong> Naroa Coretti Sanchez, Kent Larson</p>
                    <p><strong>Published:</strong> 8/5/2025</p>
                    <p><strong>Categories:</strong> cs.ET, cs.CY</p>
                    <p><strong>Summary:</strong> This study examines the behavioral and environmental implications of shared autonomous micro-mobility systems, focusing on autonomous bicycles and their integration with transit in the U.S. While prior research has addressed operational and lifecycle aspects, a critical gap remains in understanding which modes these services are likely to substitute, who is most inclined to adopt them, and how service attributes influence user decisions. We design a context-aware stated preference survey grounded in real-world trips and estimate discrete choice models, including a hybrid model incorporating latent attitudes. Findings indicate that adoption, mode shift, and environmental impacts are highly sensitive to service design. Scenarios with minimal wait and cost yield high adoption but increase emissions, while moderate waits are more likely to reduce impacts. Adoption likelihood varies with demographic characteristics, and outcomes depend on city type, context, and infrastructure assumptions. These insights can inform the development of more sustainable and equitable mobility systems.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.03516v1" target="_blank">Distribution-aware Knowledge Unification and Association for Non-exemplar Lifelong Person Re-identification</a></h3>
                    <p><strong>Authors:</strong> Shiben Liu, Mingyue Xu, Huijie Fan, Qiang Wang, Yandong Tang, Zhi Han</p>
                    <p><strong>Published:</strong> 8/5/2025</p>
                    <p><strong>Categories:</strong> cs.CV</p>
                    <p><strong>Summary:</strong> Lifelong person re-identification (LReID) encounters a key challenge: balancing the preservation of old knowledge with adaptation to new information. Existing LReID methods typically employ knowledge distillation to enforce representation alignment. However, these approaches ignore two crucial aspects: specific distribution awareness and cross-domain unified knowledge learning, both of which are essential for addressing this challenge. To overcome these limitations, we propose a novel distribution-aware knowledge unification and association (DKUA) framework where domain-style modeling is performed for each instance to propagate domain-specific representations, enhancing anti-forgetting and generalization capacity. Specifically, we design a distribution-aware model to transfer instance-level representations of the current domain into the domain-specific representations with the different domain styles, preserving learned knowledge without storing old samples. Next, we propose adaptive knowledge consolidation (AKC) to dynamically generate the unified representation as a cross-domain representation center. To further mitigate forgetting, we develop a unified knowledge association (UKA) mechanism, which explores the unified representation as a bridge to explicitly model inter-domain associations, reducing inter-domain gaps. Finally, distribution-based knowledge transfer (DKT) is proposed to prevent the current domain distribution from deviating from the cross-domain distribution center, improving adaptation capacity. Experimental results show our DKUA outperforms the existing methods by 7.6%/5.3% average mAP/R@1 improvement on anti-forgetting and generalization capacity, respectively. Our code will be publicly released.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.03515v1" target="_blank">Oxide Interface-Based Polymorphic Electronic Devices for Neuromorphic Computing</a></h3>
                    <p><strong>Authors:</strong> Soumen Pradhan, Kirill Miller, Fabian Hartmann, Merit Spring, Judith Gabel, Berengar Leikert, Silke Kuhn, Martin Kamp, Victor Lopez-Richard, Michael Sing, Ralph Claessen, Sven HÃ¶fling</p>
                    <p><strong>Published:</strong> 8/5/2025</p>
                    <p><strong>Categories:</strong> cond-mat.dis-nn, cond-mat.mtrl-sci</p>
                    <p><strong>Summary:</strong> Aside from recent advances in artificial intelligence (AI) models, specialized AI hardware is crucial to address large volumes of unstructured and dynamic data. Hardware-based AI, built on conventional complementary metal-oxidesemiconductor (CMOS)-technology, faces several critical challenges including scaling limitation of devices [1, 2], separation of computation and memory units [3] and most importantly, overall system energy efficiency [4]. While numerous materials with emergent functionalities have been proposed to overcome these limitations, scalability, reproducibility, and compatibility remain critical obstacles [5, 6]. Here, we demonstrate oxide-interface based polymorphic electronic devices with programmable transistor, memristor, and memcapacitor functionalities by manipulating the quasi-two-dimensional electron gas in LaAlO3/SrTiO3 heterostructures [7, 8] using lateral gates. A circuit utilizing two polymorphic functionalities of transistor and memcapacitor exhibits nonlinearity and short-term memory, enabling implementation in physical reservoir computing. An integrated circuit incorporating transistor and memristor functionalities is utilized for the transition from short- to long-term synaptic plasticity and for logic operations, along with in-situ logic output storage. The same circuit with advanced reconfigurable synaptic logic operations presents high-level multi-input decision-making tasks, such as patient-monitoring in healthcare applications. Our findings pave the way for oxide-based monolithic integrated circuits in a scalable, silicon compatible, energy efficient single platform, advancing both the polymorphic and neuromorphic computings.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.03514v1" target="_blank">Theatre in the Loop: A Rehearsal-Based, Collaborative Workflow for Expressive Robotic Behaviours</a></h3>
                    <p><strong>Authors:</strong> Pavlos Panagiotidis, Victor Zhi Heung Ngo, Sean Myatt, Roma Patel, Rachel Ramchurn, Alan Chamberlain, Ayse Kucukyilmaz</p>
                    <p><strong>Published:</strong> 8/5/2025</p>
                    <p><strong>Categories:</strong> cs.RO, cs.HC</p>
                    <p><strong>Summary:</strong> In this paper, we propose theatre-in-the-loop, a framework for developing expressive robot behaviours tailored to artistic performance through a director-guided puppeteering workflow. Leveraging theatrical methods, we use narrative objectives to direct a puppeteer in generating improvised robotic gestures that convey specific emotions. These improvisations are captured and curated to build a dataset of reusable movement templates for standalone playback in future autonomous performances. Initial trials demonstrate the feasibility of this approach, illustrating how the workflow enables precise sculpting of robotic gestures into coherent emotional arcs while revealing challenges posed by the robots mechanical constraints. We argue that this practice-led framework provides a model for interdisciplinary teams creating socially expressive robot behaviours, contributing to (1) theatre as an interactive training ground for human-robot interaction and (2) co-creation methodologies between humans and machines.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.03501v1" target="_blank">Training Long-Context, Multi-Turn Software Engineering Agents with Reinforcement Learning</a></h3>
                    <p><strong>Authors:</strong> Alexander Golubev, Maria Trofimova, Sergei Polezhaev, Ibragim Badertdinov, Maksim Nekrashevich, Anton Shevtsov, Simon Karasik, Sergey Abramov, Andrei Andriushchenko, Filipp Fisin, Sergei Skvortsov, Boris Yangel</p>
                    <p><strong>Published:</strong> 8/5/2025</p>
                    <p><strong>Categories:</strong> cs.LG, cs.CL, cs.SE</p>
                    <p><strong>Summary:</strong> Research on applications of Reinforcement Learning (RL) to Large Language Models (LLMs) has mostly been focused on single-turn problems, such as mathematical reasoning or single-shot code generation. While these problems can be viewed as token-level multi-turn MDPs, this view corresponds to a degenerate case of multi-turn interaction where the environment provides no feedback. This contrasts with many real-world domains, such as software engineering (SWE), which require rich multi-turn interactions with a stateful environment that responds to each action with a non-trivial observation. To bridge this gap, we demonstrate the successful application of RL to this general regime. Using a modified Decoupled Advantage Policy Optimization (DAPO) algorithm, we train an agent based on Qwen2.5-72B-Instruct to solve real-world software engineering tasks. Our approach increases the agents success rate on the SWE-bench Verified benchmark from a 20% rejection fine-tuned baseline to 39%, without relying on any teacher models. On SWE-rebench, our agent matches or outperforms leading open-weight models such as DeepSeek-V3-0324 and Qwen3-235B-A22B using an identical scaffolding, offering a viable path toward building more capable autonomous agents for complex real-world problems based on open models.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.03498v1" target="_blank">Mapping Innovation Networks: A Network-Based Approach to Actor Heterogeneity in National Innovation Systems</a></h3>
                    <p><strong>Authors:</strong> Dawoon Jeong, Taewon Kang, Saerom Si, Sangnam Lee, Wonsub Eum</p>
                    <p><strong>Published:</strong> 8/5/2025</p>
                    <p><strong>Categories:</strong> physics.soc-ph</p>
                    <p><strong>Summary:</strong> The Triple Helix model has provided a foundational framework for analyzing National Innovation Systems by highlighting the roles of universities, industries, and government research institutes. However, increasing heterogeneity within these actor groups limits the explanatory power of typological approaches. This study introduces a capability-based network methodology that maps the structural relationships among innovation actors based on the similarity of their research and development (RD) capabilities. Drawing on Economic Complexity Theory, we measure each actors revealed comparative advantage (RCA) across scientific and technological fields and construct an RD Actor Space - a proximity-based network that reflects the relational configuration of innovation capacities. Applying this method to Korean RD data, we uncover a stratified system in which central, highly diversified universities coexist with more specialized firms and government institutes. Network analysis reveals assortative and unequal structures, and hierarchical clustering further highlights layered subgroupings. By moving beyond categorical classification, this capability-based network approach provides a scalable and generalizable tool for analyzing structural complexity within national innovation systems.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.03497v1" target="_blank">EditGarment: An Instruction-Based Garment Editing Dataset Constructed with Automated MLLM Synthesis and Semantic-Aware Evaluation</a></h3>
                    <p><strong>Authors:</strong> Deqiang Yin, Junyi Guo, Huanda Lu, Fangyu Wu, Dongming Lu</p>
                    <p><strong>Published:</strong> 8/5/2025</p>
                    <p><strong>Categories:</strong> cs.CV</p>
                    <p><strong>Summary:</strong> Instruction-based garment editing enables precise image modifications via natural language, with broad applications in fashion design and customization. Unlike general editing tasks, it requires understanding garment-specific semantics and attribute dependencies. However, progress is limited by the scarcity of high-quality instruction-image pairs, as manual annotation is costly and hard to scale. While MLLMs have shown promise in automated data synthesis, their application to garment editing is constrained by imprecise instruction modeling and a lack of fashion-specific supervisory signals. To address these challenges, we present an automated pipeline for constructing a garment editing dataset. We first define six editing instruction categories aligned with real-world fashion workflows to guide the generation of balanced and diverse instruction-image triplets. Second, we introduce Fashion Edit Score, a semantic-aware evaluation metric that captures semantic dependencies between garment attributes and provides reliable supervision during construction. Using this pipeline, we construct a total of 52,257 candidate triplets and retain 20,596 high-quality triplets to build EditGarment, the first instruction-based dataset tailored to standalone garment editing. The project page is https://yindq99.github.io/EditGarment-project/.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.03496v1" target="_blank">Hyperon spin correlation in high-energy heavy-ion collisions</a></h3>
                    <p><strong>Authors:</strong> Xin-Li Sheng, Xiang-Yu Wu, Dirk H. Rischke, Xin-Nian Wang</p>
                    <p><strong>Published:</strong> 8/5/2025</p>
                    <p><strong>Categories:</strong> hep-ph, nucl-th</p>
                    <p><strong>Summary:</strong> Recent experimental data show an unexpectedly large spin alignment of $\phi$ mesons in high-energy heavy-ion collisions, which can be explained by short-distance fluctuations of strong force fields (vector $\phi$ fields) within the constituent-quark model. We calculate the hyperon spin correlations within the same model, taking into account hydrodynamic effects and a $\phi$ field fluctuating in space-time according to a Gaussian distribution. The $\Lambda\bar\Lambda$ spin correlation induced by the $\phi$ field is shown to be negative as opposed to that of $\Lambda\Lambda$ or $\bar{\Lambda}\bar{\Lambda}$. We thus propose a new net spin correlation as a sensitive probe to separate strong force effects from hydrodynamic ones. With the strength of the field fluctuations extracted from the observed $\phi$ spin alignment, we predict the collision energy dependence of the hyperon spin correlations and also investigate the dependence of net spin correlation on azimuthal-angle and rapidity difference.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.03494v1" target="_blank">Prototype-Enhanced Confidence Modeling for Cross-Modal Medical Image-Report Retrieval</a></h3>
                    <p><strong>Authors:</strong> Shreyank N Gowda, Xiaobo Jin, Christian Wagner</p>
                    <p><strong>Published:</strong> 8/5/2025</p>
                    <p><strong>Categories:</strong> cs.CV</p>
                    <p><strong>Summary:</strong> In cross-modal retrieval tasks, such as image-to-report and report-to-image retrieval, accurately aligning medical images with relevant text reports is essential but challenging due to the inherent ambiguity and variability in medical data. Existing models often struggle to capture the nuanced, multi-level semantic relationships in radiology data, leading to unreliable retrieval results. To address these issues, we propose the Prototype-Enhanced Confidence Modeling (PECM) framework, which introduces multi-level prototypes for each modality to better capture semantic variability and enhance retrieval robustness. PECM employs a dual-stream confidence estimation that leverages prototype similarity distributions and an adaptive weighting mechanism to control the impact of high-uncertainty data on retrieval rankings. Applied to radiology image-report datasets, our method achieves significant improvements in retrieval precision and consistency, effectively handling data ambiguity and advancing reliability in complex clinical scenarios. We report results on multiple different datasets and tasks including fully supervised and zero-shot retrieval obtaining performance gains of up to 10.17%, establishing in new state-of-the-art.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.03492v1" target="_blank">Quality Versus Sparsity in Image Recovery by Dictionary Learning Using Iterative Shrinkage</a></h3>
                    <p><strong>Authors:</strong> Mohammadsadegh Khoshghiaferezaee, Moritz Krauth, Shima Shabani, Michael BreuÃŸ</p>
                    <p><strong>Published:</strong> 8/5/2025</p>
                    <p><strong>Categories:</strong> cs.CV, 65K05, 68T30, I.4.5; I.2.6</p>
                    <p><strong>Summary:</strong> Sparse dictionary learning (SDL) is a fundamental technique that is useful for many image processing tasks. As an example we consider here image recovery, where SDL can be cast as a nonsmooth optimization problem. For this kind of problems, iterative shrinkage methods represent a powerful class of algorithms that are subject of ongoing research. Sparsity is an important property of the learned solutions, as exactly the sparsity enables efficient further processing or storage. The sparsity implies that a recovered image is determined as a combination of a number of dictionary elements that is as low as possible. Therefore, the question arises, to which degree sparsity should be enforced in SDL in order to not compromise recovery quality. In this paper we focus on the sparsity of solutions that can be obtained using a variety of optimization methods. It turns out that there are different sparsity regimes depending on the method in use. Furthermore, we illustrate that high sparsity does in general not compromise recovery quality, even if the recovered image is quite different from the learning database.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.03488v1" target="_blank">VQA support to Arabic Language Learning Educational Tool</a></h3>
                    <p><strong>Authors:</strong> Khaled Bachir Delassi, Lakhdar Zeggane, Hadda Cherroun, Abdelhamid Haouhat, Kaoutar Bouzouad</p>
                    <p><strong>Published:</strong> 8/5/2025</p>
                    <p><strong>Categories:</strong> cs.AI, cs.SE</p>
                    <p><strong>Summary:</strong> We address the problem of scarcity of educational Arabic Language Learning tools that advocate modern pedagogical models such as active learning which ensures language proficiency. In fact, we investigate the design and evaluation of an AI-powered educational tool designed to enhance Arabic language learning for non-native speakers with beginner-to-intermediate proficiency level. The tool leverages advanced AI models to generate interactive visual quizzes, deploying Visual Question Answering as the primary activity. Adopting a constructivist learning approach, the system encourages active learning through real-life visual quizzes, and image-based questions that focus on improving vocabulary, grammar, and comprehension. The system integrates Vision-Language Pretraining models to generate contextually relevant image description from which Large Language Model generate assignments based on customized Arabic language Learning quizzes thanks to prompting. The effectiveness of the tool is evaluated through a manual annotated benchmark consisting of 1266 real-life visual quizzes, with human participants providing feedback. The results show a suitable accuracy rates, validating the tools potential to bridge the gap in Arabic language education and highlighting the tools promise as a reliable, AI-powered resource for Arabic learners, offering personalized and interactive learning experiences.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.03485v1" target="_blank">LRQ-DiT: Log-Rotation Post-Training Quantization of Diffusion Transformers for Text-to-Image Generation</a></h3>
                    <p><strong>Authors:</strong> Lianwei Yang, Haokun Lin, Tianchen Zhao, Yichen Wu, Hongyu Zhu, Ruiqi Xie, Zhenan Sun, Yu Wang, Qingyi Gu</p>
                    <p><strong>Published:</strong> 8/5/2025</p>
                    <p><strong>Categories:</strong> cs.CV</p>
                    <p><strong>Summary:</strong> Diffusion Transformers (DiTs) have achieved impressive performance in text-to-image generation. However, their high computational cost and large parameter sizes pose significant challenges for usage in resource-constrained scenarios. Post-training quantization (PTQ) is a promising solution to reduce memory usage and accelerate inference, but existing PTQ methods suffer from severe performance degradation under extreme low-bit settings. We identify two key obstacles to low-bit post-training quantization for DiT models: (1) model weights follow a Gaussian-like distribution with long tails, causing uniform quantization to poorly allocate intervals and leading to significant errors; (2) two types of activation outliers: (i) Mild Outliers with slightly elevated values, and (ii) Salient Outliers with large magnitudes concentrated in specific channels, which disrupt activation quantization. To address these issues, we propose LRQ-DiT, an efficient and accurate PTQ framework. We introduce Twin-Log Quantization (TLQ), a log-based method that aligns well with the weight distribution and reduces quantization errors. We also propose an Adaptive Rotation Scheme (ARS) that dynamically applies Hadamard or outlier-aware rotations based on activation fluctuation, effectively mitigating the impact of both types of outliers. We evaluate LRQ-DiT on PixArt and FLUX under various bit-width settings, and validate the performance on COCO, MJHQ, and sDCI datasets. LRQ-DiT achieves low-bit quantization of DiT models while preserving image quality, outperforming existing PTQ baselines.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.03694v1" target="_blank">LongVie: Multimodal-Guided Controllable Ultra-Long Video Generation</a></h3>
                    <p><strong>Authors:</strong> Jianxiong Gao, Zhaoxi Chen, Xian Liu, Jianfeng Feng, Chenyang Si, Yanwei Fu, Yu Qiao, Ziwei Liu</p>
                    <p><strong>Published:</strong> 8/5/2025</p>
                    <p><strong>Categories:</strong> cs.CV</p>
                    <p><strong>Summary:</strong> Controllable ultra-long video generation is a fundamental yet challenging task. Although existing methods are effective for short clips, they struggle to scale due to issues such as temporal inconsistency and visual degradation. In this paper, we initially investigate and identify three key factors: separate noise initialization, independent control signal normalization, and the limitations of single-modality guidance. To address these issues, we propose LongVie, an end-to-end autoregressive framework for controllable long video generation. LongVie introduces two core designs to ensure temporal consistency: 1) a unified noise initialization strategy that maintains consistent generation across clips, and 2) global control signal normalization that enforces alignment in the control space throughout the entire video. To mitigate visual degradation, LongVie employs 3) a multi-modal control framework that integrates both dense (e.g., depth maps) and sparse (e.g., keypoints) control signals, complemented by 4) a degradation-aware training strategy that adaptively balances modality contributions over time to preserve visual quality. We also introduce LongVGenBench, a comprehensive benchmark consisting of 100 high-resolution videos spanning diverse real-world and synthetic environments, each lasting over one minute. Extensive experiments show that LongVie achieves state-of-the-art performance in long-range controllability, consistency, and quality.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.03695v1" target="_blank">Trokens: Semantic-Aware Relational Trajectory Tokens for Few-Shot Action Recognition</a></h3>
                    <p><strong>Authors:</strong> Pulkit Kumar, Shuaiyi Huang, Matthew Walmer, Sai Saketh Rambhatla, Abhinav Shrivastava</p>
                    <p><strong>Published:</strong> 8/5/2025</p>
                    <p><strong>Categories:</strong> cs.CV</p>
                    <p><strong>Summary:</strong> Video understanding requires effective modeling of both motion and appearance information, particularly for few-shot action recognition. While recent advances in point tracking have been shown to improve few-shot action recognition, two fundamental challenges persist: selecting informative points to track and effectively modeling their motion patterns. We present Trokens, a novel approach that transforms trajectory points into semantic-aware relational tokens for action recognition. First, we introduce a semantic-aware sampling strategy to adaptively distribute tracking points based on object scale and semantic relevance. Second, we develop a motion modeling framework that captures both intra-trajectory dynamics through the Histogram of Oriented Displacements (HoD) and inter-trajectory relationships to model complex action patterns. Our approach effectively combines these trajectory tokens with semantic features to enhance appearance features with motion information, achieving state-of-the-art performance across six diverse few-shot action recognition benchmarks: Something-Something-V2 (both full and small splits), Kinetics, UCF101, HMDB51, and FineGym. For project page see https://trokens-iccv25.github.io</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.03692v1" target="_blank">LiDARCrafter: Dynamic 4D World Modeling from LiDAR Sequences</a></h3>
                    <p><strong>Authors:</strong> Ao Liang, Youquan Liu, Yu Yang, Dongyue Lu, Linfeng Li, Lingdong Kong, Huaici Zhao, Wei Tsang Ooi</p>
                    <p><strong>Published:</strong> 8/5/2025</p>
                    <p><strong>Categories:</strong> cs.CV, cs.RO</p>
                    <p><strong>Summary:</strong> Generative world models have become essential data engines for autonomous driving, yet most existing efforts focus on videos or occupancy grids, overlooking the unique LiDAR properties. Extending LiDAR generation to dynamic 4D world modeling presents challenges in controllability, temporal coherence, and evaluation standardization. To this end, we present LiDARCrafter, a unified framework for 4D LiDAR generation and editing. Given free-form natural language inputs, we parse instructions into ego-centric scene graphs, which condition a tri-branch diffusion network to generate object structures, motion trajectories, and geometry. These structured conditions enable diverse and fine-grained scene editing. Additionally, an autoregressive module generates temporally coherent 4D LiDAR sequences with smooth transitions. To support standardized evaluation, we establish a comprehensive benchmark with diverse metrics spanning scene-, object-, and sequence-level aspects. Experiments on the nuScenes dataset using this benchmark demonstrate that LiDARCrafter achieves state-of-the-art performance in fidelity, controllability, and temporal consistency across all levels, paving the way for data augmentation and simulation. The code and benchmark are released to the community.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.03691v1" target="_blank">La La LiDAR: Large-Scale Layout Generation from LiDAR Data</a></h3>
                    <p><strong>Authors:</strong> Youquan Liu, Lingdong Kong, Weidong Yang, Xin Li, Ao Liang, Runnan Chen, Ben Fei, Tongliang Liu</p>
                    <p><strong>Published:</strong> 8/5/2025</p>
                    <p><strong>Categories:</strong> cs.CV, cs.RO</p>
                    <p><strong>Summary:</strong> Controllable generation of realistic LiDAR scenes is crucial for applications such as autonomous driving and robotics. While recent diffusion-based models achieve high-fidelity LiDAR generation, they lack explicit control over foreground objects and spatial relationships, limiting their usefulness for scenario simulation and safety validation. To address these limitations, we propose Large-scale Layout-guided LiDAR generation model (La La LiDAR), a novel layout-guided generative framework that introduces semantic-enhanced scene graph diffusion with relation-aware contextual conditioning for structured LiDAR layout generation, followed by foreground-aware control injection for complete scene generation. This enables customizable control over object placement while ensuring spatial and semantic consistency. To support our structured LiDAR generation, we introduce Waymo-SG and nuScenes-SG, two large-scale LiDAR scene graph datasets, along with new evaluation metrics for layout synthesis. Extensive experiments demonstrate that La La LiDAR achieves state-of-the-art performance in both LiDAR generation and downstream perception tasks, establishing a new benchmark for controllable 3D scene generation.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.03690v1" target="_blank">Veila: Panoramic LiDAR Generation from a Monocular RGB Image</a></h3>
                    <p><strong>Authors:</strong> Youquan Liu, Lingdong Kong, Weidong Yang, Ao Liang, Jianxiong Gao, Yang Wu, Xiang Xu, Xin Li, Linfeng Li, Runnan Chen, Ben Fei</p>
                    <p><strong>Published:</strong> 8/5/2025</p>
                    <p><strong>Categories:</strong> cs.CV, cs.RO</p>
                    <p><strong>Summary:</strong> Realistic and controllable panoramic LiDAR data generation is critical for scalable 3D perception in autonomous driving and robotics. Existing methods either perform unconditional generation with poor controllability or adopt text-guided synthesis, which lacks fine-grained spatial control. Leveraging a monocular RGB image as a spatial control signal offers a scalable and low-cost alternative, which remains an open problem. However, it faces three core challenges: (i) semantic and depth cues from RGB are vary spatially, complicating reliable conditioning generation; (ii) modality gaps between RGB appearance and LiDAR geometry amplify alignment errors under noisy diffusion; and (iii) maintaining structural coherence between monocular RGB and panoramic LiDAR is challenging, particularly in non-overlap regions between images and LiDAR. To address these challenges, we propose Veila, a novel conditional diffusion framework that integrates: a Confidence-Aware Conditioning Mechanism (CACM) that strengthens RGB conditioning by adaptively balancing semantic and depth cues according to their local reliability; a Geometric Cross-Modal Alignment (GCMA) for robust RGB-LiDAR alignment under noisy diffusion; and a Panoramic Feature Coherence (PFC) for enforcing global structural consistency across monocular RGB and panoramic LiDAR. Additionally, we introduce two metrics, Cross-Modal Semantic Consistency and Cross-Modal Depth Consistency, to evaluate alignment quality across modalities. Experiments on nuScenes, SemanticKITTI, and our proposed KITTI-Weather benchmark demonstrate that Veila achieves state-of-the-art generation fidelity and cross-modal consistency, while enabling generative data augmentation that improves downstream LiDAR semantic segmentation.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.03686v1" target="_blank">CompassVerifier: A Unified and Robust Verifier for LLMs Evaluation and Outcome Reward</a></h3>
                    <p><strong>Authors:</strong> Shudong Liu, Hongwei Liu, Junnan Liu, Linchen Xiao, Songyang Gao, Chengqi Lyu, Yuzhe Gu, Wenwei Zhang, Derek F. Wong, Songyang Zhang, Kai Chen</p>
                    <p><strong>Published:</strong> 8/5/2025</p>
                    <p><strong>Categories:</strong> cs.CL, cs.AI</p>
                    <p><strong>Summary:</strong> Answer verification is crucial not only for evaluating large language models (LLMs) by matching their unstructured outputs against standard answers, but also serves as the reward model to guide LLM optimization. Most evaluation frameworks rely on regularized matching or employ general LLMs for answer verification, which demands extensive, repetitive customization for regex rules or evaluation prompts. Two fundamental limitations persist in current methodologies: 1) the absence of comprehensive benchmarks that systematically evaluate verification capabilities across different LLMs; and 2) the nascent stage of verifier development, where existing approaches lack both the robustness to handle complex edge cases and the generalizability across different domains. In this work, we develop CompassVerifier, an accurate and robust lightweight verifier model for evaluation and outcome reward. It demonstrates multi-domain competency spanning math, knowledge, and diverse reasoning tasks, with the capability to process various answer types, including multi-subproblems, formulas, and sequence answers, while effectively identifying abnormal/invalid responses. We introduce VerifierBench benchmark comprising model outputs collected from multiple data sources, augmented through manual analysis of metaerror patterns to enhance CompassVerifier. We anticipate that CompassVerifier and VerifierBench will facilitate answer verification, evaluation protocols, and reinforcement learning research. Code and dataset are available at https://github.com/open-compass/CompassVerifier.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.03683v1" target="_blank">Phonon Dynamics in Spherically-Curved Analog-Gravity Bose-Einstein Condensates</a></h3>
                    <p><strong>Authors:</strong> J. Austin Chunn, Ruotong Zhai, Daniel E. Sheehy</p>
                    <p><strong>Published:</strong> 8/5/2025</p>
                    <p><strong>Categories:</strong> cond-mat.quant-gas</p>
                    <p><strong>Summary:</strong> We study the low energy phonon dynamics of a Bose-Einstein condensate (BEC) with a density profile that is equivalent, via a coordinate transformation, to phonons traveling in a \lq\lq spherical\rq\rq\ curved spacetime that realizes the Friedman-Lema\^itre-Robertson-Walker (FLRW) metric. The metric of this BEC is characterized by its curvature $\kappa$ and a time-depdendent scale factor $a(t)$, with an increase in the latter corresponding to an expansion of the analog FLRW universe. We study the propagation of classical phonons in such BECs, finding that a sudden change in the scale factor induces ripples in the wave motion. In addition, we study quantum phonon creation (or vacuum amplification) due to the scale-factor modification and quantify their entanglement.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.03679v1" target="_blank">Streaming Generated Gaussian Process Experts for Online Learning and Control</a></h3>
                    <p><strong>Authors:</strong> Zewen Yang, Dongfa Zhang, Xiaobing Dai, Fengyi Yu, Chi Zhang, Bingkun Huang, Hamid Sadeghian, Sami Haddadin</p>
                    <p><strong>Published:</strong> 8/5/2025</p>
                    <p><strong>Categories:</strong> cs.LG, cs.SY, eess.SY, stat.ML</p>
                    <p><strong>Summary:</strong> Gaussian Processes (GPs), as a nonparametric learning method, offer flexible modeling capabilities and calibrated uncertainty quantification for function approximations. Additionally, GPs support online learning by efficiently incorporating new data with polynomial-time computation, making them well-suited for safety-critical dynamical systems that require rapid adaptation. However, the inference and online updates of exact GPs, when processing streaming data, incur cubic computation time and quadratic storage memory complexity, limiting their scalability to large datasets in real-time settings. In this paper, we propose a \underline{s}treaming \underline{k}ernel-induced progressivel\underline{y} generated expert framework of \underline{G}aussian \underline{p}rocesses (SkyGP) that addresses both computational and memory constraints by maintaining a bounded set of experts, while inheriting the learning performance guarantees from exact Gaussian processes. Furthermore, two SkyGP variants are introduced, each tailored to a specific objective, either maximizing prediction accuracy (SkyGP-Dense) or improving computational efficiency (SkyGP-Fast). The effectiveness of SkyGP is validated through extensive benchmarks and real-time control experiments demonstrating its superior performance compared to state-of-the-art approaches.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.03678v1" target="_blank">More Than a Score: Probing the Impact of Prompt Specificity on LLM Code Generation</a></h3>
                    <p><strong>Authors:</strong> Yangtian Zi, Harshitha Menon, Arjun Guha</p>
                    <p><strong>Published:</strong> 8/5/2025</p>
                    <p><strong>Categories:</strong> cs.CL, cs.LG, cs.PL</p>
                    <p><strong>Summary:</strong> State-of-the-art Large Language Models (LLMs) achieve high pass@1 on general benchmarks like HumanEval but underperform on specialized suites such as ParEval. Is this due to LLMs missing domain knowledge or insufficient prompt detail is given? To answer this, we introduce PartialOrderEval, which augments any code generation benchmark with a partial order of prompts from minimal to maximally detailed. Applying it to HumanEval and both serial and OpenMP subsets of ParEval, we measure how pass@1 scales with prompt specificity. Our experiments with Llama-3.x and Qwen2.5-Coder demonstrate varying degrees of prompt sensitivity across different tasks, and a qualitative analysis highlights explicit I/O specifications, edge-case handling, and stepwise breakdowns as the key drivers of prompt detail improvement.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.03677v1" target="_blank">FairLangProc: A Python package for fairness in NLP</a></h3>
                    <p><strong>Authors:</strong> Arturo PÃ©rez-Peralta, Sandra BenÃ­tez-PeÃ±a, Rosa E. Lillo</p>
                    <p><strong>Published:</strong> 8/5/2025</p>
                    <p><strong>Categories:</strong> cs.CL, stat.ML, 68T50, I.2.7</p>
                    <p><strong>Summary:</strong> The rise in usage of Large Language Models to near ubiquitousness in recent years has risen societal concern about their applications in decision-making contexts, such as organizational justice or healthcare. This, in turn, poses questions about the fairness of these models in critical settings, which leads to the developement of different procedures to address bias in Natural Language Processing. Although many datasets, metrics and algorithms have been proposed to measure and mitigate harmful prejudice in Natural Language Processing, their implementation is diverse and far from centralized. As a response, this paper presents FairLangProc, a comprehensive Python package providing a common implementation of some of the more recent advances in fairness in Natural Language Processing providing an interface compatible with the famous Hugging Face transformers library, aiming to encourage the widespread use and democratization of bias mitigation techniques. The implementation can be found on https://github.com/arturo-perez-peralta/FairLangProc.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.03672v1" target="_blank">Inland-LOAM: Voxel-Based Structural Semantic Mapping for Inland Waterways</a></h3>
                    <p><strong>Authors:</strong> Zhongbi Luo, Yunjia Wang, Jan Swevers, Peter Slaets, Herman Bruyninckx</p>
                    <p><strong>Published:</strong> 8/5/2025</p>
                    <p><strong>Categories:</strong> cs.RO</p>
                    <p><strong>Summary:</strong> Accurate geospatial information is crucial for safe, autonomous Inland Waterway Transport (IWT), as existing charts (IENC) lack real-time detail and conventional LiDAR SLAM fails in waterway environments. These challenges lead to vertical drift and non-semantic maps, hindering autonomous navigation. This paper introduces Inland-LOAM, a LiDAR SLAM framework for waterways. It uses an improved feature extraction and a water surface planar constraint to mitigate vertical drift. A novel pipeline transforms 3D point clouds into structured 2D semantic maps using voxel-based geometric analysis, enabling real-time computation of navigational parameters like bridge clearances. An automated module extracts shorelines and exports them into a lightweight, IENC-compatible format. Evaluations on a real-world dataset show Inland-LOAM achieves superior localization accuracy over state-of-the-art methods. The generated semantic maps and shorelines align with real-world conditions, providing reliable data for enhanced situational awareness. The code and dataset will be publicly available</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.03671v1" target="_blank">Fast Computation of Path Integrals of Killed Processes Using Confined Stochastic Bridges</a></h3>
                    <p><strong>Authors:</strong> Henrique B. N. Monteiro, Daniel M. Tartakovsky</p>
                    <p><strong>Published:</strong> 8/5/2025</p>
                    <p><strong>Categories:</strong> math.PR, physics.comp-ph, 6008, 60H05, 60J70, 81S40, 82M31</p>
                    <p><strong>Summary:</strong> Expectations of path integrals of killed stochastic processes play a central role in several applications across physics, chemistry, and finance. Simulation-based evaluation of these functionals is often biased and numerically expensive due to the need to explicitly approximate stochastic paths and the challenge of correctly modeling them in the neighborhood of the killing boundary. We consider It\^{o} processes killed at the boundary of some set in the $n$-dimensional space and introduce a novel stochastic method with negligible bias and lower computational cost to evaluate path integrals without simulated paths. Our approach draws a connection between stochastic bridges and killed processes to sample only exit times and locations instead of the full path. We apply it to a Wiener process killed in the $n$-ball and explicitly derive the density of the Brownian bridge confined to the $n$-ball for $n = 1, 2, 3$. Finally, we present two numerical examples that demonstrate the efficiency and negligible bias of the novel procedure compared to an evaluation using the standard Euler-Maruyama method.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.03669v1" target="_blank">OmniShape: Zero-Shot Multi-Hypothesis Shape and Pose Estimation in the Real World</a></h3>
                    <p><strong>Authors:</strong> Katherine Liu, Sergey Zakharov, Dian Chen, Takuya Ikeda, Greg Shakhnarovich, Adrien Gaidon, Rares Ambrus</p>
                    <p><strong>Published:</strong> 8/5/2025</p>
                    <p><strong>Categories:</strong> cs.CV, cs.RO</p>
                    <p><strong>Summary:</strong> We would like to estimate the pose and full shape of an object from a single observation, without assuming known 3D model or category. In this work, we propose OmniShape, the first method of its kind to enable probabilistic pose and shape estimation. OmniShape is based on the key insight that shape completion can be decoupled into two multi-modal distributions: one capturing how measurements project into a normalized object reference frame defined by the dataset and the other modelling a prior over object geometries represented as triplanar neural fields. By training separate conditional diffusion models for these two distributions, we enable sampling multiple hypotheses from the joint pose and shape distribution. OmniShape demonstrates compelling performance on challenging real world datasets. Project website: https://tri-ml.github.io/omnishape</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.03668v1" target="_blank">CTR-Sink: Attention Sink for Language Models in Click-Through Rate Prediction</a></h3>
                    <p><strong>Authors:</strong> Zixuan Li, Binzong Geng, Jing Xiong, Yong He, Yuxuan Hu, Jian Chen, Dingwei Chen, Xiyu Chang, Liang Zhang, Linjian Mo, Chengming Li, Chuan Yuan, Zhenan Sun</p>
                    <p><strong>Published:</strong> 8/5/2025</p>
                    <p><strong>Categories:</strong> cs.CL</p>
                    <p><strong>Summary:</strong> Click-Through Rate (CTR) prediction, a core task in recommendation systems, estimates user click likelihood using historical behavioral data. Modeling user behavior sequences as text to leverage Language Models (LMs) for this task has gained traction, owing to LMs strong semantic understanding and contextual modeling capabilities. However, a critical structural gap exists: user behavior sequences consist of discrete actions connected by semantically empty separators, differing fundamentally from the coherent natural language in LM pre-training. This mismatch causes semantic fragmentation, where LM attention scatters across irrelevant tokens instead of focusing on meaningful behavior boundaries and inter-behavior relationships, degrading prediction performance. To address this, we propose $\textit{CTR-Sink}$, a novel framework introducing behavior-level attention sinks tailored for recommendation scenarios. Inspired by attention sink theory, it constructs attention focus sinks and dynamically regulates attention aggregation via external information. Specifically, we insert sink tokens between consecutive behaviors, incorporating recommendation-specific signals such as temporal distance to serve as stable attention sinks. To enhance generality, we design a two-stage training strategy that explicitly guides LM attention toward sink tokens and a attention sink mechanism that amplifies inter-sink dependencies to better capture behavioral correlations. Experiments on one industrial dataset and two open-source datasets (MovieLens, Kuairec), alongside visualization results, validate the methods effectiveness across scenarios.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.03666v1" target="_blank">Beyond risk: A proto-framework for assessing the societal impact of AI systems</a></h3>
                    <p><strong>Authors:</strong> Willem Fourie</p>
                    <p><strong>Published:</strong> 8/5/2025</p>
                    <p><strong>Categories:</strong> cs.CY, cs.AI, cs.ET</p>
                    <p><strong>Summary:</strong> In the discourse on AI regulation, responsible AI is the dominant paradigm, with the focus on mitigating the risks related to AI systems. While this focus is important and necessary, it has limited use for a systematic consideration of AIs societal impact. This paper proposes a proto-framework for assessing the societal impact of AI systems by operationalising the concept of freedom. This proto-framework is intended as a step towards a fully operationalised framework to be used in policymaking contexts. By drawing on Kantian philosophy and related contemporary interpretations, freedom is developed as the counterpart to the concept of responsibility. Two dimensions of freedom are developed in further detail: freedom as capability and freedom as opportunity. These two dimensions of freedom are then applied in a proto-framework that systematically considers AIs impact on society using the Sustainable Development Goals. This proto-framework aims to complement current risk-based approaches and thereby offers a first step towards operationalising the concept of freedom in AI regulation.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.03663v1" target="_blank">Forest vs Tree: The $(N, K)$ Trade-off in Reproducible ML Evaluation</a></h3>
                    <p><strong>Authors:</strong> Deepak Pandita, Flip Korn, Chris Welty, Christopher M. Homan</p>
                    <p><strong>Published:</strong> 8/5/2025</p>
                    <p><strong>Categories:</strong> cs.LG, cs.AI, cs.CL</p>
                    <p><strong>Summary:</strong> Reproducibility is a cornerstone of scientific validation and of the authority it confers on its results. Reproducibility in machine learning evaluations leads to greater trust, confidence, and value. However, the ground truth responses used in machine learning often necessarily come from humans, among whom disagreement is prevalent, and surprisingly little research has studied the impact of effectively ignoring disagreement in these responses, as is typically the case. One reason for the lack of research is that budgets for collecting human-annotated evaluation data are limited, and obtaining more samples from multiple annotators for each example greatly increases the per-item annotation costs. We investigate the trade-off between the number of items ($N$) and the number of responses per item ($K$) needed for reliable machine learning evaluation. We analyze a diverse collection of categorical datasets for which multiple annotations per item exist, and simulated distributions fit to these datasets, to determine the optimal $(N, K)$ configuration, given a fixed budget ($N \times K$), for collecting evaluation data and reliably comparing the performance of machine learning models. Our findings show, first, that accounting for human disagreement may come with $N \times K$ at no more than 1000 (and often much lower) for every dataset tested on at least one metric. Moreover, this minimal $N \times K$ almost always occurred for $K  10$. Furthermore, the nature of the tradeoff between $K$ and $N$ -- or if one even existed -- depends on the evaluation metric, with metrics that are more sensitive to the full distribution of responses performing better at higher levels of $K$. Our methods can be used to help ML practitioners get more effective test data by finding the optimal metrics and number of items and annotations per item to collect to get the most reliability for their budget.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.03662v1" target="_blank">Rigidity for graph product von Neumann algebras</a></h3>
                    <p><strong>Authors:</strong> Camille Horbez, Adrian Ioana</p>
                    <p><strong>Published:</strong> 8/5/2025</p>
                    <p><strong>Categories:</strong> math.OA, math.GR</p>
                    <p><strong>Summary:</strong> We establish rigidity theorems for graph product von Neumann algebras $M_\Gamma=*_{v,\Gamma}M_v$ associated to finite simple graphs $\Gamma$ and families of tracial von Neumann algebras $(M_v)_{v\in\Gamma}$. We consider the following three broad classes of vertex algebras: diffuse, diffuse amenable, and II$_1$ factors. In each of these three regimes, we exhibit a large class of graphs $\Gamma,\Lambda$ for which the following holds: any isomorphism $\theta$ between $M_\Gamma$ and $N_\Lambda$ ensures the existence of a graph isomorphism $\alpha:\Gamma\to\Lambda$, and tight relations between $\theta(M_v)$ and $N_{\alpha(v)}$ for every vertex $v\in\Gamma$, ranging from strong intertwining in both directions (in the sense of Popa), to unitary conjugacy in some cases. Our results lead to a wide range of applications to the classification of graph product von Neumann algebras and the calculation of their symmetry groups. First, we obtain general classification theorems for von Neumann algebras of right-angled Artin groups and of graph products of ICC groups. We also provide a new family of II$_1$ factors with trivial fundamental group, including all graph products of II$_1$ factors over graphs with girth at least $5$ and no vertices of degree $0$ or $1$. Finally, we compute the outer automorphism group of certain graph products of II$_1$ factors.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.03661v1" target="_blank">Automated Algorithmic Discovery for Gravitational-Wave Detection Guided by LLM-Informed Evolutionary Monte Carlo Tree Search</a></h3>
                    <p><strong>Authors:</strong> He Wang, Liang Zeng</p>
                    <p><strong>Published:</strong> 8/5/2025</p>
                    <p><strong>Categories:</strong> cs.AI, astro-ph.HE, astro-ph.IM, gr-qc</p>
                    <p><strong>Summary:</strong> Computational scientific discovery increasingly relies on algorithms to process complex data and identify meaningful patterns - yet faces persistent challenges in gravitational-wave signal identification. While existing algorithmic approaches like matched filtering (MF) and deep neural networks (DNNs) have achieved partial success, their limitations directly stem from fundamental limitations: MFs excessive computational demands arise from its reliance on predefined theoretical waveform templates, while DNNs black-box architectures obscure decision logic and introduce hidden biases. We propose Evolutionary Monte Carlo Tree Search (Evo-MCTS), a framework that addresses these limitations through systematic algorithm space exploration guided by domain-aware physical constraints. Our approach combines tree-structured search with evolutionary optimization and large language model heuristics to create interpretable algorithmic solutions. Our Evo-MCTS framework demonstrates substantial improvements, achieving a 20.2\% improvement over state-of-the-art gravitational wave detection algorithms on the MLGWSC-1 benchmark dataset. High-performing algorithm variants consistently exceed thresholds. The framework generates human-interpretable algorithmic pathways that reveal distinct performance patterns. Beyond performance improvements, our framework discovers novel algorithmic combinations, thereby establishing a transferable methodology for automated algorithmic discovery across computational science domains.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.03659v1" target="_blank">A noninvasive and nonadiabatic quantum Maxwell demon</a></h3>
                    <p><strong>Authors:</strong> Lucas Trigal, Rafael SÃ¡nchez</p>
                    <p><strong>Published:</strong> 8/5/2025</p>
                    <p><strong>Categories:</strong> cond-mat.mes-hall, quant-ph</p>
                    <p><strong>Summary:</strong> A quantum mechanical Maxwell demon is proposed in a quantum dot setting. The demon avoids continuous-measurement induced decoherence by exploiting an undetailed charge detector. The control of coherent tunneling via Landau-Zener-St\uckelberg-Majorana driving allows for efficient feedback operations with no work invested. The local violation of the second law achieves simultaneous power generation and cooling. We discuss the response current fluctuations, and the demon backaction deriving from failures, finding optimal performance in the nonadiabatic regime.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.03655v1" target="_blank">Theoretical framework for lattice QCD computations of $B\to K \ell^+ \ell^-$ and $\bar{B}_s\to \ell^+\ell^- Î³$ decays rates, including contributions from Charming Penguins</a></h3>
                    <p><strong>Authors:</strong> R. Frezzotti, G. Gagliardi, V. Lubicz, G. Martinelli, C. T. Sachrajda, F. Sanfilippo, L. Silvestrini, S. Simula, N. Tantalo</p>
                    <p><strong>Published:</strong> 8/5/2025</p>
                    <p><strong>Categories:</strong> hep-lat, hep-ph</p>
                    <p><strong>Summary:</strong> We develop a strategy for computing the $B\to K\ell^+\ell^-$ and $\bar{B}_s\to\gamma\ell^+\ell^-$ decay amplitudes using lattice QCD (where $\ell^\pm$ are charged leptons). We focus on those terms which contain complex contributions to the amplitude, due to on-shell intermediate states propagating between the weak operator and electromagnetic current(s). Such terms, which are generally estimated using model calculations and represent significant uncertainties in the phenomenological predictions for these decays, cannot be computed using standard lattice QCD techniques. It has recently been shown that such contributions can be computed using spectral-density methods and our proposed strategy, which we discuss in detail, is built on this approach. The complex contributions include the ``charming penguins (matrix elements of the current-current operators $O_1^{(c)}$ and $O_2^{(c)}$ defined in Eq. (6) below), in which the charm-quark loop can propagate long distances, particularly close to the region of charmonium resonances. They also include the contributions from the chromomagnetic operator ($O_8$ in standard notation, defined in Eq. (8) below). We discuss the renormalization of the ultra-violet divergences, and in particular those which arise due to ``contact terms, and explain how those which appear as inverse powers of the lattice spacing can be subtracted non-perturbatively. We apply the spectral density methods in an instructive exploratory computation of the charming penguin diagram in $B\to K\ell^+\ell^-$ decays in which the virtual photon is emitted from the charm-quark loop (the diagram in Fig. 1(a) below) and discuss the prospects and strategies for the reliable determination of the amplitudes in future dedicated computations.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.03654v1" target="_blank">Can Large Vision-Language Models Understand Multimodal Sarcasm?</a></h3>
                    <p><strong>Authors:</strong> Xinyu Wang, Yue Zhang, Liqiang Jing</p>
                    <p><strong>Published:</strong> 8/5/2025</p>
                    <p><strong>Categories:</strong> cs.CL, cs.CV</p>
                    <p><strong>Summary:</strong> Sarcasm is a complex linguistic phenomenon that involves a disparity between literal and intended meanings, making it challenging for sentiment analysis and other emotion-sensitive tasks. While traditional sarcasm detection methods primarily focus on text, recent approaches have incorporated multimodal information. However, the application of Large Visual Language Models (LVLMs) in Multimodal Sarcasm Analysis (MSA) remains underexplored. In this paper, we evaluate LVLMs in MSA tasks, specifically focusing on Multimodal Sarcasm Detection and Multimodal Sarcasm Explanation. Through comprehensive experiments, we identify key limitations, such as insufficient visual understanding and a lack of conceptual knowledge. To address these issues, we propose a training-free framework that integrates in-depth object extraction and external conceptual knowledge to improve the models ability to interpret and explain sarcasm in multimodal contexts. The experimental results on multiple models show the effectiveness of our proposed framework. The code is available at https://github.com/cp-cp/LVLM-MSA.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.03653v1" target="_blank">Optimized imaging prefiltering for enhanced image segmentation</a></h3>
                    <p><strong>Authors:</strong> Ronny Vallejos, Felipe Osorio, Sebastian Vidal, Grisel Britos</p>
                    <p><strong>Published:</strong> 8/5/2025</p>
                    <p><strong>Categories:</strong> stat.AP, stat.ME</p>
                    <p><strong>Summary:</strong> The Box-Cox transformation, introduced in 1964, is a widely used statistical tool for stabilizing variance and improving normality in data analysis. Its application in image processing, particularly for image enhancement, has gained increasing attention in recent years. This paper investigates the use of the Box-Cox transformation as a preprocessing step for image segmentation, with a focus on the estimation of the transformation parameter. We evaluate the effectiveness of the transformation by comparing various segmentation methods, highlighting its advantages for traditional machine learning techniques-especially in situations where no training data is available. The results demonstrate that the transformation enhances feature separability and computational efficiency, making it particularly beneficial for models like discriminant analysis. In contrast, deep learning models did not show consistent improvements, underscoring the differing impacts of the transformation across model types and image characteristics.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.03652v1" target="_blank">Maximally non-projective measurements are not always symmetric informationally complete</a></h3>
                    <p><strong>Authors:</strong> Gabriele Cobucci, Raphael Brinster, Shishir Khandelwal, Hermann Kampermann, Dagmar BruÃŸ, Nikolai Wyderka, Armin Tavakoli</p>
                    <p><strong>Published:</strong> 8/5/2025</p>
                    <p><strong>Categories:</strong> quant-ph</p>
                    <p><strong>Summary:</strong> Whereas standard quantum measurements are projective, the most general notion of a measurement is represented by positive operator-valued measures (POVMs). It is therefore natural to consider how accurately an experimenter with access only to projective measurements and classical processing can simulate POVMs. The most well-known class of non-projective measurements is called symmetric informationally complete (SIC). Such measurements are both ubiquitous in the broader scope of quantum information theory and known to be the most strongly non-projective measurements in qubit systems. Here, we show that beyond qubit systems, the SIC property is in general not associated with the most non-projective measurement. For this, we put forward a semidefinite programming criterion for detecting genuinely non-projective measurements. This method allows us to determine quantitative simulability thresholds for generic POVMs and to put forward a conjecture on which qutrit and ququart measurements that are most strongly non-projective.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.03650v1" target="_blank">Notes and computations on forbidden differences</a></h3>
                    <p><strong>Authors:</strong> Christian Dean, Haley Havard, Elizabeth Hawkins, Patch Heard, Andrew Lott, Alex Rice</p>
                    <p><strong>Published:</strong> 8/5/2025</p>
                    <p><strong>Categories:</strong> math.NT, math.CO</p>
                    <p><strong>Summary:</strong> We explore from several perspectives the following question: given $X\subseteq \mathbb{Z}$ and $N\in \mathbb{N}$, what is the maximum size $D(X,N)$ of $A\subseteq \{1,2,\dots,N\}$ before $A$ is forced to contain two distinct elements that differ by an element of $X$? The set of forbidden differences, $X$, is called \textit{intersective} if $D(X,N)=o(N)$, with the most well-studied examples being $X=S=\{n^2: n\in \mathbb{N}\}$ and $X=\mathcal{P}-1=\{p-1: p\text{ prime}\}$. In addition to some new results, including exact formulas and estimates for $D(X,N)$ in some non-intersective cases like $X=\mathcal{P}$ and $X=S+k$, $k\in \mathbb{N}$, we also provide a comprehensive survey of known bounds and extensive computational data. In particular, we utilize an existing algorithm for finding maximum cliques in graphs to determine $D(S,N)$ for $N\leq 300$ and $D(\mathcal{P}-1,N)$ for $N\leq 500$. None of these exact values appear previously in the literature.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.03645v1" target="_blank">DiWA: Diffusion Policy Adaptation with World Models</a></h3>
                    <p><strong>Authors:</strong> Akshay L Chandra, Iman Nematollahi, Chenguang Huang, Tim Welschehold, Wolfram Burgard, Abhinav Valada</p>
                    <p><strong>Published:</strong> 8/5/2025</p>
                    <p><strong>Categories:</strong> cs.RO, cs.CV, cs.LG</p>
                    <p><strong>Summary:</strong> Fine-tuning diffusion policies with reinforcement learning (RL) presents significant challenges. The long denoising sequence for each action prediction impedes effective reward propagation. Moreover, standard RL methods require millions of real-world interactions, posing a major bottleneck for practical fine-tuning. Although prior work frames the denoising process in diffusion policies as a Markov Decision Process to enable RL-based updates, its strong dependence on environment interaction remains highly inefficient. To bridge this gap, we introduce DiWA, a novel framework that leverages a world model for fine-tuning diffusion-based robotic skills entirely offline with reinforcement learning. Unlike model-free approaches that require millions of environment interactions to fine-tune a repertoire of robot skills, DiWA achieves effective adaptation using a world model trained once on a few hundred thousand offline play interactions. This results in dramatically improved sample efficiency, making the approach significantly more practical and safer for real-world robot learning. On the challenging CALVIN benchmark, DiWA improves performance across eight tasks using only offline adaptation, while requiring orders of magnitude fewer physical interactions than model-free baselines. To our knowledge, this is the first demonstration of fine-tuning diffusion policies for real-world robotic skills using an offline world model. We make the code publicly available at https://diwa.cs.uni-freiburg.de.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.03644v1" target="_blank">Are We on the Right Way for Assessing Document Retrieval-Augmented Generation?</a></h3>
                    <p><strong>Authors:</strong> Wenxuan Shen, Mingjia Wang, Yaochen Wang, Dongping Chen, Junjie Yang, Yao Wan, Weiwei Lin</p>
                    <p><strong>Published:</strong> 8/5/2025</p>
                    <p><strong>Categories:</strong> cs.CL, cs.CV, cs.IR</p>
                    <p><strong>Summary:</strong> Retrieval-Augmented Generation (RAG) systems using Multimodal Large Language Models (MLLMs) show great promise for complex document understanding, yet their development is critically hampered by inadequate evaluation. Current benchmarks often focus on specific part of document RAG system and use synthetic data with incomplete ground truth and evidence labels, therefore failing to reflect real-world bottlenecks and challenges. To overcome these limitations, we introduce Double-Bench: a new large-scale, multilingual, and multimodal evaluation system that is able to produce fine-grained assessment to each component within document RAG systems. It comprises 3,276 documents (72,880 pages) and 5,168 single- and multi-hop queries across 6 languages and 4 document types with streamlined dynamic update support for potential data contamination issues. Queries are grounded in exhaustively scanned evidence pages and verified by human experts to ensure maximum quality and completeness. Our comprehensive experiments across 9 state-of-the-art embedding models, 4 MLLMs and 4 end-to-end document RAG frameworks demonstrate the gap between text and visual embedding models is narrowing, highlighting the need in building stronger document retrieval models. Our findings also reveal the over-confidence dilemma within current document RAG frameworks that tend to provide answer even without evidence support. We hope our fully open-source Double-Bench provide a rigorous foundation for future research in advanced document RAG systems. We plan to retrieve timely corpus and release new benchmarks on an annual basis.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.03643v1" target="_blank">Uni3R: Unified 3D Reconstruction and Semantic Understanding via Generalizable Gaussian Splatting from Unposed Multi-View Images</a></h3>
                    <p><strong>Authors:</strong> Xiangyu Sun, Haoyi jiang, Liu Liu, Seungtae Nam, Gyeongjin Kang, Xinjie wang, Wei Sui, Zhizhong Su, Wenyu Liu, Xinggang Wang, Eunbyung Park</p>
                    <p><strong>Published:</strong> 8/5/2025</p>
                    <p><strong>Categories:</strong> cs.CV</p>
                    <p><strong>Summary:</strong> Reconstructing and semantically interpreting 3D scenes from sparse 2D views remains a fundamental challenge in computer vision. Conventional methods often decouple semantic understanding from reconstruction or necessitate costly per-scene optimization, thereby restricting their scalability and generalizability. In this paper, we introduce Uni3R, a novel feed-forward framework that jointly reconstructs a unified 3D scene representation enriched with open-vocabulary semantics, directly from unposed multi-view images. Our approach leverages a Cross-View Transformer to robustly integrate information across arbitrary multi-view inputs, which then regresses a set of 3D Gaussian primitives endowed with semantic feature fields. This unified representation facilitates high-fidelity novel view synthesis, open-vocabulary 3D semantic segmentation, and depth prediction, all within a single, feed-forward pass. Extensive experiments demonstrate that Uni3R establishes a new state-of-the-art across multiple benchmarks, including 25.07 PSNR on RE10K and 55.84 mIoU on ScanNet. Our work signifies a novel paradigm towards generalizable, unified 3D scene reconstruction and understanding. The code is available at https://github.com/HorizonRobotics/Uni3R.</p>
                
            
                
                    <h3><a href="http://dx.doi.org/10.4204/EPTCS.424.5" target="_blank">Visual Execution and Validation of Finite-State Machines and Pushdown Automata</a></h3>
                    <p><strong>Authors:</strong> Marco T. MorazÃ¡n, David Anthony K. Fields, AndrÃ©s M. Garced, Tijana MiniÄ‡</p>
                    <p><strong>Published:</strong> 8/5/2025</p>
                    <p><strong>Categories:</strong> cs.FL, cs.HC, cs.PL, cs.SE</p>
                    <p><strong>Summary:</strong> In Formal Languages and Automata Theory courses, students find understanding nondeterministic finite-state and pushdown automata difficult. In many cases, this means that it is challenging for them to comprehend the operational semantics of such machines and, as a consequence, determine why a word is accepted or rejected. This is not entirely surprising, because students are mostly trained to design and implement deterministic programs. Comprehension of pushdown automata is further complicated, because reasoning about the stack is necessary. A common difficulty students face, for example, is understanding that two different computations on the same word may reach the same state with different stack values. To aid student understanding, we present two novel dynamic visualization tools for FSM -- a domain-specific programming language for the Automata Theory classroom -- to support the design of such machines. These tools visualize all computations that may be performed, respectively, by a nondeterministic finite-state machine or by a pushdown automata in a stepwise manner. In addition, these tools aid the machine verification process by allowing users to visually validate whether the properties a state represents hold when a machine transitions into it.</p>
                
            
                
                    <h3><a href="http://dx.doi.org/10.4204/EPTCS.424.3" target="_blank">Teaching Introductory Functional Programming Using Haskelite</a></h3>
                    <p><strong>Authors:</strong> Pedro Vasconcelos</p>
                    <p><strong>Published:</strong> 8/5/2025</p>
                    <p><strong>Categories:</strong> cs.PL, D.3.2;D.3.4;K.3.1</p>
                    <p><strong>Summary:</strong> Learning functional programming requires learning a substitution-based computational model. While substitution should be a familiar concept from high-school algebra, students often have difficulty applying it to new settings, such as recursive definitions, algebraic data types and higher-order functions. Step-by-step interpreters have been shown to help beginners by clarifying misconceptions and improving understanding. This paper reports on the experience of using a step-by-step tracing interpreter for a subset of Haskell while teaching an introductory functional programming course at the University of Porto. We describe the use of the interpreter, present some feedback obtained from students, reflect on the lessons learned and point directions for further work.</p>
                
            
                
                    <h3><a href="http://dx.doi.org/10.4204/EPTCS.424.1" target="_blank">Design Support for Multitape Turing Machines</a></h3>
                    <p><strong>Authors:</strong> Marco T. MorazÃ¡n, Oliwia Kempinski, AndrÃ©s M. Garced</p>
                    <p><strong>Published:</strong> 8/5/2025</p>
                    <p><strong>Categories:</strong> cs.FL, cs.HC, cs.PL, cs.SE</p>
                    <p><strong>Summary:</strong> Many Formal Languages and Automata Theory courses introduce students to Turing machine extensions. One of the most widely-used extensions endows Turing machines with multiple tapes. Although multitape Turing machines are an abstraction to simplify Turing machine design, students find them no less challenging. To aid students in understanding these machines, the FSM programming language provides support for their definition and execution. This, however, has proven insufficient for many students to understand the operational semantics of such machines and to understand why such machines accept or reject a word. To address this problem, three visualization tools have been developed. The first is a dynamic visualization tool that simulates machine execution. The second is a static visualization tool that automatically renders a graphic for a multitape Turing machines transition diagram. The third is a static visualization tool that automatically renders computation graphs for multitape Turing machines. This article presents these tools and illustrates how they are used to help students design and implement multitape Turing machines. In addition, empirical data is presented that suggests these tools are well-received and found useful by students.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.03636v1" target="_blank">Likelihood Matching for Diffusion Models</a></h3>
                    <p><strong>Authors:</strong> Lei Qian, Wu Su, Yanqi Huang, Song Xi Chen</p>
                    <p><strong>Published:</strong> 8/5/2025</p>
                    <p><strong>Categories:</strong> stat.ML, cs.LG, math.ST, stat.AP, stat.ME, stat.TH</p>
                    <p><strong>Summary:</strong> We propose a Likelihood Matching approach for training diffusion models by first establishing an equivalence between the likelihood of the target data distribution and a likelihood along the sample path of the reverse diffusion. To efficiently compute the reverse sample likelihood, a quasi-likelihood is considered to approximate each reverse transition density by a Gaussian distribution with matched conditional mean and covariance, respectively. The score and Hessian functions for the diffusion generation are estimated by maximizing the quasi-likelihood, ensuring a consistent matching of both the first two transitional moments between every two time points. A stochastic sampler is introduced to facilitate computation that leverages on both the estimated score and Hessian information. We establish consistency of the quasi-maximum likelihood estimation, and provide non-asymptotic convergence guarantees for the proposed sampler, quantifying the rates of the approximation errors due to the score and Hessian estimation, dimensionality, and the number of diffusion steps. Empirical and simulation evaluations demonstrate the effectiveness of the proposed Likelihood Matching and validate the theoretical results.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.03625v1" target="_blank">AttZoom: Attention Zoom for Better Visual Features</a></h3>
                    <p><strong>Authors:</strong> Daniel DeAlcala, Aythami Morales, Julian Fierrez, Ruben Tolosana</p>
                    <p><strong>Published:</strong> 8/5/2025</p>
                    <p><strong>Categories:</strong> cs.CV, cs.AI</p>
                    <p><strong>Summary:</strong> We present Attention Zoom, a modular and model-agnostic spatial attention mechanism designed to improve feature extraction in convolutional neural networks (CNNs). Unlike traditional attention approaches that require architecture-specific integration, our method introduces a standalone layer that spatially emphasizes high-importance regions in the input. We evaluated Attention Zoom on multiple CNN backbones using CIFAR-100 and TinyImageNet, showing consistent improvements in Top-1 and Top-5 classification accuracy. Visual analyses using Grad-CAM and spatial warping reveal that our method encourages fine-grained and diverse attention patterns. Our results confirm the effectiveness and generality of the proposed layer for improving CCNs with minimal architectural overhead.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.03618v1" target="_blank">FPG-NAS: FLOPs-Aware Gated Differentiable Neural Architecture Search for Efficient 6DoF Pose Estimation</a></h3>
                    <p><strong>Authors:</strong> Nassim Ali Ousalah, Peyman Rostami, Anis Kacem, Enjie Ghorbel, Emmanuel Koumandakis, Djamila Aouada</p>
                    <p><strong>Published:</strong> 8/5/2025</p>
                    <p><strong>Categories:</strong> cs.CV</p>
                    <p><strong>Summary:</strong> We introduce FPG-NAS, a FLOPs-aware Gated Differentiable Neural Architecture Search framework for efficient 6DoF object pose estimation. Estimating 3D rotation and translation from a single image has been widely investigated yet remains computationally demanding, limiting applicability in resource-constrained scenarios. FPG-NAS addresses this by proposing a specialized differentiable NAS approach for 6DoF pose estimation, featuring a task-specific search space and a differentiable gating mechanism that enables discrete multi-candidate operator selection, thus improving architectural diversity. Additionally, a FLOPs regularization term ensures a balanced trade-off between accuracy and efficiency. The framework explores a vast search space of approximately 10\textsuperscript{92} possible architectures. Experiments on the LINEMOD and SPEED+ datasets demonstrate that FPG-NAS-derived models outperform previous methods under strict FLOPs constraints. To the best of our knowledge, FPG-NAS is the first differentiable NAS framework specifically designed for 6DoF object pose estimation.</p>
                
            
                
                    <h3><a href="http://dx.doi.org/10.1103/1gls-k3df" target="_blank">Interferometric signature of higher-order images in a parametrized framework</a></h3>
                    <p><strong>Authors:</strong> Fabiano Feleppa, Fabio Aratore, Valerio Bozza</p>
                    <p><strong>Published:</strong> 8/5/2025</p>
                    <p><strong>Categories:</strong> gr-qc, astro-ph.HE</p>
                    <p><strong>Summary:</strong> This paper investigates gravitational lensing in the strong deflection limit, focusing particularly on higher-order images produced near compact objects such as black holes and their observable impact through the visibility function. Employing a robust parametrization framework proposed by Rezzolla and Zhidenko, the study systematically explores deviations from the Schwarzschild metric. A detailed theoretical analysis of interferometric observables is provided, highlighting how higher-order images imprint distinctive, measurable patterns in the visibility function, notably characterized by a staircase-like structure. By parametrically varying metric coefficients, the analysis reveals clear dependencies between spacetime deviations and key observational signatures, specifically the step heights and periodicities in the interferometric visibility. The results enhance the theoretical groundwork for interpreting data from advanced interferometric observations, potentially enabling precise tests of general relativity and the discrimination among alternative gravitational theories.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.03614v1" target="_blank">Minimal Convolutional RNNs Accelerate Spatiotemporal Learning</a></h3>
                    <p><strong>Authors:</strong> CoÅŸku Can Horuz, Sebastian Otte, Martin V. Butz, Matthias Karlbauer</p>
                    <p><strong>Published:</strong> 8/5/2025</p>
                    <p><strong>Categories:</strong> cs.LG, cs.NE</p>
                    <p><strong>Summary:</strong> We introduce MinConvLSTM and MinConvGRU, two novel spatiotemporal models that combine the spatial inductive biases of convolutional recurrent networks with the training efficiency of minimal, parallelizable RNNs. Our approach extends the log-domain prefix-sum formulation of MinLSTM and MinGRU to convolutional architectures, enabling fully parallel training while retaining localized spatial modeling. This eliminates the need for sequential hidden state updates during teacher forcing - a major bottleneck in conventional ConvRNN models. In addition, we incorporate an exponential gating mechanism inspired by the xLSTM architecture into the MinConvLSTM, which further simplifies the log-domain computation. Our models are structurally minimal and computationally efficient, with reduced parameter count and improved scalability. We evaluate our models on two spatiotemporal forecasting tasks: Navier-Stokes dynamics and real-world geopotential data. In terms of training speed, our architectures significantly outperform standard ConvLSTMs and ConvGRUs. Moreover, our models also achieve lower prediction errors in both domains, even in closed-loop autoregressive mode. These findings demonstrate that minimal recurrent structures, when combined with convolutional input aggregation, offer a compelling and efficient alternative for spatiotemporal sequence modeling, bridging the gap between recurrent simplicity and spatial complexity.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.03613v1" target="_blank">Goedel-Prover-V2: Scaling Formal Theorem Proving with Scaffolded Data Synthesis and Self-Correction</a></h3>
                    <p><strong>Authors:</strong> Yong Lin, Shange Tang, Bohan Lyu, Ziran Yang, Jui-Hui Chung, Haoyu Zhao, Lai Jiang, Yihan Geng, Jiawei Ge, Jingruo Sun, Jiayun Wu, Jiri Gesi, Ximing Lu, David Acuna, Kaiyu Yang, Hongzhou Lin, Yejin Choi, Danqi Chen, Sanjeev Arora, Chi Jin</p>
                    <p><strong>Published:</strong> 8/5/2025</p>
                    <p><strong>Categories:</strong> cs.LG, cs.AI</p>
                    <p><strong>Summary:</strong> We introduce Goedel-Prover-V2, a series of open-source language models that set a new state-of-the-art in automated theorem proving. Built on the standard expert iteration and reinforcement learning pipeline, our approach incorporates three key innovations: (1) Scaffolded data synthesis: We generate synthetic tasks of increasing difficulty to train the model to master increasingly complex theorems; (2) Verifier-guided self-correction: We enable the model to iteratively revise its proofs by leveraging feedback from the Lean compiler; (3) Model averaging: We merge model checkpoints to mitigate the decrease in model output diversity in later stages of training. Our small model, Goedel-Prover-V2-8B, reaches 84.6% pass@32 on MiniF2F and outperforms DeepSeek-Prover-V2-671B under the same metric, despite being 80X smaller. Our flagship model, Goedel-Prover-V2-32B, achieves 88.1% on MiniF2F at pass@32 in standard mode and 90.4% in self-correction mode, outperforming prior SOTA by a large margin. Additionally, our flagship model solves 86 problems on PutnamBench at pass@184, securing the first place among open-source models on the leaderboard, surpassing DeepSeek-Prover-V2-671Bs record of solving 47 problems by pass@1024 with a significantly smaller model size and compute budget. At the time of its release (July-August 2025), Goedel-Prover-V2 achieves the strongest overall performance among all open-source theorem provers. It also ranks among the top-performing models--including closed-source systems with publicly reported performance--under a constrained test-time compute budget. Our models, code, and data are released at https://github.com/Goedel-LM/Goedel-Prover-V2.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.03612v1" target="_blank">Towards a classification of topological defects in $K3$ sigma models</a></h3>
                    <p><strong>Authors:</strong> Roberta Angius, Stefano Giaccari</p>
                    <p><strong>Published:</strong> 8/5/2025</p>
                    <p><strong>Categories:</strong> hep-th</p>
                    <p><strong>Summary:</strong> Given a $K3$ surface, a supersymmetric non-linear K3 sigma model is the internal superconformal field theory (SCFT) in a six dimensional compactification of type IIA superstring on $\mathbb{R}^{1,5} \times K3$. These models have attracted attention due to the discovery of Mathieu moonshine phenomena for the elliptic genera of K3 surfaces, and have played a pivotal role in extending Mukais theorem on classification of symplectic automorphisms of $K3$ surfaces. We report on recent progress (arXiv:2402.08719 [hep-th]) in characterizing topological defects in $K3$ models, generalizing the notion of symmetries to categories of topological operators supported on arbitrary codimension submanifolds with possibly non-invertible fusion rules. Taking advantage of the interpretation of Mukai lattice as the D-brane charge lattice, we present a number of general results for the category of topological defect lines preserving the superconformal algebra and spectral flow, obtained by studying their fusion with boundary states. While for certain K3 models infinitely many simple defects, and even a continuum, can occur, at generic points in the moduli space the category is actually trivial, i.e. it is generated by the identity defect. Furthermore, if a K3 model is at the attractor point for some BPS configuration of D-branes, then all topological defects have integral quantum dimension. We also introduce a conjecture that a continuum of topological defects arises if and only if the K3 model is a (possibly generalized) orbifold of a torus model. These general results are confirmed by the analysis of significant examples. We also point out the connection to recent studies of topological defects in the Conway moonshine module theory (arXiv:2412.21141 [hep-th],arXiv:2504.18619 [hep-th]).</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.03611v1" target="_blank">Block: Balancing Load in LLM Serving with Context, Knowledge and Predictive Scheduling</a></h3>
                    <p><strong>Authors:</strong> Wei Da, Evangelia Kalyvianaki</p>
                    <p><strong>Published:</strong> 8/5/2025</p>
                    <p><strong>Categories:</strong> cs.DC, cs.AI</p>
                    <p><strong>Summary:</strong> This paper presents Block, a distributed scheduling framework designed to optimize load balancing and auto-provisioning across instances in large language model serving frameworks by leveraging contextual information from incoming requests. Unlike popular model serving systems that rely on monolithic and heuristic task schedulers, Block operates as a fully distributed, stateless, and predictive scheduling system to achieve low overhead, reliability, and scalability. It leverages the deterministic and predictable characteristics of LLM inferences, such as host configurations, response lengths, and hardware performance, to make scheduling decisions based on accurately predicted metrics. Evaluation on a 12 GPUs cluster shows that Block significantly outperforms heuristic schedulers, boosting serving capacity by up to 16.7\% and reducing P99 tail latency by up to 49.5\%. These performance gains remain consistent across diverse models, workloads and configurations. Code and data are open-sourced.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.03609v1" target="_blank">evTransFER: A Transfer Learning Framework for Event-based Facial Expression Recognition</a></h3>
                    <p><strong>Authors:</strong> Rodrigo Verschae, Ignacio Bugueno-Cordova</p>
                    <p><strong>Published:</strong> 8/5/2025</p>
                    <p><strong>Categories:</strong> cs.CV</p>
                    <p><strong>Summary:</strong> Event-based cameras are bio-inspired vision sensors that asynchronously capture per-pixel intensity changes with microsecond latency, high temporal resolution, and high dynamic range, providing valuable information about the spatio-temporal dynamics of the scene. In the present work, we propose evTransFER, a transfer learning-based framework and architecture for face expression recognition using event-based cameras. The main contribution is a feature extractor designed to encode the spatio-temporal dynamics of faces, built by training an adversarial generative method on a different problem (facial reconstruction) and then transferring the trained encoder weights to the face expression recognition system. We show that this proposed transfer learning method greatly improves the ability to recognize facial expressions compared to training a network from scratch. In addition, we propose an architecture that incorporates an LSTM to capture longer-term facial expression dynamics, and we introduce a new event-based representation, referred to as TIE, both of which further improve the results. We evaluate the proposed framework on the event-based facial expression database e-CK+ and compare it to state-of-the-art methods. The results show that the proposed framework evTransFER achieves a 93.6\% recognition rate on the e-CK+ database, significantly improving the accuracy (25.9\% points or more) when compared to state-of-the-art performance for similar problems.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.03608v1" target="_blank">CloudBreaker: Breaking the Cloud Covers of Sentinel-2 Images using Multi-Stage Trained Conditional Flow Matching on Sentinel-1</a></h3>
                    <p><strong>Authors:</strong> Saleh Sakib Ahmed, Sara Nowreen, M. Sohel Rahman</p>
                    <p><strong>Published:</strong> 8/5/2025</p>
                    <p><strong>Categories:</strong> cs.CV, eess.IV</p>
                    <p><strong>Summary:</strong> Cloud cover and nighttime conditions remain significant limitations in satellite-based remote sensing, often restricting the availability and usability of multi-spectral imagery. In contrast, Sentinel-1 radar images are unaffected by cloud cover and can provide consistent data regardless of weather or lighting conditions. To address the challenges of limited satellite imagery, we propose CloudBreaker, a novel framework that generates high-quality multi-spectral Sentinel-2 signals from Sentinel-1 data. This includes the reconstruction of optical (RGB) images as well as critical vegetation and water indices such as NDVI and NDWI.We employed a novel multi-stage training approach based on conditional latent flow matching and, to the best of our knowledge, are the first to integrate cosine scheduling with flow matching. CloudBreaker demonstrates strong performance, achieving a Frechet Inception Distance (FID) score of 0.7432, indicating high fidelity and realism in the generated optical imagery. The model also achieved Structural Similarity Index Measure (SSIM) of 0.6156 for NDWI and 0.6874 for NDVI, indicating a high degree of structural similarity. This establishes CloudBreaker as a promising solution for a wide range of remote sensing applications where multi-spectral data is typically unavailable or unreliable</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.03602v1" target="_blank">Testing Gauss-Bonnet Gravity with DESI BAO Data</a></h3>
                    <p><strong>Authors:</strong> Praveen Kumar Dhankar, Dalale Mhamdi, Albert Munyeshyaka, Darshan Kumar, Joseph Ntahompagaze, Taoufik Oualib</p>
                    <p><strong>Published:</strong> 8/5/2025</p>
                    <p><strong>Categories:</strong> gr-qc</p>
                    <p><strong>Summary:</strong> In the present paper, we observationally constrain f (G) gravity at the background level using Type Ia supernovae from the Pantheon Plus (PP) sample, cosmic chronometer (CC) data, and the recent Baryon Acoustic Oscillation (BAO) measurements released by DESI. For the analysis, we consider two combinations of datasets: (i) PP + CC, and (ii) PP + CC + DESI BAO. In both cases, we determine the best-fit parameters by numerically solving the modified Friedmann equations for two distinct f (G) models, namely the power-law and exponential forms. This is achieved through Markov Chain Monte Carlo (MCMC) simulations. To assess the statistical significance of the f (G) models, we employ both the Akaike Information Criterion (AIC) and the Bayesian Information Criterion (BIC). Our results show that both f (G) models are statistically favored over the standard {\Lambda}CDM model. Notably, the exponential model exhibits an additional future transition at redshift closer to -0.1, indicating a possible return to a decelerating phase. This distinctive behavior sets it apart from both the power-law model and the {\Lambda}CDM scenario, which predict continued acceleration into the future.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.03600v1" target="_blank">Why Evolve When You Can Adapt? Post-Evolution Adaptation of Genetic Memory for On-the-Fly Control</a></h3>
                    <p><strong>Authors:</strong> Hamze Hammami, Eva Denisa Barbulescu, Talal Shaikh, Mouayad Aldada, Muhammad Saad Munawar</p>
                    <p><strong>Published:</strong> 8/5/2025</p>
                    <p><strong>Categories:</strong> cs.RO, cs.NE</p>
                    <p><strong>Summary:</strong> Imagine a robot controller with the ability to adapt like human synapses, dynamically rewiring itself to overcome unforeseen challenges in real time. This paper proposes a novel zero-shot adaptation mechanism for evolutionary robotics, merging a standard Genetic Algorithm (GA) controller with online Hebbian plasticity. Inspired by biological systems, the method separates learning and memory, with the genotype acting as memory and Hebbian updates handling learning. In our approach, the fitness function is leveraged as a live scaling factor for Hebbian learning, enabling the robots neural controller to adjust synaptic weights on-the-fly without additional training. This adds a dynamic adaptive layer that activates only during runtime to handle unexpected environmental changes. After the task, the robot forgets the temporary adjustments and reverts to the original weights, preserving core knowledge. We validate this hybrid GA-Hebbian controller on an e-puck robot in a T-maze navigation task with changing light conditions and obstacles.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.03599v1" target="_blank">OSINT or BULLSHINT? Exploring Open-Source Intelligence tweets about the Russo-Ukrainian War</a></h3>
                    <p><strong>Authors:</strong> Johannes Niu, Mila Stillman, Anna Kruspe</p>
                    <p><strong>Published:</strong> 8/5/2025</p>
                    <p><strong>Categories:</strong> cs.SI, cs.CL</p>
                    <p><strong>Summary:</strong> This paper examines the role of Open Source Intelligence (OSINT) on Twitter regarding the Russo-Ukrainian war, distinguishing between genuine OSINT and deceptive misinformation efforts, termed BULLSHINT. Utilizing a dataset spanning from January 2022 to July 2023, we analyze nearly 2 million tweets from approximately 1,040 users involved in discussing real-time military engagements, strategic analyses, and misinformation related to the conflict. Using sentiment analysis, partisanship detection, misinformation identification, and Named Entity Recognition (NER), we uncover communicative patterns and dissemination strategies within the OSINT community. Significant findings reveal a predominant negative sentiment influenced by war events, a nuanced distribution of pro-Ukrainian and pro-Russian partisanship, and the potential strategic manipulation of information. Additionally, we apply community detection techniques, which are able to identify distinct clusters partisanship, topics, and misinformation, highlighting the complex dynamics of information spread on social media. This research contributes to the understanding of digital warfare and misinformation dynamics, offering insights into the operationalization of OSINT in geopolitical conflicts.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.03598v1" target="_blank">DyCAF-Net: Dynamic Class-Aware Fusion Network</a></h3>
                    <p><strong>Authors:</strong> Md Abrar Jahin, Shahriar Soudeep, M. F. Mridha, Nafiz Fahad, Md. Jakir Hossen</p>
                    <p><strong>Published:</strong> 8/5/2025</p>
                    <p><strong>Categories:</strong> cs.CV, cs.LG</p>
                    <p><strong>Summary:</strong> Recent advancements in object detection rely on modular architectures with multi-scale fusion and attention mechanisms. However, static fusion heuristics and class-agnostic attention limit performance in dynamic scenes with occlusions, clutter, and class imbalance. We introduce Dynamic Class-Aware Fusion Network (DyCAF-Net) that addresses these challenges through three innovations: (1) an input-conditioned equilibrium-based neck that iteratively refines multi-scale features via implicit fixed-point modeling, (2) a dual dynamic attention mechanism that adaptively recalibrates channel and spatial responses using input- and class-dependent cues, and (3) class-aware feature adaptation that modulates features to prioritize discriminative regions for rare classes. Through comprehensive ablation studies with YOLOv8 and related architectures, alongside benchmarking against nine state-of-the-art baselines, DyCAF-Net achieves significant improvements in precision, mAP@50, and mAP@50-95 across 13 diverse benchmarks, including occlusion-heavy and long-tailed datasets. The framework maintains computational efficiency ($\sim$11.1M parameters) and competitive inference speeds, while its adaptability to scale variance, semantic overlaps, and class imbalance positions it as a robust solution for real-world detection tasks in medical imaging, surveillance, and autonomous systems.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.03597v1" target="_blank">Optimal Quantum $(r,Î´)$-Locally Repairable Codes From Matrix-Product Codes</a></h3>
                    <p><strong>Authors:</strong> Meng Cao, Kun Zhou</p>
                    <p><strong>Published:</strong> 8/5/2025</p>
                    <p><strong>Categories:</strong> quant-ph</p>
                    <p><strong>Summary:</strong> This paper studies optimal quantum $(r,\delta)$-LRCs from matrix-product (MP) codes. We establish a necessary and sufficient condition for an MP code to be an optimal $(r,\delta)$-LRC. Based on this, we present a characterization for optimal quantum $(r,\delta)$-LRCs from MP codes with nested constituent codes, and also study optimal quantum $(r,\delta)$-LRCs constructed from MP codes with non-nested constituent codes. Through Hermitian dual-containing and Euclidean dual-containing MP codes, we present five infinite families of optimal quantum $(r,\delta)$-LRCs with flexible parameters.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.03596v1" target="_blank">MetaScope: Optics-Driven Neural Network for Ultra-Micro Metalens Endoscopy</a></h3>
                    <p><strong>Authors:</strong> Wuyang Li, Wentao Pan, Xiaoyuan Liu, Zhendong Luo, Chenxin Li, Hengyu Liu, Din Ping Tsai, Mu Ku Chen, Yixuan Yuan</p>
                    <p><strong>Published:</strong> 8/5/2025</p>
                    <p><strong>Categories:</strong> cs.CV, cs.AI</p>
                    <p><strong>Summary:</strong> Miniaturized endoscopy has advanced accurate visual perception within the human body. Prevailing research remains limited to conventional cameras employing convex lenses, where the physical constraints with millimetre-scale thickness impose serious impediments on the micro-level clinical. Recently, with the emergence of meta-optics, ultra-micro imaging based on metalenses (micron-scale) has garnered great attention, serving as a promising solution. However, due to the physical difference of metalens, there is a large gap in data acquisition and algorithm research. In light of this, we aim to bridge this unexplored gap, advancing the novel metalens endoscopy. First, we establish datasets for metalens endoscopy and conduct preliminary optical simulation, identifying two derived optical issues that physically adhere to strong optical priors. Second, we propose MetaScope, a novel optics-driven neural network tailored for metalens endoscopy driven by physical optics. MetaScope comprises two novel designs: Optics-informed Intensity Adjustment (OIA), rectifying intensity decay by learning optical embeddings, and Optics-informed Chromatic Correction (OCC), mitigating chromatic aberration by learning spatial deformations informed by learned Point Spread Function (PSF) distributions. To enhance joint learning, we further deploy a gradient-guided distillation to transfer knowledge from the foundational model adaptively. Extensive experiments demonstrate that MetaScope not only outperforms state-of-the-art methods in both metalens segmentation and restoration but also achieves impressive generalized ability in real biomedical scenes.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.03594v1" target="_blank">CADD: Context aware disease deviations via restoration of brain images using normative conditional diffusion models</a></h3>
                    <p><strong>Authors:</strong> Ana Lawry Aguila, Ayodeji Ijishakin, Juan Eugenio Iglesias, Tomomi Takenaga, Yukihiro Nomura, Takeharu Yoshikawa, Osamu Abe, Shouhei Hanaoka</p>
                    <p><strong>Published:</strong> 8/5/2025</p>
                    <p><strong>Categories:</strong> eess.IV, cs.CV</p>
                    <p><strong>Summary:</strong> Applying machine learning to real-world medical data, e.g. from hospital archives, has the potential to revolutionize disease detection in brain images. However, detecting pathology in such heterogeneous cohorts is a difficult challenge. Normative modeling, a form of unsupervised anomaly detection, offers a promising approach to studying such cohorts where the ``normal behavior is modeled and can be used at subject level to detect deviations relating to disease pathology. Diffusion models have emerged as powerful tools for anomaly detection due to their ability to capture complex data distributions and generate high-quality images. Their performance relies on image restoration; differences between the original and restored images highlight potential abnormalities. However, unlike normative models, these diffusion model approaches do not incorporate clinical information which provides important context to guide the disease detection process. Furthermore, standard approaches often poorly restore healthy regions, resulting in poor reconstructions and suboptimal detection performance. We present CADD, the first conditional diffusion model for normative modeling in 3D images. To guide the healthy restoration process, we propose a novel inference inpainting strategy which balances anomaly removal with retention of subject-specific features. Evaluated on three challenging datasets, including clinical scans, which may have lower contrast, thicker slices, and motion artifacts, CADD achieves state-of-the-art performance in detecting neurological abnormalities in heterogeneous cohorts.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.03593v1" target="_blank">On the (In)Significance of Feature Selection in High-Dimensional Datasets</a></h3>
                    <p><strong>Authors:</strong> Bhavesh Neekhra, Debayan Gupta, Partha Pratim Chakravarti</p>
                    <p><strong>Published:</strong> 8/5/2025</p>
                    <p><strong>Categories:</strong> cs.LG, q-bio.GN, stat.ML</p>
                    <p><strong>Summary:</strong> Extensive research has been done on feature selection (FS) algorithms for high-dimensional datasets aiming to improve model performance, reduce computational cost and identify features of interest. We test the null hypothesis of using randomly selected features to compare against features selected by FS algorithms to validate the performance of the latter. Our results show that FS on high-dimensional datasets (in particular gene expression) in classification tasks is not useful. We find that (1) models trained on small subsets (0.02%-1% of all features) of randomly selected features almost always perform comparably to those trained on all features, and (2) a typical- sized random subset provides comparable or superior performance to that of top-k features selected in various published studies. Thus, our work challenges many feature selection results on high dimensional datasets, particularly in computational genomics. It raises serious concerns about studies that propose drug design or targeted interventions based on computationally selected genes, without further validation in a wet lab.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.03592v1" target="_blank">Frequency conversion in topological plasmonic THz photo-mixer</a></h3>
                    <p><strong>Authors:</strong> Hamid Javadi</p>
                    <p><strong>Published:</strong> 8/5/2025</p>
                    <p><strong>Categories:</strong> physics.optics, cond-mat.mtrl-sci, physics.ins-det, quant-ph</p>
                    <p><strong>Summary:</strong> We propose an alternative explanation for the observed coherent down-conversion in a laser-enabled plasmonic photo-mixer involving infrared, millimeter-wave, microwave photons. We will offer a path toward full experimental characterization of the device.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.03590v1" target="_blank">SolarSeer: Ultrafast and accurate 24-hour solar irradiance forecasts outperforming numerical weather prediction across the USA</a></h3>
                    <p><strong>Authors:</strong> Mingliang Bai, Zuliang Fang, Shengyu Tao, Siqi Xiang, Jiang Bian, Yanfei Xiang, Pengcheng Zhao, Weixin Jin, Jonathan A. Weyn, Haiyu Dong, Bin Zhang, Hongyu Sun, Kit Thambiratnam, Qi Zhang, Hongbin Sun, Xuan Zhang, Qiuwei Wu</p>
                    <p><strong>Published:</strong> 8/5/2025</p>
                    <p><strong>Categories:</strong> cs.LG, cs.CE</p>
                    <p><strong>Summary:</strong> Accurate 24-hour solar irradiance forecasting is essential for the safe and economic operation of solar photovoltaic systems. Traditional numerical weather prediction (NWP) models represent the state-of-the-art in forecasting performance but rely on computationally costly data assimilation and solving complicated partial differential equations (PDEs) that simulate atmospheric physics. Here, we introduce SolarSeer, an end-to-end large artificial intelligence (AI) model for solar irradiance forecasting across the Contiguous United States (CONUS). SolarSeer is designed to directly map the historical satellite observations to future forecasts, eliminating the computational overhead of data assimilation and PDEs solving. This efficiency allows SolarSeer to operate over 1,500 times faster than traditional NWP, generating 24-hour cloud cover and solar irradiance forecasts for the CONUS at 5-kilometer resolution in under 3 seconds. Compared with the state-of-the-art NWP in the CONUS, i.e., High-Resolution Rapid Refresh (HRRR), SolarSeer significantly reduces the root mean squared error of solar irradiance forecasting by 27.28% in reanalysis data and 15.35% across 1,800 stations. SolarSeer also effectively captures solar irradiance fluctuations and significantly enhances the first-order irradiance difference forecasting accuracy. SolarSeers ultrafast, accurate 24-hour solar irradiance forecasts provide strong support for the transition to sustainable, net-zero energy systems.</p>
                
            
        
        
            <p><em>Generated by AI News Agent</em></p>
        
    

