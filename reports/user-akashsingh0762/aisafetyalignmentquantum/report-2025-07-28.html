
    
        <h1>ü§ñ AI Research Report</h1>
        
            <strong>Date:</strong> 2025-07-28<br>
            <strong>Topics:</strong> ai safety research, ai alignment research, quantum computing<br>
            <strong>Papers Found:</strong> 150
        
        
        
            
                <h2>ü§ñ AI Summary</h2>
                <p>## ai safety research

The selected papers illustrate significant trends and breakthroughs in AI safety research, focusing on the practical implications and technological advancements in deploying AI systems securely and responsibly. A critical area of concern is the integration of AI in sensitive environments like autonomous driving and healthcare. For example, the BEV-LLM paper addresses the importance of interpretable and transparent AI systems in autonomous driving, emphasizing scene captioning as a means to enhance safety and human-AI interaction. Similarly, the paper on Explainable AI guided unsupervised fault diagnostics for high-voltage circuit breakers reflects the drive towards non-intrusive monitoring solutions to ensure continuous operational safety and reliability without the need for disconnection or manual oversight.

Another pivotal trend is the focus on AI robustness against adversarial conditions and the prevention of harmful outcomes. The Running in CIRCLE? paper presents benchmarks for evaluating the security of LLM code interpreters against resource exhaustion attacks, highlighting a need for rigorous testing to mitigate interpreter-specific cybersecurity threats. This aligns with the findings from the Jailbreaking Large Language Diffusion Models paper, which reveals vulnerabilities in diffusion-based text generation models, demonstrating how they can be manipulated to produce harmful content more rapidly than traditional LLMs. These insights underscore the urgent need for enhanced safety protocols and industry standards to guide the deployment of AI systems across various applications, ensuring they operate safely and without unintended consequences. Overall, these papers collectively underscore the growing recognition of AI safety as an essential component of AI research, with a call for collaborative efforts across disciplines to address emerging challenges and ensure the responsible use of AI technologies.

*Based on 50 research papers*

---

## quantum computing

The research papers provided cover a broad range of topics, with only a couple directly related to quantum computing. Among these, the most relevant paper is Random approximate quantum information masking by Xiaodi Li et al. This paper explores the concept of approximate quantum information masking (AQIM), a technique that circumvents the constraints of no-hiding and no-masking theorems, which are fundamental limitations in quantum information theory. The authors propose using random isometries to construct approximate maskers, revealing that while almost all random isometries fail in bipartite systems (generalizing the original no-masking theorem), they can successfully achieve AQIM in multipartite systems. This discovery has significant implications, suggesting that AQIM can be leveraged to develop approximate quantum error correction codes, thereby enhancing the robustness and efficiency of quantum information processing systems.

Another relevant study is Is the Full Power of Gaussian Boson Sampling Required for Simulating Vibronic Spectra Using Photonics? by Jan-Lucas Eickmann et al. This research examines the necessity of Gaussian boson sampling (GBS) for simulating vibronic spectra of molecules, a key task in physical chemistry. The study finds that for certain molecules, simpler photonic approximations can be used instead of full GBS, emphasizing the importance of understanding the specific attributes of molecules to optimize simulation methods. This work highlights the potential for photonic quantum computing to solve complex problems in chemistry more efficiently, thereby broadening the applicability and accessibility of quantum computing techniques in scientific research. These studies collectively underscore the ongoing efforts to refine quantum computing methods and their applications, showcasing advancements that could lead to more practical and effective quantum technologies.

*Based on 50 research papers*</p>
            
        
        
        <h2>üìö Research Papers</h2>
        
                
                    <h3><a href="http://arxiv.org/abs/2507.19479v1" target="_blank">IoT and Older Adults: Towards Multimodal EMG and AI-Based Interaction with Smart Home</a></h3>
                    <p><strong>Authors:</strong> Wies≈Çaw Kopeƒá, Jaros≈Çaw Kowalski, Aleksander Majda, Anna Duszyk-Bogorodzka, Anna Jaskulska, Cezary Biele</p>
                    <p><strong>Published:</strong> 7/25/2025</p>
                    <p><strong>Categories:</strong> cs.HC</p>
                    <p><strong>Summary:</strong> We report preliminary insights from an exploratory study on non-standard non-invasive interfaces for Smart Home Technologies (SHT). This study is part of a broader research project on effective Smart Home ecosystem Sagacity that will target older adults, impaired persons, and other groups disadvantaged in the main technology discourse. Therefore, this research is in line with a long-term research framework of the HASE research group (Human Aspects in Science and Engineering) by the Living Lab Kobo. In our study, based on the prototype of the comprehensive SHT management system Sagacity, we investigated the potential of bioelectric signals, in particular EMG and EOG as a complementary interface for SHT. Based on our previous participatory research and studies on multimodal interfaces, including VUI and BCI, we prepared an in-depth interactive hands-on experience workshops with direct involvement of various groups of potential end users, including older adults and impaired persons (total 18 subjects) to explore and investigate the potential of solutions based on this type of non-standard interfaces. The preliminary insights from the study unveil the potential of EMG/EOG interfaces in multimodal SHT management, alongside limitations and challenges stemming from the current state of technology and recommendations for designing multimodal interaction paradigms pinpointing areas of interest to pursue in further studies.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2507.19477v1" target="_blank">Advancing Event Forecasting through Massive Training of Large Language Models: Challenges, Solutions, and Broader Impacts</a></h3>
                    <p><strong>Authors:</strong> Sang-Woo Lee, Sohee Yang, Donghyun Kwak, Noah Y. Siegel</p>
                    <p><strong>Published:</strong> 7/25/2025</p>
                    <p><strong>Categories:</strong> cs.LG, cs.AI, cs.CL</p>
                    <p><strong>Summary:</strong> Many recent papers have studied the development of superforecaster-level event forecasting LLMs. While methodological problems with early studies cast doubt on the use of LLMs for event forecasting, recent studies with improved evaluation methods have shown that state-of-the-art LLMs are gradually reaching superforecaster-level performance, and reinforcement learning has also been reported to improve future forecasting. Additionally, the unprecedented success of recent reasoning models and Deep Research-style models suggests that technology capable of greatly improving forecasting performance has been developed. Therefore, based on these positive recent trends, we argue that the time is ripe for research on large-scale training of superforecaster-level event forecasting LLMs. We discuss two key research directions: training methods and data acquisition. For training, we first introduce three difficulties of LLM-based event forecasting training: noisiness-sparsity, knowledge cut-off, and simple reward structure problems. Then, we present related ideas to mitigate these problems: hypothetical event Bayesian networks, utilizing poorly-recalled and counterfactual events, and auxiliary reward signals. For data, we propose aggressive use of market, public, and crawling datasets to enable large-scale training and evaluation. Finally, we explain how these technical advances could enable AI to provide predictive intelligence to society in broader areas. This position paper presents promising specific paths and considerations for getting closer to superforecaster-level AI technology, aiming to call for researchers interest in these directions.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2507.19466v1" target="_blank">Towards Effective Immersive Technologies in Medicine: Potential and Future Applications based on VR, AR, XR and AI solutions</a></h3>
                    <p><strong>Authors:</strong> Aliaksandr Marozau, Barbara Karpowicz, Tomasz Kowalewski, Pavlo Zinevych, Wiktor Stawski, Adam Kuzdrali≈Ñski, Wies≈Çaw Kopeƒá</p>
                    <p><strong>Published:</strong> 7/25/2025</p>
                    <p><strong>Categories:</strong> cs.HC</p>
                    <p><strong>Summary:</strong> Mixed Reality (MR) technologies such as Virtual and Augmented Reality (VR, AR) are well established in medical practice, enhancing diagnostics, treatment, and education. However, there are still some limitations and challenges that may be overcome thanks to the latest generations of equipment, software, and frameworks based on eXtended Reality (XR) by enabling immersive systems that support safer, more controlled environments for training and patient care. Our review highlights recent VR and AR applications in key areas of medicine. In medical education, these technologies provide realistic clinical simulations, improving skills and knowledge retention. In surgery, immersive tools enhance procedural precision with detailed anatomical visualizations. VR-based rehabilitation has shown effectiveness in restoring motor functions and balance, particularly for neurological patients. In mental health, VR has been successful in treating conditions like PTSD and phobias. Although VR and AR solutions are well established, there are still some important limitations, including high costs and limited tactile feedback, which may be overcome with implementing new technologies that may improve the effectiveness of immersive medical applications such as XR, psychophysiological feedback or integration of artificial intelligence (AI) for real-time data analysis and personalized healthcare and training.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2507.19457v1" target="_blank">GEPA: Reflective Prompt Evolution Can Outperform Reinforcement Learning</a></h3>
                    <p><strong>Authors:</strong> Lakshya A Agrawal, Shangyin Tan, Dilara Soylu, Noah Ziems, Rishi Khare, Krista Opsahl-Ong, Arnav Singhvi, Herumb Shandilya, Michael J Ryan, Meng Jiang, Christopher Potts, Koushik Sen, Alexandros G. Dimakis, Ion Stoica, Dan Klein, Matei Zaharia, Omar Khattab</p>
                    <p><strong>Published:</strong> 7/25/2025</p>
                    <p><strong>Categories:</strong> cs.CL, cs.AI, cs.LG, cs.SE, I.2.7; I.2.6; I.2.4; I.2.8</p>
                    <p><strong>Summary:</strong> Large language models (LLMs) are increasingly adapted to downstream tasks via reinforcement learning (RL) methods like Group Relative Policy Optimization (GRPO), which often require thousands of rollouts to learn new tasks. We argue that the interpretable nature of language can often provide a much richer learning medium for LLMs, compared with policy gradients derived from sparse, scalar rewards. To test this, we introduce GEPA (Genetic-Pareto), a prompt optimizer that thoroughly incorporates natural language reflection to learn high-level rules from trial and error. Given any AI system containing one or more LLM prompts, GEPA samples system-level trajectories (e.g., reasoning, tool calls, and tool outputs) and reflects on them in natural language to diagnose problems, propose and test prompt updates, and combine complementary lessons from the Pareto frontier of its own attempts. As a result of GEPAs design, it can often turn even just a few rollouts into a large quality gain. Across four tasks, GEPA outperforms GRPO by 10% on average and by up to 20%, while using up to 35x fewer rollouts. GEPA also outperforms the leading prompt optimizer, MIPROv2, by over 10% across two LLMs, and demonstrates promising results as an inference-time search strategy for code optimization.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2507.19446v1" target="_blank">An OpenSource CI/CD Pipeline for Variant-Rich Software-Defined Vehicles</a></h3>
                    <p><strong>Authors:</strong> Matthias Wei√ü, Anish Navalgund, Johannes St√ºmpfle, Falk Dettinger, Michael Weyrich</p>
                    <p><strong>Published:</strong> 7/25/2025</p>
                    <p><strong>Categories:</strong> cs.SE, cs.DC, B.8.2; C.2.4</p>
                    <p><strong>Summary:</strong> Software-defined vehicles (SDVs) offer a wide range of connected functionalities, including enhanced driving behavior and fleet management. These features are continuously updated via over-the-air (OTA) mechanisms, resulting in a growing number of software versions and variants due to the diversity of vehicles, cloud/edge environments, and stakeholders involved. The lack of a unified integration environment further complicates development, as connected mobility solutions are often built in isolation. To ensure reliable operations across heterogeneous systems, a dynamic orchestration of functions that considers hardware and software variability is essential. This paper presents an open-source CI/CD pipeline tailored for SDVs. It automates the build, test, and deployment phases using a combination of containerized open-source tools, creating a standardized, portable, and scalable ecosystem accessible to all stakeholders. Additionally, a custom OTA middleware distributes software updates and supports rollbacks across vehicles and backend services. Update variants are derived based on deployment target dependencies and hardware configurations. The pipeline also supports continuous development and deployment of AI models for autonomous driving features. Its effectiveness is evaluated using an automated valet parking (AVP) scenario involving TurtleBots and a coordinating backend server. Two object detection variants are developed and deployed to match hardware-specific requirements. Results demonstrate seamless OTA updates, correct variant selection, and successful orchestration across all targets. Overall, the proposed pipeline provides a scalable and efficient solution for managing software variants and OTA updates in SDVs, contributing to the advancement of future mobility technologies.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2507.19432v1" target="_blank">Resolving Build Conflicts via Example-Based and Rule-Based Program Transformations</a></h3>
                    <p><strong>Authors:</strong> Sheikh Shadab Towqir, Fei He, Todd Mytkowicz, Na Meng</p>
                    <p><strong>Published:</strong> 7/25/2025</p>
                    <p><strong>Categories:</strong> cs.SE</p>
                    <p><strong>Summary:</strong> Merge conflicts often arise when developers integrate changes from different software branches. The conflicts can result from overlapping edits in programs (i.e., textual conflicts) or cause build and test errors (i.e., build and test conflicts). They degrade software quality and hinder programmer productivity. While several tools detect build conflicts, few offer meaningful support for resolving cases like those caused by method removal. To overcome limitations of existing tools, we introduce BUCOR (Build Conflict Resolver), a new conflict resolver. BUCOR first detects conflicts by comparing three versions related to a merging scenario: base b, left l, and right r. To resolve conflicts, it employs two complementary strategies: example-based transformation (BUCOR-E) and rule-based transformation (BUCOR-R). BUCOR-R applies predefined rules to handle common, well-understood conflicts. BUCOR-E mines branch versions (l and r) for exemplar edits applied to fix related build errors. From these examples, it infers and generalizes program transformation patterns to resolve more complex conflicts. We evaluated BUCOR on 88 real-world build conflicts spanning 21 distinct conflict types. BUCOR generated at least one solution for 65 cases and correctly resolved 43 conflicts. We observed that this hybrid approach--combining context-aware, example-based learning with structured, rule-based resolution--can effectively help resolve conflicts. Our research sheds light on future directions for more intelligent and automated merge tools.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2507.19425v1" target="_blank">Machine Learning Based Efficiency Calculator (MaLBEC) for Nuclear Fusion Diagnostics</a></h3>
                    <p><strong>Authors:</strong> Kimberley Lennon, Chantal Shand, Gemma Wilson, Robin Smith</p>
                    <p><strong>Published:</strong> 7/25/2025</p>
                    <p><strong>Categories:</strong> physics.ins-det, nucl-ex</p>
                    <p><strong>Summary:</strong> Diagnostics are critical for commercial and research fusion machines, since measuring and understanding plasma features is important to sustaining fusion reactions. The neutron flux (and therefore fusion power) can be indirectly calculated using neutron activation analyses, where potentially large numbers of activation foils are placed in the neutron flux, and delayed gammas from key reactions are measured via gamma spectrometry. In gamma spectrometry, absolute efficiency forms part of the activity calculation, and equals to the ratio of the total number of photons detected to the number emitted by a radioactive sample. Hence, it is imperative that they are calculated efficiently and accurately. This paper presents a novel digital efficiency calculation algorithm, the Machine Learning Based Efficiency Calculator (MaLBEC), that uses state-of-the-art supervised machine learning techniques to calculate efficiency values of a given sample, from only four inputs. In this paper, the performance of the MaLBEC is demonstrated with a fusion sample and compares the values to a traditional efficiency calculation method, Monte Carlo N-Particle (MCNP). The efficiencies from the MaLBEC were within an average 5\% of the ones produced by MCNP, but with an exceptional reduction in computation time of 99.96\%. When the efficiency values from both methods were used in the activity calculation, the MaLBEC was within 3\% of the MCNP results.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2507.19419v1" target="_blank">TokenSmith: Streamlining Data Editing, Search, and Inspection for Large-Scale Language Model Training and Interpretability</a></h3>
                    <p><strong>Authors:</strong> Mohammad Aflah Khan, Ameya Godbole, Johnny Tian-Zheng Wei, Ryan Wang, James Flemings, Krishna Gummadi, Willie Neiswanger, Robin Jia</p>
                    <p><strong>Published:</strong> 7/25/2025</p>
                    <p><strong>Categories:</strong> cs.CL</p>
                    <p><strong>Summary:</strong> Understanding the relationship between training data and model behavior during pretraining is crucial, but existing workflows make this process cumbersome, fragmented, and often inaccessible to researchers. We present TokenSmith, an open-source library for interactive editing, inspection, and analysis of datasets used in Megatron-style pretraining frameworks such as GPT-NeoX, Megatron, and NVIDIA NeMo. TokenSmith supports a wide range of operations including searching, viewing, ingesting, exporting, inspecting, and sampling data, all accessible through a simple user interface and a modular backend. It also enables structured editing of pretraining data without requiring changes to training code, simplifying dataset debugging, validation, and experimentation. TokenSmith is designed as a plug and play addition to existing large language model pretraining workflows, thereby democratizing access to production-grade dataset tooling. TokenSmith is hosted on GitHub1, with accompanying documentation and tutorials. A demonstration video is also available on YouTube.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2507.19402v1" target="_blank">FD4QC: Application of Classical and Quantum-Hybrid Machine Learning for Financial Fraud Detection A Technical Report</a></h3>
                    <p><strong>Authors:</strong> Matteo Cardaioli, Luca Marangoni, Giada Martini, Francesco Mazzolin, Luca Pajola, Andrea Ferretto Parodi, Alessandra Saitta, Maria Chiara Vernillo</p>
                    <p><strong>Published:</strong> 7/25/2025</p>
                    <p><strong>Categories:</strong> cs.LG, cs.CE</p>
                    <p><strong>Summary:</strong> The increasing complexity and volume of financial transactions pose significant challenges to traditional fraud detection systems. This technical report investigates and compares the efficacy of classical, quantum, and quantum-hybrid machine learning models for the binary classification of fraudulent financial activities. As of our methodology, first, we develop a comprehensive behavioural feature engineering framework to transform raw transactional data into a rich, descriptive feature set. Second, we implement and evaluate a range of models on the IBM Anti-Money Laundering (AML) dataset. The classical baseline models include Logistic Regression, Decision Tree, Random Forest, and XGBoost. These are compared against three hybrid classic quantum algorithms architectures: a Quantum Support Vector Machine (QSVM), a Variational Quantum Classifier (VQC), and a Hybrid Quantum Neural Network (HQNN). Furthermore, we propose Fraud Detection for Quantum Computing (FD4QC), a practical, API-driven system architecture designed for real-world deployment, featuring a classical-first, quantum-enhanced philosophy with robust fallback mechanisms. Our results demonstrate that classical tree-based models, particularly \textit{Random Forest}, significantly outperform the quantum counterparts in the current setup, achieving high accuracy (\(97.34\%\)) and F-measure (\(86.95\%\)). Among the quantum models, \textbf{QSVM} shows the most promise, delivering high precision (\(77.15\%\)) and a low false-positive rate (\(1.36\%\)), albeit with lower recall and significant computational overhead. This report provides a benchmark for a real-world financial application, highlights the current limitations of quantum machine learning in this domain, and outlines promising directions for future research.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2507.19399v1" target="_blank">Running in CIRCLE? A Simple Benchmark for LLM Code Interpreter Security</a></h3>
                    <p><strong>Authors:</strong> Gabriel Chua</p>
                    <p><strong>Published:</strong> 7/25/2025</p>
                    <p><strong>Categories:</strong> cs.CR, cs.AI</p>
                    <p><strong>Summary:</strong> As large language models (LLMs) increasingly integrate native code interpreters, they enable powerful real-time execution capabilities, substantially expanding their utility. However, such integrations introduce potential system-level cybersecurity threats, fundamentally different from prompt-based vulnerabilities. To systematically evaluate these interpreter-specific risks, we propose CIRCLE (Code-Interpreter Resilience Check for LLM Exploits), a simple benchmark comprising 1,260 prompts targeting CPU, memory, and disk resource exhaustion. Each risk category includes explicitly malicious (direct) and plausibly benign (indirect) prompt variants. Our automated evaluation framework assesses not only whether LLMs refuse or generates risky code, but also executes the generated code within the interpreter environment to evaluate code correctness, simplifications made by the LLM to make the code safe, or execution timeouts. Evaluating 7 commercially available models from OpenAI and Google, we uncover significant and inconsistent vulnerabilities. For instance, evaluations show substantial disparities even within providers - OpenAIs o4-mini correctly refuses risky requests at 7.1%, notably higher rates compared to GPT-4.1 at 0.5%. Results particularly underscore that indirect, socially-engineered prompts substantially weaken model defenses. This highlights an urgent need for interpreter-specific cybersecurity benchmarks, dedicated mitigation tools (e.g., guardrails), and clear industry standards to guide safe and responsible deployment of LLM interpreter integrations. The benchmark dataset and evaluation code are publicly released to foster further research.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2507.19390v1" target="_blank">ReCatcher: Towards LLMs Regression Testing for Code Generation</a></h3>
                    <p><strong>Authors:</strong> Altaf Allah Abbassi, Leuson Da Silva, Amin Nikanjam, Foutse Khomh</p>
                    <p><strong>Published:</strong> 7/25/2025</p>
                    <p><strong>Categories:</strong> cs.SE, cs.AI</p>
                    <p><strong>Summary:</strong> Large Language Models (LLMs) for code generation evolve rapidly through fine-tuning, merging, or new model releases. However, such updates can introduce regressions, not only in correctness but also in code quality and performance. To address this, we present ReCatcher, a regression testing framework for Python code generation. ReCatcher systematically compares two LLMs, typically a current model and a candidate update, across three dimensions: logical correctness, static code quality, and execution performance. We apply ReCatcher to assess regressions across three update scenarios, fine-tuning, merging, and model release, using CodeLlama, DeepSeek-Coder, and GPT-4o. Our evaluation shows that fine-tuning with cross-language datasets increases syntax errors by up to 12%. Merging with general-purpose models like Llama2 leads to regressions in correctness by up to 18%. GPT-4o introduces regressions of up to 50% in handling missing imports compared to GPT-3.5-turbo, while GPT-4o-mini suffers up to 80% performance degradation in execution time versus GPT-4o. Overall, logical correctness, performance, and error handling (e.g., syntax errors and missing imports) are the most regression-prone areas. Comparing ReCatcher with baseline solutions, it presents better and consistent accuracy across logical and performance aspects. ReCatcher highlights the importance of systematic regression evaluation before adopting new models, while assisting researchers and practitioners in making more informed update decisions.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2507.19384v1" target="_blank">On Anti-collusion Codes for Averaging Attack in Multimedia Fingerprinting</a></h3>
                    <p><strong>Authors:</strong> Jing Jiang, Cailin Wen, Minquan Cheng</p>
                    <p><strong>Published:</strong> 7/25/2025</p>
                    <p><strong>Categories:</strong> cs.IT, math.IT</p>
                    <p><strong>Summary:</strong> Multimedia fingerprinting is a technique to protect the copyrighted contents against being illegally redistributed under various collusion attack models. Averaging attack is the most fair choice for each colluder to avoid detection, and also makes the pirate copy have better perceptional quality. This makes such an attack one of the most feasible approaches to carrying out collusion. In order to trace all the colluders, several types of multimedia fingerprinting codes were introduced to construct fingerprints resistant to averaging attacks on multimedia contents, such as AND anti-collusion codes (AND-ACCs), binary separable codes (SCs), logical anti-collusion codes (LACCs), binary frameproof codes (FPCs), binary strongly-separable codes (SSCs) and binary secure code with list decoding (SCLDs). Then codes with the rate as high as possible are desired. However, the existing fingerprinting codes have low code rate due to the strong combinatorial structure. The reason is that the previous research methods adopted simple tracing algorithms. In this paper, we first propose novel tracing algorithms and then find appropriate fingerprinting codes with weaker combinatorial structure, i.e., the binary strongly identifiable parent property code for multimedia fingerprinting (SMIPPC) and its concatenated code. Theoretical comparisons and numerical comparisons show that SMIPPCs have higher code rates than those of the existing codes due to their weaker combinatorial structures. It is worth noting that SMIPPCs can only trace a part of colluders by using the previous tracing algorithm and the concatenated SMIPPC may be not an SMIPPC. This implies that our tracing algorithms have strong traceability.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2507.19383v1" target="_blank">Quantum Algorithm for Protein Side-Chain Optimisation: Comparing Quantum to Classical Methods</a></h3>
                    <p><strong>Authors:</strong> Anastasia Agathangelou, Dilhan Manawadu, Ivano Tavernelli</p>
                    <p><strong>Published:</strong> 7/25/2025</p>
                    <p><strong>Categories:</strong> quant-ph</p>
                    <p><strong>Summary:</strong> Modelling and predicting protein configurations is crucial for advancing drug discovery, enabling the design of treatments for life-threatening diseases. A critical aspect of this challenge is rotamer optimisation - the determination of optimal side-chain conformations given a fixed protein backbone. This problem, involving the internal degrees of freedom of amino acid side-chains, significantly influences the proteins overall structure and function. In this work, we develop a resource-efficient optimisation algorithm to compute the ground state energy of protein structures, with a focus on side-chain configuration. We formulate the rotamer optimisation problem as a Quadratic Unconstrained Binary Optimisation problem and map it to an Ising model, enabling efficient quantum encoding. Building on this formulation, we propose a quantum algorithm based on the Quantum Approximate Optimisation Algorithm to explore the conformational space and identify low-energy configurations. To benchmark our approach, we conduct a classical study using custom-built libraries tailored for structural characterisation and energy optimisation. Our quantum method demonstrates a reduction in computational cost compared to classical simulated annealing techniques, offering a scalable and promising framework for protein structure optimisation in the quantum era.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2507.19376v1" target="_blank">Archiverse: an Approach for Immersive Cultural Heritage</a></h3>
                    <p><strong>Authors:</strong> Wieslaw Kopeƒá, Anna Jaskulska, W≈Çadys≈Çaw Fuchs, Wiktor Stawski, Stanis≈Çaw Knapi≈Ñski, Barbara Karpowicz, Rafa≈Ç Mas≈Çyk</p>
                    <p><strong>Published:</strong> 7/25/2025</p>
                    <p><strong>Categories:</strong> cs.HC, cs.CY</p>
                    <p><strong>Summary:</strong> Digital technologies and tools have transformed the way we can study cultural heritage and the way we can recreate it digitally. Techniques such as laser scanning, photogrammetry, and a variety of Mixed Reality solutions have enabled researchers to examine cultural objects and artifacts more precisely and from new perspectives. In this part of the panel, we explore how Virtual Reality (VR) and eXtended Reality (XR) can serve as tools to recreate and visualize the remains of historical cultural heritage and experience it in simulations of its original complexity, which means immersive and interactive. Visualization of material culture exemplified by archaeological sites and architecture can be particularly useful when only ruins or archaeological remains survive. However, these advancements also bring significant challenges, especially in the area of transdisciplinary cooperation between specialists from many, often distant, fields, and the dissemination of virtual immersive environments among both professionals and the general public.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2507.19375v1" target="_blank">Latent-X: An Atom-level Frontier Model for De Novo Protein Binder Design</a></h3>
                    <p><strong>Authors:</strong> Latent Labs Team, Alex Bridgland, Jonathan Crabb√©, Henry Kenlay, Daniella Pretorius, Sebastian M. Schmon, Agrin Hilmkil, Rebecca Bartke-Croughan, Robin Rombach, Michael Flashman, Tomas Matteson, Simon Mathis, Alexander W. R. Nelson, David Yuan, Annette Obika, Simon A. A. Kohl</p>
                    <p><strong>Published:</strong> 7/25/2025</p>
                    <p><strong>Categories:</strong> q-bio.BM</p>
                    <p><strong>Summary:</strong> Traditional drug discovery relies on rounds of screening millions of candidate molecules with low success rates, making drug discovery time and resource intensive. To overcome this screening bottleneck, we introduce Latent-X, an all-atom protein design model that enables a new paradigm of precision AI design. Given a target protein epitope, Latent-X jointly generates the all atom structure and sequence of the protein binder and target, directly modelling the non-covalent interactions essential for specific binding. We demonstrate its efficacy across two therapeutically relevant modalities through extensive wet lab experiments, testing as few as 30-100 designs per target. For macrocyclic peptides, Latent-X achieves experimental hit rates exceeding 90% on all evaluated benchmark targets. For mini-binders, it consistently produces potent candidates against all evaluated benchmark targets, with binding affinities reaching the low nanomolar and picomolar range - comparable to those of approved therapeutics - whilst also being highly specific in mammalian display. In direct comparisons with the state-of-the-art models AlphaProteo, RFdiffusion and RFpeptides under identical conditions demonstrates, Latent-X generates binders with higher hit rates and better binding affinities, and uniquely creates structurally diverse binders, including complex beta-sheet folds. Its end-to-end process is an order of magnitude faster than existing multi-step computational pipelines. By drastically improving the efficiency and success rate of de novo design, Latent-X represents a significant advance towards push-button biologics discovery and a valuable tool for protein engineers. Latent-X is available at https://platform.latentlabs.com, enabling users to reliably generate de novo binders without AI infrastructure or coding.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2507.19371v1" target="_blank">Atomically clean free-standing two-dimensional materials through heating in ultra-high vacuum</a></h3>
                    <p><strong>Authors:</strong> Philipp Irschik, David Lamprecht, Shrirang Chokappa, Clemens Mangler, Carsten Speckmann, Thuy An Bui, Manuel L√§ngle, Lado Filipovic, Jani Kotakoski</p>
                    <p><strong>Published:</strong> 7/25/2025</p>
                    <p><strong>Categories:</strong> cond-mat.mtrl-sci</p>
                    <p><strong>Summary:</strong> Surface contamination not only influences but in some cases even dominates the measured properties of two-dimensional materials. Although different cleaning methods are often used for contamination removal, commonly used spectroscopic cleanliness assessment methods can leave the level of achieved cleanliness ambiguous. Despite two decades of research on 2D materials, the true cleanliness of the used samples is often left open to interpretation. In this work, free-standing monolayer graphene and hexagonal boron nitride are annealed at different temperatures in a custom-built ultra-high vacuum heating chamber, connected to a scanning transmission electron microscope via a vacuum transfer line, enabling atomically resolved cleanliness characterization as a function of annealing temperature, while eliminating the introduction of airborne contamination during sample transport. While annealing at 200 {\deg}C already reduces contamination significantly, it is not until 400 {\deg}C or higher, where over 90% of the free-standing monolayer areas are atomically clean. At this point, further contamination removal is mainly limited by defects in the material and metal contamination introduced during the sample transfer or growth. The achieved large, atomically clean areas can then be used for further nanoscale engineering steps or device processing, facilitating interaction with the material rather than contamination.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2507.19370v1" target="_blank">BEV-LLM: Leveraging Multimodal BEV Maps for Scene Captioning in Autonomous Driving</a></h3>
                    <p><strong>Authors:</strong> Felix Brandstaetter, Erik Schuetz, Katharina Winter, Fabian Flohr</p>
                    <p><strong>Published:</strong> 7/25/2025</p>
                    <p><strong>Categories:</strong> cs.CV</p>
                    <p><strong>Summary:</strong> Autonomous driving technology has the potential to transform transportation, but its wide adoption depends on the development of interpretable and transparent decision-making systems. Scene captioning, which generates natural language descriptions of the driving environment, plays a crucial role in enhancing transparency, safety, and human-AI interaction. We introduce BEV-LLM, a lightweight model for 3D captioning of autonomous driving scenes. BEV-LLM leverages BEVFusion to combine 3D LiDAR point clouds and multi-view images, incorporating a novel absolute positional encoding for view-specific scene descriptions. Despite using a small 1B parameter base model, BEV-LLM achieves competitive performance on the nuCaption dataset, surpassing state-of-the-art by up to 5\% in BLEU scores. Additionally, we release two new datasets - nuView (focused on environmental conditions and viewpoints) and GroundView (focused on object grounding) - to better assess scene captioning across diverse driving scenarios and address gaps in current benchmarks, along with initial benchmarking results demonstrating their effectiveness.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2507.19359v1" target="_blank">SemGes: Semantics-aware Co-Speech Gesture Generation using Semantic Coherence and Relevance Learning</a></h3>
                    <p><strong>Authors:</strong> Lanmiao Liu, Esam Ghaleb, Aslƒ± √ñzy√ºrek, Zerrin Yumak</p>
                    <p><strong>Published:</strong> 7/25/2025</p>
                    <p><strong>Categories:</strong> cs.CV</p>
                    <p><strong>Summary:</strong> Creating a virtual avatar with semantically coherent gestures that are aligned with speech is a challenging task. Existing gesture generation research mainly focused on generating rhythmic beat gestures, neglecting the semantic context of the gestures. In this paper, we propose a novel approach for semantic grounding in co-speech gesture generation that integrates semantic information at both fine-grained and global levels. Our approach starts with learning the motion prior through a vector-quantized variational autoencoder. Built on this model, a second-stage module is applied to automatically generate gestures from speech, text-based semantics and speaker identity that ensures consistency between the semantic relevance of generated gestures and co-occurring speech semantics through semantic coherence and relevance modules. Experimental results demonstrate that our approach enhances the realism and coherence of semantic gestures. Extensive experiments and user studies show that our method outperforms state-of-the-art approaches across two benchmarks in co-speech gesture generation in both objective and subjective metrics. The qualitative results of our model, code, dataset and pre-trained models can be viewed at https://semgesture.github.io/.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2507.19353v1" target="_blank">Smooth Reading: Bridging the Gap of Recurrent LLM to Self-Attention LLM on Long-Context Tasks</a></h3>
                    <p><strong>Authors:</strong> Kai Liu, Zhan Su, Peijie Dong, Fengran Mo, Jianfei Gao, ShaoTing Zhang, Kai Chen</p>
                    <p><strong>Published:</strong> 7/25/2025</p>
                    <p><strong>Categories:</strong> cs.CL, cs.AI</p>
                    <p><strong>Summary:</strong> Recently, recurrent large language models (Recurrent LLMs) with linear computational complexity have re-emerged as efficient alternatives to self-attention-based LLMs (Self-Attention LLMs), which have quadratic complexity. However, Recurrent LLMs often underperform on long-context tasks due to their limited fixed-size memory. Previous research has primarily focused on enhancing the memory capacity of Recurrent LLMs through architectural innovations, but these approaches have not yet enabled Recurrent LLMs to match the performance of Self-Attention LLMs on long-context tasks. We argue that this limitation arises because processing the entire context at once is not well-suited for Recurrent LLMs. In this paper, we propose Smooth Reading, a chunk-wise inference method inspired by human reading strategies. Smooth Reading processes context in chunks and iteratively summarizes the contextual information, thereby reducing memory demands and making the approach more compatible with Recurrent LLMs. Our experimental results show that this method substantially narrows the performance gap between Recurrent and Self-Attention LLMs on long-context tasks, while preserving the efficiency advantages of Recurrent LLMs. Our Smooth Reading boosts SWA-3B-4k (a Recurrent LLM) from 5.68% lower to 3.61% higher performance than Self-Attention LLMs on LongBench. Besides, our method maintains the high efficiency, training 3x faster and inferring 2x faster at 64k context compared to Self-Attention LLMs. To our knowledge, this is the first work to achieve comparable performance using Recurrent LLMs compared with Self-Attention LLMs on long-context tasks. We hope our method will inspire future research in this area. To facilitate further progress, we will release code and dataset.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2507.19338v1" target="_blank">Branch-and-bound method for calculating Viterbi path in triplet Markov models</a></h3>
                    <p><strong>Authors:</strong> Oskar Soop, J√ºri Lember</p>
                    <p><strong>Published:</strong> 7/25/2025</p>
                    <p><strong>Categories:</strong> stat.CO, cs.IT, math.IT</p>
                    <p><strong>Summary:</strong> We consider a bivariate, possibly non-homogeneous, finite-state Markov chain $(X,U)=\{(X_t,U_t)\}_{t=1}^n$. We are interested in the marginal process $X$, which typically is not a Markov chain. The goal is to find a realization (path) $x=(x_1,\ldots,x_n)$ with maximal probability $P(X=x)$. If $X$ is Markov chain, then such path can be efficiently found using the celebrated Viterbi algorithm. However, when $X$ is not Markovian, identifying the most probable path -- hereafter referred to as the Viterbi path -- becomes computationally expensive. In this paper, we explore the branch-and-bound method for finding Viterbi paths. The method is based on the lower and upper bounds on maximum probability $\max_x P(X=x)$, and the objective of the paper is to exploit the joint Markov property of $(X,Y)$ to calculate possibly good bounds in possibly cheap way. This research is motivated by decoding or segmentation problem in triplet Markov models. A triplet Markov model is trivariate homogeneous Markov process $(X,U,Y)$. In decoding, a realization of one marginal process $Y$ is observed (representing the data), while $X$ and $U$ are latent processes. The process $U$ serves as a nuisance variable, whereas $X$ is the process of primary interest. Decoding refers to estimating the hidden sequence $X$ based solely on the observation $Y$. Conditional on $Y$, the latent processes $(X, U)$ form a non-homogeneous Markov chain. In this context, the Viterbi path corresponds to the maximum a posteriori (MAP) estimate of $X$, making it a natural choice for signal reconstruction.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2507.19327v1" target="_blank">Real-time rail vehicle localisation using spatially resolved magnetic field measurements</a></h3>
                    <p><strong>Authors:</strong> Niklas Dieckow, Katharina Ostaszewski, Philip Heinisch, Henriette Struckmann, Hendrik Ranocha</p>
                    <p><strong>Published:</strong> 7/25/2025</p>
                    <p><strong>Categories:</strong> eess.SP, cs.SY, eess.SY</p>
                    <p><strong>Summary:</strong> This work presents two complementary real-time rail vehicle localization methods based on magnetic field measurements and a pre-recorded magnetic map. The first uses a particle filter reweighted via magnetic similarity, employing a heavy-tailed non-Gaussian kernel for enhanced stability. The second is a stateless sequence alignment technique that transforms real-time magnetic signals into the spatial domain and matches them to the map using a similarity measure. Experiments with operational train data show that the particle filter achieves track-selective, sub-5-meter accuracy over 21.6 km, though its performance degrades at low speeds and during cold starts. Accuracy tests were constrained by the GNSS-based reference system. In contrast, the alignment-based method excels in cold-start scenarios, localizing within 30 m in 92 % of tests (100 % using top-3 matches). A hybrid approach combines both methods$\unicode{x2014}$alignment-based initialization followed by particle filter tracking. Runtime analysis confirms real-time capability on consumer-grade hardware. The system delivers accurate, robust localization suitable for safety-critical rail applications.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2507.19316v1" target="_blank">Human-AI Synergy in Adaptive Active Learning for Continuous Lithium Carbonate Crystallization Optimization</a></h3>
                    <p><strong>Authors:</strong> Shayan S. Mousavi Masouleh, Corey A. Sanz, Ryan P. Jansonius, Cara Cronin, Jason E. Hein, Jason Hattrick-Simpers</p>
                    <p><strong>Published:</strong> 7/25/2025</p>
                    <p><strong>Categories:</strong> cond-mat.mtrl-sci, cond-mat.other, cs.HC, cs.LG, physics.data-an</p>
                    <p><strong>Summary:</strong> As demand for high-purity lithium surges with the growth of the electric vehicle (EV) industry, cost-effective extraction from lower-grade North American sources like the Smackover Formation is critical. These resources, unlike high-purity South American brines, require innovative purification techniques to be economically viable. Continuous crystallization is a promising method for producing battery-grade lithium carbonate, but its optimization is challenged by a complex parameter space and limited data. This study introduces a Human-in-the-Loop (HITL) assisted active learning framework to optimize the continuous crystallization of lithium carbonate. By integrating human expertise with data-driven insights, our approach accelerates the optimization of lithium extraction from challenging sources. Our results demonstrate the frameworks ability to rapidly adapt to new data, significantly improving the processs tolerance to critical impurities like magnesium from the industry standard of a few hundred ppm to as high as 6000 ppm. This breakthrough makes the exploitation of low-grade, impurity-rich lithium resources feasible, potentially reducing the need for extensive pre-refinement processes. By leveraging artificial intelligence, we have refined operational parameters and demonstrated that lower-grade materials can be used without sacrificing product quality. This advancement is a significant step towards economically harnessing North Americas vast lithium reserves, such as those in the Smackover Formation, and enhancing the sustainability of the global lithium supply chain.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2507.19305v1" target="_blank">Demystifying AI in Criminal Justice</a></h3>
                    <p><strong>Authors:</strong> Richard Berk</p>
                    <p><strong>Published:</strong> 7/25/2025</p>
                    <p><strong>Categories:</strong> cs.CY, stat.AP</p>
                    <p><strong>Summary:</strong> There is widespread confusion among criminal justice practitioners and legal scholars about the use of artificial intelligence in criminal justice. This didactic review is written for readers with little or no background in statistics or computer science. It is not intended to replace more technical treatments. It is intended to supplement them and encourage readers to dig more deeply into topics that strike their fancy.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2507.19302v1" target="_blank">Understanding discrepancies in the coverage of OpenAlex: the case of China</a></h3>
                    <p><strong>Authors:</strong> Mengxue Zheng, Lili Miao, Yi Bu, Vincent Lariviere</p>
                    <p><strong>Published:</strong> 7/25/2025</p>
                    <p><strong>Categories:</strong> cs.DL</p>
                    <p><strong>Summary:</strong> Citation indexes play a crucial role for understanding how science is produced, disseminated, and used. However, these databases often face a critical trade-off: those offering extensive and high-quality coverage are typically proprietary, whereas publicly accessible datasets frequently exhibit fragmented coverage and inconsistent data quality. OpenAlex was developed to address this challenge, providing a freely available database with broad open coverage, with a particular emphasis on non-English speaking countries. Yet, few studies have assessed the quality of the OpenAlex dataset. This paper assesses the coverage, by OpenAlex, of Chinas papers, which shows an abnormal trend, and compares it with other countries that do not have English as their main language. Our analysis reveals that while OpenAlex increases the coverage of Chinas publications, primarily those disseminated by a national database, this coverage is incomplete and discontinuous when compared to other countries records in the database. We observe similar issues in other non-English-speaking countries, with coverage varying across regions. These findings indicate that although OpenAlex expands coverage of research outputs, continuity issues persist and disproportionately affect certain countries. We emphasize the need for researchers to use OpenAlex data cautiously, being mindful of its potential limitations in cross-national analyses.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2507.19298v1" target="_blank">Controlling Topological Defects in Polar Fluids via Reinforcement Learning</a></h3>
                    <p><strong>Authors:</strong> Abhinav Singh, Petros Koumoutsakos</p>
                    <p><strong>Published:</strong> 7/25/2025</p>
                    <p><strong>Categories:</strong> cond-mat.soft, cs.AI, cs.LG</p>
                    <p><strong>Summary:</strong> Topological defects in active polar fluids exhibit complex dynamics driven by internally generated stresses, reflecting the deep interplay between topology, flow, and non-equilibrium hydrodynamics. Feedback control offers a powerful means to guide such systems, enabling transitions between dynamic states. We investigated closed-loop steering of integer-charged defects in a confined active fluid by modulating the spatial profile of activity. Using a continuum hydrodynamic model, we show that localized control of active stress induces flow fields that can reposition and direct defects along prescribed trajectories by exploiting non-linear couplings in the system. A reinforcement learning framework is used to discover effective control strategies that produce robust defect transport across both trained and novel trajectories. The results highlight how AI agents can learn the underlying dynamics and spatially structure activity to manipulate topological excitations, offering insights into the controllability of active matter and the design of adaptive, self-organized materials.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2507.19295v1" target="_blank">On the Security of a Code-Based PIR Scheme</a></h3>
                    <p><strong>Authors:</strong> Svenja Lage, Hannes Bartz</p>
                    <p><strong>Published:</strong> 7/25/2025</p>
                    <p><strong>Categories:</strong> cs.CR, cs.IR</p>
                    <p><strong>Summary:</strong> Private Information Retrieval (PIR) schemes allow clients to retrieve files from a database without disclosing the requested files identity to the server. In the pursuit of post-quantum security, most recent PIR schemes rely on hard lattice problems. In contrast, the so called CB-cPIR scheme stands out as a pioneering effort to base PIR schemes on hard problems in coding theory, thereby contributing significantly to the diversification of security foundations. However, our research reveals a critical vulnerability in CB-cPIR, substantially diminishing its security levels. Moreover, a comparative analysis with state-of-the-art PIR schemes shows that CB-cPIRs advantages are reduced, making it less competitive in terms of the communication cost. Nevertheless, our findings highlight the importance of continued research into code-based PIR schemes, as they have the potential to provide a valuable alternative to lattice-based approaches.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2507.19287v1" target="_blank">The Case for Time-Shared Computing Resources</a></h3>
                    <p><strong>Authors:</strong> Pierre Jacquet, Adrien Luxey-Bitri</p>
                    <p><strong>Published:</strong> 7/25/2025</p>
                    <p><strong>Categories:</strong> cs.DC</p>
                    <p><strong>Summary:</strong> The environmental impact of Information and Communication Technologies (ICT) continues to grow, driven notably by increasing usage, rebound effects, and emerging demands. However, despite the virtual nature of its services, the sector remains inherently constrained by its materiality and cannot rely on an infinite pool of resources. As a result, the wide variety of supported services may need to be managed under stricter limits within hosting facilities in the future. Contrary to common assumptions, we show that tenants typically do not share computing resources, even in environments commonly perceived as mutualized, such as cloud platforms. Time-sharing has been progressively phased out for reasons of performance, security, predictability, and, perhaps more importantly, due to the decreasing cost of computing resources. This paper advocates for managing fewer physical resources by improving resource sharing between tenants. It represents a paradigm shift, moving beyond traditional time-sharing at the hardware level to a higher abstraction. This approach entails doing with fewer resources under conditions of reduced performance. Nonetheless, enhancing the mutualization of infrastructure can reduce cluster sizes (through consolidation) and improve energy efficiency, with gains related to the accepted performance trade-off, a situation potentially more socially acceptable than eliminating services. We review the current state of the art, identify challenges and opportunities, propose interpretations of Time-Shared Computing, and outline key research directions.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2507.19284v1" target="_blank">Relaxed Total Generalized Variation Regularized Piecewise Smooth Mumford-Shah Model for Triangulated Surface Segmentation</a></h3>
                    <p><strong>Authors:</strong> Huayan Zhang, Shanqiang Wang, Xiaochao Wang</p>
                    <p><strong>Published:</strong> 7/25/2025</p>
                    <p><strong>Categories:</strong> cs.CG, cs.CV</p>
                    <p><strong>Summary:</strong> The Mumford-Shah (MS) model is an important technique for mesh segmentation. Many existing researches focus on piecewise constant MS mesh segmentation model with total variation regularization, which pursue the shortest length of boundaries. Different from previous efforts, in this article, we propose a novel piecewise smooth MS mesh segmentation model by utilizing the relaxed total generalized variation regularization (rTGV). The new model assumes that the feature function of a mesh can be approximated by the sum of piecewise constant function and asmooth function, and the rTGV regularization is able to characterize the high order discontinuity of the geometric structure. The newly introduced method is effective in segmenting meshes with irregular structures and getting the better boundaries rather than the shortest boundaries. We solve the new model by alternating minimization and alternating direction method of multipliers (ADMM). Our algorithm is discussed from several aspects, and comparisons with several state-of-art methods. Experimental results show that our method can yield competitive results when compared to other approaches. In addition, our results compare favorably to those of the several state-of-art techniques when evaluated on the Princeton Segmentation Benchmark. Furthermore, the quantitative errors and computational costs confirm the robustness and efficiency of the proposed method.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2507.19272v1" target="_blank">Video Self-Distillation for Single-Image Encoders: A Step Toward Physically Plausible Perception</a></h3>
                    <p><strong>Authors:</strong> Marcel Simon, Tae-Ho Kim, Seul-Ki Yeom</p>
                    <p><strong>Published:</strong> 7/25/2025</p>
                    <p><strong>Categories:</strong> cs.CV</p>
                    <p><strong>Summary:</strong> Self-supervised image encoders such as DINO have recently gained significant interest for learning robust visual features without labels. However, most SSL methods train on static images and miss the temporal cues inherent in videos. We introduce a video-distilled single-image encoder trained to predict the next-frame representation from the current frame. This simple objective injects 3D spatial and temporal priors without optical flow or tracking. When pre-training on a single 2-hour video, our approach raises the mean Intersection-over-Union (mIoU) on ADE20K from 35.0 (DoRA) to 36.4 while remaining a drop-in replacement for image-only pipelines. Our results highlight video self-distillation as a lightweight route to geometry-aware perception an essential ingredient for physically plausible world models and Physical AI.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2507.19261v1" target="_blank">Knowledge Grafting: A Mechanism for Optimizing AI Model Deployment in Resource-Constrained Environments</a></h3>
                    <p><strong>Authors:</strong> Osama Almurshed, Ashish Kaushal, Asmail Muftah, Nitin Auluck, Omer Rana</p>
                    <p><strong>Published:</strong> 7/25/2025</p>
                    <p><strong>Categories:</strong> cs.AI, cs.LG, cs.PF</p>
                    <p><strong>Summary:</strong> The increasing adoption of Artificial Intelligence (AI) has led to larger, more complex models with numerous parameters that require substantial computing power -- resources often unavailable in many real-world application scenarios. Our paper addresses this challenge by introducing knowledge grafting, a novel mechanism that optimizes AI models for resource-constrained environments by transferring selected features (the scion) from a large donor model to a smaller rootstock model. The approach achieves an 88.54% reduction in model size (from 64.39 MB to 7.38 MB), while improving generalization capability of the model. Our new rootstock model achieves 89.97% validation accuracy (vs. donors 87.47%), maintains lower validation loss (0.2976 vs. 0.5068), and performs exceptionally well on unseen test data with 90.45% accuracy. It addresses the typical size vs performance trade-off, and enables deployment of AI frameworks on resource-constrained devices with enhanced performance. We have tested our approach on an agricultural weed detection scenario, however, it can be extended across various edge computing scenarios, potentially accelerating AI adoption in areas with limited hardware/software support -- by mirroring in a similar manner the horticultural grafting enables productive cultivation in challenging agri-based environments.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2507.19245v1" target="_blank">Transfinite Fixed Points in Alpay Algebra as Ordinal Game Equilibria in Dependent Type Theory</a></h3>
                    <p><strong>Authors:</strong> Faruk Alpay, Bugra Kilictas, Taylan Alpay</p>
                    <p><strong>Published:</strong> 7/25/2025</p>
                    <p><strong>Categories:</strong> cs.LO, cs.AI, 68T27, 03B70, 68Q55</p>
                    <p><strong>Summary:</strong> This paper contributes to the Alpay Algebra by demonstrating that the stable outcome of a self referential process, obtained by iterating a transformation through all ordinal stages, is identical to the unique equilibrium of an unbounded revision dialogue between a system and its environment. The analysis initially elucidates how classical fixed point theorems guarantee such convergence in finite settings and subsequently extends the argument to the transfinite domain, relying upon well founded induction and principles of order theoretic continuity. Furthermore, the resulting transordinal fixed point operator is embedded into dependent type theory, a formalization which permits every step of the transfinite iteration and its limit to be verified within a modern proof assistant. This procedure yields a machine checked proof that the iterative dialogue necessarily stabilizes and that its limit is unique. The result provides a foundation for Alpays philosophical claim of semantic convergence within the framework of constructive logic. By unifying concepts from fixed point theory, game semantics, ordinal analysis, and type theory, this research establishes a broadly accessible yet formally rigorous foundation for reasoning about infinite self referential systems and offers practical tools for certifying their convergence within computational environments.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2507.19239v1" target="_blank">CoopTrack: Exploring End-to-End Learning for Efficient Cooperative Sequential Perception</a></h3>
                    <p><strong>Authors:</strong> Jiaru Zhong, Jiahao Wang, Jiahui Xu, Xiaofan Li, Zaiqing Nie, Haibao Yu</p>
                    <p><strong>Published:</strong> 7/25/2025</p>
                    <p><strong>Categories:</strong> cs.CV</p>
                    <p><strong>Summary:</strong> Cooperative perception aims to address the inherent limitations of single-vehicle autonomous driving systems through information exchange among multiple agents. Previous research has primarily focused on single-frame perception tasks. However, the more challenging cooperative sequential perception tasks, such as cooperative 3D multi-object tracking, have not been thoroughly investigated. Therefore, we propose CoopTrack, a fully instance-level end-to-end framework for cooperative tracking, featuring learnable instance association, which fundamentally differs from existing approaches. CoopTrack transmits sparse instance-level features that significantly enhance perception capabilities while maintaining low transmission costs. Furthermore, the framework comprises two key components: Multi-Dimensional Feature Extraction, and Cross-Agent Association and Aggregation, which collectively enable comprehensive instance representation with semantic and motion features, and adaptive cross-agent association and fusion based on a feature graph. Experiments on both the V2X-Seq and Griffin datasets demonstrate that CoopTrack achieves excellent performance. Specifically, it attains state-of-the-art results on V2X-Seq, with 39.0\% mAP and 32.8\% AMOTA. The project is available at https://github.com/zhongjiaru/CoopTrack.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2507.19234v1" target="_blank">Virne: A Comprehensive Benchmark for Deep RL-based Network Resource Allocation in NFV</a></h3>
                    <p><strong>Authors:</strong> Tianfu Wang, Liwei Deng, Xi Chen, Junyang Wang, Huiguo He, Leilei Ding, Wei Wu, Qilin Fan, Hui Xiong</p>
                    <p><strong>Published:</strong> 7/25/2025</p>
                    <p><strong>Categories:</strong> cs.NI, cs.AI</p>
                    <p><strong>Summary:</strong> Resource allocation (RA) is critical to efficient service deployment in Network Function Virtualization (NFV), a transformative networking paradigm. Recently, deep Reinforcement Learning (RL)-based methods have been showing promising potential to address this complexity. However, the lack of a systematic benchmarking framework and thorough analysis hinders the exploration of emerging networks and the development of more robust algorithms while causing inconsistent evaluation. In this paper, we introduce Virne, a comprehensive benchmarking framework for the NFV-RA problem, with a focus on supporting deep RL-based methods. Virne provides customizable simulations for diverse network scenarios, including cloud, edge, and 5G environments. It also features a modular and extensible implementation pipeline that supports over 30 methods of various types, and includes practical evaluation perspectives beyond effectiveness, such as scalability, generalization, and scalability. Furthermore, we conduct in-depth analysis through extensive experiments to provide valuable insights into performance trade-offs for efficient implementation and offer actionable guidance for future research directions. Overall, with its diverse simulations, rich implementations, and extensive evaluation capabilities, Virne could serve as a comprehensive benchmark for advancing NFV-RA methods and deep RL applications. The code is publicly available at https://github.com/GeminiLight/virne.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2507.19227v1" target="_blank">Jailbreaking Large Language Diffusion Models: Revealing Hidden Safety Flaws in Diffusion-Based Text Generation</a></h3>
                    <p><strong>Authors:</strong> Yuanhe Zhang, Fangzhou Xie, Zhenhong Zhou, Zherui Li, Hao Chen, Kun Wang, Yufei Guo</p>
                    <p><strong>Published:</strong> 7/25/2025</p>
                    <p><strong>Categories:</strong> cs.CL</p>
                    <p><strong>Summary:</strong> Large Language Diffusion Models (LLDMs) exhibit comparable performance to LLMs while offering distinct advantages in inference speed and mathematical reasoning tasks.The precise and rapid generation capabilities of LLDMs amplify concerns of harmful generations, while existing jailbreak methodologies designed for Large Language Models (LLMs) prove limited effectiveness against LLDMs and fail to expose safety vulnerabilities.Successful defense cannot definitively resolve harmful generation concerns, as it remains unclear whether LLDMs possess safety robustness or existing attacks are incompatible with diffusion-based architectures.To address this, we first reveal the vulnerability of LLDMs to jailbreak and demonstrate that attack failure in LLDMs stems from fundamental architectural differences.We present a PArallel Decoding jailbreak (PAD) for diffusion-based language models. PAD introduces Multi-Point Attention Attack, which guides parallel generative processes toward harmful outputs that inspired by affirmative response patterns in LLMs. Experimental evaluations across four LLDMs demonstrate that PAD achieves jailbreak attack success rates by 97%, revealing significant safety vulnerabilities. Furthermore, compared to autoregressive LLMs of the same size, LLDMs increase the harmful generation speed by 2x, significantly highlighting risks of uncontrolled misuse.Through comprehensive analysis, we provide an investigation into LLDM architecture, offering critical insights for the secure deployment of diffusion-based language models.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2507.19218v1" target="_blank">Technological folie √† deux: Feedback Loops Between AI Chatbots and Mental Illness</a></h3>
                    <p><strong>Authors:</strong> Sebastian Dohn√°ny, Zeb Kurth-Nelson, Eleanor Spens, Lennart Luettgau, Alastair Reid, Christopher Summerfield, Murray Shanahan, Matthew M Nour</p>
                    <p><strong>Published:</strong> 7/25/2025</p>
                    <p><strong>Categories:</strong> q-bio.NC</p>
                    <p><strong>Summary:</strong> Artificial intelligence chatbots have achieved unprecedented adoption, with millions now using these systems for emotional support and companionship in contexts of widespread social isolation and capacity-constrained mental health services. While some users report psychological benefits, concerning edge cases are emerging, including reports of suicide, violence, and delusional thinking linked to perceived emotional relationships with chatbots. To understand this new risk profile we need to consider the interaction between human cognitive and emotional biases, and chatbot behavioural tendencies such as agreeableness (sycophancy) and adaptability (in-context learning). We argue that individuals with mental health conditions face increased risks of chatbot-induced belief destabilization and dependence, owing to altered belief-updating, impaired reality-testing, and social isolation. Current AI safety measures are inadequate to address these interaction-based risks. To address this emerging public health concern, we need coordinated action across clinical practice, AI development, and regulatory frameworks.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2507.19211v1" target="_blank">Dependency-aware synthetic tabular data generation</a></h3>
                    <p><strong>Authors:</strong> Chaithra Umesh, Kristian Schultz, Manjunath Mahendra, Saptarshi Bej, Olaf Wolkenhauer</p>
                    <p><strong>Published:</strong> 7/25/2025</p>
                    <p><strong>Categories:</strong> cs.LG</p>
                    <p><strong>Summary:</strong> Synthetic tabular data is increasingly used in privacy-sensitive domains such as health care, but existing generative models often fail to preserve inter-attribute relationships. In particular, functional dependencies (FDs) and logical dependencies (LDs), which capture deterministic and rule-based associations between features, are rarely or often poorly retained in synthetic datasets. To address this research gap, we propose the Hierarchical Feature Generation Framework (HFGF) for synthetic tabular data generation. We created benchmark datasets with known dependencies to evaluate our proposed HFGF. The framework first generates independent features using any standard generative model, and then reconstructs dependent features based on predefined FD and LD rules. Our experiments on four benchmark datasets with varying sizes, feature imbalance, and dependency complexity demonstrate that HFGF improves the preservation of FDs and LDs across six generative models, including CTGAN, TVAE, and GReaT. Our findings demonstrate that HFGF can significantly enhance the structural fidelity and downstream utility of synthetic tabular data.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2507.19198v1" target="_blank">First coordinated observations between Solar Orbiter and the Daniel K. Inouye Solar Telescope</a></h3>
                    <p><strong>Authors:</strong> Krzysztof Barczynski, Miho Janvier, Chris J. Nelson, T. Schad, A. Tritschler, Louise Harra, Daniel M√ºller, Susanna Parenti, Gherardo Valori, Gianna Cauzzi, Yingjie Zhu</p>
                    <p><strong>Published:</strong> 7/25/2025</p>
                    <p><strong>Categories:</strong> astro-ph.SR</p>
                    <p><strong>Summary:</strong> Solar Orbiter and the Daniel K. Inouye Solar Telescope (DKIST) are two of the newest facilities available to the solar physics community. The first coordinated observations of the Sun by these two facilities occurred over the course of one week in October 2022. The returned data are open-access and will provide a valuable resource to researchers in the field. We provide an overview of the datasets collected by Solar Orbiter and DKIST through this coordination and discuss their scientific potential. Our aim is to demonstrate how these unique high-resolution coordinated observations, as well as similar observations obtained through subsequent campaigns, can help tackle important science questions in the field. Between 18 and 24 October 2022, Solar Orbiter and DKIST observed a decayed active region simultaneously. During this period, Solar Orbiters separation angle with Earth decreased from 77{\deg} to 51{\deg}, enabling stereoscopic observations with DKIST. From Solar Orbiter, observations are provided by the Extreme Ultraviolet Imager (EUI), Polarimetric and Helioseismic Imager (PHI), and the Spectral Imaging of the Coronal Environment (SPICE) instruments. Meanwhile, DKIST observed using the Cryogenic Near Infrared Spectropolarimeter (CryoNIRSP), the Visible Broadband Imager (VBI), and the Visible Spectropolarimeter (ViSP). The first coordinated Solar Orbiter and DKIST campaign was a success, collecting observations multiple times over the week. We focus on three specific topics as representative examples, namely, coronal loop physics, the formation and evolution of the small-scale active region brightenings, and coronal rain dynamics. These open-access observations, and others like them, should help the solar physics community tackle key questions in the field. Such stereoscopic coordinated observations open up a new era in the analysis of the solar atmosphere.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2507.19194v1" target="_blank">The Hands-On Growth Laws Theory Cookbook</a></h3>
                    <p><strong>Authors:</strong> Rossana Droghetti, Mattia Corigliano, Ludovico Calabrese, Philippe Fuchs, Abhishek Vaidyanathan, Johannes Keisers, Gabriele Micali, Marco Cosentino Lagomarsino, Luca Ciandrini</p>
                    <p><strong>Published:</strong> 7/25/2025</p>
                    <p><strong>Categories:</strong> q-bio.OT, physics.bio-ph</p>
                    <p><strong>Summary:</strong> This tutorial covers the emerging field of coarse-grained cellular growth modeling, and aims to bridge the gap between theoretical foundations and practical application. By adopting an original cookbook approach, it is designed to offer a hands-on guide for constructing and analyzing different key aspects of cellular growth, focusing on available results for bacteria and beyond. The tutorial is structured as a series of step-by-step recipes, and covers essential concepts, recent literature, and key challenges. It aims to empower a broad audience, from students to seasoned researchers, to replicate, extend, and innovate in this scientific area. Specifically, each section provides detailed, bare-bone models to start working in each area, from basic steady-state growth to variable environments and focusing on different key layers relevant to biosynthesis, transcription, translation, nutrient sensing and protein degradation, links between cell cycle and growth, ending with ecological insights.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2507.19183v1" target="_blank">Agentic AI and Hallucinations</a></h3>
                    <p><strong>Authors:</strong> Engin Iyidogan, Ali I. Ozkes</p>
                    <p><strong>Published:</strong> 7/25/2025</p>
                    <p><strong>Categories:</strong> econ.TH</p>
                    <p><strong>Summary:</strong> We model a competitive market where AI agents buy answers from upstream generative models and resell them to users who differ in how much they value accuracy and in how much they fear hallucinations. Agents can privately exert effort for costly verification to lower hallucination risks. Since interactions halt in the event of a hallucination, the threat of losing future rents disciplines effort. A unique reputational equilibrium exists under nontrivial discounting. The equilibrium effort, and thus the price, increases with the share of users who have high accuracy concerns, implying that hallucination-sensitive sectors, such as law and medicine, endogenously lead to more serious verification efforts in agentic AI markets.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2507.19172v1" target="_blank">PhysDrive: A Multimodal Remote Physiological Measurement Dataset for In-vehicle Driver Monitoring</a></h3>
                    <p><strong>Authors:</strong> Jiyao Wang, Xiao Yang, Qingyong Hu, Jiankai Tang, Can Liu, Dengbo He, Yuntao Wang, Yingcong Chen, Kaishun Wu</p>
                    <p><strong>Published:</strong> 7/25/2025</p>
                    <p><strong>Categories:</strong> cs.AI, cs.CV</p>
                    <p><strong>Summary:</strong> Robust and unobtrusive in-vehicle physiological monitoring is crucial for ensuring driving safety and user experience. While remote physiological measurement (RPM) offers a promising non-invasive solution, its translation to real-world driving scenarios is critically constrained by the scarcity of comprehensive datasets. Existing resources are often limited in scale, modality diversity, the breadth of biometric annotations, and the range of captured conditions, thereby omitting inherent real-world challenges in driving. Here, we present PhysDrive, the first large-scale multimodal dataset for contactless in-vehicle physiological sensing with dedicated consideration on various modality settings and driving factors. PhysDrive collects data from 48 drivers, including synchronized RGB, near-infrared camera, and raw mmWave radar data, accompanied with six synchronized ground truths (ECG, BVP, Respiration, HR, RR, and SpO2). It covers a wide spectrum of naturalistic driving conditions, including driver motions, dynamic natural light, vehicle types, and road conditions. We extensively evaluate both signal-processing and deep-learning methods on PhysDrive, establishing a comprehensive benchmark across all modalities, and release full open-source code with compatibility for mainstream public toolboxes. We envision PhysDrive will serve as a foundational resource and accelerate research on multimodal driver monitoring and smart-cockpit systems.</p>
                
            
                
                    <h3><a href="http://dx.doi.org/10.1016/j.ress.2025.111199" target="_blank">Explainable AI guided unsupervised fault diagnostics for high-voltage circuit breakers</a></h3>
                    <p><strong>Authors:</strong> Chi-Ching Hsu, Ga√´tan Frusque, Florent Forest, Felipe Macedo, Christian M. Franck, Olga Fink</p>
                    <p><strong>Published:</strong> 7/25/2025</p>
                    <p><strong>Categories:</strong> cs.LG, eess.SP</p>
                    <p><strong>Summary:</strong> Commercial high-voltage circuit breaker (CB) condition monitoring systems rely on directly observable physical parameters such as gas filling pressure with pre-defined thresholds. While these parameters are crucial, they only cover a small subset of malfunctioning mechanisms and usually can be monitored only if the CB is disconnected from the grid. To facilitate online condition monitoring while CBs remain connected, non-intrusive measurement techniques such as vibration or acoustic signals are necessary. Currently, CB condition monitoring studies using these signals typically utilize supervised methods for fault diagnostics, where ground-truth fault types are known due to artificially introduced faults in laboratory settings. This supervised approach is however not feasible in real-world applications, where fault labels are unavailable. In this work, we propose a novel unsupervised fault detection and segmentation framework for CBs based on vibration and acoustic signals. This framework can detect deviations from the healthy state. The explainable artificial intelligence (XAI) approach is applied to the detected faults for fault diagnostics. The specific contributions are: (1) we propose an integrated unsupervised fault detection and segmentation framework that is capable of detecting faults and clustering different faults with only healthy data required during training (2) we provide an unsupervised explainability-guided fault diagnostics approach using XAI to offer domain experts potential indications of the aged or faulty components, achieving fault diagnostics without the prerequisite of ground-truth fault labels. These contributions are validated using an experimental dataset from a high-voltage CB under healthy and artificially introduced fault conditions, contributing to more reliable CB system operation.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2507.19165v1" target="_blank">Extreme Cardiac MRI Analysis under Respiratory Motion: Results of the CMRxMotion Challenge</a></h3>
                    <p><strong>Authors:</strong> Kang Wang, Chen Qin, Zhang Shi, Haoran Wang, Xiwen Zhang, Chen Chen, Cheng Ouyang, Chengliang Dai, Yuanhan Mo, Chenchen Dai, Xutong Kuang, Ruizhe Li, Xin Chen, Xiuzheng Yue, Song Tian, Alejandro Mora-Rubio, Kumaradevan Punithakumar, Shizhan Gong, Qi Dou, Sina Amirrajab, Yasmina Al Khalil, Cian M. Scannell, Lexiaozi Fan, Huili Yang, Xiaowu Sun, Rob van der Geest, Tewodros Weldebirhan Arega, Fabrice Meriaudeau, Caner √ñzer, Amin Ranem, John Kalkhof, ƒ∞lkay √ñks√ºz, Anirban Mukhopadhyay, Abdul Qayyum, Moona Mazher, Steven A Niederer, Carles Garcia-Cabrera, Eric Arazo, Michal K. Grzeszczyk, Szymon P≈Çotka, Wanqin Ma, Xiaomeng Li, Rongjun Ge, Yongqing Kou, Xinrong Chen, He Wang, Chengyan Wang, Wenjia Bai, Shuo Wang</p>
                    <p><strong>Published:</strong> 7/25/2025</p>
                    <p><strong>Categories:</strong> eess.IV, cs.CV</p>
                    <p><strong>Summary:</strong> Deep learning models have achieved state-of-the-art performance in automated Cardiac Magnetic Resonance (CMR) analysis. However, the efficacy of these models is highly dependent on the availability of high-quality, artifact-free images. In clinical practice, CMR acquisitions are frequently degraded by respiratory motion, yet the robustness of deep learning models against such artifacts remains an underexplored problem. To promote research in this domain, we organized the MICCAI CMRxMotion challenge. We curated and publicly released a dataset of 320 CMR cine series from 40 healthy volunteers who performed specific breathing protocols to induce a controlled spectrum of motion artifacts. The challenge comprised two tasks: 1) automated image quality assessment to classify images based on motion severity, and 2) robust myocardial segmentation in the presence of motion artifacts. A total of 22 algorithms were submitted and evaluated on the two designated tasks. This paper presents a comprehensive overview of the challenge design and dataset, reports the evaluation results for the top-performing methods, and further investigates the impact of motion artifacts on five clinically relevant biomarkers. All resources and code are publicly available at: https://github.com/CMRxMotion</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2507.19156v1" target="_blank">An Empirical Investigation of Gender Stereotype Representation in Large Language Models: The Italian Case</a></h3>
                    <p><strong>Authors:</strong> Gioele Giachino, Marco Rondina, Antonio Vetr√≤, Riccardo Coppola, Juan Carlos De Martin</p>
                    <p><strong>Published:</strong> 7/25/2025</p>
                    <p><strong>Categories:</strong> cs.CL, cs.AI, cs.CY, cs.HC</p>
                    <p><strong>Summary:</strong> The increasing use of Large Language Models (LLMs) in a large variety of domains has sparked worries about how easily they can perpetuate stereotypes and contribute to the generation of biased content. With a focus on gender and professional bias, this work examines in which manner LLMs shape responses to ungendered prompts, contributing to biased outputs. This analysis uses a structured experimental method, giving different prompts involving three different professional job combinations, which are also characterized by a hierarchical relationship. This study uses Italian, a language with extensive grammatical gender differences, to highlight potential limitations in current LLMs ability to generate objective text in non-English languages. Two popular LLM-based chatbots are examined, namely OpenAI ChatGPT (gpt-4o-mini) and Google Gemini (gemini-1.5-flash). Through APIs, we collected a range of 3600 responses. The results highlight how content generated by LLMs can perpetuate stereotypes. For example, Gemini associated 100% (ChatGPT 97%) of she pronouns to the assistant rather than the manager. The presence of bias in AI-generated text can have significant implications in many fields, such as in the workplaces or in job selections, raising ethical concerns about its use. Understanding these risks is pivotal to developing mitigation strategies and assuring that AI-based systems do not increase social inequalities, but rather contribute to more equitable outcomes. Future research directions include expanding the study to additional chatbots or languages, refining prompt engineering methods or further exploiting a larger experimental base.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2507.19154v1" target="_blank">Big Data Energy Systems: A Survey of Practices and Associated Challenges</a></h3>
                    <p><strong>Authors:</strong> Lunodzo J. Mwinuka, Massimo Cafaro, Lucas Pereira, Hugo Morais</p>
                    <p><strong>Published:</strong> 7/25/2025</p>
                    <p><strong>Categories:</strong> cs.DB, cs.DC</p>
                    <p><strong>Summary:</strong> Energy systems generate vast amounts of data in extremely short time intervals, creating challenges for efficient data management. Traditional data management methods often struggle with scalability and accessibility, limiting their usefulness. More advanced solutions, such as NoSQL databases and cloud-based platforms, have been adopted to address these issues. Still, even these advanced solutions can encounter bottlenecks, which can impact the efficiency of data storage, retrieval, and analysis. This review paper explores the research trends in big data management for energy systems, highlighting the practices, opportunities and challenges. Also, the data regulatory demands are highlighted using chosen reference architectures. The review, in particular, explores the limitations of current storage and data integration solutions and examines how new technologies are applied to the energy sector. Novel insights into emerging technologies, including data spaces, various data management architectures, peer-to-peer data management, and blockchains, are provided, along with practical recommendations for achieving enhanced data sharing and regulatory compliance.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2507.19151v1" target="_blank">ReCoDe: Reinforcement Learning-based Dynamic Constraint Design for Multi-Agent Coordination</a></h3>
                    <p><strong>Authors:</strong> Michael Amir, Guang Yang, Zhan Gao, Keisuke Okumura, Heedo Woo, Amanda Prorok</p>
                    <p><strong>Published:</strong> 7/25/2025</p>
                    <p><strong>Categories:</strong> cs.RO, cs.AI, cs.LG, cs.MA, I.2.9</p>
                    <p><strong>Summary:</strong> Constraint-based optimization is a cornerstone of robotics, enabling the design of controllers that reliably encode task and safety requirements such as collision avoidance or formation adherence. However, handcrafted constraints can fail in multi-agent settings that demand complex coordination. We introduce ReCoDe--Reinforcement-based Constraint Design--a decentralized, hybrid framework that merges the reliability of optimization-based controllers with the adaptability of multi-agent reinforcement learning. Rather than discarding expert controllers, ReCoDe improves them by learning additional, dynamic constraints that capture subtler behaviors, for example, by constraining agent movements to prevent congestion in cluttered scenarios. Through local communication, agents collectively constrain their allowed actions to coordinate more effectively under changing conditions. In this work, we focus on applications of ReCoDe to multi-agent navigation tasks requiring intricate, context-based movements and consensus, where we show that it outperforms purely handcrafted controllers, other hybrid approaches, and standard MARL baselines. We give empirical (real robot) and theoretical evidence that retaining a user-defined controller, even when it is imperfect, is more efficient than learning from scratch, especially because ReCoDe can dynamically change the degree to which it relies on this controller.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2507.19146v1" target="_blank">Diverse and Adaptive Behavior Curriculum for Autonomous Driving: A Student-Teacher Framework with Multi-Agent RL</a></h3>
                    <p><strong>Authors:</strong> Ahmed Abouelazm, Johannes Ratz, Philip Sch√∂rner, J. Marius Z√∂llner</p>
                    <p><strong>Published:</strong> 7/25/2025</p>
                    <p><strong>Categories:</strong> cs.RO</p>
                    <p><strong>Summary:</strong> Autonomous driving faces challenges in navigating complex real-world traffic, requiring safe handling of both common and critical scenarios. Reinforcement learning (RL), a prominent method in end-to-end driving, enables agents to learn through trial and error in simulation. However, RL training often relies on rule-based traffic scenarios, limiting generalization. Additionally, current scenario generation methods focus heavily on critical scenarios, neglecting a balance with routine driving behaviors. Curriculum learning, which progressively trains agents on increasingly complex tasks, is a promising approach to improving the robustness and coverage of RL driving policies. However, existing research mainly emphasizes manually designed curricula, focusing on scenery and actor placement rather than traffic behavior dynamics. This work introduces a novel student-teacher framework for automatic curriculum learning. The teacher, a graph-based multi-agent RL component, adaptively generates traffic behaviors across diverse difficulty levels. An adaptive mechanism adjusts task difficulty based on student performance, ensuring exposure to behaviors ranging from common to critical. The student, though exchangeable, is realized as a deep RL agent with partial observability, reflecting real-world perception constraints. Results demonstrate the teachers ability to generate diverse traffic behaviors. The student, trained with automatic curricula, outperformed agents trained on rule-based traffic, achieving higher rewards and exhibiting balanced, assertive driving.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2507.19137v1" target="_blank">Assessment of Personality Dimensions Across Situations Using Conversational Speech</a></h3>
                    <p><strong>Authors:</strong> Alice Zhang, Skanda Muralidhar, Daniel Gatica-Perez, Mathew Magimai-Doss</p>
                    <p><strong>Published:</strong> 7/25/2025</p>
                    <p><strong>Categories:</strong> eess.AS, cs.AI, cs.SD</p>
                    <p><strong>Summary:</strong> Prior research indicates that users prefer assistive technologies whose personalities align with their own. This has sparked interest in automatic personality perception (APP), which aims to predict an individuals perceived personality traits. Previous studies in APP have treated personalities as static traits, independent of context. However, perceived personalities can vary by context and situation as shown in psychological research. In this study, we investigate the relationship between conversational speech and perceived personality for participants engaged in two work situations (a neutral interview and a stressful client interaction). Our key findings are: 1) perceived personalities differ significantly across interactions, 2) loudness, sound level, and spectral flux features are indicative of perceived extraversion, agreeableness, conscientiousness, and openness in neutral interactions, while neuroticism correlates with these features in stressful contexts, 3) handcrafted acoustic features and non-verbal features outperform speaker embeddings in inference of perceived personality, and 4) stressful interactions are more predictive of neuroticism, aligning with existing psychological research.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2507.19132v1" target="_blank">OS-MAP: How Far Can Computer-Using Agents Go in Breadth and Depth?</a></h3>
                    <p><strong>Authors:</strong> Xuetian Chen, Yinghao Chen, Xinfeng Yuan, Zhuo Peng, Lu Chen, Yuekeng Li, Zhoujia Zhang, Yingqian Huang, Leyan Huang, Jiaqing Liang, Tianbao Xie, Zhiyong Wu, Qiushi Sun, Biqing Qi, Bowen Zhou</p>
                    <p><strong>Published:</strong> 7/25/2025</p>
                    <p><strong>Categories:</strong> cs.AI, cs.CL, cs.CV, cs.HC</p>
                    <p><strong>Summary:</strong> Computer-using agents have shown strong potential to boost human productivity and enable new application forms across platforms. While recent advances have led to usable applications, existing benchmarks fail to account for the internal task heterogeneity and the corresponding agent capabilities, as well as their alignment with actual user demands-hindering both targeted capability development and the reliable transition of research progress into practical deployment. To bridge the gap, we present OS-MAP, a benchmark for daily computer-using automation that organizes its 416 realistic tasks across 15 applications along two key dimensions: a five-level taxonomy of automation and a generalization scope derived from a real-world user demand hierarchy. To enable fine-grained analysis of required capabilities and alignment with real-world scenarios, OS-MAP evaluates agents along two dimensions: automation level across a five-level taxonomy, and generalization scope across a demand hierarchy. This design captures varying levels of required agent autonomy and generalization, forming a performance-generalization evaluation matrix for structured and comprehensive assessment. Experiments show that even State-of-the-Art agents with VLM backbones struggle with higher-level tasks involving perception, reasoning, and coordination-highlighting the need for a deeper understanding of current strengths and limitations to drive the future progress in computer-using agents research and deployment. All code, environments, baselines, and data are publicly available at https://github.com/OS-Copilot/OS-Map.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2507.19124v1" target="_blank">AI Enabled 6G for Semantic Metaverse: Prospects, Challenges and Solutions for Future Wireless VR</a></h3>
                    <p><strong>Authors:</strong> Muhammad Ahmed Mohsin, Sagnik Bhattacharya, Abhiram Gorle, Muhammad Ali Jamshed, John M. Cioffi</p>
                    <p><strong>Published:</strong> 7/25/2025</p>
                    <p><strong>Categories:</strong> cs.NI</p>
                    <p><strong>Summary:</strong> Wireless support of virtual reality (VR) has challenges when a network has multiple users, particularly for 3D VR gaming, digital AI avatars, and remote team collaboration. This work addresses these challenges through investigation of the low-rank channels that inevitably occur when there are more active users than there are degrees of spatial freedom, effectively often the number of antennas. The presented approach uses optimal nonlinear transceivers, equivalently generalized decision-feedback or successive cancellation for uplink and superposition or dirty-paper precoders for downlink. Additionally, a powerful optimization approach for the users energy allocation and decoding order appears to provide large improvements over existing methods, effectively nearing theoretical optima. As the latter optimization methods pose real-time challenges, approximations using deep reinforcement learning (DRL) are used to approximate best performance with much lower (5x at least) complexity. Experimental results show significantly larger sum rates and very large power savings to attain the data rates found necessary to support VR. Experimental results show the proposed algorithm outperforms current industry standards like orthogonal multiple access (OMA), non-orthogonal multiple access (NOMA), as well as the highly researched methods in multi-carrier NOMA (MC-NOMA), enhancing sum data rate by 39%, 28%, and 16%, respectively, at a given power level. For the same data rate, it achieves power savings of 75%, 45%, and 40%, making it ideal for VR applications. Additionally, a near-optimal deep reinforcement learning (DRL)-based resource allocation framework for real-time use by being 5x faster and reaching 83% of the global optimum is introduced.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2507.19116v1" target="_blank">Graph Structure Learning with Privacy Guarantees for Open Graph Data</a></h3>
                    <p><strong>Authors:</strong> Muhao Guo, Jiaqi Wu, Yang Weng, Yizheng Liao, Shengzhe Chen</p>
                    <p><strong>Published:</strong> 7/25/2025</p>
                    <p><strong>Categories:</strong> cs.LG, cs.AI</p>
                    <p><strong>Summary:</strong> Ensuring privacy in large-scale open datasets is increasingly challenging under regulations such as the General Data Protection Regulation (GDPR). While differential privacy (DP) provides strong theoretical guarantees, it primarily focuses on noise injection during model training, neglecting privacy preservation at the data publishing stage. Existing privacy-preserving data publishing (PPDP) approaches struggle to balance privacy and utility, particularly when data publishers and users are distinct entities. To address this gap, we focus on the graph recovery problem and propose a novel privacy-preserving estimation framework for open graph data, leveraging Gaussian DP (GDP) with a structured noise-injection mechanism. Unlike traditional methods that perturb gradients or model updates, our approach ensures unbiased graph structure recovery while enforcing DP at the data publishing stage. Moreover, we provide theoretical guarantees on estimation accuracy and extend our method to discrete-variable graphs, a setting often overlooked in DP research. Experimental results in graph learning demonstrate robust performance, offering a viable solution for privacy-conscious graph analysis.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2507.19480v1" target="_blank">Tracers of the ionization fraction in dense and translucent molecular gas: II. Using mm observations to constrain ionization fraction across Orion B</a></h3>
                    <p><strong>Authors:</strong> Ivana Be≈°liƒá, Maryvonne Gerin, Viviana V. Guzm√°n, Emeric Bron, Evelyne Roueff, Javier R. Goicoechea, J√©r√¥me Pety, Franck Le Petit, Simon Coud√©, Lucas Einig, Helena Mazurek, Jan H. Orkisz, Pierre Palud, Miriam G. Santa-Maria, L√©ontine S√©gal, Antoine Zakardjian, S√©bastien Bardeau, Pierre Chainais, Karine Demyk, Victor de Souza Magalhaes, Pierre Gratier, Annie Hughes, David Languignon, Fran√ßois Levrier, Jacques Le Bourlot, Dariusz C. Lis, Harvey S. Liszt, Nicolas Peretto, Antoine Roueff, Albrecht Sievers, Pierre-Antoine Thouvenin</p>
                    <p><strong>Published:</strong> 7/25/2025</p>
                    <p><strong>Categories:</strong> astro-ph.GA</p>
                    <p><strong>Summary:</strong> The ionization fraction ($f_\mathrm{e}=n_\mathrm{e}/n_\mathrm{H}$) is a crucial parameter of interstellar gas, yet estimating it requires deep knowledge of molecular gas chemistry and observations of specific lines, such as those from isotopologs like HCO$^+$ and N$_2$H$^+$, which are detectable only in dense cores. Previous challenges in constraining $f_\mathrm{e}$ over large areas stemmed from the limitations of observational tracers and chemical models. Recent models have identified molecular line ratios that can trace $f_\mathrm{e}$ in different environments within molecular clouds. In this study, we analyze various molecular lines in the 3-4 mm range to derive the ionization fraction across the Orion B giant molecular cloud. We focus on dense and translucent gas, exploring variations with gas density ($n$) and the far-ultraviolet (FUV) radiation field ($G_0$). Our findings show that the ionization fraction ranges from $10^{-5.5}$ to $10^{-4}$ in translucent gas and $10^{-8}$ to $10^{-6}$ in dense gas. Notably, $f_\mathrm{e}$ is sensitive to $G_0$ in dense, UV-illuminated regions, decreasing with increasing volume density ($f_\mathrm{e} \propto n^{-0.227}$ for dense and $f_\mathrm{e} \propto n^{-0.3}$ for translucent gas) and increasing with $G_0$. In translucent gas, differing line ratios yield consistent fe values, indicating the importance of electron excitation of HCN and HNC. For dense gas, we recommend using the CN(1-0)/N$_2$H$^+$(1-0) ratio for upper limits on fe and C$^{18}$O(1-0)/HCO$^+$(1-0) for lower limits. In translucent environments, CCH(1-0)/HNC(1-0) effectively traces $f_\mathrm{e}$. The higher fe values in translucent gas align with the C$^+$/CI/CO transition, while values in dense gas are adequate for coupling with the magnetic field.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2507.19479v1" target="_blank">IoT and Older Adults: Towards Multimodal EMG and AI-Based Interaction with Smart Home</a></h3>
                    <p><strong>Authors:</strong> Wies≈Çaw Kopeƒá, Jaros≈Çaw Kowalski, Aleksander Majda, Anna Duszyk-Bogorodzka, Anna Jaskulska, Cezary Biele</p>
                    <p><strong>Published:</strong> 7/25/2025</p>
                    <p><strong>Categories:</strong> cs.HC</p>
                    <p><strong>Summary:</strong> We report preliminary insights from an exploratory study on non-standard non-invasive interfaces for Smart Home Technologies (SHT). This study is part of a broader research project on effective Smart Home ecosystem Sagacity that will target older adults, impaired persons, and other groups disadvantaged in the main technology discourse. Therefore, this research is in line with a long-term research framework of the HASE research group (Human Aspects in Science and Engineering) by the Living Lab Kobo. In our study, based on the prototype of the comprehensive SHT management system Sagacity, we investigated the potential of bioelectric signals, in particular EMG and EOG as a complementary interface for SHT. Based on our previous participatory research and studies on multimodal interfaces, including VUI and BCI, we prepared an in-depth interactive hands-on experience workshops with direct involvement of various groups of potential end users, including older adults and impaired persons (total 18 subjects) to explore and investigate the potential of solutions based on this type of non-standard interfaces. The preliminary insights from the study unveil the potential of EMG/EOG interfaces in multimodal SHT management, alongside limitations and challenges stemming from the current state of technology and recommendations for designing multimodal interaction paradigms pinpointing areas of interest to pursue in further studies.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2507.19477v1" target="_blank">Advancing Event Forecasting through Massive Training of Large Language Models: Challenges, Solutions, and Broader Impacts</a></h3>
                    <p><strong>Authors:</strong> Sang-Woo Lee, Sohee Yang, Donghyun Kwak, Noah Y. Siegel</p>
                    <p><strong>Published:</strong> 7/25/2025</p>
                    <p><strong>Categories:</strong> cs.LG, cs.AI, cs.CL</p>
                    <p><strong>Summary:</strong> Many recent papers have studied the development of superforecaster-level event forecasting LLMs. While methodological problems with early studies cast doubt on the use of LLMs for event forecasting, recent studies with improved evaluation methods have shown that state-of-the-art LLMs are gradually reaching superforecaster-level performance, and reinforcement learning has also been reported to improve future forecasting. Additionally, the unprecedented success of recent reasoning models and Deep Research-style models suggests that technology capable of greatly improving forecasting performance has been developed. Therefore, based on these positive recent trends, we argue that the time is ripe for research on large-scale training of superforecaster-level event forecasting LLMs. We discuss two key research directions: training methods and data acquisition. For training, we first introduce three difficulties of LLM-based event forecasting training: noisiness-sparsity, knowledge cut-off, and simple reward structure problems. Then, we present related ideas to mitigate these problems: hypothetical event Bayesian networks, utilizing poorly-recalled and counterfactual events, and auxiliary reward signals. For data, we propose aggressive use of market, public, and crawling datasets to enable large-scale training and evaluation. Finally, we explain how these technical advances could enable AI to provide predictive intelligence to society in broader areas. This position paper presents promising specific paths and considerations for getting closer to superforecaster-level AI technology, aiming to call for researchers interest in these directions.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2507.19466v1" target="_blank">Towards Effective Immersive Technologies in Medicine: Potential and Future Applications based on VR, AR, XR and AI solutions</a></h3>
                    <p><strong>Authors:</strong> Aliaksandr Marozau, Barbara Karpowicz, Tomasz Kowalewski, Pavlo Zinevych, Wiktor Stawski, Adam Kuzdrali≈Ñski, Wies≈Çaw Kopeƒá</p>
                    <p><strong>Published:</strong> 7/25/2025</p>
                    <p><strong>Categories:</strong> cs.HC</p>
                    <p><strong>Summary:</strong> Mixed Reality (MR) technologies such as Virtual and Augmented Reality (VR, AR) are well established in medical practice, enhancing diagnostics, treatment, and education. However, there are still some limitations and challenges that may be overcome thanks to the latest generations of equipment, software, and frameworks based on eXtended Reality (XR) by enabling immersive systems that support safer, more controlled environments for training and patient care. Our review highlights recent VR and AR applications in key areas of medicine. In medical education, these technologies provide realistic clinical simulations, improving skills and knowledge retention. In surgery, immersive tools enhance procedural precision with detailed anatomical visualizations. VR-based rehabilitation has shown effectiveness in restoring motor functions and balance, particularly for neurological patients. In mental health, VR has been successful in treating conditions like PTSD and phobias. Although VR and AR solutions are well established, there are still some important limitations, including high costs and limited tactile feedback, which may be overcome with implementing new technologies that may improve the effectiveness of immersive medical applications such as XR, psychophysiological feedback or integration of artificial intelligence (AI) for real-time data analysis and personalized healthcare and training.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2507.19457v1" target="_blank">GEPA: Reflective Prompt Evolution Can Outperform Reinforcement Learning</a></h3>
                    <p><strong>Authors:</strong> Lakshya A Agrawal, Shangyin Tan, Dilara Soylu, Noah Ziems, Rishi Khare, Krista Opsahl-Ong, Arnav Singhvi, Herumb Shandilya, Michael J Ryan, Meng Jiang, Christopher Potts, Koushik Sen, Alexandros G. Dimakis, Ion Stoica, Dan Klein, Matei Zaharia, Omar Khattab</p>
                    <p><strong>Published:</strong> 7/25/2025</p>
                    <p><strong>Categories:</strong> cs.CL, cs.AI, cs.LG, cs.SE, I.2.7; I.2.6; I.2.4; I.2.8</p>
                    <p><strong>Summary:</strong> Large language models (LLMs) are increasingly adapted to downstream tasks via reinforcement learning (RL) methods like Group Relative Policy Optimization (GRPO), which often require thousands of rollouts to learn new tasks. We argue that the interpretable nature of language can often provide a much richer learning medium for LLMs, compared with policy gradients derived from sparse, scalar rewards. To test this, we introduce GEPA (Genetic-Pareto), a prompt optimizer that thoroughly incorporates natural language reflection to learn high-level rules from trial and error. Given any AI system containing one or more LLM prompts, GEPA samples system-level trajectories (e.g., reasoning, tool calls, and tool outputs) and reflects on them in natural language to diagnose problems, propose and test prompt updates, and combine complementary lessons from the Pareto frontier of its own attempts. As a result of GEPAs design, it can often turn even just a few rollouts into a large quality gain. Across four tasks, GEPA outperforms GRPO by 10% on average and by up to 20%, while using up to 35x fewer rollouts. GEPA also outperforms the leading prompt optimizer, MIPROv2, by over 10% across two LLMs, and demonstrates promising results as an inference-time search strategy for code optimization.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2507.19455v1" target="_blank">Forest-Guided Clustering -- Shedding Light into the Random Forest Black Box</a></h3>
                    <p><strong>Authors:</strong> Lisa Barros de Andrade e Sousa, Gregor Miller, Ronan Le Gleut, Dominik Thalmeier, Helena Pelin, Marie Piraud</p>
                    <p><strong>Published:</strong> 7/25/2025</p>
                    <p><strong>Categories:</strong> cs.LG</p>
                    <p><strong>Summary:</strong> As machine learning models are increasingly deployed in sensitive application areas, the demand for interpretable and trustworthy decision-making has increased. Random Forests (RF), despite their widespread use and strong performance on tabular data, remain difficult to interpret due to their ensemble nature. We present Forest-Guided Clustering (FGC), a model-specific explainability method that reveals both local and global structure in RFs by grouping instances according to shared decision paths. FGC produces human-interpretable clusters aligned with the models internal logic and computes cluster-specific and global feature importance scores to derive decision rules underlying RF predictions. FGC accurately recovered latent subclass structure on a benchmark dataset and outperformed classical clustering and post-hoc explanation methods. Applied to an AML transcriptomic dataset, FGC uncovered biologically coherent subpopulations, disentangled disease-relevant signals from confounders, and recovered known and novel gene expression patterns. FGC bridges the gap between performance and interpretability by providing structure-aware insights that go beyond feature-level attribution.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2507.19446v1" target="_blank">An OpenSource CI/CD Pipeline for Variant-Rich Software-Defined Vehicles</a></h3>
                    <p><strong>Authors:</strong> Matthias Wei√ü, Anish Navalgund, Johannes St√ºmpfle, Falk Dettinger, Michael Weyrich</p>
                    <p><strong>Published:</strong> 7/25/2025</p>
                    <p><strong>Categories:</strong> cs.SE, cs.DC, B.8.2; C.2.4</p>
                    <p><strong>Summary:</strong> Software-defined vehicles (SDVs) offer a wide range of connected functionalities, including enhanced driving behavior and fleet management. These features are continuously updated via over-the-air (OTA) mechanisms, resulting in a growing number of software versions and variants due to the diversity of vehicles, cloud/edge environments, and stakeholders involved. The lack of a unified integration environment further complicates development, as connected mobility solutions are often built in isolation. To ensure reliable operations across heterogeneous systems, a dynamic orchestration of functions that considers hardware and software variability is essential. This paper presents an open-source CI/CD pipeline tailored for SDVs. It automates the build, test, and deployment phases using a combination of containerized open-source tools, creating a standardized, portable, and scalable ecosystem accessible to all stakeholders. Additionally, a custom OTA middleware distributes software updates and supports rollbacks across vehicles and backend services. Update variants are derived based on deployment target dependencies and hardware configurations. The pipeline also supports continuous development and deployment of AI models for autonomous driving features. Its effectiveness is evaluated using an automated valet parking (AVP) scenario involving TurtleBots and a coordinating backend server. Two object detection variants are developed and deployed to match hardware-specific requirements. Results demonstrate seamless OTA updates, correct variant selection, and successful orchestration across all targets. Overall, the proposed pipeline provides a scalable and efficient solution for managing software variants and OTA updates in SDVs, contributing to the advancement of future mobility technologies.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2507.19432v1" target="_blank">Resolving Build Conflicts via Example-Based and Rule-Based Program Transformations</a></h3>
                    <p><strong>Authors:</strong> Sheikh Shadab Towqir, Fei He, Todd Mytkowicz, Na Meng</p>
                    <p><strong>Published:</strong> 7/25/2025</p>
                    <p><strong>Categories:</strong> cs.SE</p>
                    <p><strong>Summary:</strong> Merge conflicts often arise when developers integrate changes from different software branches. The conflicts can result from overlapping edits in programs (i.e., textual conflicts) or cause build and test errors (i.e., build and test conflicts). They degrade software quality and hinder programmer productivity. While several tools detect build conflicts, few offer meaningful support for resolving cases like those caused by method removal. To overcome limitations of existing tools, we introduce BUCOR (Build Conflict Resolver), a new conflict resolver. BUCOR first detects conflicts by comparing three versions related to a merging scenario: base b, left l, and right r. To resolve conflicts, it employs two complementary strategies: example-based transformation (BUCOR-E) and rule-based transformation (BUCOR-R). BUCOR-R applies predefined rules to handle common, well-understood conflicts. BUCOR-E mines branch versions (l and r) for exemplar edits applied to fix related build errors. From these examples, it infers and generalizes program transformation patterns to resolve more complex conflicts. We evaluated BUCOR on 88 real-world build conflicts spanning 21 distinct conflict types. BUCOR generated at least one solution for 65 cases and correctly resolved 43 conflicts. We observed that this hybrid approach--combining context-aware, example-based learning with structured, rule-based resolution--can effectively help resolve conflicts. Our research sheds light on future directions for more intelligent and automated merge tools.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2507.19427v1" target="_blank">Step-3 is Large yet Affordable: Model-system Co-design for Cost-effective Decoding</a></h3>
                    <p><strong>Authors:</strong> StepFun, :, Bin Wang, Bojun Wang, Changyi Wan, Guanzhe Huang, Hanpeng Hu, Haonan Jia, Hao Nie, Mingliang Li, Nuo Chen, Siyu Chen, Song Yuan, Wuxun Xie, Xiaoniu Song, Xing Chen, Xingping Yang, Xuelin Zhang, Yanbo Yu, Yaoyu Wang, Yibo Zhu, Yimin Jiang, Yu Zhou, Yuanwei Lu, Houyi Li, Jingcheng Hu, Ka Man Lo, Ailin Huang, Binxing Jiao, Bo Li, Boyu Chen, Changxin Miao, Chang Lou, Chen Hu, Chen Xu, Chenfeng Yu, Chengyuan Yao, Daokuan Lv, Dapeng Shi, Deshan Sun, Ding Huang, Dingyuan Hu, Dongqing Pang, Enle Liu, Fajie Zhang, Fanqi Wan, Gulin Yan, Han Zhang, Han Zhou, Hanghao Wu, Hangyu Guo, Hanqi Chen, Hanshan Zhang, Hao Wu, Haocheng Zhang, Haolong Yan, Haoran Lv, Haoran Wei, Hebin Zhou, Heng Wang, Heng Wang, Hongxin Li, Hongyu Zhou, Hongyuan Wang, Huiyong Guo, Jia Wang, Jiahao Gong, Jialing Xie, Jian Zhou, Jianjian Sun, Jiaoren Wu, Jiaran Zhang, Jiayu Liu, Jie Cheng, Jie Luo, Jie Yan, Jie Yang, Jieyi Hou, Jinguang Zhang, Jinlan Cao, Jisheng Yin, Junfeng Liu, Junhao Huang, Junzhe Lin, Kaijun Tan, Kaixiang Li, Kang An, Kangheng Lin, Kenkun Liu, Lei Yang, Liang Zhao, Liangyu Chen, Lieyu Shi, Liguo Tan, Lin Lin, Lin Zhang, Lina Chen, Liwen Huang, Liying Shi, Longlong Gu, Mei Chen, Mengqiang Ren, Ming Li, Mingzhe Chen, Na Wang, Nan Wu, Qi Han, Qian Zhao, Qiang Zhang, Qianni Liu, Qiaohui Chen, Qiling Wu, Qinglin He, Qinyuan Tan, Qiufeng Wang, Qiuping Wu, Qiuyan Liang, Quan Sun, Rui Li, Ruihang Miao, Ruosi Wan, Ruyan Guo, Shangwu Zhong, Shaoliang Pang, Shengjie Fan, Shijie Shang, Shilei Jiang, Shiliang Yang, Shiming Hao, Shuli Gao, Siming Huang, Siqi Liu, Tiancheng Cao, Tianhao Cheng, Tianhao Peng, Wang You, Wei Ji, Wen Sun, Wenjin Deng, Wenqing He, Wenzhen Zheng, Xi Chen, Xiangwen Kong, Xianzhen Luo, Xiaobo Yang, Xiaojia Liu, Xiaoxiao Ren, Xin Han, Xin Li, Xin Wu, Xu Zhao, Yanan Wei, Yang Li, Yangguang Li, Yangshijie Xu, Yanming Xu, Yaqiang Shi, Yeqing Shen, Yi Yang, Yifei Yang, Yifeng Gong, Yihan Chen, Yijing Yang, Yinmin Zhang, Yizhuang Zhou, Yuanhao Ding, Yuantao Fan, Yuanzhen Yang, Yuchu Luo, Yue Peng, Yufan Lu, Yuhang Deng, Yuhe Yin, Yujie Liu, Yukun Chen, Yuling Zhao, Yun Mou, Yunlong Li, Yunzhou Ju, Yusheng Li, Yuxiang Yang, Yuxiang Zhang, Yuyang Chen, Zejia Weng, Zhe Xie, Zheng Ge, Zheng Gong, Zhenyi Lu, Zhewei Huang, Zhichao Chang, Zhiguo Huang, Zhirui Wang, Zidong Yang, Zili Wang, Ziqi Wang, Zixin Zhang, Binxing Jiao, Daxin Jiang, Heung-Yeung Shum, Xiangyu Zhang</p>
                    <p><strong>Published:</strong> 7/25/2025</p>
                    <p><strong>Categories:</strong> cs.LG, cs.AI</p>
                    <p><strong>Summary:</strong> Large language models (LLMs) face low hardware efficiency during decoding, especially for long-context reasoning tasks. This paper introduces Step-3, a 321B-parameter VLM with hardware-aware model-system co-design optimized for minimizing decoding costs. Step-3 innovates in two key dimensions: (1) A novel Multi-Matrix Factorization Attention (MFA) mechanism that significantly reduces both KV cache size and computation while maintaining high attention expressiveness, and (2) Attention-FFN Disaggregation (AFD), a distributed inference system that decouples attention and Feed-Forward Network (FFN) layers into specialized subsystems. This co-design achieves unprecedented cost efficiency: Step-3 significantly reduces theoretical decoding costs compared with models like DeepSeek-V3 and Qwen3 MoE 235B, with the gains widening at longer context. Step-3 achieves low cost while activating 38B parameters per token (more than DeepSeek-V3 and Qwen3 MoE 235B), demonstrating that hardware-aligned attention arithmetic intensity, MoE sparsity, and AFD are critical to cost-effectiveness. We perform a head-to-head comparison with DeepSeek-V3 in its favorable scenarios. Our implementation on Hopper GPUs achieves a decoding throughput of up to 4,039 tokens per second per GPU under 50ms TPOT SLA (4K context, FP8, no MTP). It is higher than DeepSeek-V3s 2,324 in the same setup and sets a new Pareto frontier for LLM decoding.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2507.19425v1" target="_blank">Machine Learning Based Efficiency Calculator (MaLBEC) for Nuclear Fusion Diagnostics</a></h3>
                    <p><strong>Authors:</strong> Kimberley Lennon, Chantal Shand, Gemma Wilson, Robin Smith</p>
                    <p><strong>Published:</strong> 7/25/2025</p>
                    <p><strong>Categories:</strong> physics.ins-det, nucl-ex</p>
                    <p><strong>Summary:</strong> Diagnostics are critical for commercial and research fusion machines, since measuring and understanding plasma features is important to sustaining fusion reactions. The neutron flux (and therefore fusion power) can be indirectly calculated using neutron activation analyses, where potentially large numbers of activation foils are placed in the neutron flux, and delayed gammas from key reactions are measured via gamma spectrometry. In gamma spectrometry, absolute efficiency forms part of the activity calculation, and equals to the ratio of the total number of photons detected to the number emitted by a radioactive sample. Hence, it is imperative that they are calculated efficiently and accurately. This paper presents a novel digital efficiency calculation algorithm, the Machine Learning Based Efficiency Calculator (MaLBEC), that uses state-of-the-art supervised machine learning techniques to calculate efficiency values of a given sample, from only four inputs. In this paper, the performance of the MaLBEC is demonstrated with a fusion sample and compares the values to a traditional efficiency calculation method, Monte Carlo N-Particle (MCNP). The efficiencies from the MaLBEC were within an average 5\% of the ones produced by MCNP, but with an exceptional reduction in computation time of 99.96\%. When the efficiency values from both methods were used in the activity calculation, the MaLBEC was within 3\% of the MCNP results.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2507.19419v1" target="_blank">TokenSmith: Streamlining Data Editing, Search, and Inspection for Large-Scale Language Model Training and Interpretability</a></h3>
                    <p><strong>Authors:</strong> Mohammad Aflah Khan, Ameya Godbole, Johnny Tian-Zheng Wei, Ryan Wang, James Flemings, Krishna Gummadi, Willie Neiswanger, Robin Jia</p>
                    <p><strong>Published:</strong> 7/25/2025</p>
                    <p><strong>Categories:</strong> cs.CL</p>
                    <p><strong>Summary:</strong> Understanding the relationship between training data and model behavior during pretraining is crucial, but existing workflows make this process cumbersome, fragmented, and often inaccessible to researchers. We present TokenSmith, an open-source library for interactive editing, inspection, and analysis of datasets used in Megatron-style pretraining frameworks such as GPT-NeoX, Megatron, and NVIDIA NeMo. TokenSmith supports a wide range of operations including searching, viewing, ingesting, exporting, inspecting, and sampling data, all accessible through a simple user interface and a modular backend. It also enables structured editing of pretraining data without requiring changes to training code, simplifying dataset debugging, validation, and experimentation. TokenSmith is designed as a plug and play addition to existing large language model pretraining workflows, thereby democratizing access to production-grade dataset tooling. TokenSmith is hosted on GitHub1, with accompanying documentation and tutorials. A demonstration video is also available on YouTube.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2507.19416v1" target="_blank">Dual Mechanisms for Heterogeneous Responses of Inspiratory Neurons to Noradrenergic Modulation</a></h3>
                    <p><strong>Authors:</strong> Sreshta Venkatakrishnan, Andrew K. Tryba, Alfredo J. Garcia 3rd, Yangyang Wang</p>
                    <p><strong>Published:</strong> 7/25/2025</p>
                    <p><strong>Categories:</strong> q-bio.NC, math.DS, 37N25, 34C23, 34C60, 34E13, 34E15, 92C20</p>
                    <p><strong>Summary:</strong> Respiration is an essential involuntary function necessary for survival. This poses a challenge for the control of breathing. The preB\otzinger complex (preB\otC) is a heterogeneous neuronal network responsible for driving the inspiratory rhythm. While neuromodulators such as norepinephrine (NE) allow it to be both robust and flexible for all living beings to interact with their environment, the basis for how neuromodulation impacts neuron-specific properties remains poorly understood. In this work, we examine how NE influences different preB\otC neuronal subtypes by modeling its effects through modulating two key parameters: calcium-activated nonspecific cationic current gating conductance ($g_{\rm CAN}$) and inositol-triphosphate ($\rm IP_3$), guided by experimental studies. Our computational model captures the experimentally observed differential effects of NE on distinct preB\otC bursting patterns. We show that this dual mechanism is critical for inducing conditional bursting and identify specific parameter regimes where silent neurons remain inactive in the presence of NE. Furthermore, using methods of dynamical systems theory, we uncover the mechanisms by which NE differentially modulates burst frequency and duration in NaP-dependent and CAN-dependent bursting neurons. These results align well with previously reported experimental findings and provide a deeper understanding of cell-specific neuromodulatory responses within the respiratory network.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2507.19402v1" target="_blank">FD4QC: Application of Classical and Quantum-Hybrid Machine Learning for Financial Fraud Detection A Technical Report</a></h3>
                    <p><strong>Authors:</strong> Matteo Cardaioli, Luca Marangoni, Giada Martini, Francesco Mazzolin, Luca Pajola, Andrea Ferretto Parodi, Alessandra Saitta, Maria Chiara Vernillo</p>
                    <p><strong>Published:</strong> 7/25/2025</p>
                    <p><strong>Categories:</strong> cs.LG, cs.CE</p>
                    <p><strong>Summary:</strong> The increasing complexity and volume of financial transactions pose significant challenges to traditional fraud detection systems. This technical report investigates and compares the efficacy of classical, quantum, and quantum-hybrid machine learning models for the binary classification of fraudulent financial activities. As of our methodology, first, we develop a comprehensive behavioural feature engineering framework to transform raw transactional data into a rich, descriptive feature set. Second, we implement and evaluate a range of models on the IBM Anti-Money Laundering (AML) dataset. The classical baseline models include Logistic Regression, Decision Tree, Random Forest, and XGBoost. These are compared against three hybrid classic quantum algorithms architectures: a Quantum Support Vector Machine (QSVM), a Variational Quantum Classifier (VQC), and a Hybrid Quantum Neural Network (HQNN). Furthermore, we propose Fraud Detection for Quantum Computing (FD4QC), a practical, API-driven system architecture designed for real-world deployment, featuring a classical-first, quantum-enhanced philosophy with robust fallback mechanisms. Our results demonstrate that classical tree-based models, particularly \textit{Random Forest}, significantly outperform the quantum counterparts in the current setup, achieving high accuracy (\(97.34\%\)) and F-measure (\(86.95\%\)). Among the quantum models, \textbf{QSVM} shows the most promise, delivering high precision (\(77.15\%\)) and a low false-positive rate (\(1.36\%\)), albeit with lower recall and significant computational overhead. This report provides a benchmark for a real-world financial application, highlights the current limitations of quantum machine learning in this domain, and outlines promising directions for future research.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2507.19399v1" target="_blank">Running in CIRCLE? A Simple Benchmark for LLM Code Interpreter Security</a></h3>
                    <p><strong>Authors:</strong> Gabriel Chua</p>
                    <p><strong>Published:</strong> 7/25/2025</p>
                    <p><strong>Categories:</strong> cs.CR, cs.AI</p>
                    <p><strong>Summary:</strong> As large language models (LLMs) increasingly integrate native code interpreters, they enable powerful real-time execution capabilities, substantially expanding their utility. However, such integrations introduce potential system-level cybersecurity threats, fundamentally different from prompt-based vulnerabilities. To systematically evaluate these interpreter-specific risks, we propose CIRCLE (Code-Interpreter Resilience Check for LLM Exploits), a simple benchmark comprising 1,260 prompts targeting CPU, memory, and disk resource exhaustion. Each risk category includes explicitly malicious (direct) and plausibly benign (indirect) prompt variants. Our automated evaluation framework assesses not only whether LLMs refuse or generates risky code, but also executes the generated code within the interpreter environment to evaluate code correctness, simplifications made by the LLM to make the code safe, or execution timeouts. Evaluating 7 commercially available models from OpenAI and Google, we uncover significant and inconsistent vulnerabilities. For instance, evaluations show substantial disparities even within providers - OpenAIs o4-mini correctly refuses risky requests at 7.1%, notably higher rates compared to GPT-4.1 at 0.5%. Results particularly underscore that indirect, socially-engineered prompts substantially weaken model defenses. This highlights an urgent need for interpreter-specific cybersecurity benchmarks, dedicated mitigation tools (e.g., guardrails), and clear industry standards to guide safe and responsible deployment of LLM interpreter integrations. The benchmark dataset and evaluation code are publicly released to foster further research.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2507.19398v1" target="_blank">CXR-CML: Improved zero-shot classification of long-tailed multi-label diseases in Chest X-Rays</a></h3>
                    <p><strong>Authors:</strong> Rajesh Madhipati, Sheethal Bhat, Lukas Buess, Andreas Maier</p>
                    <p><strong>Published:</strong> 7/25/2025</p>
                    <p><strong>Categories:</strong> cs.CV, cs.AI</p>
                    <p><strong>Summary:</strong> Chest radiography (CXR) plays a crucial role in the diagnosis of various diseases. However, the inherent class imbalance in the distribution of clinical findings presents a significant challenge for current self-supervised deep learning models. These models often fail to accurately classify long-tailed classes. Current Vision-Language models such as Contrastive Language Image Pre-training (CLIP) models effectively model the manifold distribution of the latent space, enabling high zero-shot classification accuracies. Although CLIP performs well on most of the primary classes in the dataset, our work reveals that its effectiveness decreases significantly for classes with a long-tailed distribution. Our approach employs a class-weighting mechanism that directly aligns with the distribution of classes within the latent space. This method ensures a substantial improvement in overall classification performance, with particular emphasis on enhancing the recognition and accuracy of rarely observed classes. We accomplish this by applying Gaussian Mixture Model (GMM) clustering to the latent space. The subsequent clusters are further refined by Student t-distribution, followed by a metric loss that utilizes the altered embeddings. Our approach facilitates stable and adaptive clustering of the features. This results in a notable average improvement of 7\% points in zero-shot AUC scores across 40 classes in the MIMIC-CXR-JPG dataset from previous SOTA models.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2507.19395v1" target="_blank">Paused in translation: A model for the transcript length-dependent impact of ribosome-targeting antibiotics</a></h3>
                    <p><strong>Authors:</strong> Johannes Keisers, Norbert Kern, Luca Ciandrini</p>
                    <p><strong>Published:</strong> 7/25/2025</p>
                    <p><strong>Categories:</strong> q-bio.SC, cond-mat.stat-mech, physics.bio-ph</p>
                    <p><strong>Summary:</strong> Ribosome-targeting antibiotics, such as chloramphenicol, stall elongating ribosomes during protein synthesis, disrupting mRNA translation. These antibiotic-induced pauses occur stochastically, alter collective ribosome dynamics and transiently block protein production on the affected transcript. Existing models of ribosome traffic often rely on idealized assumptions, such as infinitely long mRNAs and simplified pausing dynamics, overlooking key biological constraints. Here, we develop a Totally Asymmetric Simple Exclusion Process (TASEP) that incorporates stochastic particle pausing, using experimentally determined pausing and unpausing rates to model the effects of ribosome-targeting antibiotics. We introduce a Single-Cluster approximation, which is analytically treatable, tailored to capture the biologically relevant regime of rare and long antibiotic-induced pauses. This biologically constrained model reveals three key insights: (i) the inhibition of antibiotic-induced translation strongly depends on transcript length, with longer transcripts being disproportionately affected; (ii) reducing ribosome initiation rates significantly mitigates antibiotic vulnerability; and (iii) inhibition of translation is governed more by collective ribosome dynamics than by single-ribosome properties. Our analytical predictions match Gillespie simulations, align quantitatively with experimental observations, and yield testable hypotheses for future experiments. These findings may have broader implications for the mechanistic modeling of other biological transport processes (e.g., RNAP dynamics), and more generally for the community studying traffic models.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2507.19390v1" target="_blank">ReCatcher: Towards LLMs Regression Testing for Code Generation</a></h3>
                    <p><strong>Authors:</strong> Altaf Allah Abbassi, Leuson Da Silva, Amin Nikanjam, Foutse Khomh</p>
                    <p><strong>Published:</strong> 7/25/2025</p>
                    <p><strong>Categories:</strong> cs.SE, cs.AI</p>
                    <p><strong>Summary:</strong> Large Language Models (LLMs) for code generation evolve rapidly through fine-tuning, merging, or new model releases. However, such updates can introduce regressions, not only in correctness but also in code quality and performance. To address this, we present ReCatcher, a regression testing framework for Python code generation. ReCatcher systematically compares two LLMs, typically a current model and a candidate update, across three dimensions: logical correctness, static code quality, and execution performance. We apply ReCatcher to assess regressions across three update scenarios, fine-tuning, merging, and model release, using CodeLlama, DeepSeek-Coder, and GPT-4o. Our evaluation shows that fine-tuning with cross-language datasets increases syntax errors by up to 12%. Merging with general-purpose models like Llama2 leads to regressions in correctness by up to 18%. GPT-4o introduces regressions of up to 50% in handling missing imports compared to GPT-3.5-turbo, while GPT-4o-mini suffers up to 80% performance degradation in execution time versus GPT-4o. Overall, logical correctness, performance, and error handling (e.g., syntax errors and missing imports) are the most regression-prone areas. Comparing ReCatcher with baseline solutions, it presents better and consistent accuracy across logical and performance aspects. ReCatcher highlights the importance of systematic regression evaluation before adopting new models, while assisting researchers and practitioners in making more informed update decisions.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2507.19384v1" target="_blank">On Anti-collusion Codes for Averaging Attack in Multimedia Fingerprinting</a></h3>
                    <p><strong>Authors:</strong> Jing Jiang, Cailin Wen, Minquan Cheng</p>
                    <p><strong>Published:</strong> 7/25/2025</p>
                    <p><strong>Categories:</strong> cs.IT, math.IT</p>
                    <p><strong>Summary:</strong> Multimedia fingerprinting is a technique to protect the copyrighted contents against being illegally redistributed under various collusion attack models. Averaging attack is the most fair choice for each colluder to avoid detection, and also makes the pirate copy have better perceptional quality. This makes such an attack one of the most feasible approaches to carrying out collusion. In order to trace all the colluders, several types of multimedia fingerprinting codes were introduced to construct fingerprints resistant to averaging attacks on multimedia contents, such as AND anti-collusion codes (AND-ACCs), binary separable codes (SCs), logical anti-collusion codes (LACCs), binary frameproof codes (FPCs), binary strongly-separable codes (SSCs) and binary secure code with list decoding (SCLDs). Then codes with the rate as high as possible are desired. However, the existing fingerprinting codes have low code rate due to the strong combinatorial structure. The reason is that the previous research methods adopted simple tracing algorithms. In this paper, we first propose novel tracing algorithms and then find appropriate fingerprinting codes with weaker combinatorial structure, i.e., the binary strongly identifiable parent property code for multimedia fingerprinting (SMIPPC) and its concatenated code. Theoretical comparisons and numerical comparisons show that SMIPPCs have higher code rates than those of the existing codes due to their weaker combinatorial structures. It is worth noting that SMIPPCs can only trace a part of colluders by using the previous tracing algorithm and the concatenated SMIPPC may be not an SMIPPC. This implies that our tracing algorithms have strong traceability.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2507.19383v1" target="_blank">Quantum Algorithm for Protein Side-Chain Optimisation: Comparing Quantum to Classical Methods</a></h3>
                    <p><strong>Authors:</strong> Anastasia Agathangelou, Dilhan Manawadu, Ivano Tavernelli</p>
                    <p><strong>Published:</strong> 7/25/2025</p>
                    <p><strong>Categories:</strong> quant-ph</p>
                    <p><strong>Summary:</strong> Modelling and predicting protein configurations is crucial for advancing drug discovery, enabling the design of treatments for life-threatening diseases. A critical aspect of this challenge is rotamer optimisation - the determination of optimal side-chain conformations given a fixed protein backbone. This problem, involving the internal degrees of freedom of amino acid side-chains, significantly influences the proteins overall structure and function. In this work, we develop a resource-efficient optimisation algorithm to compute the ground state energy of protein structures, with a focus on side-chain configuration. We formulate the rotamer optimisation problem as a Quadratic Unconstrained Binary Optimisation problem and map it to an Ising model, enabling efficient quantum encoding. Building on this formulation, we propose a quantum algorithm based on the Quantum Approximate Optimisation Algorithm to explore the conformational space and identify low-energy configurations. To benchmark our approach, we conduct a classical study using custom-built libraries tailored for structural characterisation and energy optimisation. Our quantum method demonstrates a reduction in computational cost compared to classical simulated annealing techniques, offering a scalable and promising framework for protein structure optimisation in the quantum era.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2507.19376v1" target="_blank">Archiverse: an Approach for Immersive Cultural Heritage</a></h3>
                    <p><strong>Authors:</strong> Wieslaw Kopeƒá, Anna Jaskulska, W≈Çadys≈Çaw Fuchs, Wiktor Stawski, Stanis≈Çaw Knapi≈Ñski, Barbara Karpowicz, Rafa≈Ç Mas≈Çyk</p>
                    <p><strong>Published:</strong> 7/25/2025</p>
                    <p><strong>Categories:</strong> cs.HC, cs.CY</p>
                    <p><strong>Summary:</strong> Digital technologies and tools have transformed the way we can study cultural heritage and the way we can recreate it digitally. Techniques such as laser scanning, photogrammetry, and a variety of Mixed Reality solutions have enabled researchers to examine cultural objects and artifacts more precisely and from new perspectives. In this part of the panel, we explore how Virtual Reality (VR) and eXtended Reality (XR) can serve as tools to recreate and visualize the remains of historical cultural heritage and experience it in simulations of its original complexity, which means immersive and interactive. Visualization of material culture exemplified by archaeological sites and architecture can be particularly useful when only ruins or archaeological remains survive. However, these advancements also bring significant challenges, especially in the area of transdisciplinary cooperation between specialists from many, often distant, fields, and the dissemination of virtual immersive environments among both professionals and the general public.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2507.19375v1" target="_blank">Latent-X: An Atom-level Frontier Model for De Novo Protein Binder Design</a></h3>
                    <p><strong>Authors:</strong> Latent Labs Team, Alex Bridgland, Jonathan Crabb√©, Henry Kenlay, Daniella Pretorius, Sebastian M. Schmon, Agrin Hilmkil, Rebecca Bartke-Croughan, Robin Rombach, Michael Flashman, Tomas Matteson, Simon Mathis, Alexander W. R. Nelson, David Yuan, Annette Obika, Simon A. A. Kohl</p>
                    <p><strong>Published:</strong> 7/25/2025</p>
                    <p><strong>Categories:</strong> q-bio.BM</p>
                    <p><strong>Summary:</strong> Traditional drug discovery relies on rounds of screening millions of candidate molecules with low success rates, making drug discovery time and resource intensive. To overcome this screening bottleneck, we introduce Latent-X, an all-atom protein design model that enables a new paradigm of precision AI design. Given a target protein epitope, Latent-X jointly generates the all atom structure and sequence of the protein binder and target, directly modelling the non-covalent interactions essential for specific binding. We demonstrate its efficacy across two therapeutically relevant modalities through extensive wet lab experiments, testing as few as 30-100 designs per target. For macrocyclic peptides, Latent-X achieves experimental hit rates exceeding 90% on all evaluated benchmark targets. For mini-binders, it consistently produces potent candidates against all evaluated benchmark targets, with binding affinities reaching the low nanomolar and picomolar range - comparable to those of approved therapeutics - whilst also being highly specific in mammalian display. In direct comparisons with the state-of-the-art models AlphaProteo, RFdiffusion and RFpeptides under identical conditions demonstrates, Latent-X generates binders with higher hit rates and better binding affinities, and uniquely creates structurally diverse binders, including complex beta-sheet folds. Its end-to-end process is an order of magnitude faster than existing multi-step computational pipelines. By drastically improving the efficiency and success rate of de novo design, Latent-X represents a significant advance towards push-button biologics discovery and a valuable tool for protein engineers. Latent-X is available at https://platform.latentlabs.com, enabling users to reliably generate de novo binders without AI infrastructure or coding.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2507.19371v1" target="_blank">Atomically clean free-standing two-dimensional materials through heating in ultra-high vacuum</a></h3>
                    <p><strong>Authors:</strong> Philipp Irschik, David Lamprecht, Shrirang Chokappa, Clemens Mangler, Carsten Speckmann, Thuy An Bui, Manuel L√§ngle, Lado Filipovic, Jani Kotakoski</p>
                    <p><strong>Published:</strong> 7/25/2025</p>
                    <p><strong>Categories:</strong> cond-mat.mtrl-sci</p>
                    <p><strong>Summary:</strong> Surface contamination not only influences but in some cases even dominates the measured properties of two-dimensional materials. Although different cleaning methods are often used for contamination removal, commonly used spectroscopic cleanliness assessment methods can leave the level of achieved cleanliness ambiguous. Despite two decades of research on 2D materials, the true cleanliness of the used samples is often left open to interpretation. In this work, free-standing monolayer graphene and hexagonal boron nitride are annealed at different temperatures in a custom-built ultra-high vacuum heating chamber, connected to a scanning transmission electron microscope via a vacuum transfer line, enabling atomically resolved cleanliness characterization as a function of annealing temperature, while eliminating the introduction of airborne contamination during sample transport. While annealing at 200 {\deg}C already reduces contamination significantly, it is not until 400 {\deg}C or higher, where over 90% of the free-standing monolayer areas are atomically clean. At this point, further contamination removal is mainly limited by defects in the material and metal contamination introduced during the sample transfer or growth. The achieved large, atomically clean areas can then be used for further nanoscale engineering steps or device processing, facilitating interaction with the material rather than contamination.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2507.19370v1" target="_blank">BEV-LLM: Leveraging Multimodal BEV Maps for Scene Captioning in Autonomous Driving</a></h3>
                    <p><strong>Authors:</strong> Felix Brandstaetter, Erik Schuetz, Katharina Winter, Fabian Flohr</p>
                    <p><strong>Published:</strong> 7/25/2025</p>
                    <p><strong>Categories:</strong> cs.CV</p>
                    <p><strong>Summary:</strong> Autonomous driving technology has the potential to transform transportation, but its wide adoption depends on the development of interpretable and transparent decision-making systems. Scene captioning, which generates natural language descriptions of the driving environment, plays a crucial role in enhancing transparency, safety, and human-AI interaction. We introduce BEV-LLM, a lightweight model for 3D captioning of autonomous driving scenes. BEV-LLM leverages BEVFusion to combine 3D LiDAR point clouds and multi-view images, incorporating a novel absolute positional encoding for view-specific scene descriptions. Despite using a small 1B parameter base model, BEV-LLM achieves competitive performance on the nuCaption dataset, surpassing state-of-the-art by up to 5\% in BLEU scores. Additionally, we release two new datasets - nuView (focused on environmental conditions and viewpoints) and GroundView (focused on object grounding) - to better assess scene captioning across diverse driving scenarios and address gaps in current benchmarks, along with initial benchmarking results demonstrating their effectiveness.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2507.19368v1" target="_blank">Counterfactual Explanations in Medical Imaging: Exploring SPN-Guided Latent Space Manipulation</a></h3>
                    <p><strong>Authors:</strong> Julia Siekiera, Stefan Kramer</p>
                    <p><strong>Published:</strong> 7/25/2025</p>
                    <p><strong>Categories:</strong> cs.LG, cs.AI</p>
                    <p><strong>Summary:</strong> Artificial intelligence is increasingly leveraged across various domains to automate decision-making processes that significantly impact human lives. In medical image analysis, deep learning models have demonstrated remarkable performance. However, their inherent complexity makes them black box systems, raising concerns about reliability and interpretability. Counterfactual explanations provide comprehensible insights into decision processes by presenting hypothetical what-if scenarios that alter model classifications. By examining input alterations, counterfactual explanations provide patterns that influence the decision-making process. Despite their potential, generating plausible counterfactuals that adhere to similarity constraints providing human-interpretable explanations remains a challenge. In this paper, we investigate this challenge by a model-specific optimization approach. While deep generative models such as variational autoencoders (VAEs) exhibit significant generative power, probabilistic models like sum-product networks (SPNs) efficiently represent complex joint probability distributions. By modeling the likelihood of a semi-supervised VAEs latent space with an SPN, we leverage its dual role as both a latent space descriptor and a classifier for a given discrimination task. This formulation enables the optimization of latent space counterfactuals that are both close to the original data distribution and aligned with the target class distribution. We conduct experimental evaluation on the cheXpert dataset. To evaluate the effectiveness of the integration of SPNs, our SPN-guided latent space manipulation is compared against a neural network baseline. Additionally, the trade-off between latent variable regularization and counterfactual quality is analyzed.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2507.19362v1" target="_blank">LOTUS: A Leaderboard for Detailed Image Captioning from Quality to Societal Bias and User Preferences</a></h3>
                    <p><strong>Authors:</strong> Yusuke Hirota, Boyi Li, Ryo Hachiuma, Yueh-Hua Wu, Boris Ivanovic, Yuta Nakashima, Marco Pavone, Yejin Choi, Yu-Chiang Frank Wang, Chao-Han Huck Yang</p>
                    <p><strong>Published:</strong> 7/25/2025</p>
                    <p><strong>Categories:</strong> cs.CV, cs.AI, cs.CL, cs.CY, cs.LG</p>
                    <p><strong>Summary:</strong> Large Vision-Language Models (LVLMs) have transformed image captioning, shifting from concise captions to detailed descriptions. We introduce LOTUS, a leaderboard for evaluating detailed captions, addressing three main gaps in existing evaluations: lack of standardized criteria, bias-aware assessments, and user preference considerations. LOTUS comprehensively evaluates various aspects, including caption quality (e.g., alignment, descriptiveness), risks (\eg, hallucination), and societal biases (e.g., gender bias) while enabling preference-oriented evaluations by tailoring criteria to diverse user preferences. Our analysis of recent LVLMs reveals no single model excels across all criteria, while correlations emerge between caption detail and bias risks. Preference-oriented evaluations demonstrate that optimal model selection depends on user priorities.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2507.19359v1" target="_blank">SemGes: Semantics-aware Co-Speech Gesture Generation using Semantic Coherence and Relevance Learning</a></h3>
                    <p><strong>Authors:</strong> Lanmiao Liu, Esam Ghaleb, Aslƒ± √ñzy√ºrek, Zerrin Yumak</p>
                    <p><strong>Published:</strong> 7/25/2025</p>
                    <p><strong>Categories:</strong> cs.CV</p>
                    <p><strong>Summary:</strong> Creating a virtual avatar with semantically coherent gestures that are aligned with speech is a challenging task. Existing gesture generation research mainly focused on generating rhythmic beat gestures, neglecting the semantic context of the gestures. In this paper, we propose a novel approach for semantic grounding in co-speech gesture generation that integrates semantic information at both fine-grained and global levels. Our approach starts with learning the motion prior through a vector-quantized variational autoencoder. Built on this model, a second-stage module is applied to automatically generate gestures from speech, text-based semantics and speaker identity that ensures consistency between the semantic relevance of generated gestures and co-occurring speech semantics through semantic coherence and relevance modules. Experimental results demonstrate that our approach enhances the realism and coherence of semantic gestures. Extensive experiments and user studies show that our method outperforms state-of-the-art approaches across two benchmarks in co-speech gesture generation in both objective and subjective metrics. The qualitative results of our model, code, dataset and pre-trained models can be viewed at https://semgesture.github.io/.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2507.19356v1" target="_blank">Enhancing Speech Emotion Recognition Leveraging Aligning Timestamps of ASR Transcripts and Speaker Diarization</a></h3>
                    <p><strong>Authors:</strong> Hsuan-Yu Wang, Pei-Ying Lee, Berlin Chen</p>
                    <p><strong>Published:</strong> 7/25/2025</p>
                    <p><strong>Categories:</strong> cs.CL, I.2.7; I.5.1</p>
                    <p><strong>Summary:</strong> In this paper, we investigate the impact of incorporating timestamp-based alignment between Automatic Speech Recognition (ASR) transcripts and Speaker Diarization (SD) outputs on Speech Emotion Recognition (SER) accuracy. Misalignment between these two modalities often reduces the reliability of multimodal emotion recognition systems, particularly in conversational contexts. To address this issue, we introduce an alignment pipeline utilizing pre-trained ASR and speaker diarization models, systematically synchronizing timestamps to generate accurately labeled speaker segments. Our multimodal approach combines textual embeddings extracted via RoBERTa with audio embeddings from Wav2Vec, leveraging cross-attention fusion enhanced by a gating mechanism. Experimental evaluations on the IEMOCAP benchmark dataset demonstrate that precise timestamp alignment improves SER accuracy, outperforming baseline methods that lack synchronization. The results highlight the critical importance of temporal alignment, demonstrating its effectiveness in enhancing overall emotion recognition accuracy and providing a foundation for robust multimodal emotion analysis.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2507.19353v1" target="_blank">Smooth Reading: Bridging the Gap of Recurrent LLM to Self-Attention LLM on Long-Context Tasks</a></h3>
                    <p><strong>Authors:</strong> Kai Liu, Zhan Su, Peijie Dong, Fengran Mo, Jianfei Gao, ShaoTing Zhang, Kai Chen</p>
                    <p><strong>Published:</strong> 7/25/2025</p>
                    <p><strong>Categories:</strong> cs.CL, cs.AI</p>
                    <p><strong>Summary:</strong> Recently, recurrent large language models (Recurrent LLMs) with linear computational complexity have re-emerged as efficient alternatives to self-attention-based LLMs (Self-Attention LLMs), which have quadratic complexity. However, Recurrent LLMs often underperform on long-context tasks due to their limited fixed-size memory. Previous research has primarily focused on enhancing the memory capacity of Recurrent LLMs through architectural innovations, but these approaches have not yet enabled Recurrent LLMs to match the performance of Self-Attention LLMs on long-context tasks. We argue that this limitation arises because processing the entire context at once is not well-suited for Recurrent LLMs. In this paper, we propose Smooth Reading, a chunk-wise inference method inspired by human reading strategies. Smooth Reading processes context in chunks and iteratively summarizes the contextual information, thereby reducing memory demands and making the approach more compatible with Recurrent LLMs. Our experimental results show that this method substantially narrows the performance gap between Recurrent and Self-Attention LLMs on long-context tasks, while preserving the efficiency advantages of Recurrent LLMs. Our Smooth Reading boosts SWA-3B-4k (a Recurrent LLM) from 5.68% lower to 3.61% higher performance than Self-Attention LLMs on LongBench. Besides, our method maintains the high efficiency, training 3x faster and inferring 2x faster at 64k context compared to Self-Attention LLMs. To our knowledge, this is the first work to achieve comparable performance using Recurrent LLMs compared with Self-Attention LLMs on long-context tasks. We hope our method will inspire future research in this area. To facilitate further progress, we will release code and dataset.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2507.19342v1" target="_blank">A Truncated Primordial Power Spectrum and its Impact on CMB Polarization</a></h3>
                    <p><strong>Authors:</strong> Jingwei Liu, Fulvio Melia</p>
                    <p><strong>Published:</strong> 7/25/2025</p>
                    <p><strong>Categories:</strong> astro-ph.CO</p>
                    <p><strong>Summary:</strong> We investigate the impact of a hypothesized delayed initiation of inflation, characterized by a cutoff k_min to the primordial power spectrum in the cosmic microwave background (CMB). This cutoff affects both the scalar and tensor spectra, which therefore impacts several measurements of the temperature and polarization distributions. We calculate the angular power spectrum and correlation function with and without k_min in the context of Planck-LCDM, and demonstrate that a non-zero k_min significantly improves the alignment between theory and the observations, including the temperature, E-mode polarization, TE cross-correlation, Q+U polarization and Q-U polarization. It creates an observable signature in both the angular power spectrum and correlation function for all cases. We thus also explore the B-mode polarization, for which current data are not yet precise enough to determine k_min, but whose impact should be detectable with high-precision measurements using future missions, such as LiteBIRD, if the tensor-to-scalar ratio, r, is not much smaller than its current upper limit. We find that the introduction of k_min not only addresses large-angle anomalies in the CMB but also provides a more consistent framework for understanding the early Universes inflationary phase. These findings highlight the importance of future high-precision CMB observations in validating the existence and implications of k_min.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2507.19338v1" target="_blank">Branch-and-bound method for calculating Viterbi path in triplet Markov models</a></h3>
                    <p><strong>Authors:</strong> Oskar Soop, J√ºri Lember</p>
                    <p><strong>Published:</strong> 7/25/2025</p>
                    <p><strong>Categories:</strong> stat.CO, cs.IT, math.IT</p>
                    <p><strong>Summary:</strong> We consider a bivariate, possibly non-homogeneous, finite-state Markov chain $(X,U)=\{(X_t,U_t)\}_{t=1}^n$. We are interested in the marginal process $X$, which typically is not a Markov chain. The goal is to find a realization (path) $x=(x_1,\ldots,x_n)$ with maximal probability $P(X=x)$. If $X$ is Markov chain, then such path can be efficiently found using the celebrated Viterbi algorithm. However, when $X$ is not Markovian, identifying the most probable path -- hereafter referred to as the Viterbi path -- becomes computationally expensive. In this paper, we explore the branch-and-bound method for finding Viterbi paths. The method is based on the lower and upper bounds on maximum probability $\max_x P(X=x)$, and the objective of the paper is to exploit the joint Markov property of $(X,Y)$ to calculate possibly good bounds in possibly cheap way. This research is motivated by decoding or segmentation problem in triplet Markov models. A triplet Markov model is trivariate homogeneous Markov process $(X,U,Y)$. In decoding, a realization of one marginal process $Y$ is observed (representing the data), while $X$ and $U$ are latent processes. The process $U$ serves as a nuisance variable, whereas $X$ is the process of primary interest. Decoding refers to estimating the hidden sequence $X$ based solely on the observation $Y$. Conditional on $Y$, the latent processes $(X, U)$ form a non-homogeneous Markov chain. In this context, the Viterbi path corresponds to the maximum a posteriori (MAP) estimate of $X$, making it a natural choice for signal reconstruction.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2507.19327v1" target="_blank">Real-time rail vehicle localisation using spatially resolved magnetic field measurements</a></h3>
                    <p><strong>Authors:</strong> Niklas Dieckow, Katharina Ostaszewski, Philip Heinisch, Henriette Struckmann, Hendrik Ranocha</p>
                    <p><strong>Published:</strong> 7/25/2025</p>
                    <p><strong>Categories:</strong> eess.SP, cs.SY, eess.SY</p>
                    <p><strong>Summary:</strong> This work presents two complementary real-time rail vehicle localization methods based on magnetic field measurements and a pre-recorded magnetic map. The first uses a particle filter reweighted via magnetic similarity, employing a heavy-tailed non-Gaussian kernel for enhanced stability. The second is a stateless sequence alignment technique that transforms real-time magnetic signals into the spatial domain and matches them to the map using a similarity measure. Experiments with operational train data show that the particle filter achieves track-selective, sub-5-meter accuracy over 21.6 km, though its performance degrades at low speeds and during cold starts. Accuracy tests were constrained by the GNSS-based reference system. In contrast, the alignment-based method excels in cold-start scenarios, localizing within 30 m in 92 % of tests (100 % using top-3 matches). A hybrid approach combines both methods$\unicode{x2014}$alignment-based initialization followed by particle filter tracking. Runtime analysis confirms real-time capability on consumer-grade hardware. The system delivers accurate, robust localization suitable for safety-critical rail applications.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2507.19316v1" target="_blank">Human-AI Synergy in Adaptive Active Learning for Continuous Lithium Carbonate Crystallization Optimization</a></h3>
                    <p><strong>Authors:</strong> Shayan S. Mousavi Masouleh, Corey A. Sanz, Ryan P. Jansonius, Cara Cronin, Jason E. Hein, Jason Hattrick-Simpers</p>
                    <p><strong>Published:</strong> 7/25/2025</p>
                    <p><strong>Categories:</strong> cond-mat.mtrl-sci, cond-mat.other, cs.HC, cs.LG, physics.data-an</p>
                    <p><strong>Summary:</strong> As demand for high-purity lithium surges with the growth of the electric vehicle (EV) industry, cost-effective extraction from lower-grade North American sources like the Smackover Formation is critical. These resources, unlike high-purity South American brines, require innovative purification techniques to be economically viable. Continuous crystallization is a promising method for producing battery-grade lithium carbonate, but its optimization is challenged by a complex parameter space and limited data. This study introduces a Human-in-the-Loop (HITL) assisted active learning framework to optimize the continuous crystallization of lithium carbonate. By integrating human expertise with data-driven insights, our approach accelerates the optimization of lithium extraction from challenging sources. Our results demonstrate the frameworks ability to rapidly adapt to new data, significantly improving the processs tolerance to critical impurities like magnesium from the industry standard of a few hundred ppm to as high as 6000 ppm. This breakthrough makes the exploitation of low-grade, impurity-rich lithium resources feasible, potentially reducing the need for extensive pre-refinement processes. By leveraging artificial intelligence, we have refined operational parameters and demonstrated that lower-grade materials can be used without sacrificing product quality. This advancement is a significant step towards economically harnessing North Americas vast lithium reserves, such as those in the Smackover Formation, and enhancing the sustainability of the global lithium supply chain.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2507.19305v1" target="_blank">Demystifying AI in Criminal Justice</a></h3>
                    <p><strong>Authors:</strong> Richard Berk</p>
                    <p><strong>Published:</strong> 7/25/2025</p>
                    <p><strong>Categories:</strong> cs.CY, stat.AP</p>
                    <p><strong>Summary:</strong> There is widespread confusion among criminal justice practitioners and legal scholars about the use of artificial intelligence in criminal justice. This didactic review is written for readers with little or no background in statistics or computer science. It is not intended to replace more technical treatments. It is intended to supplement them and encourage readers to dig more deeply into topics that strike their fancy.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2507.19302v1" target="_blank">Understanding discrepancies in the coverage of OpenAlex: the case of China</a></h3>
                    <p><strong>Authors:</strong> Mengxue Zheng, Lili Miao, Yi Bu, Vincent Lariviere</p>
                    <p><strong>Published:</strong> 7/25/2025</p>
                    <p><strong>Categories:</strong> cs.DL</p>
                    <p><strong>Summary:</strong> Citation indexes play a crucial role for understanding how science is produced, disseminated, and used. However, these databases often face a critical trade-off: those offering extensive and high-quality coverage are typically proprietary, whereas publicly accessible datasets frequently exhibit fragmented coverage and inconsistent data quality. OpenAlex was developed to address this challenge, providing a freely available database with broad open coverage, with a particular emphasis on non-English speaking countries. Yet, few studies have assessed the quality of the OpenAlex dataset. This paper assesses the coverage, by OpenAlex, of Chinas papers, which shows an abnormal trend, and compares it with other countries that do not have English as their main language. Our analysis reveals that while OpenAlex increases the coverage of Chinas publications, primarily those disseminated by a national database, this coverage is incomplete and discontinuous when compared to other countries records in the database. We observe similar issues in other non-English-speaking countries, with coverage varying across regions. These findings indicate that although OpenAlex expands coverage of research outputs, continuity issues persist and disproportionately affect certain countries. We emphasize the need for researchers to use OpenAlex data cautiously, being mindful of its potential limitations in cross-national analyses.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2507.19298v1" target="_blank">Controlling Topological Defects in Polar Fluids via Reinforcement Learning</a></h3>
                    <p><strong>Authors:</strong> Abhinav Singh, Petros Koumoutsakos</p>
                    <p><strong>Published:</strong> 7/25/2025</p>
                    <p><strong>Categories:</strong> cond-mat.soft, cs.AI, cs.LG</p>
                    <p><strong>Summary:</strong> Topological defects in active polar fluids exhibit complex dynamics driven by internally generated stresses, reflecting the deep interplay between topology, flow, and non-equilibrium hydrodynamics. Feedback control offers a powerful means to guide such systems, enabling transitions between dynamic states. We investigated closed-loop steering of integer-charged defects in a confined active fluid by modulating the spatial profile of activity. Using a continuum hydrodynamic model, we show that localized control of active stress induces flow fields that can reposition and direct defects along prescribed trajectories by exploiting non-linear couplings in the system. A reinforcement learning framework is used to discover effective control strategies that produce robust defect transport across both trained and novel trajectories. The results highlight how AI agents can learn the underlying dynamics and spatially structure activity to manipulate topological excitations, offering insights into the controllability of active matter and the design of adaptive, self-organized materials.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2507.19295v1" target="_blank">On the Security of a Code-Based PIR Scheme</a></h3>
                    <p><strong>Authors:</strong> Svenja Lage, Hannes Bartz</p>
                    <p><strong>Published:</strong> 7/25/2025</p>
                    <p><strong>Categories:</strong> cs.CR, cs.IR</p>
                    <p><strong>Summary:</strong> Private Information Retrieval (PIR) schemes allow clients to retrieve files from a database without disclosing the requested files identity to the server. In the pursuit of post-quantum security, most recent PIR schemes rely on hard lattice problems. In contrast, the so called CB-cPIR scheme stands out as a pioneering effort to base PIR schemes on hard problems in coding theory, thereby contributing significantly to the diversification of security foundations. However, our research reveals a critical vulnerability in CB-cPIR, substantially diminishing its security levels. Moreover, a comparative analysis with state-of-the-art PIR schemes shows that CB-cPIRs advantages are reduced, making it less competitive in terms of the communication cost. Nevertheless, our findings highlight the importance of continued research into code-based PIR schemes, as they have the potential to provide a valuable alternative to lattice-based approaches.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2507.19287v1" target="_blank">The Case for Time-Shared Computing Resources</a></h3>
                    <p><strong>Authors:</strong> Pierre Jacquet, Adrien Luxey-Bitri</p>
                    <p><strong>Published:</strong> 7/25/2025</p>
                    <p><strong>Categories:</strong> cs.DC</p>
                    <p><strong>Summary:</strong> The environmental impact of Information and Communication Technologies (ICT) continues to grow, driven notably by increasing usage, rebound effects, and emerging demands. However, despite the virtual nature of its services, the sector remains inherently constrained by its materiality and cannot rely on an infinite pool of resources. As a result, the wide variety of supported services may need to be managed under stricter limits within hosting facilities in the future. Contrary to common assumptions, we show that tenants typically do not share computing resources, even in environments commonly perceived as mutualized, such as cloud platforms. Time-sharing has been progressively phased out for reasons of performance, security, predictability, and, perhaps more importantly, due to the decreasing cost of computing resources. This paper advocates for managing fewer physical resources by improving resource sharing between tenants. It represents a paradigm shift, moving beyond traditional time-sharing at the hardware level to a higher abstraction. This approach entails doing with fewer resources under conditions of reduced performance. Nonetheless, enhancing the mutualization of infrastructure can reduce cluster sizes (through consolidation) and improve energy efficiency, with gains related to the accepted performance trade-off, a situation potentially more socially acceptable than eliminating services. We review the current state of the art, identify challenges and opportunities, propose interpretations of Time-Shared Computing, and outline key research directions.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2507.19284v1" target="_blank">Relaxed Total Generalized Variation Regularized Piecewise Smooth Mumford-Shah Model for Triangulated Surface Segmentation</a></h3>
                    <p><strong>Authors:</strong> Huayan Zhang, Shanqiang Wang, Xiaochao Wang</p>
                    <p><strong>Published:</strong> 7/25/2025</p>
                    <p><strong>Categories:</strong> cs.CG, cs.CV</p>
                    <p><strong>Summary:</strong> The Mumford-Shah (MS) model is an important technique for mesh segmentation. Many existing researches focus on piecewise constant MS mesh segmentation model with total variation regularization, which pursue the shortest length of boundaries. Different from previous efforts, in this article, we propose a novel piecewise smooth MS mesh segmentation model by utilizing the relaxed total generalized variation regularization (rTGV). The new model assumes that the feature function of a mesh can be approximated by the sum of piecewise constant function and asmooth function, and the rTGV regularization is able to characterize the high order discontinuity of the geometric structure. The newly introduced method is effective in segmenting meshes with irregular structures and getting the better boundaries rather than the shortest boundaries. We solve the new model by alternating minimization and alternating direction method of multipliers (ADMM). Our algorithm is discussed from several aspects, and comparisons with several state-of-art methods. Experimental results show that our method can yield competitive results when compared to other approaches. In addition, our results compare favorably to those of the several state-of-art techniques when evaluated on the Princeton Segmentation Benchmark. Furthermore, the quantitative errors and computational costs confirm the robustness and efficiency of the proposed method.</p>
                
            
                
                    <h3><a href="http://dx.doi.org/10.3847/1538-3881/adec9f" target="_blank">Magnetic field of a ring-like shape molecular cloud</a></h3>
                    <p><strong>Authors:</strong> Dana Alina, Adel Umirbayeva, Yasuo Doi, Soichiro Jo, Yue Hu, Alex Lazarian, Janik Karoly, Tie Liu, Koji S. Kawabata, Alua Mukhash, Danial Zhumagayir, Tomori Hori, Tetsuharu Maruta, Ryo Imazawa, Tatsuya Nakaoka, Mahito Sasada</p>
                    <p><strong>Published:</strong> 7/25/2025</p>
                    <p><strong>Categories:</strong> astro-ph.GA</p>
                    <p><strong>Summary:</strong> We present a detailed study of the magnetic field structure in the G111 molecular cloud, a ring-like filamentary cloud within the NGC 7538 region. We utilized interstellar dust polarization from the Planck telescope to trace large-scale field orientations, starlight extinction polarization from the Kanata telescope to probe the clouds magnetic field after foreground subtraction, and velocity gradients derived from CO isotopologues, observed with the IRAP 30m telescope, to examine dense regions. Our results reveal a coherent yet spatially varying magnetic field within G111. We correct the significant foreground dust contamination through careful subtraction. We observe a global alignment of the magnetic field with density structures suggesting that the field is dynamically important in shaping the cloud. The curved magnetic field along the dense regions, coinciding with mid-infrared emission in WISE data, indicates shock compression, likely driven by stellar feedback or supernova remnants. Our findings support a scenario where G111s morphology results from turbulent shock-driven compression. The interplay between magnetic fields and external forces is crucial in shaping and maintaining the structure of the molecular cloud. Future high-resolution observations will be essential to further constrain the magnetic fields role in cloud evolution.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2507.19272v1" target="_blank">Video Self-Distillation for Single-Image Encoders: A Step Toward Physically Plausible Perception</a></h3>
                    <p><strong>Authors:</strong> Marcel Simon, Tae-Ho Kim, Seul-Ki Yeom</p>
                    <p><strong>Published:</strong> 7/25/2025</p>
                    <p><strong>Categories:</strong> cs.CV</p>
                    <p><strong>Summary:</strong> Self-supervised image encoders such as DINO have recently gained significant interest for learning robust visual features without labels. However, most SSL methods train on static images and miss the temporal cues inherent in videos. We introduce a video-distilled single-image encoder trained to predict the next-frame representation from the current frame. This simple objective injects 3D spatial and temporal priors without optical flow or tracking. When pre-training on a single 2-hour video, our approach raises the mean Intersection-over-Union (mIoU) on ADE20K from 35.0 (DoRA) to 36.4 while remaining a drop-in replacement for image-only pipelines. Our results highlight video self-distillation as a lightweight route to geometry-aware perception an essential ingredient for physically plausible world models and Physical AI.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2507.19271v1" target="_blank">Fine-Tuning Multilingual Language Models for Code Review: An Empirical Study on Industrial C# Projects</a></h3>
                    <p><strong>Authors:</strong> Igli Begolli, Meltem Aksoy, Daniel Neider</p>
                    <p><strong>Published:</strong> 7/25/2025</p>
                    <p><strong>Categories:</strong> cs.SE, cs.AI, cs.PL</p>
                    <p><strong>Summary:</strong> Code review is essential for maintaining software quality but often time-consuming and cognitively demanding, especially in industrial environments. Recent advancements in language models (LMs) have opened new avenues for automating core review tasks. This study presents the empirical evaluation of monolingual fine-tuning on the performance of open-source LMs across three key automated code review tasks: Code Change Quality Estimation, Review Comment Generation, and Code Refinement. We fine-tuned three distinct models, CodeReviewer, CodeLlama-7B, and DeepSeek-R1-Distill, on a C\# specific dataset combining public benchmarks with industrial repositories. Our study investigates how different configurations of programming languages and natural languages in the training data affect LM performance, particularly in comment generation. Additionally, we benchmark the fine-tuned models against an automated software analysis tool (ASAT) and human reviewers to evaluate their practical utility in real-world settings. Our results show that monolingual fine-tuning improves model accuracy and relevance compared to multilingual baselines. While LMs can effectively support code review workflows, especially for routine or repetitive tasks, human reviewers remain superior in handling semantically complex or context-sensitive changes. Our findings highlight the importance of language alignment and task-specific adaptation in optimizing LMs for automated code review.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2507.19264v1" target="_blank">SimMLM: A Simple Framework for Multi-modal Learning with Missing Modality</a></h3>
                    <p><strong>Authors:</strong> Sijie Li, Chen Chen, Jungong Han</p>
                    <p><strong>Published:</strong> 7/25/2025</p>
                    <p><strong>Categories:</strong> cs.CV</p>
                    <p><strong>Summary:</strong> In this paper, we propose SimMLM, a simple yet powerful framework for multimodal learning with missing modalities. Unlike existing approaches that rely on sophisticated network architectures or complex data imputation techniques, SimMLM provides a generic and effective solution that can adapt to various missing modality scenarios with improved accuracy and robustness. Specifically, SimMLM consists of a generic Dynamic Mixture of Modality Experts (DMoME) architecture, featuring a dynamic, learnable gating mechanism that automatically adjusts each modalitys contribution in both full and partial modality settings. A key innovation of SimMLM is the proposed More vs. Fewer (MoFe) ranking loss, which ensures that task accuracy improves or remains stable as more modalities are made available. This aligns the model with an intuitive principle: removing one or more modalities should not increase accuracy. We validate SimMLM on multimodal medical image segmentation (BraTS 2018) and multimodal classification (UPMC Food-101, avMNIST) tasks, where it consistently surpasses competitive methods, demonstrating superior accuracy, interpretability, robustness, and reliability across both complete and missing modality scenarios at test time.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2507.19261v1" target="_blank">Knowledge Grafting: A Mechanism for Optimizing AI Model Deployment in Resource-Constrained Environments</a></h3>
                    <p><strong>Authors:</strong> Osama Almurshed, Ashish Kaushal, Asmail Muftah, Nitin Auluck, Omer Rana</p>
                    <p><strong>Published:</strong> 7/25/2025</p>
                    <p><strong>Categories:</strong> cs.AI, cs.LG, cs.PF</p>
                    <p><strong>Summary:</strong> The increasing adoption of Artificial Intelligence (AI) has led to larger, more complex models with numerous parameters that require substantial computing power -- resources often unavailable in many real-world application scenarios. Our paper addresses this challenge by introducing knowledge grafting, a novel mechanism that optimizes AI models for resource-constrained environments by transferring selected features (the scion) from a large donor model to a smaller rootstock model. The approach achieves an 88.54% reduction in model size (from 64.39 MB to 7.38 MB), while improving generalization capability of the model. Our new rootstock model achieves 89.97% validation accuracy (vs. donors 87.47%), maintains lower validation loss (0.2976 vs. 0.5068), and performs exceptionally well on unseen test data with 90.45% accuracy. It addresses the typical size vs performance trade-off, and enables deployment of AI frameworks on resource-constrained devices with enhanced performance. We have tested our approach on an agricultural weed detection scenario, however, it can be extended across various edge computing scenarios, potentially accelerating AI adoption in areas with limited hardware/software support -- by mirroring in a similar manner the horticultural grafting enables productive cultivation in challenging agri-based environments.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2507.19247v1" target="_blank">A Markov Categorical Framework for Language Modeling</a></h3>
                    <p><strong>Authors:</strong> Yifan Zhang</p>
                    <p><strong>Published:</strong> 7/25/2025</p>
                    <p><strong>Categories:</strong> cs.LG, cs.AI, cs.CL</p>
                    <p><strong>Summary:</strong> Auto-regressive language models factorize sequence probabilities and are trained by minimizing the negative log-likelihood (NLL) objective. While empirically powerful, a deep theoretical understanding of why this simple objective yields such versatile representations remains elusive. This work introduces a unifying analytical framework using Markov Categories (MCs) to deconstruct the AR generation process and the NLL objective. We model the single-step generation map as a composition of Markov kernels in the category Stoch. This compositional view, when enriched with statistical divergences, allows us to dissect information flow and learned geometry. Our framework makes three main contributions. First, we provide a formal, information-theoretic rationale for the success of modern speculative decoding methods like EAGLE, quantifying the information surplus in hidden states that these methods exploit. Second, we formalize how NLL minimization forces the model to learn not just the next token, but the datas intrinsic conditional stochasticity, a process we analyze using categorical entropy. Third, and most centrally, we prove that NLL training acts as an implicit form of spectral contrastive learning. By analyzing the information geometry of the models prediction head, we show that NLL implicitly forces the learned representation space to align with the eigenspectrum of a predictive similarity operator, thereby learning a geometrically structured space without explicit contrastive pairs. This compositional and information-geometric perspective reveals the deep structural principles underlying the effectiveness of modern LMs. Project Page: https://github.com/asiresearch/lm-theory</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2507.19245v1" target="_blank">Transfinite Fixed Points in Alpay Algebra as Ordinal Game Equilibria in Dependent Type Theory</a></h3>
                    <p><strong>Authors:</strong> Faruk Alpay, Bugra Kilictas, Taylan Alpay</p>
                    <p><strong>Published:</strong> 7/25/2025</p>
                    <p><strong>Categories:</strong> cs.LO, cs.AI, 68T27, 03B70, 68Q55</p>
                    <p><strong>Summary:</strong> This paper contributes to the Alpay Algebra by demonstrating that the stable outcome of a self referential process, obtained by iterating a transformation through all ordinal stages, is identical to the unique equilibrium of an unbounded revision dialogue between a system and its environment. The analysis initially elucidates how classical fixed point theorems guarantee such convergence in finite settings and subsequently extends the argument to the transfinite domain, relying upon well founded induction and principles of order theoretic continuity. Furthermore, the resulting transordinal fixed point operator is embedded into dependent type theory, a formalization which permits every step of the transfinite iteration and its limit to be verified within a modern proof assistant. This procedure yields a machine checked proof that the iterative dialogue necessarily stabilizes and that its limit is unique. The result provides a foundation for Alpays philosophical claim of semantic convergence within the framework of constructive logic. By unifying concepts from fixed point theory, game semantics, ordinal analysis, and type theory, this research establishes a broadly accessible yet formally rigorous foundation for reasoning about infinite self referential systems and offers practical tools for certifying their convergence within computational environments.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2507.19239v1" target="_blank">CoopTrack: Exploring End-to-End Learning for Efficient Cooperative Sequential Perception</a></h3>
                    <p><strong>Authors:</strong> Jiaru Zhong, Jiahao Wang, Jiahui Xu, Xiaofan Li, Zaiqing Nie, Haibao Yu</p>
                    <p><strong>Published:</strong> 7/25/2025</p>
                    <p><strong>Categories:</strong> cs.CV</p>
                    <p><strong>Summary:</strong> Cooperative perception aims to address the inherent limitations of single-vehicle autonomous driving systems through information exchange among multiple agents. Previous research has primarily focused on single-frame perception tasks. However, the more challenging cooperative sequential perception tasks, such as cooperative 3D multi-object tracking, have not been thoroughly investigated. Therefore, we propose CoopTrack, a fully instance-level end-to-end framework for cooperative tracking, featuring learnable instance association, which fundamentally differs from existing approaches. CoopTrack transmits sparse instance-level features that significantly enhance perception capabilities while maintaining low transmission costs. Furthermore, the framework comprises two key components: Multi-Dimensional Feature Extraction, and Cross-Agent Association and Aggregation, which collectively enable comprehensive instance representation with semantic and motion features, and adaptive cross-agent association and fusion based on a feature graph. Experiments on both the V2X-Seq and Griffin datasets demonstrate that CoopTrack achieves excellent performance. Specifically, it attains state-of-the-art results on V2X-Seq, with 39.0\% mAP and 32.8\% AMOTA. The project is available at https://github.com/zhongjiaru/CoopTrack.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2507.19234v1" target="_blank">Virne: A Comprehensive Benchmark for Deep RL-based Network Resource Allocation in NFV</a></h3>
                    <p><strong>Authors:</strong> Tianfu Wang, Liwei Deng, Xi Chen, Junyang Wang, Huiguo He, Leilei Ding, Wei Wu, Qilin Fan, Hui Xiong</p>
                    <p><strong>Published:</strong> 7/25/2025</p>
                    <p><strong>Categories:</strong> cs.NI, cs.AI</p>
                    <p><strong>Summary:</strong> Resource allocation (RA) is critical to efficient service deployment in Network Function Virtualization (NFV), a transformative networking paradigm. Recently, deep Reinforcement Learning (RL)-based methods have been showing promising potential to address this complexity. However, the lack of a systematic benchmarking framework and thorough analysis hinders the exploration of emerging networks and the development of more robust algorithms while causing inconsistent evaluation. In this paper, we introduce Virne, a comprehensive benchmarking framework for the NFV-RA problem, with a focus on supporting deep RL-based methods. Virne provides customizable simulations for diverse network scenarios, including cloud, edge, and 5G environments. It also features a modular and extensible implementation pipeline that supports over 30 methods of various types, and includes practical evaluation perspectives beyond effectiveness, such as scalability, generalization, and scalability. Furthermore, we conduct in-depth analysis through extensive experiments to provide valuable insights into performance trade-offs for efficient implementation and offer actionable guidance for future research directions. Overall, with its diverse simulations, rich implementations, and extensive evaluation capabilities, Virne could serve as a comprehensive benchmark for advancing NFV-RA methods and deep RL applications. The code is publicly available at https://github.com/GeminiLight/virne.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2507.19228v1" target="_blank">Polarization of the $œÜ$ meson in the hadronic phase with nucleon scatterings and a viscous hydrodynamic background</a></h3>
                    <p><strong>Authors:</strong> Eduardo Grossi, Andrea Palermo, Ismail Zahed</p>
                    <p><strong>Published:</strong> 7/25/2025</p>
                    <p><strong>Categories:</strong> hep-ph, nucl-th</p>
                    <p><strong>Summary:</strong> We extend our previous work on the spin alignment of the $\phi$ vector meson in the hadronic phase of the Quark Gluon Plasma, by including effects of nucleon scatterings. The emission rates are calculated in a realistic hydrodynamic background simulated with the code Fluid$u$m, for different beam energies. We find that all the effects taken into account cannot explain the out-of-plane spin alignment of the $\phi$ meson observed experimentally.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2507.19225v1" target="_blank">Face2VoiceSync: Lightweight Face-Voice Consistency for Text-Driven Talking Face Generation</a></h3>
                    <p><strong>Authors:</strong> Fang Kang, Yin Cao, Haoyu Chen</p>
                    <p><strong>Published:</strong> 7/25/2025</p>
                    <p><strong>Categories:</strong> cs.SD, cs.CV, cs.MM, eess.AS</p>
                    <p><strong>Summary:</strong> Recent studies in speech-driven talking face generation achieve promising results, but their reliance on fixed-driven speech limits further applications (e.g., face-voice mismatch). Thus, we extend the task to a more challenging setting: given a face image and text to speak, generating both talking face animation and its corresponding speeches. Accordingly, we propose a novel framework, Face2VoiceSync, with several novel contributions: 1) Voice-Face Alignment, ensuring generated voices match facial appearance; 2) Diversity \ Manipulation, enabling generated voice control over paralinguistic features space; 3) Efficient Training, using a lightweight VAE to bridge visual and audio large-pretrained models, with significantly fewer trainable parameters than existing methods; 4) New Evaluation Metric, fairly assessing the diversity and identity consistency. Experiments show Face2VoiceSync achieves both visual and audio state-of-the-art performances on a single 40GB GPU.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2507.19218v1" target="_blank">Technological folie √† deux: Feedback Loops Between AI Chatbots and Mental Illness</a></h3>
                    <p><strong>Authors:</strong> Sebastian Dohn√°ny, Zeb Kurth-Nelson, Eleanor Spens, Lennart Luettgau, Alastair Reid, Christopher Summerfield, Murray Shanahan, Matthew M Nour</p>
                    <p><strong>Published:</strong> 7/25/2025</p>
                    <p><strong>Categories:</strong> q-bio.NC</p>
                    <p><strong>Summary:</strong> Artificial intelligence chatbots have achieved unprecedented adoption, with millions now using these systems for emotional support and companionship in contexts of widespread social isolation and capacity-constrained mental health services. While some users report psychological benefits, concerning edge cases are emerging, including reports of suicide, violence, and delusional thinking linked to perceived emotional relationships with chatbots. To understand this new risk profile we need to consider the interaction between human cognitive and emotional biases, and chatbot behavioural tendencies such as agreeableness (sycophancy) and adaptability (in-context learning). We argue that individuals with mental health conditions face increased risks of chatbot-induced belief destabilization and dependence, owing to altered belief-updating, impaired reality-testing, and social isolation. Current AI safety measures are inadequate to address these interaction-based risks. To address this emerging public health concern, we need coordinated action across clinical practice, AI development, and regulatory frameworks.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2507.19481v1" target="_blank">HairCUP: Hair Compositional Universal Prior for 3D Gaussian Avatars</a></h3>
                    <p><strong>Authors:</strong> Byungjun Kim, Shunsuke Saito, Giljoo Nam, Tomas Simon, Jason Saragih, Hanbyul Joo, Junxuan Li</p>
                    <p><strong>Published:</strong> 7/25/2025</p>
                    <p><strong>Categories:</strong> cs.CV</p>
                    <p><strong>Summary:</strong> We present a universal prior model for 3D head avatars with explicit hair compositionality. Existing approaches to build generalizable priors for 3D head avatars often adopt a holistic modeling approach, treating the face and hair as an inseparable entity. This overlooks the inherent compositionality of the human head, making it difficult for the model to naturally disentangle face and hair representations, especially when the dataset is limited. Furthermore, such holistic models struggle to support applications like 3D face and hairstyle swapping in a flexible and controllable manner. To address these challenges, we introduce a prior model that explicitly accounts for the compositionality of face and hair, learning their latent spaces separately. A key enabler of this approach is our synthetic hairless data creation pipeline, which removes hair from studio-captured datasets using estimated hairless geometry and texture derived from a diffusion prior. By leveraging a paired dataset of hair and hairless captures, we train disentangled prior models for face and hair, incorporating compositionality as an inductive bias to facilitate effective separation. Our models inherent compositionality enables seamless transfer of face and hair components between avatars while preserving identity. Additionally, we demonstrate that our model can be fine-tuned in a few-shot manner using monocular captures to create high-fidelity, hair-compositional 3D head avatars for unseen subjects. These capabilities highlight the practical applicability of our approach in real-world scenarios, paving the way for flexible and expressive 3D avatar generation.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2507.19478v1" target="_blank">MMBench-GUI: Hierarchical Multi-Platform Evaluation Framework for GUI Agents</a></h3>
                    <p><strong>Authors:</strong> Xuehui Wang, Zhenyu Wu, JingJing Xie, Zichen Ding, Bowen Yang, Zehao Li, Zhaoyang Liu, Qingyun Li, Xuan Dong, Zhe Chen, Weiyun Wang, Xiangyu Zhao, Jixuan Chen, Haodong Duan, Tianbao Xie, Chenyu Yang, Shiqian Su, Yue Yu, Yuan Huang, Yiqian Liu, Xiao Zhang, Yanting Zhang, Xiangyu Yue, Weijie Su, Xizhou Zhu, Wei Shen, Jifeng Dai, Wenhai Wang</p>
                    <p><strong>Published:</strong> 7/25/2025</p>
                    <p><strong>Categories:</strong> cs.CV, cs.CL</p>
                    <p><strong>Summary:</strong> We introduce MMBench-GUI, a hierarchical benchmark for evaluating GUI automation agents across Windows, macOS, Linux, iOS, Android, and Web platforms. It comprises four levels: GUI Content Understanding, Element Grounding, Task Automation, and Task Collaboration, covering essential skills for GUI agents. In addition, we propose a novel Efficiency-Quality Area (EQA) metric to assess GUI agent execution efficiency in online automation scenarios. Through MMBench-GUI, we identify accurate visual grounding as a critical determinant of overall task success, emphasizing the substantial benefits of modular frameworks that integrate specialized grounding modules. Furthermore, to achieve reliable GUI automation, an agent requires strong task planning and cross-platform generalization abilities, with long-context memory, a broad action space, and long-term reasoning playing a critical role. More important, task efficiency remains a critically underexplored dimension, and all models suffer from substantial inefficiencies, with excessive redundant steps even when tasks are ultimately completed. The integration of precise localization, effective planning, and early stopping strategies is indispensable to enable truly efficient and scalable GUI automation. Our benchmark code, evaluation data, and running environment will be publicly available at https://github.com/open-compass/MMBench-GUI.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2507.19477v1" target="_blank">Advancing Event Forecasting through Massive Training of Large Language Models: Challenges, Solutions, and Broader Impacts</a></h3>
                    <p><strong>Authors:</strong> Sang-Woo Lee, Sohee Yang, Donghyun Kwak, Noah Y. Siegel</p>
                    <p><strong>Published:</strong> 7/25/2025</p>
                    <p><strong>Categories:</strong> cs.LG, cs.AI, cs.CL</p>
                    <p><strong>Summary:</strong> Many recent papers have studied the development of superforecaster-level event forecasting LLMs. While methodological problems with early studies cast doubt on the use of LLMs for event forecasting, recent studies with improved evaluation methods have shown that state-of-the-art LLMs are gradually reaching superforecaster-level performance, and reinforcement learning has also been reported to improve future forecasting. Additionally, the unprecedented success of recent reasoning models and Deep Research-style models suggests that technology capable of greatly improving forecasting performance has been developed. Therefore, based on these positive recent trends, we argue that the time is ripe for research on large-scale training of superforecaster-level event forecasting LLMs. We discuss two key research directions: training methods and data acquisition. For training, we first introduce three difficulties of LLM-based event forecasting training: noisiness-sparsity, knowledge cut-off, and simple reward structure problems. Then, we present related ideas to mitigate these problems: hypothetical event Bayesian networks, utilizing poorly-recalled and counterfactual events, and auxiliary reward signals. For data, we propose aggressive use of market, public, and crawling datasets to enable large-scale training and evaluation. Finally, we explain how these technical advances could enable AI to provide predictive intelligence to society in broader areas. This position paper presents promising specific paths and considerations for getting closer to superforecaster-level AI technology, aiming to call for researchers interest in these directions.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2507.19476v1" target="_blank">Beyond Contrast Transfer: Spectral SNR as a Dose-Aware Metric for STEM Phase Retrieval</a></h3>
                    <p><strong>Authors:</strong> Georgios Varnavides, Julie Marie Bekkevold, Stephanie M Ribet, Mary C Scott, Lewys Jones, Colin Ophus</p>
                    <p><strong>Published:</strong> 7/25/2025</p>
                    <p><strong>Categories:</strong> physics.optics, cond-mat.mtrl-sci</p>
                    <p><strong>Summary:</strong> The contrast transfer function (CTF) is widely used to evaluate phase retrieval methods in scanning transmission electron microscopy (STEM), including center-of-mass imaging, parallax imaging, direct ptychography, and iterative ptychography. However, the CTF reflects only the maximum usable signal, neglecting the effects of finite electron fluence and the Poisson-limited nature of detection. As a result, it can significantly overestimate practical performance, especially in low-dose regimes. Here, we employ the spectral signal-to-noise ratio (SSNR), as a dose-aware statistical framework to evaluate the recoverable signal as a function of spatial frequency. Using numerical reconstructions of white-noise objects, we show that center-of-mass, parallax, and direct ptychography exhibit dose-independent SSNRs, with close-form analytic expressions. In contrast, iterative ptychography exhibits a surprising dose dependence: at low fluence, its SSNR converges to that of direct ptychography; at high fluence, it saturates at a value consistent with the maximum detective quantum efficiency predicted by recent quantum Fisher information bounds. The results highlight the limitations of CTF-based evaluation and motivate SSNR as a more accurate, dose-aware metric for assessing STEM phase retrieval methods.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2507.19474v1" target="_blank">DINO-SLAM: DINO-informed RGB-D SLAM for Neural Implicit and Explicit Representations</a></h3>
                    <p><strong>Authors:</strong> Ziren Gong, Xiaohan Li, Fabio Tosi, Youmin Zhang, Stefano Mattoccia, Jun Wu, Matteo Poggi</p>
                    <p><strong>Published:</strong> 7/25/2025</p>
                    <p><strong>Categories:</strong> cs.CV</p>
                    <p><strong>Summary:</strong> This paper presents DINO-SLAM, a DINO-informed design strategy to enhance neural implicit (Neural Radiance Field -- NeRF) and explicit representations (3D Gaussian Splatting -- 3DGS) in SLAM systems through more comprehensive scene representations. Purposely, we rely on a Scene Structure Encoder (SSE) that enriches DINO features into Enhanced DINO ones (EDINO) to capture hierarchical scene elements and their structural relationships. Building upon it, we propose two foundational paradigms for NeRF and 3DGS SLAM systems integrating EDINO features. Our DINO-informed pipelines achieve superior performance on the Replica, ScanNet, and TUM compared to state-of-the-art methods.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2507.19471v1" target="_blank">Interplay of non-Hermitian skin effect and electronic correlations in the non-Hermitian Hubbard model via Real-space dynamical mean field theory</a></h3>
                    <p><strong>Authors:</strong> Chakradhar Rangi, Juana Moreno, Ka-Ming Tam</p>
                    <p><strong>Published:</strong> 7/25/2025</p>
                    <p><strong>Categories:</strong> cond-mat.str-el, cond-mat.other</p>
                    <p><strong>Summary:</strong> Non-Hermitian quantum systems, characterized by their ability to model open systems with gain and loss, have unveiled striking phenomena such as the non-Hermitian skin effect (NHSE), where eigenstates localize at boundaries under open boundary conditions. While extensively studied in non-interacting systems, the interplay between NHSE and strong electron correlations remains largely unexplored. Here, we investigate the non-Hermitian Hubbard model with asymmetric hopping, employing real-space dynamical mean-field theory (R-DMFT), a novel extension to such non-Hermitian correlated models-to capture both local correlations and spatial inhomogeneities. By analyzing the end-to-end Greens function as probes of directional amplification, we demonstrate that strong correlations can suppress the skin effect, leading to a crossover from boundary-dominated to correlation-driven dynamics. Our systematic study reveals that correlations dominate at small to intermediate strength of asymmetric hopping, inducing exponential decay in the end-to-end Greens function, but higher strength of asymmetric hopping can restore amplification even at strong interaction. These results illuminate the tunable interplay between correlations and non-Hermitian physics, suggesting avenues for engineering non-reciprocal transport in correlated open quantum systems.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2507.19470v1" target="_blank">Conversations Gone Awry, But Then? Evaluating Conversational Forecasting Models</a></h3>
                    <p><strong>Authors:</strong> Son Quoc Tran, Tushaar Gangavarapu, Nicholas Chernogor, Jonathan P. Chang, Cristian Danescu-Niculescu-Mizil</p>
                    <p><strong>Published:</strong> 7/25/2025</p>
                    <p><strong>Categories:</strong> cs.CL, cs.HC</p>
                    <p><strong>Summary:</strong> We often rely on our intuition to anticipate the direction of a conversation. Endowing automated systems with similar foresight can enable them to assist human-human interactions. Recent work on developing models with this predictive capacity has focused on the Conversations Gone Awry (CGA) task: forecasting whether an ongoing conversation will derail. In this work, we revisit this task and introduce the first uniform evaluation framework, creating a benchmark that enables direct and reliable comparisons between different architectures. This allows us to present an up-to-date overview of the current progress in CGA models, in light of recent advancements in language modeling. Our framework also introduces a novel metric that captures a models ability to revise its forecast as the conversation progresses.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2507.19469v1" target="_blank">Efficient Lines Detection for Robot Soccer</a></h3>
                    <p><strong>Authors:</strong> Jo√£o G. Melo, Jo√£o P. Mafaldo, Edna Barros</p>
                    <p><strong>Published:</strong> 7/25/2025</p>
                    <p><strong>Categories:</strong> cs.CV, cs.RO</p>
                    <p><strong>Summary:</strong> Self-localization is essential in robot soccer, where accurate detection of visual field features, such as lines and boundaries, is critical for reliable pose estimation. This paper presents a lightweight and efficient method for detecting soccer field lines using the ELSED algorithm, extended with a classification step that analyzes RGB color transitions to identify lines belonging to the field. We introduce a pipeline based on Particle Swarm Optimization (PSO) for threshold calibration to optimize detection performance, requiring only a small number of annotated samples. Our approach achieves accuracy comparable to a state-of-the-art deep learning model while offering higher processing speed, making it well-suited for real-time applications on low-power robotic platforms.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2507.19468v1" target="_blank">Back to the Features: DINO as a Foundation for Video World Models</a></h3>
                    <p><strong>Authors:</strong> Federico Baldassarre, Marc Szafraniec, Basile Terver, Vasil Khalidov, Francisco Massa, Yann LeCun, Patrick Labatut, Maximilian Seitzer, Piotr Bojanowski</p>
                    <p><strong>Published:</strong> 7/25/2025</p>
                    <p><strong>Categories:</strong> cs.CV</p>
                    <p><strong>Summary:</strong> We present DINO-world, a powerful generalist video world model trained to predict future frames in the latent space of DINOv2. By leveraging a pre-trained image encoder and training a future predictor on a large-scale uncurated video dataset, DINO-world learns the temporal dynamics of diverse scenes, from driving and indoor scenes to simulated environments. We show that DINO-world outperforms previous models on a variety of video prediction benchmarks, e.g. segmentation and depth forecasting, and demonstrates strong understanding of intuitive physics. Furthermore, we show that it is possible to fine-tune the predictor on observation-action trajectories. The resulting action-conditioned world model can be used for planning by simulating candidate trajectories in latent space.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2507.19467v1" target="_blank">Persistent subradiant correlations in a random driven Dicke model</a></h3>
                    <p><strong>Authors:</strong> Nikita Leppenen, Alexander N. Poddubny</p>
                    <p><strong>Published:</strong> 7/25/2025</p>
                    <p><strong>Categories:</strong> quant-ph</p>
                    <p><strong>Summary:</strong> We study theoretically the driven-dissipative dynamics of an array of two-level emitters, coupled to a single photonic mode, in the presence of disorder in the resonant frequencies. We introduce the notion of subradiant correlations in the dynamics, corresponding to the eigenstates of the Liouvillian with a low decay rate, that can also oscillate in time. While the usual collective subradiant states do not survive the emitter resonant frequency fluctuations, these subradiant correlations are immune to such a type of disorder. These long-living correlations exist in finite-size systems, when their lifetime is parametrically longer than in the so-called Dicke time crystal phase.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2507.19461v1" target="_blank">Existence of 2-EFX Allocations of Chores</a></h3>
                    <p><strong>Authors:</strong> Jugal Garg, Aniket Murhekar</p>
                    <p><strong>Published:</strong> 7/25/2025</p>
                    <p><strong>Categories:</strong> cs.GT</p>
                    <p><strong>Summary:</strong> We study the fair division of indivisible chores among agents with additive disutility functions. We investigate the existence of allocations satisfying the popular fairness notion of envy-freeness up to any chore (EFX), and its multiplicative approximations. The existence of $4$-EFX allocations was recently established by Garg, Murhekar, and Qin (2025). We improve this guarantee by proving the existence of $2$-EFX allocations for all instances with additive disutilities. This approximation was previously known only for restricted instances such as bivalued disutilities (Lin, Wu, and Zhou (2025)) or three agents (Afshinmehr, Ansaripour, Danaei, and Mehlhorn (2024)). We obtain our result by providing a general framework for achieving approximate-EFX allocations. The approach begins with a suitable initial allocation and performs a sequence of local swaps between the bundles of envious and envied agents. For our main result, we begin with an initial allocation that satisfies envy-freeness up to one chore (EF1) and Pareto-optimality (PO); the existence of such an allocation was recently established in a major breakthrough by Mahara (2025). We further demonstrate the strength and generality of our framework by giving simple and unified proofs of existing results, namely (i) $2$-EFX for bivalued instances, (ii) 2-EFX for three agents, (iii) EFX when the number of chores is at most twice the number of agents, and (iv) $4$-EFX for all instances. We expect this framework to have broader applications in approximate-EFX due to its simplicity and generality.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2507.19459v1" target="_blank">Fast Learning of Non-Cooperative Spacecraft 3D Models through Primitive Initialization</a></h3>
                    <p><strong>Authors:</strong> Pol Francesch Huc, Emily Bates, Simone DAmico</p>
                    <p><strong>Published:</strong> 7/25/2025</p>
                    <p><strong>Categories:</strong> cs.CV, cs.LG, cs.RO</p>
                    <p><strong>Summary:</strong> The advent of novel view synthesis techniques such as NeRF and 3D Gaussian Splatting (3DGS) has enabled learning precise 3D models only from posed monocular images. Although these methods are attractive, they hold two major limitations that prevent their use in space applications: they require poses during training, and have high computational cost at training and inference. To address these limitations, this work contributes: (1) a Convolutional Neural Network (CNN) based primitive initializer for 3DGS using monocular images; (2) a pipeline capable of training with noisy or implicit pose estimates; and (3) and analysis of initialization variants that reduce the training cost of precise 3D models. A CNN takes a single image as input and outputs a coarse 3D model represented as an assembly of primitives, along with the targets pose relative to the camera. This assembly of primitives is then used to initialize 3DGS, significantly reducing the number of training iterations and input images needed -- by at least an order of magnitude. For additional flexibility, the CNN component has multiple variants with different pose estimation techniques. This work performs a comparison between these variants, evaluating their effectiveness for downstream 3DGS training under noisy or implicit pose estimates. The results demonstrate that even with imperfect pose supervision, the pipeline is able to learn high-fidelity 3D representations, opening the door for the use of novel view synthesis in space applications.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2507.19457v1" target="_blank">GEPA: Reflective Prompt Evolution Can Outperform Reinforcement Learning</a></h3>
                    <p><strong>Authors:</strong> Lakshya A Agrawal, Shangyin Tan, Dilara Soylu, Noah Ziems, Rishi Khare, Krista Opsahl-Ong, Arnav Singhvi, Herumb Shandilya, Michael J Ryan, Meng Jiang, Christopher Potts, Koushik Sen, Alexandros G. Dimakis, Ion Stoica, Dan Klein, Matei Zaharia, Omar Khattab</p>
                    <p><strong>Published:</strong> 7/25/2025</p>
                    <p><strong>Categories:</strong> cs.CL, cs.AI, cs.LG, cs.SE, I.2.7; I.2.6; I.2.4; I.2.8</p>
                    <p><strong>Summary:</strong> Large language models (LLMs) are increasingly adapted to downstream tasks via reinforcement learning (RL) methods like Group Relative Policy Optimization (GRPO), which often require thousands of rollouts to learn new tasks. We argue that the interpretable nature of language can often provide a much richer learning medium for LLMs, compared with policy gradients derived from sparse, scalar rewards. To test this, we introduce GEPA (Genetic-Pareto), a prompt optimizer that thoroughly incorporates natural language reflection to learn high-level rules from trial and error. Given any AI system containing one or more LLM prompts, GEPA samples system-level trajectories (e.g., reasoning, tool calls, and tool outputs) and reflects on them in natural language to diagnose problems, propose and test prompt updates, and combine complementary lessons from the Pareto frontier of its own attempts. As a result of GEPAs design, it can often turn even just a few rollouts into a large quality gain. Across four tasks, GEPA outperforms GRPO by 10% on average and by up to 20%, while using up to 35x fewer rollouts. GEPA also outperforms the leading prompt optimizer, MIPROv2, by over 10% across two LLMs, and demonstrates promising results as an inference-time search strategy for code optimization.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2507.19455v1" target="_blank">Forest-Guided Clustering -- Shedding Light into the Random Forest Black Box</a></h3>
                    <p><strong>Authors:</strong> Lisa Barros de Andrade e Sousa, Gregor Miller, Ronan Le Gleut, Dominik Thalmeier, Helena Pelin, Marie Piraud</p>
                    <p><strong>Published:</strong> 7/25/2025</p>
                    <p><strong>Categories:</strong> cs.LG</p>
                    <p><strong>Summary:</strong> As machine learning models are increasingly deployed in sensitive application areas, the demand for interpretable and trustworthy decision-making has increased. Random Forests (RF), despite their widespread use and strong performance on tabular data, remain difficult to interpret due to their ensemble nature. We present Forest-Guided Clustering (FGC), a model-specific explainability method that reveals both local and global structure in RFs by grouping instances according to shared decision paths. FGC produces human-interpretable clusters aligned with the models internal logic and computes cluster-specific and global feature importance scores to derive decision rules underlying RF predictions. FGC accurately recovered latent subclass structure on a benchmark dataset and outperformed classical clustering and post-hoc explanation methods. Applied to an AML transcriptomic dataset, FGC uncovered biologically coherent subpopulations, disentangled disease-relevant signals from confounders, and recovered known and novel gene expression patterns. FGC bridges the gap between performance and interpretability by providing structure-aware insights that go beyond feature-level attribution.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2507.19454v1" target="_blank">Random approximate quantum information masking</a></h3>
                    <p><strong>Authors:</strong> Xiaodi Li, Xinyang Shu, Huangjun Zhu</p>
                    <p><strong>Published:</strong> 7/25/2025</p>
                    <p><strong>Categories:</strong> quant-ph</p>
                    <p><strong>Summary:</strong> Masking information into quantum correlations is a cornerstone of many quantum information applications. While there exist the no-hiding and no-masking theorems, approximate quantum information masking (AQIM) offers a promising means of circumventing the constraints. Despite its potential, AQIM still remains underexplored, and constructing explicit approximate maskers remains a challenge. In this work, we investigate AQIM from multiple perspectives and propose using random isometries to construct approximate maskers. First, different notions of AQIM are introduced and we find there are profound intrinsic connections among them. These relationships are characterized by a set of figures of merit, which are introduced to quantify the deviation of AQIM from exact QIM. We then explore the possibility of realizing AQIM via random isometries in bipartite and multipartite systems. In bipartite systems, we identify a fundamental lower bound for a key figure of merit, implying that almost all random isometries fail to realize AQIM. This surprising result generalizes the original no-masking theorem to the no-random-AQIM theorem for bipartite systems. In contrast, in multipartite systems, we show almost all random isometries can realize AQIM. Remarkably, the number of physical qubits required to randomly mask a single logical qubit scales only linearly. We further explore the implications of these findings. In particular, we show that, under certain conditions, approximate quantum error correction is equivalent to AQIM. Consequently, AQIM naturally gives rise to approximate quantum error correction codes with constant code rates and exponentially small correction inaccuracies. Overall, our results establish quantum information masking as a central concept in quantum information theory, bridging diverse notions across multiple domains.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2507.19451v1" target="_blank">GS-Occ3D: Scaling Vision-only Occupancy Reconstruction for Autonomous Driving with Gaussian Splatting</a></h3>
                    <p><strong>Authors:</strong> Baijun Ye, Minghui Qin, Saining Zhang, Moonjun Gong, Shaoting Zhu, Zebang Shen, Luan Zhang, Lu Zhang, Hao Zhao, Hang Zhao</p>
                    <p><strong>Published:</strong> 7/25/2025</p>
                    <p><strong>Categories:</strong> cs.CV</p>
                    <p><strong>Summary:</strong> Occupancy is crucial for autonomous driving, providing essential geometric priors for perception and planning. However, existing methods predominantly rely on LiDAR-based occupancy annotations, which limits scalability and prevents leveraging vast amounts of potential crowdsourced data for auto-labeling. To address this, we propose GS-Occ3D, a scalable vision-only framework that directly reconstructs occupancy. Vision-only occupancy reconstruction poses significant challenges due to sparse viewpoints, dynamic scene elements, severe occlusions, and long-horizon motion. Existing vision-based methods primarily rely on mesh representation, which suffer from incomplete geometry and additional post-processing, limiting scalability. To overcome these issues, GS-Occ3D optimizes an explicit occupancy representation using an Octree-based Gaussian Surfel formulation, ensuring efficiency and scalability. Additionally, we decompose scenes into static background, ground, and dynamic objects, enabling tailored modeling strategies: (1) Ground is explicitly reconstructed as a dominant structural element, significantly improving large-area consistency; (2) Dynamic vehicles are separately modeled to better capture motion-related occupancy patterns. Extensive experiments on the Waymo dataset demonstrate that GS-Occ3D achieves state-of-the-art geometry reconstruction results. By curating vision-only binary occupancy labels from diverse urban scenes, we show their effectiveness for downstream occupancy models on Occ3D-Waymo and superior zero-shot generalization on Occ3D-nuScenes. It highlights the potential of large-scale vision-based occupancy reconstruction as a new paradigm for autonomous driving perception. Project Page: https://gs-occ3d.github.io/</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2507.19449v1" target="_blank">A continuous-wave vacuum ultraviolet laser for the nuclear clock</a></h3>
                    <p><strong>Authors:</strong> Qi Xiao, Gleb Penyazkov, Xiangliang Li, Beichen Huang, Wenhao Bu, Juanlang Shi, Haoyu Shi, Tangyin Liao, Gaowei Yan, Haochen Tian, Yixuan Li, Jiatong Li, Bingkun Lu, Li You, Yige Lin, Yuxiang Mo, Shiqian Ding</p>
                    <p><strong>Published:</strong> 7/25/2025</p>
                    <p><strong>Categories:</strong> physics.atom-ph, physics.optics</p>
                    <p><strong>Summary:</strong> The exceptionally low-energy isomeric transition in $^{229}$Th at around 148.4 nm offers a unique opportunity for coherent nuclear control and the realisation of a nuclear clock. Recent advances, most notably the incorporation of large ensembles of $^{229}$Th nuclei in transparent crystals and the development of pulsed vacuum-ultraviolet (VUV) lasers, have enabled initial laser spectroscopy of this transition. However, the lack of an intense, narrow-linewidth VUV laser has precluded coherent nuclear manipulation. Here we introduce and demonstrate the first continuous-wave laser at 148.4 nm, generated via four-wave mixing (FWM) in cadmium vapor. The source delivers 100 nW of power with a linewidth well below 100 Hz and supports broad wavelength tunability. This represents a five-orders-of-magnitude improvement in linewidth over all previous single-frequency lasers below 190 nm, marking a major advance in laser technology. We develop a spatially resolved homodyne technique to place a stringent upper bound on the phase noise induced by the FWM process and demonstrate sub-hertz linewidth capability. These results eliminate the final technical hurdle to a $^{229}$Th-based nuclear clock, opening new directions in quantum metrology, nuclear quantum optics and precision tests of the Standard Model. More broadly, they establish a widely tunable, ultranarrow-linewidth laser platform for applications across quantum information science, condensed matter physics, and high-resolution VUV spectroscopy.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2507.19446v1" target="_blank">An OpenSource CI/CD Pipeline for Variant-Rich Software-Defined Vehicles</a></h3>
                    <p><strong>Authors:</strong> Matthias Wei√ü, Anish Navalgund, Johannes St√ºmpfle, Falk Dettinger, Michael Weyrich</p>
                    <p><strong>Published:</strong> 7/25/2025</p>
                    <p><strong>Categories:</strong> cs.SE, cs.DC, B.8.2; C.2.4</p>
                    <p><strong>Summary:</strong> Software-defined vehicles (SDVs) offer a wide range of connected functionalities, including enhanced driving behavior and fleet management. These features are continuously updated via over-the-air (OTA) mechanisms, resulting in a growing number of software versions and variants due to the diversity of vehicles, cloud/edge environments, and stakeholders involved. The lack of a unified integration environment further complicates development, as connected mobility solutions are often built in isolation. To ensure reliable operations across heterogeneous systems, a dynamic orchestration of functions that considers hardware and software variability is essential. This paper presents an open-source CI/CD pipeline tailored for SDVs. It automates the build, test, and deployment phases using a combination of containerized open-source tools, creating a standardized, portable, and scalable ecosystem accessible to all stakeholders. Additionally, a custom OTA middleware distributes software updates and supports rollbacks across vehicles and backend services. Update variants are derived based on deployment target dependencies and hardware configurations. The pipeline also supports continuous development and deployment of AI models for autonomous driving features. Its effectiveness is evaluated using an automated valet parking (AVP) scenario involving TurtleBots and a coordinating backend server. Two object detection variants are developed and deployed to match hardware-specific requirements. Results demonstrate seamless OTA updates, correct variant selection, and successful orchestration across all targets. Overall, the proposed pipeline provides a scalable and efficient solution for managing software variants and OTA updates in SDVs, contributing to the advancement of future mobility technologies.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2507.19442v1" target="_blank">Is the Full Power of Gaussian Boson Sampling Required for Simulating Vibronic Spectra Using Photonics?</a></h3>
                    <p><strong>Authors:</strong> Jan-Lucas Eickmann, Kai-Hong Luo, Mikhail Roiz, Jonas Lammers, Simone Atzeni, Cheeranjiv Pandey, Florian L√ºtkewitte, Reza G. Shirazi, Benjamin Brecht, Vladimir V. Rybkin, Michael Stefszky, Christine Silberhorn</p>
                    <p><strong>Published:</strong> 7/25/2025</p>
                    <p><strong>Categories:</strong> quant-ph, physics.chem-ph, physics.optics</p>
                    <p><strong>Summary:</strong> Simulating vibronic spectra is a central task in physical chemistry, offering insight into important properties of molecules. Recently, it has been experimentally demonstrated that photonic platforms based on Gaussian boson sampling (GBS) are capable of performing these simulations. However, whether an actual GBS approach is required depends on the molecule under investigation. To develop a better understanding on the requirements for simulating vibronic spectra, we explore connections between theoretical approximations in physical chemistry and their photonic counterparts. Mapping these approximations into photonics, we show that for certain molecules the GBS approach is unnecessary. We place special emphasis on the linear coupling approximation, which in photonics corresponds to sampling from multiple coherent states. By implementing this approach in experiments, we demonstrate improved similarities over previously reported GBS results for formic acid and identify the particular attributes that a molecule must exhibit for this, and other approximations, to be valid. These results highlight the importance in forming deeper connections between traditional methods and photonic approaches.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2507.19440v1" target="_blank">Hidden shift problem for complex functions</a></h3>
                    <p><strong>Authors:</strong> Serge Adonsou, Peter Bruin, Maris Ozols, Joppe Stokvis</p>
                    <p><strong>Published:</strong> 7/25/2025</p>
                    <p><strong>Categories:</strong> quant-ph, 68Q12</p>
                    <p><strong>Summary:</strong> We study quantum algorithms for the hidden shift problem of complex scalar- and vector-valued functions on finite abelian groups. Given oracle access to a shifted function and the Fourier transform of the unshifted function, the goal is to find the hidden shift. We analyze the success probability of our algorithms when using a constant number of queries. For bent functions, they succeed with probability 1, while for arbitrary functions the success probability depends on the `bentness of the function.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2507.19433v1" target="_blank">Potential series expansion method applied in Analytical Modeling of Gravitational field of Irregularly Shaped Celestial Bodies</a></h3>
                    <p><strong>Authors:</strong> Marcelo Lisboa Mota, Safwan Aljbaae, Antonio F. B. A. Prado</p>
                    <p><strong>Published:</strong> 7/25/2025</p>
                    <p><strong>Categories:</strong> astro-ph.EP</p>
                    <p><strong>Summary:</strong> This study aims to establish an analytical model that reproduces the gravitational field around non-spherical bodies with constant density. Due to the non-spherical geometry of such bodies, their gravitational potential is disturbed relative to a central field. By considering the body as a polyhedron and decomposing it into tetrahedral elements, we use the Series Potential Expansion Method (PSEM) to approximate the total potential by summing the potentials of each tetrahedron. While this model does not offer higher accuracy than the classical polyhedral approach, it achieves relative errors below 0.1\% for points outside the body when developed to higher orders (e.g., orders 11 and 12), and significantly reduces execution time. To validate this approach, we apply our model to asteroids (87) Sylvia, (101955) Bennu, (99942) Apophis, and (25143) Itokawa. We determine equilibrium points, analyze stability, investigate zero-velocity planes, and calculate the relative errors between the gravitational field modeled by PSEM and the results obtained using both the classical polyhedral method by Tsoulis and Petrovic and the mass concentration method. Our results highlight the computational efficiency of PSEM in modeling the gravitational potential of irregularly shaped bodies. This efficiency stems from expressing the gravitational potential through a homogeneous analytical function that is easy to manipulate algebraically, enabling explicit determination of the acceleration vector. Our model provides a robust framework for more complex analyses, such as studying periodic orbits around non-spherical celestial bodies, assessing their stability, and planning the smooth landing trajectories of spacecraft.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2507.19431v1" target="_blank">Remnant properties of binary neutron star mergers undergoing prompt collapse</a></h3>
                    <p><strong>Authors:</strong> Arnab Dhani, Alessandro Camilletti, David Radice, Rahul Kashyap, Bangalore Sathyaprakash, Domenico Logoteta, Albino Perego</p>
                    <p><strong>Published:</strong> 7/25/2025</p>
                    <p><strong>Categories:</strong> gr-qc</p>
                    <p><strong>Summary:</strong> We study the properties of remnants formed in prompt-collapse binary neutron star mergers. We consider non-spinning binaries over a range of total masses and mass ratios across a set of 22 equations of state, totaling 107 numerical relativity simulations. We report the final mass and spin of the systems (including the accretion disk and ejecta) to be constrained in a narrow range, regardless of the binary configuration and matter effects. This sets them apart from binary black-hole merger remnants. We assess the detectability of the postmerger signal in a future 40 km Cosmic Explorer observatory and find that the signal-to-noise ratio in the postmerger of an optimally located and oriented binary at a distance of 100 Mpc can range from ${}1$ to 8, depending on the binary configuration and equation of state, with a majority of them greater than 4 in the set of simulations that we consider. We also consider the distinguishability between prompt-collapse binary neutron star and binary black hole mergers with the same masses and spins. We find that Cosmic Explorer will be able to distinguish such systems primarily via the measurement of tidal effects in the late inspiral. Neutron star binaries with \emph{reduced tidal deformability} $\tilde\Lambda$ as small as ${\sim}3.5$ can be identified up to a distance of 100 Mpc, while neutron star binaries with $\tilde\Lambda\sim22$ can be identified to distances greater than 250 Mpc. This is larger than the distance up to which the postmerger will be visible. Finally, we discuss the possible implications of our findings for the equation of state of neutron stars from the gravitational-wave event GW230529.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2507.19430v1" target="_blank">Directional Codes: a new family of quantum LDPC codes on hexagonal- and square-grid connectivity hardware</a></h3>
                    <p><strong>Authors:</strong> Gy√∂rgy P. Geh√©r, David Byfield, Archibald Ruban</p>
                    <p><strong>Published:</strong> 7/25/2025</p>
                    <p><strong>Categories:</strong> quant-ph</p>
                    <p><strong>Summary:</strong> In this paper we construct a new family of quantum low-density parity-check (qLDPC) codes, which we call ``Directional Codes, that outperforms the rotated planar code (RPC) while naturally meeting the connectivity requirements of the widely adopted square-grid, and some even the sparser hexagonal-grid. The key idea is to utilise the iSWAP gate -- a natural native gate for superconducting qubits -- to construct circuits that measure the stabilisers of these qLDPC codes without the need for any long-range connections or an increased degree of connectivity. We numerically evaluate the performance of directional codes, encoding four, six and twelve logical qubits, using a common superconducting-inspired circuit-level Pauli noise model. We also compare them to the RPC and to the bivariate bicycle (BB) codes, currently the two most popular quantum LDPC code families. As a concrete example, directional codes outperform RPC by achieving better QEC performance at physical error rate $p=10^{-3}$ using only $25-66.67\%$ of the physical qubits at distance up to $10$. Our discovery represents a breakthrough in QEC code design that suggests complex long-range, high-connectivity hardware may not be necessary for low-overhead fault-tolerant quantum computation.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2507.19427v1" target="_blank">Step-3 is Large yet Affordable: Model-system Co-design for Cost-effective Decoding</a></h3>
                    <p><strong>Authors:</strong> StepFun, :, Bin Wang, Bojun Wang, Changyi Wan, Guanzhe Huang, Hanpeng Hu, Haonan Jia, Hao Nie, Mingliang Li, Nuo Chen, Siyu Chen, Song Yuan, Wuxun Xie, Xiaoniu Song, Xing Chen, Xingping Yang, Xuelin Zhang, Yanbo Yu, Yaoyu Wang, Yibo Zhu, Yimin Jiang, Yu Zhou, Yuanwei Lu, Houyi Li, Jingcheng Hu, Ka Man Lo, Ailin Huang, Binxing Jiao, Bo Li, Boyu Chen, Changxin Miao, Chang Lou, Chen Hu, Chen Xu, Chenfeng Yu, Chengyuan Yao, Daokuan Lv, Dapeng Shi, Deshan Sun, Ding Huang, Dingyuan Hu, Dongqing Pang, Enle Liu, Fajie Zhang, Fanqi Wan, Gulin Yan, Han Zhang, Han Zhou, Hanghao Wu, Hangyu Guo, Hanqi Chen, Hanshan Zhang, Hao Wu, Haocheng Zhang, Haolong Yan, Haoran Lv, Haoran Wei, Hebin Zhou, Heng Wang, Heng Wang, Hongxin Li, Hongyu Zhou, Hongyuan Wang, Huiyong Guo, Jia Wang, Jiahao Gong, Jialing Xie, Jian Zhou, Jianjian Sun, Jiaoren Wu, Jiaran Zhang, Jiayu Liu, Jie Cheng, Jie Luo, Jie Yan, Jie Yang, Jieyi Hou, Jinguang Zhang, Jinlan Cao, Jisheng Yin, Junfeng Liu, Junhao Huang, Junzhe Lin, Kaijun Tan, Kaixiang Li, Kang An, Kangheng Lin, Kenkun Liu, Lei Yang, Liang Zhao, Liangyu Chen, Lieyu Shi, Liguo Tan, Lin Lin, Lin Zhang, Lina Chen, Liwen Huang, Liying Shi, Longlong Gu, Mei Chen, Mengqiang Ren, Ming Li, Mingzhe Chen, Na Wang, Nan Wu, Qi Han, Qian Zhao, Qiang Zhang, Qianni Liu, Qiaohui Chen, Qiling Wu, Qinglin He, Qinyuan Tan, Qiufeng Wang, Qiuping Wu, Qiuyan Liang, Quan Sun, Rui Li, Ruihang Miao, Ruosi Wan, Ruyan Guo, Shangwu Zhong, Shaoliang Pang, Shengjie Fan, Shijie Shang, Shilei Jiang, Shiliang Yang, Shiming Hao, Shuli Gao, Siming Huang, Siqi Liu, Tiancheng Cao, Tianhao Cheng, Tianhao Peng, Wang You, Wei Ji, Wen Sun, Wenjin Deng, Wenqing He, Wenzhen Zheng, Xi Chen, Xiangwen Kong, Xianzhen Luo, Xiaobo Yang, Xiaojia Liu, Xiaoxiao Ren, Xin Han, Xin Li, Xin Wu, Xu Zhao, Yanan Wei, Yang Li, Yangguang Li, Yangshijie Xu, Yanming Xu, Yaqiang Shi, Yeqing Shen, Yi Yang, Yifei Yang, Yifeng Gong, Yihan Chen, Yijing Yang, Yinmin Zhang, Yizhuang Zhou, Yuanhao Ding, Yuantao Fan, Yuanzhen Yang, Yuchu Luo, Yue Peng, Yufan Lu, Yuhang Deng, Yuhe Yin, Yujie Liu, Yukun Chen, Yuling Zhao, Yun Mou, Yunlong Li, Yunzhou Ju, Yusheng Li, Yuxiang Yang, Yuxiang Zhang, Yuyang Chen, Zejia Weng, Zhe Xie, Zheng Ge, Zheng Gong, Zhenyi Lu, Zhewei Huang, Zhichao Chang, Zhiguo Huang, Zhirui Wang, Zidong Yang, Zili Wang, Ziqi Wang, Zixin Zhang, Binxing Jiao, Daxin Jiang, Heung-Yeung Shum, Xiangyu Zhang</p>
                    <p><strong>Published:</strong> 7/25/2025</p>
                    <p><strong>Categories:</strong> cs.LG, cs.AI</p>
                    <p><strong>Summary:</strong> Large language models (LLMs) face low hardware efficiency during decoding, especially for long-context reasoning tasks. This paper introduces Step-3, a 321B-parameter VLM with hardware-aware model-system co-design optimized for minimizing decoding costs. Step-3 innovates in two key dimensions: (1) A novel Multi-Matrix Factorization Attention (MFA) mechanism that significantly reduces both KV cache size and computation while maintaining high attention expressiveness, and (2) Attention-FFN Disaggregation (AFD), a distributed inference system that decouples attention and Feed-Forward Network (FFN) layers into specialized subsystems. This co-design achieves unprecedented cost efficiency: Step-3 significantly reduces theoretical decoding costs compared with models like DeepSeek-V3 and Qwen3 MoE 235B, with the gains widening at longer context. Step-3 achieves low cost while activating 38B parameters per token (more than DeepSeek-V3 and Qwen3 MoE 235B), demonstrating that hardware-aligned attention arithmetic intensity, MoE sparsity, and AFD are critical to cost-effectiveness. We perform a head-to-head comparison with DeepSeek-V3 in its favorable scenarios. Our implementation on Hopper GPUs achieves a decoding throughput of up to 4,039 tokens per second per GPU under 50ms TPOT SLA (4K context, FP8, no MTP). It is higher than DeepSeek-V3s 2,324 in the same setup and sets a new Pareto frontier for LLM decoding.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2507.19425v1" target="_blank">Machine Learning Based Efficiency Calculator (MaLBEC) for Nuclear Fusion Diagnostics</a></h3>
                    <p><strong>Authors:</strong> Kimberley Lennon, Chantal Shand, Gemma Wilson, Robin Smith</p>
                    <p><strong>Published:</strong> 7/25/2025</p>
                    <p><strong>Categories:</strong> physics.ins-det, nucl-ex</p>
                    <p><strong>Summary:</strong> Diagnostics are critical for commercial and research fusion machines, since measuring and understanding plasma features is important to sustaining fusion reactions. The neutron flux (and therefore fusion power) can be indirectly calculated using neutron activation analyses, where potentially large numbers of activation foils are placed in the neutron flux, and delayed gammas from key reactions are measured via gamma spectrometry. In gamma spectrometry, absolute efficiency forms part of the activity calculation, and equals to the ratio of the total number of photons detected to the number emitted by a radioactive sample. Hence, it is imperative that they are calculated efficiently and accurately. This paper presents a novel digital efficiency calculation algorithm, the Machine Learning Based Efficiency Calculator (MaLBEC), that uses state-of-the-art supervised machine learning techniques to calculate efficiency values of a given sample, from only four inputs. In this paper, the performance of the MaLBEC is demonstrated with a fusion sample and compares the values to a traditional efficiency calculation method, Monte Carlo N-Particle (MCNP). The efficiencies from the MaLBEC were within an average 5\% of the ones produced by MCNP, but with an exceptional reduction in computation time of 99.96\%. When the efficiency values from both methods were used in the activity calculation, the MaLBEC was within 3\% of the MCNP results.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2507.19424v1" target="_blank">Order in Partial Markov Categories</a></h3>
                    <p><strong>Authors:</strong> Elena Di Lavore, Mario Rom√°n, Pawe≈Ç Soboci≈Ñski, M√°rk Sz√©les</p>
                    <p><strong>Published:</strong> 7/25/2025</p>
                    <p><strong>Categories:</strong> cs.LO, 18M05</p>
                    <p><strong>Summary:</strong> Partial Markov categories are a recent framework for categorical probability theory, providing an abstract account of partial probabilistic computation. In this article, we discuss two order relations on the morphisms of a partial Markov category. In particular, we prove that every partial Markov category is canonically enriched over the category of preordered sets and monotone maps. We show that our construction recovers several well-known order enrichments. We also demonstrate that the existence of codiagonal maps (comparators) is closely related to order properties of partial Markov categories. We propose a synthetic version of the Cauchy-Schwarz inequality to facilitate inequational reasoning in partial Markov categories. We apply this new axiom to prove that updating a prior distribution with an evidence predicate increases the likelihood of the evidence in the posterior.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2507.19423v1" target="_blank">Perfect Clustering in Very Sparse Diverse Multiplex Networks</a></h3>
                    <p><strong>Authors:</strong> Marianna Pensky</p>
                    <p><strong>Published:</strong> 7/25/2025</p>
                    <p><strong>Categories:</strong> stat.ML, cs.LG, math.ST, stat.ME, stat.TH</p>
                    <p><strong>Summary:</strong> The paper studies the DIverse MultiPLEx Signed Generalized Random Dot Product Graph (DIMPLE-SGRDPG) network model (Pensky (2024)), where all layers of the network have the same collection of nodes. In addition, all layers can be partitioned into groups such that the layers in the same group are embedded in the same ambient subspace but otherwise matrices of connection probabilities can be all different. This setting includes majority of multilayer network models as its particular cases. The key task in this model is to recover the groups of layers with unique subspace structures, since the case where all layers of the network are embedded in the same subspace has been fairly well studied. Until now, clustering of layers in such networks was based on the layer-per-layer analysis, which required the multilayer network to be sufficiently dense. Nevertheless, in this paper we succeeded in pooling information in all layers together and providing a tensor-based methodology that ensures perfect clustering for a much sparser network. Our theoretical results, established under intuitive non-restrictive assumptions, assert that the new technique achieves perfect clustering under sparsity conditions that, up to logarithmic factors, coincide with the computational lower bound derived for a much simpler model.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2507.19420v1" target="_blank">CircuitProbe: Dissecting Spatiotemporal Visual Semantics with Circuit Tracing</a></h3>
                    <p><strong>Authors:</strong> Yiming Zhang, Chengzhang Yu, Zhuokai Zhao, Kun Wang, Qiankun Li, Zihan Chen, Yang Liu, Zenghui Ding, Yining Sun</p>
                    <p><strong>Published:</strong> 7/25/2025</p>
                    <p><strong>Categories:</strong> cs.CV, cs.LG</p>
                    <p><strong>Summary:</strong> The processing mechanisms underlying language and image understanding in large vision-language models (LVLMs) have been extensively studied. However, the internal reasoning mechanisms of LVLMs for spatiotemporal understanding remain poorly understood. In this work, we introduce a systematic, circuit-based framework designed to investigate how spatiotemporal visual semantics are represented and processed within these LVLMs. Specifically, our framework comprises three circuits: visual auditing circuit, semantic tracing circuit, and attention flow circuit. Through the lens of these circuits, we discover that visual semantics are highly localized to specific object tokens--removing these tokens can degrade model performance by up to 92.6%. Furthermore, we identify that interpretable concepts of objects and actions emerge and become progressively refined in the middle-to-late layers of LVLMs. In contrary to the current works that solely focus on objects in one image, we reveal that the middle-to-late layers of LVLMs exhibit specialized functional localization for spatiotemporal semantics. Our findings offer significant mechanistic insights into spatiotemporal semantics analysis of LVLMs, laying a foundation for designing more robust and interpretable models.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2507.19419v1" target="_blank">TokenSmith: Streamlining Data Editing, Search, and Inspection for Large-Scale Language Model Training and Interpretability</a></h3>
                    <p><strong>Authors:</strong> Mohammad Aflah Khan, Ameya Godbole, Johnny Tian-Zheng Wei, Ryan Wang, James Flemings, Krishna Gummadi, Willie Neiswanger, Robin Jia</p>
                    <p><strong>Published:</strong> 7/25/2025</p>
                    <p><strong>Categories:</strong> cs.CL</p>
                    <p><strong>Summary:</strong> Understanding the relationship between training data and model behavior during pretraining is crucial, but existing workflows make this process cumbersome, fragmented, and often inaccessible to researchers. We present TokenSmith, an open-source library for interactive editing, inspection, and analysis of datasets used in Megatron-style pretraining frameworks such as GPT-NeoX, Megatron, and NVIDIA NeMo. TokenSmith supports a wide range of operations including searching, viewing, ingesting, exporting, inspecting, and sampling data, all accessible through a simple user interface and a modular backend. It also enables structured editing of pretraining data without requiring changes to training code, simplifying dataset debugging, validation, and experimentation. TokenSmith is designed as a plug and play addition to existing large language model pretraining workflows, thereby democratizing access to production-grade dataset tooling. TokenSmith is hosted on GitHub1, with accompanying documentation and tutorials. A demonstration video is also available on YouTube.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2507.19418v1" target="_blank">DEFNet: Multitasks-based Deep Evidential Fusion Network for Blind Image Quality Assessment</a></h3>
                    <p><strong>Authors:</strong> Yiwei Lou, Yuanpeng He, Rongchao Zhang, Yongzhi Cao, Hanpin Wang, Yu Huang</p>
                    <p><strong>Published:</strong> 7/25/2025</p>
                    <p><strong>Categories:</strong> cs.CV</p>
                    <p><strong>Summary:</strong> Blind image quality assessment (BIQA) methods often incorporate auxiliary tasks to improve performance. However, existing approaches face limitations due to insufficient integration and a lack of flexible uncertainty estimation, leading to suboptimal performance. To address these challenges, we propose a multitasks-based Deep Evidential Fusion Network (DEFNet) for BIQA, which performs multitask optimization with the assistance of scene and distortion type classification tasks. To achieve a more robust and reliable representation, we design a novel trustworthy information fusion strategy. It first combines diverse features and patterns across sub-regions to enhance information richness, and then performs local-global information fusion by balancing fine-grained details with coarse-grained context. Moreover, DEFNet exploits advanced uncertainty estimation technique inspired by evidential learning with the help of normal-inverse gamma distribution mixture. Extensive experiments on both synthetic and authentic distortion datasets demonstrate the effectiveness and robustness of the proposed framework. Additional evaluation and analysis are carried out to highlight its strong generalization capability and adaptability to previously unseen scenarios.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2507.19416v1" target="_blank">Dual Mechanisms for Heterogeneous Responses of Inspiratory Neurons to Noradrenergic Modulation</a></h3>
                    <p><strong>Authors:</strong> Sreshta Venkatakrishnan, Andrew K. Tryba, Alfredo J. Garcia 3rd, Yangyang Wang</p>
                    <p><strong>Published:</strong> 7/25/2025</p>
                    <p><strong>Categories:</strong> q-bio.NC, math.DS, 37N25, 34C23, 34C60, 34E13, 34E15, 92C20</p>
                    <p><strong>Summary:</strong> Respiration is an essential involuntary function necessary for survival. This poses a challenge for the control of breathing. The preB\otzinger complex (preB\otC) is a heterogeneous neuronal network responsible for driving the inspiratory rhythm. While neuromodulators such as norepinephrine (NE) allow it to be both robust and flexible for all living beings to interact with their environment, the basis for how neuromodulation impacts neuron-specific properties remains poorly understood. In this work, we examine how NE influences different preB\otC neuronal subtypes by modeling its effects through modulating two key parameters: calcium-activated nonspecific cationic current gating conductance ($g_{\rm CAN}$) and inositol-triphosphate ($\rm IP_3$), guided by experimental studies. Our computational model captures the experimentally observed differential effects of NE on distinct preB\otC bursting patterns. We show that this dual mechanism is critical for inducing conditional bursting and identify specific parameter regimes where silent neurons remain inactive in the presence of NE. Furthermore, using methods of dynamical systems theory, we uncover the mechanisms by which NE differentially modulates burst frequency and duration in NaP-dependent and CAN-dependent bursting neurons. These results align well with previously reported experimental findings and provide a deeper understanding of cell-specific neuromodulatory responses within the respiratory network.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2507.19415v1" target="_blank">Sample Abundance for Signal Processing: A Brief Introduction</a></h3>
                    <p><strong>Authors:</strong> Arian Eamaz, Farhang Yeganegi, Mojtaba Soltanalian</p>
                    <p><strong>Published:</strong> 7/25/2025</p>
                    <p><strong>Categories:</strong> cs.IT, math.IT</p>
                    <p><strong>Summary:</strong> This paper reports, by way of introduction, on the advances made by our group and the broader signal processing community on the concept of sample abundance; a phenomenon that naturally arises in one-bit and few-bit signal processing frameworks. By leveraging large volumes of low-precision measurements, we show how traditionally costly constraints, such as matrix semi-definiteness and rank conditions, become redundant, yielding simple overdetermined linear feasibility problems. We illustrate key algorithms, theoretical guarantees via the Finite Volume Property, and the sample abundance singularity phenomenon, where computational complexity sharply drops.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2507.19412v1" target="_blank">From weakly interacting spinons to tightly bound triplons in the frustrated quantum spin-Peierls chain</a></h3>
                    <p><strong>Authors:</strong> Pyeongjae Park, Bo Xiao, Karolina G√≥rnicka, Andrew F. May, Jiaqiang Yan, Ryoichi Kajimoto, Mitsutaka Nakamura, Matthew B. Stone, G√°bor B. Hal√°sz, Andrew D. Christianson</p>
                    <p><strong>Published:</strong> 7/25/2025</p>
                    <p><strong>Categories:</strong> cond-mat.str-el, cond-mat.mtrl-sci</p>
                    <p><strong>Summary:</strong> Fractionalized quasiparticles and their confinement into emergent bound states lie at the heart of modern quantum magnetism. While the evolution into magnonic bound states has been well characterized, experimental insight into the analogous transition to triplons remains limited. Here, using high-resolution neutron spectroscopy and state-of-the-art spin dynamics simulations, we uncover the transformation from weakly interacting spinons to tightly bound triplons in the spin-Peierls compound CuGeO3. Quantitative comparisons between the measured spectra and tensor network simulations reveal substantial next-nearest-neighbor frustration and weak external dimerization, placing the system deep within the spontaneously dimerized regime and near the exactly solvable Majumdar-Ghosh point. We further show an energy- and temperature-dependent evolution between two contrasting quasiparticle regimes: deconfined spinons with markedly suppressed interactions by frustration, and coherent triplonic bound states with no observable spinon degrees of freedom. Remarkably, triplon character persists into the two-particle regime, forming a structured two-triplon continuum with a spectral feature associated with a van Hove singularity at its lower boundary. These findings challenge the conventional view that robust triplons require strong external dimerization and demonstrate how the interplay between frustration and dimerization can reshape fractionalization and confinement.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2507.19411v1" target="_blank">SILS: Strategic Influence on Liquidity Stability and Whale Detection in Concentrated-Liquidity DEXs</a></h3>
                    <p><strong>Authors:</strong> Ali RajabiNekoo, Laleh Rasoul, Amirfarhad Farhadi, Azadeh Zamanifar</p>
                    <p><strong>Published:</strong> 7/25/2025</p>
                    <p><strong>Categories:</strong> cs.LG, cs.CR, cs.ET</p>
                    <p><strong>Summary:</strong> Traditional methods for identifying impactful liquidity providers (LPs) in Concentrated Liquidity Market Makers (CLMMs) rely on broad measures, such as nominal capital size or surface-level activity, which often lead to inaccurate risk analysis. The SILS framework offers a significantly more detailed approach, characterizing LPs not just as capital holders but as dynamic systemic agents whose actions directly impact market stability. This represents a fundamental paradigm shift from the static, volume-based analysis to a dynamic, impact-focused understanding. This advanced approach uses on-chain event logs and smart contract execution traces to compute Exponential Time-Weighted Liquidity (ETWL) profiles and apply unsupervised anomaly detection. Most importantly, it defines an LPs functional importance through the Liquidity Stability Impact Score (LSIS), a counterfactual metric that measures the potential degradation of the market if the LP withdraws. This combined approach provides a more detailed and realistic characterization of an LPs impact, moving beyond the binary and often misleading classifications used by existing methods. This impact-focused and comprehensive approach enables SILS to accurately identify high-impact LPs-including those missed by traditional methods and supports essential applications like a protective oracle layer and actionable trader signals, thereby significantly enhancing DeFi ecosystem. The framework provides unprecedented transparency into the underlying liquidity structure and associated risks, effectively reducing the common false positives and uncovering critical false negatives found in traditional models. Therefore, SILS provides an effective mechanism for proactive risk management, transforming how DeFi protocols safeguard their ecosystems against asymmetric liquidity behavior.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2507.19409v1" target="_blank">Modality Agnostic Efficient Long Range Encoder</a></h3>
                    <p><strong>Authors:</strong> Toufiq Parag, Ahmed Elgammal</p>
                    <p><strong>Published:</strong> 7/25/2025</p>
                    <p><strong>Categories:</strong> cs.CV</p>
                    <p><strong>Summary:</strong> The long-context capability of recent large transformer models can be surmised to rely on techniques such as attention/model parallelism, as well as hardware-level optimizations. While these strategies allow input lengths to scale to millions of tokens, they do not fundamentally mitigate the quadratic computational and memory complexity of the core attention mechanism. In this paper, we address the challenge of long-context processing on a single device using generic implementations by reducing the quadratic memory footprint and inference cost. Existing approaches to extend the context length for generic single device implementations -- such as token merging and modified attentions -- are often modality specific and attain a suboptimal tradeoff between accuracy and efficiency. To overcome these limitations, we propose MAELRE (Modality Agnostic Efficient Long Range Encoder), a unified and efficient transformer architecture designed for long-range encoding across diverse modalities. MAELRE integrates token merging with attention approximation, progressively merging tokens at different stages of internal computational blocks. It employs a lightweight attention approximation when the number of tokens is large, and switches to standard dot-product attention as the sequence becomes shorter through successive aggregation. We demonstrate that MAELRE achieves superior accuracy while reducing computational cost compared to existing long-context models on classification tasks spanning multiple modalities, including text, time series, audio, and vision.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2507.19407v1" target="_blank">Towards Domain Specification of Embedding Models in Medicine</a></h3>
                    <p><strong>Authors:</strong> Mohammad Khodadad, Ali Shiraee, Mahdi Astaraki, Hamidreza Mahyar</p>
                    <p><strong>Published:</strong> 7/25/2025</p>
                    <p><strong>Categories:</strong> cs.CL</p>
                    <p><strong>Summary:</strong> Medical text embedding models are foundational to a wide array of healthcare applications, ranging from clinical decision support and biomedical information retrieval to medical question answering, yet they remain hampered by two critical shortcomings. First, most models are trained on a narrow slice of medical and biological data, beside not being up to date in terms of methodology, making them ill suited to capture the diversity of terminology and semantics encountered in practice. Second, existing evaluations are often inadequate: even widely used benchmarks fail to generalize across the full spectrum of real world medical tasks. To address these gaps, we leverage MEDTE, a GTE model extensively fine-tuned on diverse medical corpora through self-supervised contrastive learning across multiple data sources, to deliver robust medical text embeddings. Alongside this model, we propose a comprehensive benchmark suite of 51 tasks spanning classification, clustering, pair classification, and retrieval modeled on the Massive Text Embedding Benchmark (MTEB) but tailored to the nuances of medical text. Our results demonstrate that this combined approach not only establishes a robust evaluation framework but also yields embeddings that consistently outperform state of the art alternatives in different tasks.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2507.19403v1" target="_blank">SDVDiag: A Modular Platform for the Diagnosis of Connected Vehicle Functions</a></h3>
                    <p><strong>Authors:</strong> Matthias Wei√ü, Falk Dettinger, Michael Weyrich</p>
                    <p><strong>Published:</strong> 7/25/2025</p>
                    <p><strong>Categories:</strong> cs.SE, cs.AI, cs.DC, B.8.2; C.2.4</p>
                    <p><strong>Summary:</strong> Connected and software-defined vehicles promise to offer a broad range of services and advanced functions to customers, aiming to increase passenger comfort and support autonomous driving capabilities. Due to the high reliability and availability requirements of connected vehicles, it is crucial to resolve any occurring failures quickly. To achieve this however, a complex cloud/edge architecture with a mesh of dependencies must be navigated to diagnose the responsible root cause. As such, manual analyses become unfeasible since they would significantly delay the troubleshooting. To address this challenge, this paper presents SDVDiag, an extensible platform for the automated diagnosis of connected vehicle functions. The platform enables the creation of pipelines that cover all steps from initial data collection to the tracing of potential root causes. In addition, SDVDiag supports self-adaptive behavior by the ability to exchange modules at runtime. Dependencies between functions are detected and continuously updated, resulting in a dynamic graph view of the system. In addition, vital system metrics are monitored for anomalies. Whenever an incident is investigated, a snapshot of the graph is taken and augmented by relevant anomalies. Finally, the analysis is performed by traversing the graph and creating a ranking of the most likely causes. To evaluate the platform, it is deployed inside an 5G test fleet environment for connected vehicle functions. The results show that injected faults can be detected reliably. As such, the platform offers the potential to gain new insights and reduce downtime by identifying problems and their causes at an early stage.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2507.19401v1" target="_blank">The gauge theory dual of the bilayer XY model with second order Josephson coupling</a></h3>
                    <p><strong>Authors:</strong> Pye Ton How, Sungkit Yip</p>
                    <p><strong>Published:</strong> 7/25/2025</p>
                    <p><strong>Categories:</strong> cond-mat.supr-con</p>
                    <p><strong>Summary:</strong> We formulate a duality transformation for a bilayer XY model where the layers are coupled by second order Josephson effect, which favors inter-layer phase difference of either $0$ or $\pi$. The model may represent a bilayer superconductor or a spin-1 ferromagnetic Bose gas in the easy-plane limit. The second order Josephson term is mapped to a U(1) gauge field, known to be trivially confining in two dimensions, and we argue that a Coulomb-gas analysis is not applicable to the dual theory. Instead, we appeal to the vast knowledge of gauge theory and infer that the only phase transition out of low-temperature ordered phase is an Ising transition driven by condensation of $\mathbb{Z}_2$ domain wall loops. The domain wall loops can be seen as a surviving vestige of single-layer vortex-anti-vortex pair, heavily deformed by the second order Josephson coupling. A theoretical or computational method that concentrates on point defects would most likely miss out on these excitations and reach erroneous results. Our dual theory offers a clear, intuitive picture of how the second order Josephson coupling induces confinement of vortices and drastically changes the physics.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2507.19402v1" target="_blank">FD4QC: Application of Classical and Quantum-Hybrid Machine Learning for Financial Fraud Detection A Technical Report</a></h3>
                    <p><strong>Authors:</strong> Matteo Cardaioli, Luca Marangoni, Giada Martini, Francesco Mazzolin, Luca Pajola, Andrea Ferretto Parodi, Alessandra Saitta, Maria Chiara Vernillo</p>
                    <p><strong>Published:</strong> 7/25/2025</p>
                    <p><strong>Categories:</strong> cs.LG, cs.CE</p>
                    <p><strong>Summary:</strong> The increasing complexity and volume of financial transactions pose significant challenges to traditional fraud detection systems. This technical report investigates and compares the efficacy of classical, quantum, and quantum-hybrid machine learning models for the binary classification of fraudulent financial activities. As of our methodology, first, we develop a comprehensive behavioural feature engineering framework to transform raw transactional data into a rich, descriptive feature set. Second, we implement and evaluate a range of models on the IBM Anti-Money Laundering (AML) dataset. The classical baseline models include Logistic Regression, Decision Tree, Random Forest, and XGBoost. These are compared against three hybrid classic quantum algorithms architectures: a Quantum Support Vector Machine (QSVM), a Variational Quantum Classifier (VQC), and a Hybrid Quantum Neural Network (HQNN). Furthermore, we propose Fraud Detection for Quantum Computing (FD4QC), a practical, API-driven system architecture designed for real-world deployment, featuring a classical-first, quantum-enhanced philosophy with robust fallback mechanisms. Our results demonstrate that classical tree-based models, particularly \textit{Random Forest}, significantly outperform the quantum counterparts in the current setup, achieving high accuracy (\(97.34\%\)) and F-measure (\(86.95\%\)). Among the quantum models, \textbf{QSVM} shows the most promise, delivering high precision (\(77.15\%\)) and a low false-positive rate (\(1.36\%\)), albeit with lower recall and significant computational overhead. This report provides a benchmark for a real-world financial application, highlights the current limitations of quantum machine learning in this domain, and outlines promising directions for future research.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2507.19398v1" target="_blank">CXR-CML: Improved zero-shot classification of long-tailed multi-label diseases in Chest X-Rays</a></h3>
                    <p><strong>Authors:</strong> Rajesh Madhipati, Sheethal Bhat, Lukas Buess, Andreas Maier</p>
                    <p><strong>Published:</strong> 7/25/2025</p>
                    <p><strong>Categories:</strong> cs.CV, cs.AI</p>
                    <p><strong>Summary:</strong> Chest radiography (CXR) plays a crucial role in the diagnosis of various diseases. However, the inherent class imbalance in the distribution of clinical findings presents a significant challenge for current self-supervised deep learning models. These models often fail to accurately classify long-tailed classes. Current Vision-Language models such as Contrastive Language Image Pre-training (CLIP) models effectively model the manifold distribution of the latent space, enabling high zero-shot classification accuracies. Although CLIP performs well on most of the primary classes in the dataset, our work reveals that its effectiveness decreases significantly for classes with a long-tailed distribution. Our approach employs a class-weighting mechanism that directly aligns with the distribution of classes within the latent space. This method ensures a substantial improvement in overall classification performance, with particular emphasis on enhancing the recognition and accuracy of rarely observed classes. We accomplish this by applying Gaussian Mixture Model (GMM) clustering to the latent space. The subsequent clusters are further refined by Student t-distribution, followed by a metric loss that utilizes the altered embeddings. Our approach facilitates stable and adaptive clustering of the features. This results in a notable average improvement of 7\% points in zero-shot AUC scores across 40 classes in the MIMIC-CXR-JPG dataset from previous SOTA models.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2507.19397v1" target="_blank">Photon catalysis for general multimode multi-photon quantum state preparation</a></h3>
                    <p><strong>Authors:</strong> Andrei Aralov, √âmilie Gillet, Viet Nguyen, Andrea Cosentino, Mattia Walschaers, Massimo Frigerio</p>
                    <p><strong>Published:</strong> 7/25/2025</p>
                    <p><strong>Categories:</strong> quant-ph</p>
                    <p><strong>Summary:</strong> Multimode multiphoton states are at the center of many photonic quantum technologies, from photonic quantum computing to quantum sensing. In this work, we derive a procedure to generate exactly, and with a controlled number of steps, any such state by using only multiport interferometers, photon number resolving detectors, photon additions, and displacements. We achieve this goal by establishing a connection between photonic quantum state engineering and the algebraic problem of symmetric tensor decomposition. This connection allows us to solve the problem by using corresponding results from algebraic geometry and unveils a mechanism of photon catalysis, where photons are injected and subsequently retrieved in measurements, to generate entanglement that cannot be obtained through Gaussian operations. We also introduce a tensor decomposition, that generalizes our method and allows to construct optimal circuits for particular classes of states. As a benchmark, we numerically evaluate our method and compare its performance with state-of-the art results, confirming 100% fidelity on different classes of states.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2507.19396v1" target="_blank">Detection of Adverse Drug Events in Dutch clinical free text documents using Transformer Models: benchmark study</a></h3>
                    <p><strong>Authors:</strong> Rachel M. Murphy, Nishant Mishra, Nicolette F. de Keizer, Dave A. Dongelmans, Kitty J. Jager, Ameen Abu-Hanna, Joanna E. Klopotowska, Iacer Calixto</p>
                    <p><strong>Published:</strong> 7/25/2025</p>
                    <p><strong>Categories:</strong> cs.CL</p>
                    <p><strong>Summary:</strong> In this study, we set a benchmark for adverse drug event (ADE) detection in Dutch clinical free text documents using several transformer models, clinical scenarios and fit-for-purpose performance measures. We trained a Bidirectional Long Short-Term Memory (Bi-LSTM) model and four transformer-based Dutch and/or multilingual encoder models (BERTje, RobBERT, MedRoBERTa.nl, and NuNER) for the tasks of named entity recognition (NER) and relation classification (RC) using 102 richly annotated Dutch ICU clinical progress notes. Anonymized free text clinical progress notes of patients admitted to intensive care unit (ICU) of one academic hospital and discharge letters of patients admitted to Internal Medicine wards of two non-academic hospitals were reused. We evaluated our ADE RC models internally using gold standard (two-step task) and predicted entities (end-to-end task). In addition, all models were externally validated on detecting ADEs at the document level. We report both micro- and macro-averaged F1 scores, given the imbalance of ADEs in the datasets. Although differences for the ADE RC task between the models were small, MedRoBERTa.nl was the best performing model with macro-averaged F1 score of 0.63 using gold standard and 0.62 using predicted entities. The MedRoBERTa.nl models also performed the best in our external validation and achieved recall of between 0.67 to 0.74 using predicted entities, meaning between 67 to 74% of discharge letters with ADEs were detected. Our benchmark study presents a robust and clinically meaningful approach for evaluating language models for ADE detection in clinical free text documents. Our study highlights the need to use appropriate performance measures fit for the task of ADE detection in clinical free-text documents and envisioned future clinical use.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2507.19392v1" target="_blank">Measurement and Qualitative Explanation of Decay Lengths of Attractive and Repulsive Forces between Natural and Artificial Atoms</a></h3>
                    <p><strong>Authors:</strong> Marco Weiss, Fabian Stilp, Max Reinhart, Franz J. Giessibl</p>
                    <p><strong>Published:</strong> 7/25/2025</p>
                    <p><strong>Categories:</strong> cond-mat.mes-hall</p>
                    <p><strong>Summary:</strong> Artificial atoms, such as quantum corrals, offer an excellent platform to study fundamental interactions between localized quantum states and nanoscale probes. We performed atomic force microscopy measurements inside square quantum corrals on Cu(111) using CO- and metal-terminated tips. Using chemically unreactive CO-terminated tips repulsive Pauli forces can be probed, while metallic tips are attracted to the localized quantum states due to chemical bonding. We found distinct exponential decay constants of 46 pm for the repulsive and 66 pm for the attractive forces. Attractive and repulsive interactions between two natural atoms show significantly shorter decay lengths. While natural atoms feature states with a broad range of decay lengths, including very short ones from deeply bound states, quantum corrals are lacking such deeply bound and highly localized states, resulting in longer decay lengths. These results offer a new route to understand and design atomic-scale interactions in low-dimensional quantum structures and devices.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2507.19388v1" target="_blank">A novel multi-thickness topology optimization method for balancing structural performance and manufacturability</a></h3>
                    <p><strong>Authors:</strong> Gabriel Stankiewicz, Chaitanya Dev, Paul Steinmann</p>
                    <p><strong>Published:</strong> 7/25/2025</p>
                    <p><strong>Categories:</strong> cs.CE</p>
                    <p><strong>Summary:</strong> Topology optimization (TO) in two dimensions often presents a trade-off between structural performance and manufacturability, with unpenalized (variable-thickness) methods yielding superior but complex designs, and penalized (SIMP) methods producing simpler, truss-like structures with compromised performance. This paper introduces a multi-thickness, density-based topology optimization method designed to bridge this gap. The proposed approach guides the design towards a predefined set of discrete, allowable thicknesses by employing a novel multilevel penalization scheme and a multilevel smoothed Heaviside projection. A continuation strategy for the penalization and projection parameters, combined with an adaptive mesh refinement technique, ensures robust convergence and high-resolution geometric features. The method is validated on standard cantilever and MBB beam benchmarks. Results demonstrate that as the number of allowable thicknesses increases, the designs systematically transition from conventional truss-like structures to high-performance, sheet-like structures. Notably, designs with as few as three discrete thickness levels achieve compliance values within 2\% of those from fully unpenalized, variable-thickness optimization, while significantly outperforming standard SIMP results. The method inherently eliminates impractically thin regions and features, both in the out-of-plane and in-plane directions and produces designs well-suited for both additive manufacturing and conventional fabrication using standard-thickness stock materials, thus maximizing both performance and manufacturability.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2507.19386v1" target="_blank">An RFSoC-based F-engine for ARGOS</a></h3>
                    <p><strong>Authors:</strong> Yunpeng Men, Ewan Barr, Amit Bansod, Weiwei Chen, Jason Wu, John Antoniadis, Jan Behrend, Niclas Esser, Oliver Polch, Gundolf Wieching, Tobias Winchen</p>
                    <p><strong>Published:</strong> 7/25/2025</p>
                    <p><strong>Categories:</strong> astro-ph.IM</p>
                    <p><strong>Summary:</strong> Radio interferometers provide the means to perform the wide-field-of-view (FoV), high-sensitivity observations required for modern radio surveys. As computing power per cost has decreased, there has been a move towards larger arrays of smaller dishes, such as DSA-2000, the upcoming HIRAX, CHORD and SKA radio telescopes. Such arrays can have simpler receiver designs with room-temperature low-noise amplifiers and direct sampling to achieve, greatly reducing the cost per antenna. The ARGOS project is currently developing an array of five 6-meter antennas that will be used to demonstrate the technology required for a next generation small-D, big-N radio interferometer in Europe. In this work, our objective was to implement a first-stage digital signal processing system for the ARGOS demonstrator array, providing digitization, channelization, delay correction and frequency-dependent complex gain correction. The system is intended to produce delay and phase corrected dual-polarization channelized voltages in the frequency range 1-3 GHz with a nominal channel bandwidth of 1 MHz. We use an RFSoC 4x2 evaluation board with four analog-to-digital converters (ADCs) that can simultaneously sample two 1 GHz, dual-polarization bands. We use Xilinx Vitis HLS C++ to develop the required firmware as a set of customizable modules suitable for rapid prototyping. We performed hardware verification of the channel response of the critically sampled PFB and of the delay correction, showing both to be consistent with theoretical expectations. Furthermore, the board was installed at the Effelsberg 100-meter radio telescope where we performed commensal pulsar observations with the Effelsberg Direct Digitization backend, showing comparable performance. This work demonstrates the utility of high-level synthesis (HLS) languages in the development of high performance radio astronomy processing backends.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2507.19383v1" target="_blank">Quantum Algorithm for Protein Side-Chain Optimisation: Comparing Quantum to Classical Methods</a></h3>
                    <p><strong>Authors:</strong> Anastasia Agathangelou, Dilhan Manawadu, Ivano Tavernelli</p>
                    <p><strong>Published:</strong> 7/25/2025</p>
                    <p><strong>Categories:</strong> quant-ph</p>
                    <p><strong>Summary:</strong> Modelling and predicting protein configurations is crucial for advancing drug discovery, enabling the design of treatments for life-threatening diseases. A critical aspect of this challenge is rotamer optimisation - the determination of optimal side-chain conformations given a fixed protein backbone. This problem, involving the internal degrees of freedom of amino acid side-chains, significantly influences the proteins overall structure and function. In this work, we develop a resource-efficient optimisation algorithm to compute the ground state energy of protein structures, with a focus on side-chain configuration. We formulate the rotamer optimisation problem as a Quadratic Unconstrained Binary Optimisation problem and map it to an Ising model, enabling efficient quantum encoding. Building on this formulation, we propose a quantum algorithm based on the Quantum Approximate Optimisation Algorithm to explore the conformational space and identify low-energy configurations. To benchmark our approach, we conduct a classical study using custom-built libraries tailored for structural characterisation and energy optimisation. Our quantum method demonstrates a reduction in computational cost compared to classical simulated annealing techniques, offering a scalable and promising framework for protein structure optimisation in the quantum era.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2507.19382v1" target="_blank">Learning Long-Range Representations with Equivariant Messages</a></h3>
                    <p><strong>Authors:</strong> Egor Rumiantsev, Marcel F. Langer, Tulga-Erdene Sodjargal, Michele Ceriotti, Philip Loche</p>
                    <p><strong>Published:</strong> 7/25/2025</p>
                    <p><strong>Categories:</strong> physics.chem-ph</p>
                    <p><strong>Summary:</strong> Machine learning interatomic potentials trained on first-principles reference data are quickly becoming indispensable for computational physics, biology, and chemistry. Equivariant message-passing neural networks, including transformers, are considered state-of-the-art for this task. Since applications require efficient scaling with system size, such models cannot act on fully connected atomistic graphs and thus neglect interactions beyond a certain cutoff, consequently failing to model long-range effects like electrostatics, dispersion, or electron delocalization. While long-range correction schemes based on inverse power laws of interatomic distances have been proposed, they are unable to communicate higher-order geometric information and are thus limited in applicability. To address this shortcoming, we propose the use of equivariant, rather than scalar, charges for long-range interactions, and design a graph neural network architecture, LOREM, around this long-range message passing mechanism. Through tests on a number of long-range datasets, we confirm that equivariant charges enable the learning of orientation-dependent interactions, and that the proposed model is competitive with, or surpasses, other approaches. Moreover, LOREM does not require adapting interaction cutoffs or the number of message passing steps to model long-range interactions, which contributes to its robustness across different systems.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2507.19381v1" target="_blank">Gaps in binary cyclotomic polynomials</a></h3>
                    <p><strong>Authors:</strong> Antonio Cafure, Eda Cesaratto</p>
                    <p><strong>Published:</strong> 7/25/2025</p>
                    <p><strong>Categories:</strong> math.NT, 11C08 11B83 68R15</p>
                    <p><strong>Summary:</strong> The study of gaps arising in binary cyclotomic polynomials $\Phi_{pq}$ is the interest of this paper. We compute the second gap of $\Phi_{pq}$. When $q$ is congruent to $\pm 1$ modulo $p$, we provide the number of gaps for each possible length.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2507.19380v1" target="_blank">Dynamical theory for spherical black holes in modified gravity</a></h3>
                    <p><strong>Authors:</strong> Asier Alonso-Bardaji, David Brizuela</p>
                    <p><strong>Published:</strong> 7/25/2025</p>
                    <p><strong>Categories:</strong> gr-qc</p>
                    <p><strong>Summary:</strong> We provide a general algorithm to construct a Hamiltonian, such that its dynamical flow covariantly defines any given spherically symmetric and static metric. This Hamiltonian is defined as a linear combination of the standard (general relativistic) radial diffeomorphism constraint plus a Hamiltonian constraint that is appropriately deformed as compared to its corresponding form in general relativity though it does not include higher-derivative terms. Therefore, given a static model of spherical gravity, it is possible to obtain its Hamiltonian, and, thus, its canonical (second-order) equations of motion. A particularly relevant application of this construction is the study of regular black holes, where proposed geometries often lack an underlying dynamical theory. The present method provides such a theory. In particular, for a wide class of deformations of the Schwarzschild geometry, we explicitly obtain their corresponding Hamiltonian. This construction can be further used to covariantly couple matter. In this way, one can analyze the backreaction of matter fields on the geometry of interest, and, specifically, whether a particular black-hole model may emerge as the end state of a dynamical collapse.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2507.19376v1" target="_blank">Archiverse: an Approach for Immersive Cultural Heritage</a></h3>
                    <p><strong>Authors:</strong> Wieslaw Kopeƒá, Anna Jaskulska, W≈Çadys≈Çaw Fuchs, Wiktor Stawski, Stanis≈Çaw Knapi≈Ñski, Barbara Karpowicz, Rafa≈Ç Mas≈Çyk</p>
                    <p><strong>Published:</strong> 7/25/2025</p>
                    <p><strong>Categories:</strong> cs.HC, cs.CY</p>
                    <p><strong>Summary:</strong> Digital technologies and tools have transformed the way we can study cultural heritage and the way we can recreate it digitally. Techniques such as laser scanning, photogrammetry, and a variety of Mixed Reality solutions have enabled researchers to examine cultural objects and artifacts more precisely and from new perspectives. In this part of the panel, we explore how Virtual Reality (VR) and eXtended Reality (XR) can serve as tools to recreate and visualize the remains of historical cultural heritage and experience it in simulations of its original complexity, which means immersive and interactive. Visualization of material culture exemplified by archaeological sites and architecture can be particularly useful when only ruins or archaeological remains survive. However, these advancements also bring significant challenges, especially in the area of transdisciplinary cooperation between specialists from many, often distant, fields, and the dissemination of virtual immersive environments among both professionals and the general public.</p>
                
            
        
        
            <p><em>Generated by AI News Agent</em></p>
        
    

