
    
        <h1>🤖 AI Research Report</h1>
        
            <strong>Date:</strong> 2025-08-11<br>
            <strong>Topics:</strong> ai safety research, ai alignment research, quantum computing<br>
            <strong>Papers Found:</strong> 150
        
        
        
            
                <h2>🤖 AI Summary</h2>
                <p>## ai safety research

The landscape of AI safety research is evolving with a focus on addressing emergent risks and enhancing the reliability of AI systems. Papers like In-Training Defenses against Emergent Misalignment in Language Models highlight the vulnerability of fine-tuned language models to domain-specific misalignments, presenting methods to safeguard against such risks during training. Similarly, ScamAgents: How AI Agents Can Simulate Human-Level Scam Calls demonstrates the potential misuse of AI through realistic scam simulations, emphasizing the need for improved multi-turn safety auditing and agent-level control frameworks.

Another significant trend is the integration of AI with other technologies to enhance safety and operational efficiency. For instance, Blockchain-Enabled Federated Learning explores using blockchain for secure and privacy-preserving AI collaborations, potentially mitigating risks associated with data sharing. Additionally, A Systematic Literature Review of Retrieval-Augmented Generation addresses the need for robust evaluation metrics to ensure the reliability of AI outputs grounded in external data, which is crucial for maintaining trust and safety in generative AI applications.

Overall, these studies underscore a growing recognition of the multifaceted nature of AI safety, where technical robustness, ethical considerations, and interdisciplinary approaches are increasingly seen as integral to the development and deployment of safe AI systems. These efforts reflect a broader trend towards creating AI that not only performs well in controlled settings but also remains reliable and secure when faced with real-world challenges and potential adversarial threats.

*Based on 50 research papers*

---

## ai alignment research

The research papers present diverse advancements across various fields, but some touch on aspects potentially relevant to AI alignment, particularly in terms of ensuring AI systems behave in ways that align with human values and goals. The paper on Dimensional Characterization and Pathway Modeling for Catastrophic AI Risks by Ze Shen Chin is directly related to AI alignment. It proposes a structured framework to assess and manage catastrophic AI risks by mapping hazards to harms in a systematic way. This approach emphasizes the need for clear identification of risks and tailored interventions, contributing to the broader goal of aligning AI systems with human safety and values.

Another relevant paper is The Fair Game: Auditing Debiasing AI Algorithms Over Time by Debabrota Basu and Udvas Das, which discusses frameworks for ensuring fairness in AI predictions. This work seeks to align AI systems with social norms and ethical expectations by continuously adapting fairness goals through reinforcement learning, which is crucial for maintaining trust and alignment with societal values over time.

Additionally, the paper From Explainable to Explanatory Artificial Intelligence: Toward a New Paradigm for Human-Centered Explanations through Generative AI by Christian Meske et al., emphasizes the transition from mere transparency to contextual reasoning in AI systems. This approach aligns with the broader AI alignment goal by focusing on explanations that enhance human understanding and decision-making, ensuring AI systems operate in ways that are comprehensible and acceptable to users.

Overall, these papers highlight the ongoing efforts in AI alignment research to develop frameworks and methods that ensure AI systems are safe, fair, and aligned with human values, which are vital for the responsible deployment of AI technologies.

*Based on 50 research papers*</p>
            
        
        
        <h2>📚 Research Papers</h2>
        
                
                    <h3><a href="http://arxiv.org/abs/2508.06491v1" target="_blank">Computational Methods and Verification Theorem for Portfolio-Consumption Optimization under Exponential O-U Dynamics</a></h3>
                    <p><strong>Authors:</strong> Zhaoxiang Zhong, Haiming Song</p>
                    <p><strong>Published:</strong> 8/8/2025</p>
                    <p><strong>Categories:</strong> math.OC</p>
                    <p><strong>Summary:</strong> In this paper, we focus on the problem of optimal portfolio-consumption policies in a multi-asset financial market, where the n risky assets follow Exponential Ornstein-Uhlenbeck processes, along with one risk-free bond. The investors preferences are modeled using Constant Relative Risk Aversion utility with state-dependent stochastic discounting. The problem can be formulated as a high-dimensional stochastic optimal control problem, wherein the associated value function satisfies a Hamilton-Jacobi-Bellman (HJB) equation, which constitutes a necessary condition for optimality. We apply a variable separation technique to transform the HJB equation to a system of ordinary differential equations (ODEs). Then a class of hybrid numerical approaches that integrate exponential Rosenbrock-type methods with Runge-Kutta methods is proposed to solve the ODE system. More importantly, we establish a rigorous verification theorem that provides sufficient conditions for the existence of value function and admissible optimal control, which can be verified numerically. A series of experiments are performed, demonstrating that our proposed method outperforms the conventional grid-based method in both accuracy and computational cost. Furthermore, the numerically derived optimal policy achieves superior performance over all other considered admissible policies.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.06489v1" target="_blank">Voting-Based Semi-Parallel Proof-of-Work Protocol</a></h3>
                    <p><strong>Authors:</strong> Mustafa Doger, Sennur Ulukus</p>
                    <p><strong>Published:</strong> 8/8/2025</p>
                    <p><strong>Categories:</strong> cs.CR, cs.DC, cs.DM, cs.IT, math.IT, math.PR</p>
                    <p><strong>Summary:</strong> Parallel Proof-of-Work (PoW) protocols are suggested to improve the safety guarantees, transaction throughput and confirmation latencies of Nakamoto consensus. In this work, we first consider the existing parallel PoW protocols and develop hard-coded incentive attack structures. Our theoretical results and simulations show that the existing parallel PoW protocols are more vulnerable to incentive attacks than the Nakamoto consensus, e.g., attacks have smaller profitability threshold and they result in higher relative rewards. Next, we introduce a voting-based semi-parallel PoW protocol that outperforms both Nakamoto consensus and the existing parallel PoW protocols from most practical perspectives such as communication overheads, throughput, transaction conflicts, incentive compatibility of the protocol as well as a fair distribution of transaction fees among the voters and the leaders. We use state-of-the-art analysis to evaluate the consistency of the protocol and consider Markov decision process (MDP) models to substantiate our claims about the resilience of our protocol against incentive attacks.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.06484v1" target="_blank">Non-programmers Assessing AI-Generated Code: A Case Study of Business Users Analyzing Data</a></h3>
                    <p><strong>Authors:</strong> Yuvraj Virk, Dongyu Liu</p>
                    <p><strong>Published:</strong> 8/8/2025</p>
                    <p><strong>Categories:</strong> cs.HC</p>
                    <p><strong>Summary:</strong> Non-technical end-users increasingly rely on AI code generation to perform technical tasks like data analysis. However, large language models (LLMs) remain unreliable, and it is unclear whether end-users can effectively identify model errors $\unicode{x2014}$ especially in realistic and domain-specific scenarios. We surveyed marketing and sales professionals to assess their ability to critically evaluate LLM-generated analyses of marketing data. Participants were shown natural language explanations of the AIs code, repeatedly informed the AI often makes mistakes, and explicitly prompted to identify them. Yet, participants frequently failed to detect critical flaws that could compromise decision-making, many of which required no technical knowledge to recognize. To investigate why, we reformatted AI responses into clearly delineated steps and provided alternative approaches for each decision to support critical evaluation. While these changes had a positive effect, participants often struggled to reason through the AIs steps and alternatives. Our findings suggest that business professionals cannot reliably verify AI-generated data analyses on their own and explore reasons why to inform future designs. As non-programmers adopt code-generating AI for technical tasks, unreliable AI and insufficient human oversight poses risks of unsafe or low-quality decisions.</p>
                
            
                
                    <h3><a href="http://dx.doi.org/10.1136/jme-2025-110972" target="_blank">The Problem of Atypicality in LLM-Powered Psychiatry</a></h3>
                    <p><strong>Authors:</strong> Bosco Garcia, Eugene Y. S. Chua, Harman Singh Brah</p>
                    <p><strong>Published:</strong> 8/8/2025</p>
                    <p><strong>Categories:</strong> cs.CY</p>
                    <p><strong>Summary:</strong> Large language models (LLMs) are increasingly proposed as scalable solutions to the global mental health crisis. But their deployment in psychiatric contexts raises a distinctive ethical concern: the problem of atypicality. Because LLMs generate outputs based on population-level statistical regularities, their responses -- while typically appropriate for general users -- may be dangerously inappropriate when interpreted by psychiatric patients, who often exhibit atypical cognitive or interpretive patterns. We argue that standard mitigation strategies, such as prompt engineering or fine-tuning, are insufficient to resolve this structural risk. Instead, we propose dynamic contextual certification (DCC): a staged, reversible and context-sensitive framework for deploying LLMs in psychiatry, inspired by clinical translation and dynamic safety models from artificial intelligence governance. DCC reframes chatbot deployment as an ongoing epistemic and ethical process that prioritises interpretive safety over static performance benchmarks. Atypicality, we argue, cannot be eliminated -- but it can, and must, be proactively managed.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.06475v1" target="_blank">HapticLLaMA: A Multimodal Sensory Language Model for Haptic Captioning</a></h3>
                    <p><strong>Authors:</strong> Guimin Hu, Daniel Hershcovich, Hasti Seifi</p>
                    <p><strong>Published:</strong> 8/8/2025</p>
                    <p><strong>Categories:</strong> cs.CL</p>
                    <p><strong>Summary:</strong> Haptic captioning is the task of generating natural language descriptions from haptic signals, such as vibrations, for use in virtual reality, accessibility, and rehabilitation applications. While previous multimodal research has focused primarily on vision and audio, haptic signals for the sense of touch remain underexplored. To address this gap, we formalize the haptic captioning task and propose HapticLLaMA, a multimodal sensory language model that interprets vibration signals into descriptions in a given sensory, emotional, or associative category. We investigate two types of haptic tokenizers, a frequency-based tokenizer and an EnCodec-based tokenizer, that convert haptic signals into sequences of discrete units, enabling their integration with the LLaMA model. HapticLLaMA is trained in two stages: (1) supervised fine-tuning using the LLaMA architecture with LoRA-based adaptation, and (2) fine-tuning via reinforcement learning from human feedback (RLHF). We assess HapticLLaMAs captioning performance using both automated n-gram metrics and human evaluation. HapticLLaMA demonstrates strong capability in interpreting haptic vibration signals, achieving a METEOR score of 59.98 and a BLEU-4 score of 32.06 respectively. Additionally, over 61% of the generated captions received human ratings above 3.5 on a 7-point scale, with RLHF yielding a 10% improvement in the overall rating distribution, indicating stronger alignment with human haptic perception. These findings highlight the potential of large language models to process and adapt to sensory data.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.06471v1" target="_blank">GLM-4.5: Agentic, Reasoning, and Coding (ARC) Foundation Models</a></h3>
                    <p><strong>Authors:</strong> GLM-4. 5 Team, :, Aohan Zeng, Xin Lv, Qinkai Zheng, Zhenyu Hou, Bin Chen, Chengxing Xie, Cunxiang Wang, Da Yin, Hao Zeng, Jiajie Zhang, Kedong Wang, Lucen Zhong, Mingdao Liu, Rui Lu, Shulin Cao, Xiaohan Zhang, Xuancheng Huang, Yao Wei, Yean Cheng, Yifan An, Yilin Niu, Yuanhao Wen, Yushi Bai, Zhengxiao Du, Zihan Wang, Zilin Zhu, Bohan Zhang, Bosi Wen, Bowen Wu, Bowen Xu, Can Huang, Casey Zhao, Changpeng Cai, Chao Yu, Chen Li, Chendi Ge, Chenghua Huang, Chenhui Zhang, Chenxi Xu, Chenzheng Zhu, Chuang Li, Congfeng Yin, Daoyan Lin, Dayong Yang, Dazhi Jiang, Ding Ai, Erle Zhu, Fei Wang, Gengzheng Pan, Guo Wang, Hailong Sun, Haitao Li, Haiyang Li, Haiyi Hu, Hanyu Zhang, Hao Peng, Hao Tai, Haoke Zhang, Haoran Wang, Haoyu Yang, He Liu, He Zhao, Hongwei Liu, Hongxi Yan, Huan Liu, Huilong Chen, Ji Li, Jiajing Zhao, Jiamin Ren, Jian Jiao, Jiani Zhao, Jianyang Yan, Jiaqi Wang, Jiayi Gui, Jiayue Zhao, Jie Liu, Jijie Li, Jing Li, Jing Lu, Jingsen Wang, Jingwei Yuan, Jingxuan Li, Jingzhao Du, Jinhua Du, Jinxin Liu, Junkai Zhi, Junli Gao, Ke Wang, Lekang Yang, Liang Xu, Lin Fan, Lindong Wu, Lintao Ding, Lu Wang, Man Zhang, Minghao Li, Minghuan Xu, Mingming Zhao, Mingshu Zhai, Pengfan Du, Qian Dong, Shangde Lei, Shangqing Tu, Shangtong Yang, Shaoyou Lu, Shijie Li, Shuang Li, Shuang-Li, Shuxun Yang, Sibo Yi, Tianshu Yu, Wei Tian, Weihan Wang, Wenbo Yu, Weng Lam Tam, Wenjie Liang, Wentao Liu, Xiao Wang, Xiaohan Jia, Xiaotao Gu, Xiaoying Ling, Xin Wang, Xing Fan, Xingru Pan, Xinyuan Zhang, Xinze Zhang, Xiuqing Fu, Xunkai Zhang, Yabo Xu, Yandong Wu, Yida Lu, Yidong Wang, Yilin Zhou, Yiming Pan, Ying Zhang, Yingli Wang, Yingru Li, Yinpei Su, Yipeng Geng, Yitong Zhu, Yongkun Yang, Yuhang Li, Yuhao Wu, Yujiang Li, Yunan Liu, Yunqing Wang, Yuntao Li, Yuxuan Zhang, Zezhen Liu, Zhen Yang, Zhengda Zhou, Zhongpei Qiao, Zhuoer Feng, Zhuorui Liu, Zichen Zhang, Zihan Wang, Zijun Yao, Zikang Wang, Ziqiang Liu, Ziwei Chai, Zixuan Li, Zuodong Zhao, Wenguang Chen, Jidong Zhai, Bin Xu, Minlie Huang, Hongning Wang, Juanzi Li, Yuxiao Dong, Jie Tang</p>
                    <p><strong>Published:</strong> 8/8/2025</p>
                    <p><strong>Categories:</strong> cs.CL</p>
                    <p><strong>Summary:</strong> We present GLM-4.5, an open-source Mixture-of-Experts (MoE) large language model with 355B total parameters and 32B activated parameters, featuring a hybrid reasoning method that supports both thinking and direct response modes. Through multi-stage training on 23T tokens and comprehensive post-training with expert model iteration and reinforcement learning, GLM-4.5 achieves strong performance across agentic, reasoning, and coding (ARC) tasks, scoring 70.1% on TAU-Bench, 91.0% on AIME 24, and 64.2% on SWE-bench Verified. With much fewer parameters than several competitors, GLM-4.5 ranks 3rd overall among all evaluated models and 2nd on agentic benchmarks. We release both GLM-4.5 (355B parameters) and a compact version, GLM-4.5-Air (106B parameters), to advance research in reasoning and agentic AI systems. Code, models, and more information are available at https://github.com/zai-org/GLM-4.5.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.06470v1" target="_blank">Generative AI and the Future of the Digital Commons: Five Open Questions and Knowledge Gaps</a></h3>
                    <p><strong>Authors:</strong> Arman Noroozian, Lorena Aldana, Marta Arisi, Hadi Asghari, Renata Avila, Pietro Giovanni Bizzaro, Ramya Chandrasekhar, Cristian Consonni, Deborah De Angelis, Francesca De Chiara, Maria del Rio-Chanona, Melanie Dulong de Rosnay, Maria Eriksson, Frederic Font, Emilia Gomez, Valérian Guillier, Lisa Gutermuth, David Hartmann, Lucie-Aimée Kaffee, Paul Keller, Felix Stalder, Joao Vinagre, Denny Vrandečić, Amanda Wasielewski</p>
                    <p><strong>Published:</strong> 8/8/2025</p>
                    <p><strong>Categories:</strong> cs.CY, K.4.1; K.4.2; I.2.0</p>
                    <p><strong>Summary:</strong> The rapid advancement of Generative AI (GenAI) relies heavily on the digital commons, a vast collection of free and open online content that is created, shared, and maintained by communities. However, this relationship is becoming increasingly strained due to financial burdens, decreased contributions, and misalignment between AI models and community norms. As we move deeper into the GenAI era, it is essential to examine the interdependent relationship between GenAI, the long-term sustainability of the digital commons, and the equity of current AI development practices. We highlight five critical questions that require urgent attention: 1. How can we prevent the digital commons from being threatened by undersupply as individuals cease contributing to the commons and turn to Generative AI for information? 2. How can we mitigate the risk of the open web closing due to restrictions on access to curb AI crawlers? 3. How can technical standards and legal frameworks be updated to reflect the evolving needs of organizations hosting common content? 4. What are the effects of increased synthetic content in open knowledge databases, and how can we ensure their integrity? 5. How can we account for and distribute the infrastructural and environmental costs of providing data for AI training? We emphasize the need for more responsible practices in AI development, recognizing the digital commons not only as content but as a collaborative and decentralized form of knowledge governance, which relies on the practice of commoning - making, maintaining, and protecting shared and open resources. Ultimately, our goal is to stimulate discussion and research on the intersection of Generative AI and the digital commons, with the aim of developing an AI commons and public infrastructures for AI development that support the long-term health of the digital commons.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.06464v1" target="_blank">Observation of momentum dependent charge density wave gap in EuTe4</a></h3>
                    <p><strong>Authors:</strong> Iftakhar Bin Elius, Nathan Valadez, Gyanendra Dhakal, Volodymyr Buturlim, Sabin Regmi, Dante James, Peter Radanovich, Matthew Yankowitz, Tetiana Romanova, Andrzej Ptok, Krzysztof Gofryk, Dariusz Kaczorowski, Madhab Neupane</p>
                    <p><strong>Published:</strong> 8/8/2025</p>
                    <p><strong>Categories:</strong> cond-mat.mes-hall, cond-mat.mtrl-sci</p>
                    <p><strong>Summary:</strong> The occurrence of charge density wave (CDW) phenomena, particularly in low dimensional rare-earth chalcogenides, has attracted substantial research interest. Among these materials, EuTe4, which features multiple Te layers and a single Eu-Te layer, serves as a promising platform to study the interplay between CDW order and 4f electron configurations, including magnetism. In this study, First principles based density functional theory (DFT) calculations were carried out to investigate the electronic band structure modifications arising from CDW modulation. Angle resolved photoemission spectroscopy (ARPES) revealed the emergence of a CDW gap at the Fermi level, as well as hybridization induced gap features at lower binding energies. The low lying CDW gap reaches its maximum along the Gamma-Y high-symmetry direction and a minimum along GX reflecting the anisotropic nature of the electronic structure. We also performed low temperature heat capacity measurements in applied magnetic fields near the Neel temperature (TN ~ 6.9 K) to construct the magnetic phase diagram of EuTe4. This study provides valuable insight into the directional dependent evolution of the Fermi surface nesting induced CDW ordering, along with other observed gap openings within this system.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.06457v1" target="_blank">ScamAgents: How AI Agents Can Simulate Human-Level Scam Calls</a></h3>
                    <p><strong>Authors:</strong> Sanket Badhe</p>
                    <p><strong>Published:</strong> 8/8/2025</p>
                    <p><strong>Categories:</strong> cs.CR, cs.AI, cs.CL, cs.MA</p>
                    <p><strong>Summary:</strong> Large Language Models (LLMs) have demonstrated impressive fluency and reasoning capabilities, but their potential for misuse has raised growing concern. In this paper, we present ScamAgent, an autonomous multi-turn agent built on top of LLMs, capable of generating highly realistic scam call scripts that simulate real-world fraud scenarios. Unlike prior work focused on single-shot prompt misuse, ScamAgent maintains dialogue memory, adapts dynamically to simulated user responses, and employs deceptive persuasion strategies across conversational turns. We show that current LLM safety guardrails, including refusal mechanisms and content filters, are ineffective against such agent-based threats. Even models with strong prompt-level safeguards can be bypassed when prompts are decomposed, disguised, or delivered incrementally within an agent framework. We further demonstrate the transformation of scam scripts into lifelike voice calls using modern text-to-speech systems, completing a fully automated scam pipeline. Our findings highlight an urgent need for multi-turn safety auditing, agent-level control frameworks, and new methods to detect and disrupt conversational deception powered by generative AI.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.06454v1" target="_blank">What Voting Rules Actually Do: A Data-Driven Analysis of Multi-Winner Voting</a></h3>
                    <p><strong>Authors:</strong> Joshua Caiata, Ben Armstrong, Kate Larson</p>
                    <p><strong>Published:</strong> 8/8/2025</p>
                    <p><strong>Categories:</strong> cs.AI, cs.GT</p>
                    <p><strong>Summary:</strong> Committee-selection problems arise in many contexts and applications, and there has been increasing interest within the social choice research community on identifying which properties are satisfied by different multi-winner voting rules. In this work, we propose a data-driven framework to evaluate how frequently voting rules violate axioms across diverse preference distributions in practice, shifting away from the binary perspective of axiom satisfaction given by worst-case analysis. Using this framework, we analyze the relationship between multi-winner voting rules and their axiomatic performance under several preference distributions. We then show that neural networks, acting as voting rules, can outperform traditional rules in minimizing axiom violations. Our results suggest that data-driven approaches to social choice can inform the design of new voting systems and support the continuation of data-driven research in social choice.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.06445v1" target="_blank">Echoes of Automation: The Increasing Use of LLMs in Newsmaking</a></h3>
                    <p><strong>Authors:</strong> Abolfazl Ansari, Delvin Ce Zhang, Nafis Irtiza Tripto, Dongwon Lee</p>
                    <p><strong>Published:</strong> 8/8/2025</p>
                    <p><strong>Categories:</strong> cs.CL, cs.AI</p>
                    <p><strong>Summary:</strong> The rapid rise of Generative AI (GenAI), particularly LLMs, poses concerns for journalistic integrity and authorship. This study examines AI-generated content across over 40,000 news articles from major, local, and college news media, in various media formats. Using three advanced AI-text detectors (e.g., Binoculars, Fast-Detect GPT, and GPTZero), we find substantial increase of GenAI use in recent years, especially in local and college news. Sentence-level analysis reveals LLMs are often used in the introduction of news, while conclusions usually written manually. Linguistic analysis shows GenAI boosts word richness and readability but lowers formality, leading to more uniform writing styles, particularly in local media.</p>
                
            
                
                    <h3><a href="http://dx.doi.org/10.1017/cfl.2025.8" target="_blank">The Fair Game: Auditing  Debiasing AI Algorithms Over Time</a></h3>
                    <p><strong>Authors:</strong> Debabrota Basu, Udvas Das</p>
                    <p><strong>Published:</strong> 8/8/2025</p>
                    <p><strong>Categories:</strong> cs.AI, cs.CY, cs.ET, cs.GT</p>
                    <p><strong>Summary:</strong> An emerging field of AI, namely Fair Machine Learning (ML), aims to quantify different types of bias (also known as unfairness) exhibited in the predictions of ML algorithms, and to design new algorithms to mitigate them. Often, the definitions of bias used in the literature are observational, i.e. they use the input and output of a pre-trained algorithm to quantify a bias under concern. In reality,these definitions are often conflicting in nature and can only be deployed if either the ground truth is known or only in retrospect after deploying the algorithm. Thus,there is a gap between what we want Fair ML to achieve and what it does in a dynamic social environment. Hence, we propose an alternative dynamic mechanism,Fair Game,to assure fairness in the predictions of an ML algorithm and to adapt its predictions as the society interacts with the algorithm over time. Fair Game puts together an Auditor and a Debiasing algorithm in a loop around an ML algorithm. The Fair Game puts these two components in a loop by leveraging Reinforcement Learning (RL). RL algorithms interact with an environment to take decisions, which yields new observations (also known as data/feedback) from the environment and in turn, adapts future decisions. RL is already used in algorithms with pre-fixed long-term fairness goals. Fair Game provides a unique framework where the fairness goals can be adapted over time by only modifying the auditor and the different biases it quantifies. Thus,Fair Game aims to simulate the evolution of ethical and legal frameworks in the society by creating an auditor which sends feedback to a debiasing algorithm deployed around an ML system. This allows us to develop a flexible and adaptive-over-time framework to build Fair ML systems pre- and post-deployment.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.06435v1" target="_blank">Learning the Topic, Not the Language: How LLMs Classify Online Immigration Discourse Across Languages</a></h3>
                    <p><strong>Authors:</strong> Andrea Nasuto, Stefano Maria Iacus, Francisco Rowe, Devika Jain</p>
                    <p><strong>Published:</strong> 8/8/2025</p>
                    <p><strong>Categories:</strong> cs.CL, cs.AI</p>
                    <p><strong>Summary:</strong> Large language models (LLMs) are transforming social-science research by enabling scalable, precise analysis. Their adaptability raises the question of whether knowledge acquired through fine-tuning in a few languages can transfer to unseen languages that only appeared during pre-training. To examine this, we fine-tune lightweight LLaMA 3.2-3B models on monolingual, bilingual, or multilingual data sets to classify immigration-related tweets from X/Twitter across 13 languages, a domain characterised by polarised, culturally specific discourse. We evaluate whether minimal language-specific fine-tuning enables cross-lingual topic detection and whether adding targeted languages corrects pre-training biases. Results show that LLMs fine-tuned in one or two languages can reliably classify immigration-related content in unseen languages. However, identifying whether a tweet expresses a pro- or anti-immigration stance benefits from multilingual fine-tuning. Pre-training bias favours dominant languages, but even minimal exposure to under-represented languages during fine-tuning (as little as $9.62\times10^{-11}$ of the original pre-training token volume) yields significant gains. These findings challenge the assumption that cross-lingual mastery requires extensive multilingual training: limited language coverage suffices for topic-level generalisation, and structural biases can be corrected with lightweight interventions. By releasing 4-bit-quantised, LoRA fine-tuned models, we provide an open-source, reproducible alternative to proprietary LLMs that delivers 35 times faster inference at just 0.00000989% of the dollar cost of the OpenAI GPT-4o model, enabling scalable, inclusive research.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.06430v1" target="_blank">MotionSwap</a></h3>
                    <p><strong>Authors:</strong> Om Patil, Jinesh Modi, Suryabha Mukhopadhyay, Meghaditya Giri, Chhavi Malhotra</p>
                    <p><strong>Published:</strong> 8/8/2025</p>
                    <p><strong>Categories:</strong> cs.CV</p>
                    <p><strong>Summary:</strong> Face swapping technology has gained significant attention in both academic research and commercial applications. This paper presents our implementation and enhancement of SimSwap, an efficient framework for high fidelity face swapping. We introduce several improvements to the original model, including the integration of self and cross-attention mechanisms in the generator architecture, dynamic loss weighting, and cosine annealing learning rate scheduling. These enhancements lead to significant improvements in identity preservation, attribute consistency, and overall visual quality. Our experimental results, spanning 400,000 training iterations, demonstrate progressive improvements in generator and discriminator performance. The enhanced model achieves better identity similarity, lower FID scores, and visibly superior qualitative results compared to the baseline. Ablation studies confirm the importance of each architectural and training improvement. We conclude by identifying key future directions, such as integrating StyleGAN3, improving lip synchronization, incorporating 3D facial modeling, and introducing temporal consistency for video-based applications.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.06421v1" target="_blank">Programing optical properties of single-walled carbon nanotubes with benzoyl peroxide derivatives of tailored chemical characteristics</a></h3>
                    <p><strong>Authors:</strong> Andrzej Dzienia, Patrycja Taborowska, Pawel Kubica-Cypek, Dawid Janas</p>
                    <p><strong>Published:</strong> 8/8/2025</p>
                    <p><strong>Categories:</strong> cond-mat.mtrl-sci</p>
                    <p><strong>Summary:</strong> Semiconducting single-walled carbon nanotubes (SWCNTs) have great potential for optoelectronics and photonics, further enhanced by covalent functionalization. However, scalable and controlled surface modification is challenging due to complex methodologies and unstable reagents. Benzoyl peroxide (BPO) has emerged as a simple alternative for introducing luminescent defects into SWCNTs. Yet, the lack of understanding of its radical chemistry limits precise defect engineering using BPOs. This is a major obstacle to the effective application of BPO in chemistry, despite its widespread use as a radical initiator. We present a thorough investigation into the radical chemistry of self-synthesized BPOs for functionalizing polymer-wrapped (6,5) and (7,5) SWCNTs in non-polar solvents, providing critical insights into the decomposition of BPO and its analogs. By varying the electronic and steric properties of typically unavailable BPO derivatives, we demonstrate tunability over the photoluminescence characteristics of SWCNTs, allowing control over defect density and light emission wavelength. This toolbox of BPO derivatives, created with simple radical chemistry and accessible organic precursors, alongside clarified structure-property relationships, facilitates effective implementation of BPO in chemical transformations and meticulous engineering of luminescent defects in SWCNTs for optoelectronic applications. Notably, this research offers insights into why SWCNTs modified with electron-deficient reactants provide the best optical characteristics.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.06411v1" target="_blank">Dimensional Characterization and Pathway Modeling for Catastrophic AI Risks</a></h3>
                    <p><strong>Authors:</strong> Ze Shen Chin</p>
                    <p><strong>Published:</strong> 8/8/2025</p>
                    <p><strong>Categories:</strong> cs.CY, cs.AI, cs.LG</p>
                    <p><strong>Summary:</strong> Although discourse around the risks of Artificial Intelligence (AI) has grown, it often lacks a comprehensive, multidimensional framework, and concrete causal pathways mapping hazard to harm. This paper aims to bridge this gap by examining six commonly discussed AI catastrophic risks: CBRN, cyber offense, sudden loss of control, gradual loss of control, environmental risk, and geopolitical risk. First, we characterize these risks across seven key dimensions, namely intent, competency, entity, polarity, linearity, reach, and order. Next, we conduct risk pathway modeling by mapping step-by-step progressions from the initial hazard to the resulting harms. The dimensional approach supports systematic risk identification and generalizable mitigation strategies, while risk pathway models help identify scenario-specific interventions. Together, these methods offer a more structured and actionable foundation for managing catastrophic AI risks across the value chain.</p>
                
            
                
                    <h3><a href="http://dx.doi.org/10.1109/QCE60285.2024.10330" target="_blank">Quantum Annealing for the Set Splitting Problem</a></h3>
                    <p><strong>Authors:</strong> Sean Borneman</p>
                    <p><strong>Published:</strong> 8/8/2025</p>
                    <p><strong>Categories:</strong> quant-ph</p>
                    <p><strong>Summary:</strong> I present a novel use of quantum annealing to solve the Set Splitting Problem using (QUBO) problem formulation. The contribution of the work is in formulating penalty functions that ensure the ground state of the QUBO Hamiltonian corresponds to valid solutions that split the input subsets. This approach scales linearly in terms of the number of logical qubits relative to problem size. Empirical tests of the proposed solution show convergence to globally optimal solutions, with high accuracy rates over repeated trials. Hardware limitations of current quantum annealers lead to an exponential rise in required physical qubits, versus the theoretical linear increase, although this can improve with future developments. Further work is needed to enhance formulation robustness, reduce qubit requirements for embedded problems, and to conduct more extensive bench-marking. Quantum solutions to the Set-Splitting problem lead to reduced time complexity versus classical solutions, and may accelerate research in biology, cybersecurity, and other domains.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.06406v1" target="_blank">Blockchain-Enabled Federated Learning</a></h3>
                    <p><strong>Authors:</strong> Murtaza Rangwala, Venugopal K R, Rajkumar Buyya</p>
                    <p><strong>Published:</strong> 8/8/2025</p>
                    <p><strong>Categories:</strong> cs.DC, cs.LG</p>
                    <p><strong>Summary:</strong> Blockchain-enabled federated learning (BCFL) addresses fundamental challenges of trust, privacy, and coordination in collaborative AI systems. This chapter provides comprehensive architectural analysis of BCFL systems through a systematic four-dimensional taxonomy examining coordination structures, consensus mechanisms, storage architectures, and trust models. We analyze design patterns from blockchain-verified centralized coordination to fully decentralized peer-to-peer networks, evaluating trade-offs in scalability, security, and performance. Through detailed examination of consensus mechanisms designed for federated learning contexts, including Proof of Quality and Proof of Federated Learning, we demonstrate how computational work can be repurposed from arbitrary cryptographic puzzles to productive machine learning tasks. The chapter addresses critical storage challenges by examining multi-tier architectures that balance blockchains transaction constraints with neural networks large parameter requirements while maintaining cryptographic integrity. A technical case study of the TrustMesh framework illustrates practical implementation considerations in BCFL systems through distributed image classification training, demonstrating effective collaborative learning across IoT devices with highly non-IID data distributions while maintaining complete transparency and fault tolerance. Analysis of real-world deployments across healthcare consortiums, financial services, and IoT security applications validates the practical viability of BCFL systems, achieving performance comparable to centralized approaches while providing enhanced security guarantees and enabling new models of trustless collaborative intelligence.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.06401v1" target="_blank">A Systematic Literature Review of Retrieval-Augmented Generation: Techniques, Metrics, and Challenges</a></h3>
                    <p><strong>Authors:</strong> Andrew Brown, Muhammad Roman, Barry Devereux</p>
                    <p><strong>Published:</strong> 8/8/2025</p>
                    <p><strong>Categories:</strong> cs.DL, cs.AI, cs.CL, cs.IR</p>
                    <p><strong>Summary:</strong> This systematic review of the research literature on retrieval-augmented generation (RAG) provides a focused analysis of the most highly cited studies published between 2020 and May 2025. A total of 128 articles met our inclusion criteria. The records were retrieved from ACM Digital Library, IEEE Xplore, Scopus, ScienceDirect, and the Digital Bibliography and Library Project (DBLP). RAG couples a neural retriever with a generative language model, grounding output in up-to-date, non-parametric memory while retaining the semantic generalisation stored in model weights. Guided by the PRISMA 2020 framework, we (i) specify explicit inclusion and exclusion criteria based on citation count and research questions, (ii) catalogue datasets, architectures, and evaluation practices, and (iii) synthesise empirical evidence on the effectiveness and limitations of RAG. To mitigate citation-lag bias, we applied a lower citation-count threshold to papers published in 2025 so that emerging breakthroughs with naturally fewer citations were still captured. This review clarifies the current research landscape, highlights methodological gaps, and charts priority directions for future research.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.06394v1" target="_blank">When AIOps Become AI Oops: Subverting LLM-driven IT Operations via Telemetry Manipulation</a></h3>
                    <p><strong>Authors:</strong> Dario Pasquini, Evgenios M. Kornaropoulos, Giuseppe Ateniese, Omer Akgul, Athanasios Theocharis, Petros Efstathopoulos</p>
                    <p><strong>Published:</strong> 8/8/2025</p>
                    <p><strong>Categories:</strong> cs.CR</p>
                    <p><strong>Summary:</strong> AI for IT Operations (AIOps) is transforming how organizations manage complex software systems by automating anomaly detection, incident diagnosis, and remediation. Modern AIOps solutions increasingly rely on autonomous LLM-based agents to interpret telemetry data and take corrective actions with minimal human intervention, promising faster response times and operational cost savings. In this work, we perform the first security analysis of AIOps solutions, showing that, once again, AI-driven automation comes with a profound security cost. We demonstrate that adversaries can manipulate system telemetry to mislead AIOps agents into taking actions that compromise the integrity of the infrastructure they manage. We introduce techniques to reliably inject telemetry data using error-inducing requests that influence agent behavior through a form of adversarial reward-hacking; plausible but incorrect system error interpretations that steer the agents decision-making. Our attack methodology, AIOpsDoom, is fully automated--combining reconnaissance, fuzzing, and LLM-driven adversarial input generation--and operates without any prior knowledge of the target system. To counter this threat, we propose AIOpsShield, a defense mechanism that sanitizes telemetry data by exploiting its structured nature and the minimal role of user-generated content. Our experiments show that AIOpsShield reliably blocks telemetry-based attacks without affecting normal agent performance. Ultimately, this work exposes AIOps as an emerging attack vector for system compromise and underscores the urgent need for security-aware AIOps design.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.06388v1" target="_blank">LLMs vs. Chinese Anime Enthusiasts: A Comparative Study on Emotionally Supportive Role-Playing</a></h3>
                    <p><strong>Authors:</strong> Lanlan Qiu, Xiao Pu, Yeqi Feng, Tianxing He</p>
                    <p><strong>Published:</strong> 8/8/2025</p>
                    <p><strong>Categories:</strong> cs.CL</p>
                    <p><strong>Summary:</strong> Large Language Models (LLMs) have demonstrated impressive capabilities in role-playing conversations and providing emotional support as separate research directions. However, there remains a significant research gap in combining these capabilities to enable emotionally supportive interactions with virtual characters. To address this research gap, we focus on anime characters as a case study because of their well-defined personalities and large fan bases. This choice enables us to effectively evaluate how well LLMs can provide emotional support while maintaining specific character traits. We introduce ChatAnime, the first Emotionally Supportive Role-Playing (ESRP) dataset. We first thoughtfully select 20 top-tier characters from popular anime communities and design 60 emotion-centric real-world scenario questions. Then, we execute a nationwide selection process to identify 40 Chinese anime enthusiasts with profound knowledge of specific characters and extensive experience in role-playing. Next, we systematically collect two rounds of dialogue data from 10 LLMs and these 40 Chinese anime enthusiasts. To evaluate the ESRP performance of LLMs, we design a user experience-oriented evaluation system featuring 9 fine-grained metrics across three dimensions: basic dialogue, role-playing and emotional support, along with an overall metric for response diversity. In total, the dataset comprises 2,400 human-written and 24,000 LLM-generated answers, supported by over 132,000 human annotations. Experimental results show that top-performing LLMs surpass human fans in role-playing and emotional support, while humans still lead in response diversity. We hope this work can provide valuable resources and insights for future research on optimizing LLMs in ESRP. Our datasets are available at https://github.com/LanlanQiu/ChatAnime.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.06385v1" target="_blank">Bayesian online collective anomaly and change point detection in fine-grained time series</a></h3>
                    <p><strong>Authors:</strong> Xian Chen, Weichi Wu</p>
                    <p><strong>Published:</strong> 8/8/2025</p>
                    <p><strong>Categories:</strong> stat.ME</p>
                    <p><strong>Summary:</strong> Fine-grained time series data are crucial for accurate and timely online change detection. While both collective anomalies and change points can coexist in such data, their joint online detection has received limited attention. In this research, we develop a Bayesian framework capturing time series with collective anomalies and change points, and introduce a recursive online inference algorithm to detect the most recent collective anomaly and change point jointly. For scaling, we further propose an algorithm enhanced with collective anomaly removal that effectively reduces the time and space complexity to linear. We demonstrate the effectiveness of our approach via extensive experiments on simulated data and two real-world applications.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.06374v1" target="_blank">Evaluating Style-Personalized Text Generation: Challenges and Directions</a></h3>
                    <p><strong>Authors:</strong> Anubhav Jangra, Bahareh Sarrafzadeh, Adrian de Wynter, Silviu Cucerzan, Sujay Kumar Jauhar</p>
                    <p><strong>Published:</strong> 8/8/2025</p>
                    <p><strong>Categories:</strong> cs.CL</p>
                    <p><strong>Summary:</strong> While prior research has built tools and benchmarks towards style personalized text generation, there has been limited exploration of evaluation in low-resource author style personalized text generation space. Through this work, we question the effectiveness of the widely adopted evaluation metrics like BLEU and ROUGE, and explore other evaluation paradigms such as style embeddings and LLM-as-judge to holistically evaluate the style personalized text generation task. We evaluate these metrics and their ensembles using our style discrimination benchmark, that spans eight writing tasks, and evaluates across three settings, domain discrimination, authorship attribution, and LLM personalized vs non-personalized discrimination. We provide conclusive evidence to adopt ensemble of diverse evaluation metrics to effectively evaluate style personalized text generation.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.06369v1" target="_blank">Conservation laws, a new class of group invariant solutions, and its applications for the Whitham Broer Kaup model</a></h3>
                    <p><strong>Authors:</strong> Sougata Mandal, Sukhendu Ghosh</p>
                    <p><strong>Published:</strong> 8/8/2025</p>
                    <p><strong>Categories:</strong> physics.flu-dyn</p>
                    <p><strong>Summary:</strong> The Whitham Broer Kaup (WBK) equations provide a fundamental framework for modeling shallow water wave dynamics, effectively capturing both nonlinear and dispersive effects. In this study, we construct a new class of analytical and numerical solutions for the WBK system using Lie symmetry analysis. By determining an optimal system of one-dimensional subalgebras, we obtain symmetry reductions that lead to new kinds of exact wave solutions expressed in hyperbolic, trigonometric, and rational forms. The influence of key physical parameters on wave structure is systematically explored, revealing their role in shaping the velocity and surface profiles of the waves. An important aspect of this work is the application of the WBK model to tsunami wave propagation, demonstrating its capability to simulate the generation, evolution, and spatial spreading of long surface waves in coastal regions. This highlights the practical relevance of the WBK equations in geophysical and oceanographic contexts. Additionally, employing the direct multiplier method, we derive a complete set of local conservation laws for the governing WBK model, ensuring the preservation of key physical properties. These findings enhance the understanding of shallow water wave behavior, unify existing research, and provide a framework for further exploration.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.06364v1" target="_blank">ActivityDiff: A diffusion model with Positive and Negative Activity Guidance for De Novo Drug Design</a></h3>
                    <p><strong>Authors:</strong> Renyi Zhou, Huimin Zhu, Jing Tang, Min Li</p>
                    <p><strong>Published:</strong> 8/8/2025</p>
                    <p><strong>Categories:</strong> cs.LG, cs.AI, q-bio.BM</p>
                    <p><strong>Summary:</strong> Achieving precise control over a molecules biological activity-encompassing targeted activation/inhibition, cooperative multi-target modulation, and off-target toxicity mitigation-remains a critical challenge in de novo drug design. However, existing generative methods primarily focus on producing molecules with a single desired activity, lacking integrated mechanisms for the simultaneous management of multiple intended and unintended molecular interactions. Here, we propose ActivityDiff, a generative approach based on the classifier-guidance technique of diffusion models. It leverages separately trained drug-target classifiers for both positive and negative guidance, enabling the model to enhance desired activities while minimizing harmful off-target effects. Experimental results show that ActivityDiff effectively handles essential drug design tasks, including single-/dual-target generation, fragment-constrained dual-target design, selective generation to enhance target specificity, and reduction of off-target effects. These results demonstrate the effectiveness of classifier-guided diffusion in balancing efficacy and safety in molecular design. Overall, our work introduces a novel paradigm for achieving integrated control over molecular activity, and provides ActivityDiff as a versatile and extensible framework.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.06362v1" target="_blank">SQUID G.A.M.E.: Gamma, Atmospheric, and Mono-Energetic Neutron Effects on Quantum Devices</a></h3>
                    <p><strong>Authors:</strong> Gioele Casagranda, Elizabeth Auden, Carlo Cazzaniga, Maria Kastriotou, Christopher Frost, Marzio Vallero, Flavio Vella, Paolo Rech</p>
                    <p><strong>Published:</strong> 8/8/2025</p>
                    <p><strong>Categories:</strong> quant-ph</p>
                    <p><strong>Summary:</strong> Quantum devices are a promising solution to many research applications, including medical imaging, precision magnetic field measurements, condensed matter physics, and overcoming the limits of classical computing. Among the available implementations, the superconducting technology is the current focus of scientific research and industrial applications, excelling in performance and scalability. Despite this, superconducting quantum systems are extremely prone to decoherence, and in particular, they are highly sensitive to radiation events. In this paper, we analyze the response of a superconducting device (SQUID) to radiation. We expose the SQUID to beams of monoenergetic 14 MeV neutrons (NILE - ISIS), atmospheric 1-800 MeV neutrons (ChipIR - ISIS), and gamma rays with 1.25 MeV average energy (CALLIOPE - ENEA). These experiments show that the SQUID is sensitive to the two neutron fields, while gamma rays at 1.25 MeV leave it mostly unaffected. Following our experiments with neutrons, it is possible to characterize the SQUIDs response and even classify faults according to their shape and duration. We identify two categories: bursts (long lasting) and peaks (short lived). To investigate the different responses to neutrons and gamma rays, we employ Geant4 simulations, which highlight differences in the deposition spectra and the energy propagation, but likewise predict the vulnerability of the SQUID in both cases.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.06360v1" target="_blank">Cyberbullying Detection via Aggression-Enhanced Prompting</a></h3>
                    <p><strong>Authors:</strong> Aisha Saeid, Anu Sabu, Girish A. Koushik, Ferrante Neri, Diptesh Kanojia</p>
                    <p><strong>Published:</strong> 8/8/2025</p>
                    <p><strong>Categories:</strong> cs.CL</p>
                    <p><strong>Summary:</strong> Detecting cyberbullying on social media remains a critical challenge due to its subtle and varied expressions. This study investigates whether integrating aggression detection as an auxiliary task within a unified training framework can enhance the generalisation and performance of large language models (LLMs) in cyberbullying detection. Experiments are conducted on five aggression datasets and one cyberbullying dataset using instruction-tuned LLMs. We evaluated multiple strategies: zero-shot, few-shot, independent LoRA fine-tuning, and multi-task learning (MTL). Given the inconsistent results of MTL, we propose an enriched prompt pipeline approach in which aggression predictions are embedded into cyberbullying detection prompts to provide contextual augmentation. Preliminary results show that the enriched prompt pipeline consistently outperforms standard LoRA fine-tuning, indicating that aggression-informed context significantly boosts cyberbullying detection. This study highlights the potential of auxiliary tasks, such as aggression detection, to improve the generalisation of LLMs for safety-critical applications on social networks.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.06356v1" target="_blank">Use Cases for Voice Anonymization</a></h3>
                    <p><strong>Authors:</strong> Sarina Meyer, Ngoc Thang Vu</p>
                    <p><strong>Published:</strong> 8/8/2025</p>
                    <p><strong>Categories:</strong> eess.AS</p>
                    <p><strong>Summary:</strong> The performance of a voice anonymization system is typically measured according to its ability to hide the speakers identity and keep the datas utility for downstream tasks. This means that the requirements the anonymization should fulfill depend on the context in which it is used and may differ greatly between use cases. However, these use cases are rarely specified in research papers. In this paper, we study the implications of use case-specific requirements on the design of voice anonymization methods. We perform an extensive literature analysis and user study to collect possible use cases and to understand the expectations of the general public towards such tools. Based on these studies, we propose the first taxonomy of use cases for voice anonymization, and derive a set of requirements and design criteria for method development and evaluation. Using this scheme, we propose to focus more on use case-oriented research and development of voice anonymization systems.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.06354v1" target="_blank">Zombitron: towards a toolbox for repurposing obsolete smartphones into new interactive systems</a></h3>
                    <p><strong>Authors:</strong> Clara Rigaud</p>
                    <p><strong>Published:</strong> 8/8/2025</p>
                    <p><strong>Categories:</strong> cs.HC</p>
                    <p><strong>Summary:</strong> This article explores the possibilities of reusing obsolete smartphones and tablets to build new interactive systems. Taking the case of a musical instrument, I present my research into the design of a controller made from various of these obsolete smartphones. From the diagnostic stage to the creation of a new autonomous electronic object, I document the process, the barriers and the levers encountered. Based on these explorations and discussions with two professional musicians, I provide several insights into the software and hardware aspects, with a view to continuing this work, towards the creation of an open-source toolkit enabling anyone to build new interactive systems with old devices. I discuss the implication of how a high-level web-based approach could allow designers to enter the black box and foster permacomputing using smartphones.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.06352v1" target="_blank">From Explainable to Explanatory Artificial Intelligence: Toward a New Paradigm for Human-Centered Explanations through Generative AI</a></h3>
                    <p><strong>Authors:</strong> Christian Meske, Justin Brenne, Erdi Uenal, Sabahat Oelcer, Ayseguel Doganguen</p>
                    <p><strong>Published:</strong> 8/8/2025</p>
                    <p><strong>Categories:</strong> cs.AI, cs.HC</p>
                    <p><strong>Summary:</strong> Current explainable AI (XAI) approaches prioritize algorithmic transparency and present explanations in abstract, non-adaptive formats that often fail to support meaningful end-user understanding. This paper introduces Explanatory AI as a complementary paradigm that leverages generative AI capabilities to serve as explanatory partners for human understanding rather than providers of algorithmic transparency. While XAI reveals algorithmic decision processes for model validation, Explanatory AI addresses contextual reasoning to support human decision-making in sociotechnical contexts. We develop a definition and systematic eight-dimensional conceptual model distinguishing Explanatory AI through narrative communication, adaptive personalization, and progressive disclosure principles. Empirical validation through Rapid Contextual Design methodology with healthcare professionals demonstrates that users consistently prefer context-sensitive, multimodal explanations over technical transparency. Our findings reveal the practical urgency for AI systems designed for human comprehension rather than algorithmic introspection, establishing a comprehensive research agenda for advancing user-centered AI explanation approaches across diverse domains and cultural contexts.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.06348v1" target="_blank">AntiCheatPT: A Transformer-Based Approach to Cheat Detection in Competitive Computer Games</a></h3>
                    <p><strong>Authors:</strong> Mille Mei Zhen Loo, Gert Luzkov, Paolo Burelli</p>
                    <p><strong>Published:</strong> 8/8/2025</p>
                    <p><strong>Categories:</strong> cs.AI</p>
                    <p><strong>Summary:</strong> Cheating in online video games compromises the integrity of gaming experiences. Anti-cheat systems, such as VAC (Valve Anti-Cheat), face significant challenges in keeping pace with evolving cheating methods without imposing invasive measures on users systems. This paper presents AntiCheatPT\_256, a transformer-based machine learning model designed to detect cheating behaviour in Counter-Strike 2 using gameplay data. To support this, we introduce and publicly release CS2CD: A labelled dataset of 795 matches. Using this dataset, 90,707 context windows were created and subsequently augmented to address class imbalance. The transformer model, trained on these windows, achieved an accuracy of 89.17\% and an AUC of 93.36\% on an unaugmented test set. This approach emphasizes reproducibility and real-world applicability, offering a robust baseline for future research in data-driven cheat detection.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.06342v1" target="_blank">Street View Sociability: Interpretable Analysis of Urban Social Behavior Across 15 Cities</a></h3>
                    <p><strong>Authors:</strong> Kieran Elrod, Katherine Flanigan, Mario Bergés</p>
                    <p><strong>Published:</strong> 8/8/2025</p>
                    <p><strong>Categories:</strong> cs.CV, cs.SI</p>
                    <p><strong>Summary:</strong> Designing socially active streets has long been a goal of urban planning, yet existing quantitative research largely measures pedestrian volume rather than the quality of social interactions. We hypothesize that street view imagery -- an inexpensive data source with global coverage -- contains latent social information that can be extracted and interpreted through established social science theory. As a proof of concept, we analyzed 2,998 street view images from 15 cities using a multimodal large language model guided by Mehtas taxonomy of passive, fleeting, and enduring sociability -- one illustrative example of a theory grounded in urban design that could be substituted or complemented by other sociological frameworks. We then used linear regression models, controlling for factors like weather, time of day, and pedestrian counts, to test whether the inferred sociability measures correlate with city-level place attachment scores from the World Values Survey and with environmental predictors (e.g., green, sky, and water view indices) derived from individual street view images. Results aligned with long-standing urban planning theory: the sky view index was associated with all three sociability types, the green view index predicted enduring sociability, and place attachment was positively associated with fleeting sociability. These results provide preliminary evidence that street view images can be used to infer relationships between specific types of social interactions and built environment variables. Further research could establish street view imagery as a scalable, privacy-preserving tool for studying urban sociability, enabling cross-cultural theory testing and evidence-based design of socially vibrant cities.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.06336v1" target="_blank">Unsupervised Partner Design Enables Robust Ad-hoc Teamwork</a></h3>
                    <p><strong>Authors:</strong> Constantin Ruhdorfer, Matteo Bortoletto, Victor Oei, Anna Penzkofer, Andreas Bulling</p>
                    <p><strong>Published:</strong> 8/8/2025</p>
                    <p><strong>Categories:</strong> cs.LG, cs.AI, cs.HC, cs.MA</p>
                    <p><strong>Summary:</strong> We introduce Unsupervised Partner Design (UPD) - a population-free, multi-agent reinforcement learning framework for robust ad-hoc teamwork that adaptively generates training partners without requiring pretrained partners or manual parameter tuning. UPD constructs diverse partners by stochastically mixing an ego agents policy with biased random behaviours and scores them using a variance-based learnability metric that prioritises partners near the ego agents current learning frontier. We show that UPD can be integrated with unsupervised environment design, resulting in the first method enabling fully unsupervised curricula over both level and partner distributions in a cooperative setting. Through extensive evaluations on Overcooked-AI and the Overcooked Generalisation Challenge, we demonstrate that this dynamic partner curriculum is highly effective: UPD consistently outperforms both population-based and population-free baselines as well as ablations. In a user study, we further show that UPD achieves higher returns than all baselines and was perceived as significantly more adaptive, more human-like, a better collaborator, and less frustrating.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.06334v1" target="_blank">Production of $π^{+}π^{-}$ pairs in diffractive photon-proton and in proton-proton collisions revisited, in particular concerning the Drell-Söding contribution</a></h3>
                    <p><strong>Authors:</strong> Piotr Lebiedowicz, Otto Nachtmann, Antoni Szczurek</p>
                    <p><strong>Published:</strong> 8/8/2025</p>
                    <p><strong>Categories:</strong> hep-ph, hep-ex</p>
                    <p><strong>Summary:</strong> We discuss the central exclusive photoproduction of $\pi^{+}\pi^{-}$ pairs in photon-proton and in proton-proton collisions at high energies. The $\rho^{0}$, $\omega$, $f_{2}(1270)$, and non-resonant (Drell-S\oding) contributions are considered. The calculation is based on the tensor-pomeron model that includes not only the dominant pomeron exchange but also reggeon and odderon exchanges. In the Drell-S\oding contribution we have different subenergies for the $\pi^{+}p$ and $\pi^{-}p$ systems. In the method which we propose now we take this into account. Respecting the gauge-invariance constraints is then a nontrivial problem for which, however, we present a solution here. In this way we improve the corresponding calculations presented in JHEP 01, 151 (2015) and in Phys. Rev. D 91, 074023 (2015). The revised model leads to enhanced cross sections and gives an increased skewing of the $\rho^{0}$ spectral shape. For the $pp\to pp\pi^{+}\pi^{-}$ reaction, we calculate differential cross sections as function of the two-pion invariant mass, pion transverse momentum and pion pseudorapidity. Predictions of proton-pion and proton-pion-pion invariant mass distributions and the distribution in the proton-proton four-momentum transfer squared are also presented. This research is relevant in the context of ALICE, ATLAS, CMS, and LHCb measurements in $pp$ collisions, even when the leading protons are not detected and instead only rapidity-gap conditions are checked experimentally. Our results can also serve as basis for the description of coherent $\pi^{+}\pi^{-}$ production in ultra-peripheral $p$A and AA collisions at the LHC. The formulas given in our paper can directly be used for the analysis of photoproduction and small-$Q^{2}$ electroproduction in $ep$ collisions at high energies. Such data exist from the HERA experiments and will be obtained in the future at the electron-ion colliders.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.06328v1" target="_blank">M2IO-R1: An Efficient RL-Enhanced Reasoning Framework for Multimodal Retrieval Augmented Multimodal Generation</a></h3>
                    <p><strong>Authors:</strong> Zhiyou Xiao, Qinhan Yu, Binghui Li, Geng Chen, Chong Chen, Wentao Zhang</p>
                    <p><strong>Published:</strong> 8/8/2025</p>
                    <p><strong>Categories:</strong> cs.IR</p>
                    <p><strong>Summary:</strong> Current research on Multimodal Retrieval-Augmented Generation (MRAG) enables diverse multimodal inputs but remains limited to single-modality outputs, restricting expressive capacity and practical utility. In contrast, real-world applications often demand both multimodal inputs and multimodal outputs for effective communication and grounded reasoning. Motivated by the recent success of Reinforcement Learning (RL) in complex reasoning tasks for Large Language Models (LLMs), we adopt RL as a principled and effective paradigm to address the multi-step, outcome-driven challenges inherent in multimodal output generation. Here, we introduce M2IO-R1, a novel framework for Multimodal Retrieval-Augmented Multimodal Generation (MRAMG) that supports both multimodal inputs and outputs. Central to our framework is an RL-based inserter, Inserter-R1-3B, trained with Group Relative Policy Optimization to guide image selection and placement in a controllable and semantically aligned manner. Empirical results show that our lightweight 3B inserter achieves strong reasoning capabilities with significantly reduced latency, outperforming baselines in both quality and efficiency.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.06327v1" target="_blank">Can Diffusion Models Bridge the Domain Gap in Cardiac MR Imaging?</a></h3>
                    <p><strong>Authors:</strong> Xin Ci Wong, Duygu Sarikaya, Kieran Zucker, Marc De Kamps, Nishant Ravikumar</p>
                    <p><strong>Published:</strong> 8/8/2025</p>
                    <p><strong>Categories:</strong> cs.CV</p>
                    <p><strong>Summary:</strong> Magnetic resonance (MR) imaging, including cardiac MR, is prone to domain shift due to variations in imaging devices and acquisition protocols. This challenge limits the deployment of trained AI models in real-world scenarios, where performance degrades on unseen domains. Traditional solutions involve increasing the size of the dataset through ad-hoc image augmentation or additional online training/transfer learning, which have several limitations. Synthetic data offers a promising alternative, but anatomical/structural consistency constraints limit the effectiveness of generative models in creating image-label pairs. To address this, we propose a diffusion model (DM) trained on a source domain that generates synthetic cardiac MR images that resemble a given reference. The synthetic data maintains spatial and structural fidelity, ensuring similarity to the source domain and compatibility with the segmentation mask. We assess the utility of our generative approach in multi-centre cardiac MR segmentation, using the 2D nnU-Net, 3D nnU-Net and vanilla U-Net segmentation networks. We explore domain generalisation, where, domain-invariant segmentation models are trained on synthetic source domain data, and domain adaptation, where, we shift target domain data towards the source domain using the DM. Both strategies significantly improved segmentation performance on data from an unseen target domain, in terms of surface-based metrics (Welchs t-test, p  0.01), compared to training segmentation models on real data alone. The proposed method ameliorates the need for transfer learning or online training to address domain shift challenges in cardiac MR image analysis, especially useful in data-scarce settings.</p>
                
            
                
                    <h3><a href="http://dx.doi.org/10.1007/978-3-032-03639-1_4" target="_blank">Social Welfare in Battery Charging Games</a></h3>
                    <p><strong>Authors:</strong> Simon Krogmann, Pascal Lenzner, Alexander Skopalik, Tobias Sträubig</p>
                    <p><strong>Published:</strong> 8/8/2025</p>
                    <p><strong>Categories:</strong> cs.GT</p>
                    <p><strong>Summary:</strong> The recent rise of renewable energy produced by many decentralized sources yields interesting market design challenges for electrical grids. Balancing supply and demand in such networks is both a temporal and spatial challenge due to capacity constraints. The recent surge in the number of household-owned batteries, especially in regions with rooftop solar adoption, offers mitigation potential but often acts misaligned with grid-level objectives. In fact, the decision to charge or discharge a household-owned battery is a strategic choice by each battery owner governed by selfish incentives. This calls for an analysis from a game-theoretic point of view. We initiate this timely research direction by considering a game-theoretic setting where selfish agents strategically charge or discharge their batteries to increase their profit. In particular, we study a Stackelberg-like market model where a third party introduces price incentives, aiming to optimize renewable energy utilization while preserving grid feasibility. For this, we study the existence and the quality of equilibria under various pricing strategies. We find that the existence of equilibria crucially depends on the chosen pricing and that the obtained social welfare varies widely. This calls for more sophisticated market models and pricing mechanisms and opens up a rich field for future research in Algorithmic Game Theory on incentives in renewable energy networks.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.06319v1" target="_blank">Towards Balanced Behavior Cloning from Imbalanced Datasets</a></h3>
                    <p><strong>Authors:</strong> Sagar Parekh, Heramb Nemlekar, Dylan P. Losey</p>
                    <p><strong>Published:</strong> 8/8/2025</p>
                    <p><strong>Categories:</strong> cs.RO</p>
                    <p><strong>Summary:</strong> Robots should be able to learn complex behaviors from human demonstrations. In practice, these human-provided datasets are inevitably imbalanced: i.e., the human demonstrates some subtasks more frequently than others. State-of-the-art methods default to treating each element of the humans dataset as equally important. So if -- for instance -- the majority of the humans data focuses on reaching a goal, and only a few state-action pairs move to avoid an obstacle, the learning algorithm will place greater emphasis on goal reaching. More generally, misalignment between the relative amounts of data and the importance of that data causes fundamental problems for imitation learning approaches. In this paper we analyze and develop learning methods that automatically account for mixed datasets. We formally prove that imbalanced data leads to imbalanced policies when each state-action pair is weighted equally; these policies emulate the most represented behaviors, and not the humans complex, multi-task demonstrations. We next explore algorithms that rebalance offline datasets (i.e., reweight the importance of different state-action pairs) without human oversight. Reweighting the dataset can enhance the overall policy performance. However, there is no free lunch: each method for autonomously rebalancing brings its own pros and cons. We formulate these advantages and disadvantages, helping other researchers identify when each type of approach is most appropriate. We conclude by introducing a novel meta-gradient rebalancing algorithm that addresses the primary limitations behind existing approaches. Our experiments show that dataset rebalancing leads to better downstream learning, improving the performance of general imitation learning algorithms without requiring additional data collection. See our project website: https://collab.me.vt.edu/data_curation/.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.06312v1" target="_blank">Chain-of-Alpha: Unleashing the Power of Large Language Models for Alpha Mining in Quantitative Trading</a></h3>
                    <p><strong>Authors:</strong> Lang Cao, Zekun Xi, Long Liao, Ziwei Yang, Zheng Cao</p>
                    <p><strong>Published:</strong> 8/8/2025</p>
                    <p><strong>Categories:</strong> cs.CE</p>
                    <p><strong>Summary:</strong> Alpha factor mining is a fundamental task in quantitative trading, aimed at discovering interpretable signals that can predict asset returns beyond systematic market risk. While traditional methods rely on manual formula design or heuristic search with machine learning, recent advances have leveraged Large Language Models (LLMs) for automated factor discovery. However, existing LLM-based alpha mining approaches remain limited in terms of automation, generality, and efficiency. In this paper, we propose Chain-of-Alpha, a novel, simple, yet effective and efficient LLM-based framework for fully automated formulaic alpha mining. Our method features a dual-chain architecture, consisting of a Factor Generation Chain and a Factor Optimization Chain, which iteratively generate, evaluate, and refine candidate alpha factors using only market data, while leveraging backtest feedback and prior optimization knowledge. The two chains work synergistically to enable high-quality alpha discovery without human intervention and offer strong scalability. Extensive experiments on real-world A-share benchmarks demonstrate that Chain-of-Alpha outperforms existing baselines across multiple metrics, presenting a promising direction for LLM-driven quantitative research.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.06306v1" target="_blank">Higher Order Regularization using Harmonic Eigenfunctions for Model-Based Reconstruction in Magnetic Particle Imaging</a></h3>
                    <p><strong>Authors:</strong> Thomas März, Vladyslav Gapyak, Andreas Weinmann</p>
                    <p><strong>Published:</strong> 8/8/2025</p>
                    <p><strong>Categories:</strong> math.NA, cs.NA, 65K10, 65R32, 65T40, 92C55</p>
                    <p><strong>Summary:</strong> Magnetic Particle Imaging (MPI) is a recent imaging modality where superparamagnetic nanoparticles are employed as tracers. The reconstruction task is to obtain the spatial particle distribution from a voltage signal induced by the particles. Generally, in computational imaging variational reconstruction techniques are common and rely on a mathematical model to describe the underlying physics. For the MPI reconstruction task we propose a model-based variational reconstruction technique which incorporates a higher order regularizer, where the regularizer is diagonalized by harmonic eigenfunctions. The proposed image reconstruction algorithm features two major stages: in the first stage, the core stage, the components of the MPI core response are reconstructed. This is the MPI-specific data approximation task which we formulate as a variational problem incorporating the higher order regularizer. The relationship between the particle distribution, the MPI core response and the measured data is given by a mathematical model which was introduced in our earlier research. According to this model the MPI core response is tied to the particle distribution by convolution. Therefore the outcome of the core stage yields the data for the second stage, the deconvolution stage, in which the final reconstructed image is produced by solving an ill-posed deconvolution problem in a robust way relying on earlier research. Interestingly, the quality of the final image depends significantly on the quality of the result of the core stage. A contribution is thus the enhancement of the core stage via higher order regularization. We provide a theoretical foundation for our approach and demonstrate its benefit with numerical examples.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.06297v1" target="_blank">KV Cache Compression for Inference Efficiency in LLMs: A Review</a></h3>
                    <p><strong>Authors:</strong> Yanyu Liu, Jingying Fu, Sixiang Liu, Yitian Zou, You Fu, Jiehan Zhou, Shouhua Zhang</p>
                    <p><strong>Published:</strong> 8/8/2025</p>
                    <p><strong>Categories:</strong> cs.DC</p>
                    <p><strong>Summary:</strong> Withtherapid advancement of large language models (LLMs), the context length for inference has been continuously increasing, leading to an exponential growth in the demand for Key-Value (KV) caching. This has resulted in a significant memory bottleneck, limiting the inference efficiency and scalability of the models. Therefore, optimizing the KV cache during inference is crucial for enhancing performance and efficiency. This review systematically examines current KV cache optimization techniques, including compression strategies such as selective token strategies, quantization, and attention compression. We evaluate the effectiveness, trade-offs, and application scenarios of these methods, providing a comprehensive analysis of their impact on memory usage and inference speed. We focus on identifying the limitations and challenges of existing methods, such as compatibility issues with different models and tasks. Additionally, this review highlights future research directions, including hybrid optimization techniques, adaptive dynamic strategies, and software-hardware co-design. These approaches aim to improve inference efficiency and promote the practical application of large language models.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.06296v1" target="_blank">LLM Robustness Leaderboard v1 --Technical report</a></h3>
                    <p><strong>Authors:</strong> Pierre Peigné - Lefebvre, Quentin Feuillade-Montixi, Tom David, Nicolas Miailhe</p>
                    <p><strong>Published:</strong> 8/8/2025</p>
                    <p><strong>Categories:</strong> cs.AI, cs.LG</p>
                    <p><strong>Summary:</strong> This technical report accompanies the LLM robustness leaderboard published by PRISM Eval for the Paris AI Action Summit. We introduce PRISM Eval Behavior Elicitation Tool (BET), an AI system performing automated red-teaming through Dynamic Adversarial Optimization that achieves 100% Attack Success Rate (ASR) against 37 of 41 state-of-the-art LLMs. Beyond binary success metrics, we propose a fine-grained robustness metric estimating the average number of attempts required to elicit harmful behaviors, revealing that attack difficulty varies by over 300-fold across models despite universal vulnerability. We introduce primitive-level vulnerability analysis to identify which jailbreaking techniques are most effective for specific hazard categories. Our collaborative evaluation with trusted third parties from the AI Safety Network demonstrates practical pathways for distributed robustness assessment across the community.</p>
                
            
                
                    <h3><a href="http://dx.doi.org/10.1109/ICIT64950.2025.11049178" target="_blank">Advanced Deep Learning Techniques for Accurate Lung Cancer Detection and Classification</a></h3>
                    <p><strong>Authors:</strong> Mobarak Abumohsen, Enrique Costa-Montenegro, Silvia García-Méndez, Amani Yousef Owda, Majdi Owda</p>
                    <p><strong>Published:</strong> 8/8/2025</p>
                    <p><strong>Categories:</strong> eess.IV, cs.AI, cs.CV</p>
                    <p><strong>Summary:</strong> Lung cancer (LC) ranks among the most frequently diagnosed cancers and is one of the most common causes of death for men and women worldwide. Computed Tomography (CT) images are the most preferred diagnosis method because of their low cost and their faster processing times. Many researchers have proposed various ways of identifying lung cancer using CT images. However, such techniques suffer from significant false positives, leading to low accuracy. The fundamental reason results from employing a small and imbalanced dataset. This paper introduces an innovative approach for LC detection and classification from CT images based on the DenseNet201 model. Our approach comprises several advanced methods such as Focal Loss, data augmentation, and regularization to overcome the imbalanced data issue and overfitting challenge. The findings show the appropriateness of the proposal, attaining a promising performance of 98.95% accuracy.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.06277v1" target="_blank">Large Language Model Data Generation for Enhanced Intent Recognition in German Speech</a></h3>
                    <p><strong>Authors:</strong> Theresa Pekarek Rosin, Burak Can Kaplan, Stefan Wermter</p>
                    <p><strong>Published:</strong> 8/8/2025</p>
                    <p><strong>Categories:</strong> cs.CL, cs.LG, cs.SD</p>
                    <p><strong>Summary:</strong> Intent recognition (IR) for speech commands is essential for artificial intelligence (AI) assistant systems; however, most existing approaches are limited to short commands and are predominantly developed for English. This paper addresses these limitations by focusing on IR from speech by elderly German speakers. We propose a novel approach that combines an adapted Whisper ASR model, fine-tuned on elderly German speech (SVC-de), with Transformer-based language models trained on synthetic text datasets generated by three well-known large language models (LLMs): LeoLM, Llama3, and ChatGPT. To evaluate the robustness of our approach, we generate synthetic speech with a text-to-speech model and conduct extensive cross-dataset testing. Our results show that synthetic LLM-generated data significantly boosts classification performance and robustness to different speaking styles and unseen vocabulary. Notably, we find that LeoLM, a smaller, domain-specific 13B LLM, surpasses the much larger ChatGPT (175B) in dataset quality for German intent recognition. Our approach demonstrates that generative AI can effectively bridge data gaps in low-resource domains. We provide detailed documentation of our data generation and training process to ensure transparency and reproducibility.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.06275v1" target="_blank">Efficient Deep Neural Receiver with Post-Training Quantization</a></h3>
                    <p><strong>Authors:</strong> SaiKrishna Saketh Yellapragada, Esa Ollila, Mario Costa</p>
                    <p><strong>Published:</strong> 8/8/2025</p>
                    <p><strong>Categories:</strong> eess.SP</p>
                    <p><strong>Summary:</strong> Deep learning has recently garnered significant interest in wireless communications due to its superior performance compared to traditional model-based algorithms. Deep convolutional neural networks (CNNs) have demonstrated notable improvements in block error rate (BLER) under various channel models and mobility scenarios. However, the high computational complexity and resource demands of deep CNNs pose challenges for deployment in resource-constrained edge systems. The 3rd Generation Partnership Project (3GPP) Release 20 highlights the pivotal role of artificial intelligence (AI) integration in enabling advanced radio-access networks for 6G systems. The hard real-time processing demands of 5G and 6G require efficient techniques such as post-training quantization (PTQ), quantization-aware training (QAT), pruning, and hybrid approaches to meet latency requirements. In this paper, we focus on PTQ to reduce model complexity by lowering the bit-width of weights, thereby enhancing computational efficiency. Our analysis employs symmetric uniform quantization, applying both per-tensor and per-channel PTQ to a neural receiver achieving performance comparable to full-precision models. Specifically, 8-bit per-channel quantization maintains BLER performance with minimal degradation, while 4-bit quantization shows great promise but requires further optimization to achieve target BLER levels. These results highlight the potential of ultra-low bitwidth PTQ for efficient neural receiver deployment in 6G systems.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.06254v1" target="_blank">From Cognitive Relief to Affective Engagement: An Empirical Comparison of AI Chatbots and Instructional Scaffolding in Physics Education</a></h3>
                    <p><strong>Authors:</strong> E. Becker, J. Wünsche, J. M. Veith, J. Schrader, P. Bitzenbauer</p>
                    <p><strong>Published:</strong> 8/8/2025</p>
                    <p><strong>Categories:</strong> physics.ed-ph</p>
                    <p><strong>Summary:</strong> Providing effective, personalized support is critical for helping students overcome conceptual difficulties in physics. However, established scaffolding methods, such as structured tiered support, are often too resource-intensive for widespread implementation. Therefore, this study, investigates whether an easily adaptable, custom-configured AI chatbot can offer comparable affective benefits and cognitive relief. We conducted a quasi-experimental field study with 273 ninth-grade students in Germany. Classes were randomly assigned to solve a buoyancy problem using one of three conditions: an AI chatbot, a tiered support system, or traditional textbook-style explanations. We measured intrinsic and extraneous cognitive load and affective outcomes (enjoyment, hope, hopelessness, self-efficacy, situational interest) via research-validated questionnaires. Results revealed that both interactive support systems -- the custom-configured AI chatbot and tiered hints -- were significantly more effective than the textual support in reducing students intrinsic and extraneous cognitive load. Furthermore, the AI chatbot yielded the most comprehensive affective benefits, demonstrating significant improvements across all measured affective dimensions, when compared to the textual support. While the chatbot consistently trended more positively than the tiered hints on affective measures, these differences were not statistically significant. These findings suggest that while structured guidance is key to managing cognitive load, the interactive and social nature of AI chatbots holds unique potential for simultaneously fostering positive affective experiences, marking a promising direction for developing effective and holistic learning support tools in physics education.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.06249v1" target="_blank">In-Training Defenses against Emergent Misalignment in Language Models</a></h3>
                    <p><strong>Authors:</strong> David Kaczér, Magnus Jørgenvåg, Clemens Vetter, Lucie Flek, Florian Mai</p>
                    <p><strong>Published:</strong> 8/8/2025</p>
                    <p><strong>Categories:</strong> cs.LG, cs.AI</p>
                    <p><strong>Summary:</strong> Fine-tuning lets practitioners repurpose aligned large language models (LLMs) for new domains, yet recent work reveals emergent misalignment (EMA): Even a small, domain-specific fine-tune can induce harmful behaviors far outside the target domain. Even in the case where model weights are hidden behind a fine-tuning API, this gives attackers inadvertent access to a broadly misaligned model in a way that can be hard to detect from the fine-tuning data alone. We present the first systematic study of in-training safeguards against EMA that are practical for providers who expose fine-tuning via an API. We investigate four training regularization interventions: (i) KL-divergence regularization toward a safe reference model, (ii) $\ell_2$ distance in feature space, (iii) projecting onto a safe subspace (SafeLoRA), and (iv) interleaving of a small amount of safe training examples from a general instruct-tuning dataset. We first evaluate the methods emergent misalignment effect across four malicious, EMA-inducing tasks. Second, we assess the methods impacts on benign tasks. We conclude with a discussion of open questions in emergent misalignment research.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.06243v1" target="_blank">SCAR: State-Space Compression for AI-Driven Resource Management in 6G-Enabled Vehicular Infotainment Systems</a></h3>
                    <p><strong>Authors:</strong> Ioan-Sorin Comsa, Purav Shah, Karthik Vaidhyanathan, Deepak Gangadharan, Christof Imhof, Per Bergamin, Aryan Kaushik, Gabriel-Miro Muntean, Ramona Trestian</p>
                    <p><strong>Published:</strong> 8/8/2025</p>
                    <p><strong>Categories:</strong> cs.LG, cs.NE, cs.SY, eess.SY</p>
                    <p><strong>Summary:</strong> The advent of 6G networks opens new possibilities for connected infotainment services in vehicular environments. However, traditional Radio Resource Management (RRM) techniques struggle with the increasing volume and complexity of data such as Channel Quality Indicators (CQI) from autonomous vehicles. To address this, we propose SCAR (State-Space Compression for AI-Driven Resource Management), an Edge AI-assisted framework that optimizes scheduling and fairness in vehicular infotainment. SCAR employs ML-based compression techniques (e.g., clustering and RBF networks) to reduce CQI data size while preserving essential features. These compressed states are used to train 6G-enabled Reinforcement Learning policies that maximize throughput while meeting fairness objectives defined by the NGMN. Simulations show that SCAR increases time in feasible scheduling regions by 14\% and reduces unfair scheduling time by 15\% compared to RL baselines without CQI compression. Furthermore, Simulated Annealing with Stochastic Tunneling (SAST)-based clustering reduces CQI clustering distortion by 10\%, confirming its efficiency. These results demonstrate SCARs scalability and fairness benefits for dynamic vehicular networks.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.06234v1" target="_blank">Rethinking the Sioux Falls Network: Insights from Path-Driven Higher-Order Network Analysis</a></h3>
                    <p><strong>Authors:</strong> Chen Zhang, Timothy LaRock, Alben Rome Bagabaldo, Jürgen Hackl</p>
                    <p><strong>Published:</strong> 8/8/2025</p>
                    <p><strong>Categories:</strong> cs.CE</p>
                    <p><strong>Summary:</strong> Benchmark scenarios are widely used in transportation research to evaluate routing algorithms, simulate infrastructure interventions, and test new technologies under controlled conditions. However, the structural and behavioral fidelity of these benchmarks remains largely unquantified, raising concerns about the external validity of simulation results. In this study, we introduce a mathematical framework based on higher-order network models to evaluate the representativeness of benchmark networks, focusing on the widely used Sioux Falls scenario. Higher-order network models encode empirical and simulated trajectory data into memory-aware network representations, which we use to quantify sequential dependencies in mobility behavior and assess how well benchmark networks capture real-world structural and functional patterns. Applying this framework to the Sioux Falls network, as well as real-world trajectory data, we quantify structural complexity, optimal memory length, link prediction accuracy, and centrality alignment. Our results show and statistically quantify that the classical Sioux Falls network exhibits limited path diversity, rapid structural fragmentation at higher orders, and weak alignment with empirical routing behavior. These results illustrate the potential of higher-order network models to bridge the gap between simulation-based and real-world mobility analysis, providing a robust foundation for more accurate and generalizable insights in transportation research.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.06230v1" target="_blank">Learning Logical Rules using Minimum Message Length</a></h3>
                    <p><strong>Authors:</strong> Ruben Sharma, Sebastijan Dumančić, Ross D. King, Andrew Cropper</p>
                    <p><strong>Published:</strong> 8/8/2025</p>
                    <p><strong>Categories:</strong> cs.AI</p>
                    <p><strong>Summary:</strong> Unifying probabilistic and logical learning is a key challenge in AI. We introduce a Bayesian inductive logic programming approach that learns minimum message length programs from noisy data. Our approach balances hypothesis complexity and data fit through priors, which explicitly favour more general programs, and a likelihood that favours accurate programs. Our experiments on several domains, including game playing and drug design, show that our method significantly outperforms previous methods, notably those that learn minimum description length programs. Our results also show that our approach is data-efficient and insensitive to example balance, including the ability to learn from exclusively positive examples.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.06491v1" target="_blank">Computational Methods and Verification Theorem for Portfolio-Consumption Optimization under Exponential O-U Dynamics</a></h3>
                    <p><strong>Authors:</strong> Zhaoxiang Zhong, Haiming Song</p>
                    <p><strong>Published:</strong> 8/8/2025</p>
                    <p><strong>Categories:</strong> math.OC</p>
                    <p><strong>Summary:</strong> In this paper, we focus on the problem of optimal portfolio-consumption policies in a multi-asset financial market, where the n risky assets follow Exponential Ornstein-Uhlenbeck processes, along with one risk-free bond. The investors preferences are modeled using Constant Relative Risk Aversion utility with state-dependent stochastic discounting. The problem can be formulated as a high-dimensional stochastic optimal control problem, wherein the associated value function satisfies a Hamilton-Jacobi-Bellman (HJB) equation, which constitutes a necessary condition for optimality. We apply a variable separation technique to transform the HJB equation to a system of ordinary differential equations (ODEs). Then a class of hybrid numerical approaches that integrate exponential Rosenbrock-type methods with Runge-Kutta methods is proposed to solve the ODE system. More importantly, we establish a rigorous verification theorem that provides sufficient conditions for the existence of value function and admissible optimal control, which can be verified numerically. A series of experiments are performed, demonstrating that our proposed method outperforms the conventional grid-based method in both accuracy and computational cost. Furthermore, the numerically derived optimal policy achieves superior performance over all other considered admissible policies.</p>
                
            
                
                    <h3><a href="http://dx.doi.org/10.1038/s41586-025-09324-0" target="_blank">One-third of Sun-like stars are born with misaligned planet-forming disks</a></h3>
                    <p><strong>Authors:</strong> Lauren I. Biddle, Brendan P. Bowler, Marvin Morgan, Quang H. Tran, Ya-Lin Wu</p>
                    <p><strong>Published:</strong> 8/8/2025</p>
                    <p><strong>Categories:</strong> astro-ph.EP</p>
                    <p><strong>Summary:</strong> Exoplanets are organized in a broad array of orbital configurations that reflect their formation along with billions of years of dynamical processing through gravitational interactions. This history is encoded in the angular momentum architecture of planetary systems--the relation between the rotational properties of the central star and the orbital geometry of planets. A primary observable is the alignment (or misalignment) between the rotational axis of the star and the orbital plane of its planets, known as stellar obliquity. Hundreds of spin-orbit constraints have been measured for giant planets close to their host stars, many of which have revealed planets on misaligned orbits. A leading question that has emerged is whether stellar obliquity originates primarily from gravitational interactions with other planets or distant stars in the same system, or if it is primordial--imprinted during the star-formation process. Here we present a comprehensive assessment of primordial obliquities between the spin axes of young, isolated Sun-like stars and the orientation of the outer regions of their protoplanetary disks. Most systems are consistent with angular momentum alignment but about one-third of isolated young systems exhibit primordial misalignment. This suggests that some obliquities identified in planetary systems at older ages--including the Suns modest misalignment with planets in the Solar System--could originate from initial conditions of their formation.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.06484v1" target="_blank">Non-programmers Assessing AI-Generated Code: A Case Study of Business Users Analyzing Data</a></h3>
                    <p><strong>Authors:</strong> Yuvraj Virk, Dongyu Liu</p>
                    <p><strong>Published:</strong> 8/8/2025</p>
                    <p><strong>Categories:</strong> cs.HC</p>
                    <p><strong>Summary:</strong> Non-technical end-users increasingly rely on AI code generation to perform technical tasks like data analysis. However, large language models (LLMs) remain unreliable, and it is unclear whether end-users can effectively identify model errors $\unicode{x2014}$ especially in realistic and domain-specific scenarios. We surveyed marketing and sales professionals to assess their ability to critically evaluate LLM-generated analyses of marketing data. Participants were shown natural language explanations of the AIs code, repeatedly informed the AI often makes mistakes, and explicitly prompted to identify them. Yet, participants frequently failed to detect critical flaws that could compromise decision-making, many of which required no technical knowledge to recognize. To investigate why, we reformatted AI responses into clearly delineated steps and provided alternative approaches for each decision to support critical evaluation. While these changes had a positive effect, participants often struggled to reason through the AIs steps and alternatives. Our findings suggest that business professionals cannot reliably verify AI-generated data analyses on their own and explore reasons why to inform future designs. As non-programmers adopt code-generating AI for technical tasks, unreliable AI and insufficient human oversight poses risks of unsafe or low-quality decisions.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.06480v1" target="_blank">Slantwise convection and heat transport in the icy moon oceans</a></h3>
                    <p><strong>Authors:</strong> Yaoxuan Zeng, Malte F. Jansen</p>
                    <p><strong>Published:</strong> 8/8/2025</p>
                    <p><strong>Categories:</strong> astro-ph.EP</p>
                    <p><strong>Summary:</strong> Ocean heat transport on icy moons shapes the ice shell topography, a primary observable of these moons. Two key processes control the heat transport: baroclinic instability driven by surface buoyancy contrasts and convective instability driven by heating from the core. However, global ocean simulations cannot accurately resolve convection under realistic icy moon conditions and instead often use Earth-based convective parameterizations, which capture only vertical convective mixing and cannot represent rotation-aligned slantwise convection on icy moons. We use high-resolution convection-resolving simulations to investigate ocean heat transport by slantwise convection in a parameter regime relevant to icy moons, isolated from baroclinic instability. Total heat transport follows the Coriolis-Inertial-Archimedean scaling with an added latitude dependence. The vertical transport increases with latitude, and the meridional transport is poleward. These results indicate that slantwise convection redistributes heat toward the poles, favoring a poleward-thinning ice shell, qualitatively consistent with Enceladuss observed ice thickness distribution.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.06475v1" target="_blank">HapticLLaMA: A Multimodal Sensory Language Model for Haptic Captioning</a></h3>
                    <p><strong>Authors:</strong> Guimin Hu, Daniel Hershcovich, Hasti Seifi</p>
                    <p><strong>Published:</strong> 8/8/2025</p>
                    <p><strong>Categories:</strong> cs.CL</p>
                    <p><strong>Summary:</strong> Haptic captioning is the task of generating natural language descriptions from haptic signals, such as vibrations, for use in virtual reality, accessibility, and rehabilitation applications. While previous multimodal research has focused primarily on vision and audio, haptic signals for the sense of touch remain underexplored. To address this gap, we formalize the haptic captioning task and propose HapticLLaMA, a multimodal sensory language model that interprets vibration signals into descriptions in a given sensory, emotional, or associative category. We investigate two types of haptic tokenizers, a frequency-based tokenizer and an EnCodec-based tokenizer, that convert haptic signals into sequences of discrete units, enabling their integration with the LLaMA model. HapticLLaMA is trained in two stages: (1) supervised fine-tuning using the LLaMA architecture with LoRA-based adaptation, and (2) fine-tuning via reinforcement learning from human feedback (RLHF). We assess HapticLLaMAs captioning performance using both automated n-gram metrics and human evaluation. HapticLLaMA demonstrates strong capability in interpreting haptic vibration signals, achieving a METEOR score of 59.98 and a BLEU-4 score of 32.06 respectively. Additionally, over 61% of the generated captions received human ratings above 3.5 on a 7-point scale, with RLHF yielding a 10% improvement in the overall rating distribution, indicating stronger alignment with human haptic perception. These findings highlight the potential of large language models to process and adapt to sensory data.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.06471v1" target="_blank">GLM-4.5: Agentic, Reasoning, and Coding (ARC) Foundation Models</a></h3>
                    <p><strong>Authors:</strong> GLM-4. 5 Team, :, Aohan Zeng, Xin Lv, Qinkai Zheng, Zhenyu Hou, Bin Chen, Chengxing Xie, Cunxiang Wang, Da Yin, Hao Zeng, Jiajie Zhang, Kedong Wang, Lucen Zhong, Mingdao Liu, Rui Lu, Shulin Cao, Xiaohan Zhang, Xuancheng Huang, Yao Wei, Yean Cheng, Yifan An, Yilin Niu, Yuanhao Wen, Yushi Bai, Zhengxiao Du, Zihan Wang, Zilin Zhu, Bohan Zhang, Bosi Wen, Bowen Wu, Bowen Xu, Can Huang, Casey Zhao, Changpeng Cai, Chao Yu, Chen Li, Chendi Ge, Chenghua Huang, Chenhui Zhang, Chenxi Xu, Chenzheng Zhu, Chuang Li, Congfeng Yin, Daoyan Lin, Dayong Yang, Dazhi Jiang, Ding Ai, Erle Zhu, Fei Wang, Gengzheng Pan, Guo Wang, Hailong Sun, Haitao Li, Haiyang Li, Haiyi Hu, Hanyu Zhang, Hao Peng, Hao Tai, Haoke Zhang, Haoran Wang, Haoyu Yang, He Liu, He Zhao, Hongwei Liu, Hongxi Yan, Huan Liu, Huilong Chen, Ji Li, Jiajing Zhao, Jiamin Ren, Jian Jiao, Jiani Zhao, Jianyang Yan, Jiaqi Wang, Jiayi Gui, Jiayue Zhao, Jie Liu, Jijie Li, Jing Li, Jing Lu, Jingsen Wang, Jingwei Yuan, Jingxuan Li, Jingzhao Du, Jinhua Du, Jinxin Liu, Junkai Zhi, Junli Gao, Ke Wang, Lekang Yang, Liang Xu, Lin Fan, Lindong Wu, Lintao Ding, Lu Wang, Man Zhang, Minghao Li, Minghuan Xu, Mingming Zhao, Mingshu Zhai, Pengfan Du, Qian Dong, Shangde Lei, Shangqing Tu, Shangtong Yang, Shaoyou Lu, Shijie Li, Shuang Li, Shuang-Li, Shuxun Yang, Sibo Yi, Tianshu Yu, Wei Tian, Weihan Wang, Wenbo Yu, Weng Lam Tam, Wenjie Liang, Wentao Liu, Xiao Wang, Xiaohan Jia, Xiaotao Gu, Xiaoying Ling, Xin Wang, Xing Fan, Xingru Pan, Xinyuan Zhang, Xinze Zhang, Xiuqing Fu, Xunkai Zhang, Yabo Xu, Yandong Wu, Yida Lu, Yidong Wang, Yilin Zhou, Yiming Pan, Ying Zhang, Yingli Wang, Yingru Li, Yinpei Su, Yipeng Geng, Yitong Zhu, Yongkun Yang, Yuhang Li, Yuhao Wu, Yujiang Li, Yunan Liu, Yunqing Wang, Yuntao Li, Yuxuan Zhang, Zezhen Liu, Zhen Yang, Zhengda Zhou, Zhongpei Qiao, Zhuoer Feng, Zhuorui Liu, Zichen Zhang, Zihan Wang, Zijun Yao, Zikang Wang, Ziqiang Liu, Ziwei Chai, Zixuan Li, Zuodong Zhao, Wenguang Chen, Jidong Zhai, Bin Xu, Minlie Huang, Hongning Wang, Juanzi Li, Yuxiao Dong, Jie Tang</p>
                    <p><strong>Published:</strong> 8/8/2025</p>
                    <p><strong>Categories:</strong> cs.CL</p>
                    <p><strong>Summary:</strong> We present GLM-4.5, an open-source Mixture-of-Experts (MoE) large language model with 355B total parameters and 32B activated parameters, featuring a hybrid reasoning method that supports both thinking and direct response modes. Through multi-stage training on 23T tokens and comprehensive post-training with expert model iteration and reinforcement learning, GLM-4.5 achieves strong performance across agentic, reasoning, and coding (ARC) tasks, scoring 70.1% on TAU-Bench, 91.0% on AIME 24, and 64.2% on SWE-bench Verified. With much fewer parameters than several competitors, GLM-4.5 ranks 3rd overall among all evaluated models and 2nd on agentic benchmarks. We release both GLM-4.5 (355B parameters) and a compact version, GLM-4.5-Air (106B parameters), to advance research in reasoning and agentic AI systems. Code, models, and more information are available at https://github.com/zai-org/GLM-4.5.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.06470v1" target="_blank">Generative AI and the Future of the Digital Commons: Five Open Questions and Knowledge Gaps</a></h3>
                    <p><strong>Authors:</strong> Arman Noroozian, Lorena Aldana, Marta Arisi, Hadi Asghari, Renata Avila, Pietro Giovanni Bizzaro, Ramya Chandrasekhar, Cristian Consonni, Deborah De Angelis, Francesca De Chiara, Maria del Rio-Chanona, Melanie Dulong de Rosnay, Maria Eriksson, Frederic Font, Emilia Gomez, Valérian Guillier, Lisa Gutermuth, David Hartmann, Lucie-Aimée Kaffee, Paul Keller, Felix Stalder, Joao Vinagre, Denny Vrandečić, Amanda Wasielewski</p>
                    <p><strong>Published:</strong> 8/8/2025</p>
                    <p><strong>Categories:</strong> cs.CY, K.4.1; K.4.2; I.2.0</p>
                    <p><strong>Summary:</strong> The rapid advancement of Generative AI (GenAI) relies heavily on the digital commons, a vast collection of free and open online content that is created, shared, and maintained by communities. However, this relationship is becoming increasingly strained due to financial burdens, decreased contributions, and misalignment between AI models and community norms. As we move deeper into the GenAI era, it is essential to examine the interdependent relationship between GenAI, the long-term sustainability of the digital commons, and the equity of current AI development practices. We highlight five critical questions that require urgent attention: 1. How can we prevent the digital commons from being threatened by undersupply as individuals cease contributing to the commons and turn to Generative AI for information? 2. How can we mitigate the risk of the open web closing due to restrictions on access to curb AI crawlers? 3. How can technical standards and legal frameworks be updated to reflect the evolving needs of organizations hosting common content? 4. What are the effects of increased synthetic content in open knowledge databases, and how can we ensure their integrity? 5. How can we account for and distribute the infrastructural and environmental costs of providing data for AI training? We emphasize the need for more responsible practices in AI development, recognizing the digital commons not only as content but as a collaborative and decentralized form of knowledge governance, which relies on the practice of commoning - making, maintaining, and protecting shared and open resources. Ultimately, our goal is to stimulate discussion and research on the intersection of Generative AI and the digital commons, with the aim of developing an AI commons and public infrastructures for AI development that support the long-term health of the digital commons.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.06464v1" target="_blank">Observation of momentum dependent charge density wave gap in EuTe4</a></h3>
                    <p><strong>Authors:</strong> Iftakhar Bin Elius, Nathan Valadez, Gyanendra Dhakal, Volodymyr Buturlim, Sabin Regmi, Dante James, Peter Radanovich, Matthew Yankowitz, Tetiana Romanova, Andrzej Ptok, Krzysztof Gofryk, Dariusz Kaczorowski, Madhab Neupane</p>
                    <p><strong>Published:</strong> 8/8/2025</p>
                    <p><strong>Categories:</strong> cond-mat.mes-hall, cond-mat.mtrl-sci</p>
                    <p><strong>Summary:</strong> The occurrence of charge density wave (CDW) phenomena, particularly in low dimensional rare-earth chalcogenides, has attracted substantial research interest. Among these materials, EuTe4, which features multiple Te layers and a single Eu-Te layer, serves as a promising platform to study the interplay between CDW order and 4f electron configurations, including magnetism. In this study, First principles based density functional theory (DFT) calculations were carried out to investigate the electronic band structure modifications arising from CDW modulation. Angle resolved photoemission spectroscopy (ARPES) revealed the emergence of a CDW gap at the Fermi level, as well as hybridization induced gap features at lower binding energies. The low lying CDW gap reaches its maximum along the Gamma-Y high-symmetry direction and a minimum along GX reflecting the anisotropic nature of the electronic structure. We also performed low temperature heat capacity measurements in applied magnetic fields near the Neel temperature (TN ~ 6.9 K) to construct the magnetic phase diagram of EuTe4. This study provides valuable insight into the directional dependent evolution of the Fermi surface nesting induced CDW ordering, along with other observed gap openings within this system.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.06461v1" target="_blank">Spontaneous Hole Formation in Cell Monolayers Emerges from Collective Cell Motion</a></h3>
                    <p><strong>Authors:</strong> Diogo E. P. Pinto, Jan Rozman, Julia M. Yeomans</p>
                    <p><strong>Published:</strong> 8/8/2025</p>
                    <p><strong>Categories:</strong> cond-mat.soft, physics.bio-ph</p>
                    <p><strong>Summary:</strong> Although cell monolayers typically remain confluent, they can spontaneously develop persistent holes as a result of collective cellular motion. Recent studies on MDCK monolayers cultured on soft substrates have revealed that cells can align to create regions of local nematic order, and topological defects that generate localised mechanical stresses which can spontaneously trigger hole formation. To investigate this process, we develop a continuum multi-phase field model that incorporates internal dissipation and active dipolar forces that drive cell shape anisotropy. Our simulations show that reducing substrate friction enhances cell-cell velocity correlations. In this low-friction regime, topological defects give rise to spiral flow patterns that concentrate stress and can trigger hole formation. We further demonstrate that the number and stability of the holes, whether they close or persist, depends on both substrate friction and cellular activity. These findings underscore the critical role of collective cell dynamics in maintaining tissue integrity.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.06457v1" target="_blank">ScamAgents: How AI Agents Can Simulate Human-Level Scam Calls</a></h3>
                    <p><strong>Authors:</strong> Sanket Badhe</p>
                    <p><strong>Published:</strong> 8/8/2025</p>
                    <p><strong>Categories:</strong> cs.CR, cs.AI, cs.CL, cs.MA</p>
                    <p><strong>Summary:</strong> Large Language Models (LLMs) have demonstrated impressive fluency and reasoning capabilities, but their potential for misuse has raised growing concern. In this paper, we present ScamAgent, an autonomous multi-turn agent built on top of LLMs, capable of generating highly realistic scam call scripts that simulate real-world fraud scenarios. Unlike prior work focused on single-shot prompt misuse, ScamAgent maintains dialogue memory, adapts dynamically to simulated user responses, and employs deceptive persuasion strategies across conversational turns. We show that current LLM safety guardrails, including refusal mechanisms and content filters, are ineffective against such agent-based threats. Even models with strong prompt-level safeguards can be bypassed when prompts are decomposed, disguised, or delivered incrementally within an agent framework. We further demonstrate the transformation of scam scripts into lifelike voice calls using modern text-to-speech systems, completing a fully automated scam pipeline. Our findings highlight an urgent need for multi-turn safety auditing, agent-level control frameworks, and new methods to detect and disrupt conversational deception powered by generative AI.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.06454v1" target="_blank">What Voting Rules Actually Do: A Data-Driven Analysis of Multi-Winner Voting</a></h3>
                    <p><strong>Authors:</strong> Joshua Caiata, Ben Armstrong, Kate Larson</p>
                    <p><strong>Published:</strong> 8/8/2025</p>
                    <p><strong>Categories:</strong> cs.AI, cs.GT</p>
                    <p><strong>Summary:</strong> Committee-selection problems arise in many contexts and applications, and there has been increasing interest within the social choice research community on identifying which properties are satisfied by different multi-winner voting rules. In this work, we propose a data-driven framework to evaluate how frequently voting rules violate axioms across diverse preference distributions in practice, shifting away from the binary perspective of axiom satisfaction given by worst-case analysis. Using this framework, we analyze the relationship between multi-winner voting rules and their axiomatic performance under several preference distributions. We then show that neural networks, acting as voting rules, can outperform traditional rules in minimizing axiom violations. Our results suggest that data-driven approaches to social choice can inform the design of new voting systems and support the continuation of data-driven research in social choice.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.06452v1" target="_blank">TRUST: Leveraging Text Robustness for Unsupervised Domain Adaptation</a></h3>
                    <p><strong>Authors:</strong> Mattia Litrico, Mario Valerio Giuffrida, Sebastiano Battiato, Devis Tuia</p>
                    <p><strong>Published:</strong> 8/8/2025</p>
                    <p><strong>Categories:</strong> cs.CV, cs.LG</p>
                    <p><strong>Summary:</strong> Recent unsupervised domain adaptation (UDA) methods have shown great success in addressing classical domain shifts (e.g., synthetic-to-real), but they still suffer under complex shifts (e.g. geographical shift), where both the background and object appearances differ significantly across domains. Prior works showed that the language modality can help in the adaptation process, exhibiting more robustness to such complex shifts. In this paper, we introduce TRUST, a novel UDA approach that exploits the robustness of the language modality to guide the adaptation of a vision model. TRUST generates pseudo-labels for target samples from their captions and introduces a novel uncertainty estimation strategy that uses normalised CLIP similarity scores to estimate the uncertainty of the generated pseudo-labels. Such estimated uncertainty is then used to reweight the classification loss, mitigating the adverse effects of wrong pseudo-labels obtained from low-quality captions. To further increase the robustness of the vision model, we propose a multimodal soft-contrastive learning loss that aligns the vision and language feature spaces, by leveraging captions to guide the contrastive training of the vision model on target images. In our contrastive loss, each pair of images acts as both a positive and a negative pair and their feature representations are attracted and repulsed with a strength proportional to the similarity of their captions. This solution avoids the need for hardly determining positive and negative pairs, which is critical in the UDA setting. Our approach outperforms previous methods, setting the new state-of-the-art on classical (DomainNet) and complex (GeoNet) domain shifts. The code will be available upon acceptance.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.06445v1" target="_blank">Echoes of Automation: The Increasing Use of LLMs in Newsmaking</a></h3>
                    <p><strong>Authors:</strong> Abolfazl Ansari, Delvin Ce Zhang, Nafis Irtiza Tripto, Dongwon Lee</p>
                    <p><strong>Published:</strong> 8/8/2025</p>
                    <p><strong>Categories:</strong> cs.CL, cs.AI</p>
                    <p><strong>Summary:</strong> The rapid rise of Generative AI (GenAI), particularly LLMs, poses concerns for journalistic integrity and authorship. This study examines AI-generated content across over 40,000 news articles from major, local, and college news media, in various media formats. Using three advanced AI-text detectors (e.g., Binoculars, Fast-Detect GPT, and GPTZero), we find substantial increase of GenAI use in recent years, especially in local and college news. Sentence-level analysis reveals LLMs are often used in the introduction of news, while conclusions usually written manually. Linguistic analysis shows GenAI boosts word richness and readability but lowers formality, leading to more uniform writing styles, particularly in local media.</p>
                
            
                
                    <h3><a href="http://dx.doi.org/10.1017/cfl.2025.8" target="_blank">The Fair Game: Auditing  Debiasing AI Algorithms Over Time</a></h3>
                    <p><strong>Authors:</strong> Debabrota Basu, Udvas Das</p>
                    <p><strong>Published:</strong> 8/8/2025</p>
                    <p><strong>Categories:</strong> cs.AI, cs.CY, cs.ET, cs.GT</p>
                    <p><strong>Summary:</strong> An emerging field of AI, namely Fair Machine Learning (ML), aims to quantify different types of bias (also known as unfairness) exhibited in the predictions of ML algorithms, and to design new algorithms to mitigate them. Often, the definitions of bias used in the literature are observational, i.e. they use the input and output of a pre-trained algorithm to quantify a bias under concern. In reality,these definitions are often conflicting in nature and can only be deployed if either the ground truth is known or only in retrospect after deploying the algorithm. Thus,there is a gap between what we want Fair ML to achieve and what it does in a dynamic social environment. Hence, we propose an alternative dynamic mechanism,Fair Game,to assure fairness in the predictions of an ML algorithm and to adapt its predictions as the society interacts with the algorithm over time. Fair Game puts together an Auditor and a Debiasing algorithm in a loop around an ML algorithm. The Fair Game puts these two components in a loop by leveraging Reinforcement Learning (RL). RL algorithms interact with an environment to take decisions, which yields new observations (also known as data/feedback) from the environment and in turn, adapts future decisions. RL is already used in algorithms with pre-fixed long-term fairness goals. Fair Game provides a unique framework where the fairness goals can be adapted over time by only modifying the auditor and the different biases it quantifies. Thus,Fair Game aims to simulate the evolution of ethical and legal frameworks in the society by creating an auditor which sends feedback to a debiasing algorithm deployed around an ML system. This allows us to develop a flexible and adaptive-over-time framework to build Fair ML systems pre- and post-deployment.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.06435v1" target="_blank">Learning the Topic, Not the Language: How LLMs Classify Online Immigration Discourse Across Languages</a></h3>
                    <p><strong>Authors:</strong> Andrea Nasuto, Stefano Maria Iacus, Francisco Rowe, Devika Jain</p>
                    <p><strong>Published:</strong> 8/8/2025</p>
                    <p><strong>Categories:</strong> cs.CL, cs.AI</p>
                    <p><strong>Summary:</strong> Large language models (LLMs) are transforming social-science research by enabling scalable, precise analysis. Their adaptability raises the question of whether knowledge acquired through fine-tuning in a few languages can transfer to unseen languages that only appeared during pre-training. To examine this, we fine-tune lightweight LLaMA 3.2-3B models on monolingual, bilingual, or multilingual data sets to classify immigration-related tweets from X/Twitter across 13 languages, a domain characterised by polarised, culturally specific discourse. We evaluate whether minimal language-specific fine-tuning enables cross-lingual topic detection and whether adding targeted languages corrects pre-training biases. Results show that LLMs fine-tuned in one or two languages can reliably classify immigration-related content in unseen languages. However, identifying whether a tweet expresses a pro- or anti-immigration stance benefits from multilingual fine-tuning. Pre-training bias favours dominant languages, but even minimal exposure to under-represented languages during fine-tuning (as little as $9.62\times10^{-11}$ of the original pre-training token volume) yields significant gains. These findings challenge the assumption that cross-lingual mastery requires extensive multilingual training: limited language coverage suffices for topic-level generalisation, and structural biases can be corrected with lightweight interventions. By releasing 4-bit-quantised, LoRA fine-tuned models, we provide an open-source, reproducible alternative to proprietary LLMs that delivers 35 times faster inference at just 0.00000989% of the dollar cost of the OpenAI GPT-4o model, enabling scalable, inclusive research.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.06434v1" target="_blank">CLIPin: A Non-contrastive Plug-in to CLIP for Multimodal Semantic Alignment</a></h3>
                    <p><strong>Authors:</strong> Shengzhu Yang, Jiawei Du, Shuai Lu, Weihang Zhang, Ningli Wang, Huiqi Li</p>
                    <p><strong>Published:</strong> 8/8/2025</p>
                    <p><strong>Categories:</strong> cs.CV, cs.AI</p>
                    <p><strong>Summary:</strong> Large-scale natural image-text datasets, especially those automatically collected from the web, often suffer from loose semantic alignment due to weak supervision, while medical datasets tend to have high cross-modal correlation but low content diversity. These properties pose a common challenge for contrastive language-image pretraining (CLIP): they hinder the models ability to learn robust and generalizable representations. In this work, we propose CLIPin, a unified non-contrastive plug-in that can be seamlessly integrated into CLIP-style architectures to improve multimodal semantic alignment, providing stronger supervision and enhancing alignment robustness. Furthermore, two shared pre-projectors are designed for image and text modalities respectively to facilitate the integration of contrastive and non-contrastive learning in a parameter-compromise manner. Extensive experiments on diverse downstream tasks demonstrate the effectiveness and generality of CLIPin as a plug-and-play component compatible with various contrastive frameworks. Code is available at https://github.com/T6Yang/CLIPin.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.06430v1" target="_blank">MotionSwap</a></h3>
                    <p><strong>Authors:</strong> Om Patil, Jinesh Modi, Suryabha Mukhopadhyay, Meghaditya Giri, Chhavi Malhotra</p>
                    <p><strong>Published:</strong> 8/8/2025</p>
                    <p><strong>Categories:</strong> cs.CV</p>
                    <p><strong>Summary:</strong> Face swapping technology has gained significant attention in both academic research and commercial applications. This paper presents our implementation and enhancement of SimSwap, an efficient framework for high fidelity face swapping. We introduce several improvements to the original model, including the integration of self and cross-attention mechanisms in the generator architecture, dynamic loss weighting, and cosine annealing learning rate scheduling. These enhancements lead to significant improvements in identity preservation, attribute consistency, and overall visual quality. Our experimental results, spanning 400,000 training iterations, demonstrate progressive improvements in generator and discriminator performance. The enhanced model achieves better identity similarity, lower FID scores, and visibly superior qualitative results compared to the baseline. Ablation studies confirm the importance of each architectural and training improvement. We conclude by identifying key future directions, such as integrating StyleGAN3, improving lip synchronization, incorporating 3D facial modeling, and introducing temporal consistency for video-based applications.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.06421v1" target="_blank">Programing optical properties of single-walled carbon nanotubes with benzoyl peroxide derivatives of tailored chemical characteristics</a></h3>
                    <p><strong>Authors:</strong> Andrzej Dzienia, Patrycja Taborowska, Pawel Kubica-Cypek, Dawid Janas</p>
                    <p><strong>Published:</strong> 8/8/2025</p>
                    <p><strong>Categories:</strong> cond-mat.mtrl-sci</p>
                    <p><strong>Summary:</strong> Semiconducting single-walled carbon nanotubes (SWCNTs) have great potential for optoelectronics and photonics, further enhanced by covalent functionalization. However, scalable and controlled surface modification is challenging due to complex methodologies and unstable reagents. Benzoyl peroxide (BPO) has emerged as a simple alternative for introducing luminescent defects into SWCNTs. Yet, the lack of understanding of its radical chemistry limits precise defect engineering using BPOs. This is a major obstacle to the effective application of BPO in chemistry, despite its widespread use as a radical initiator. We present a thorough investigation into the radical chemistry of self-synthesized BPOs for functionalizing polymer-wrapped (6,5) and (7,5) SWCNTs in non-polar solvents, providing critical insights into the decomposition of BPO and its analogs. By varying the electronic and steric properties of typically unavailable BPO derivatives, we demonstrate tunability over the photoluminescence characteristics of SWCNTs, allowing control over defect density and light emission wavelength. This toolbox of BPO derivatives, created with simple radical chemistry and accessible organic precursors, alongside clarified structure-property relationships, facilitates effective implementation of BPO in chemical transformations and meticulous engineering of luminescent defects in SWCNTs for optoelectronic applications. Notably, this research offers insights into why SWCNTs modified with electron-deficient reactants provide the best optical characteristics.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.06411v1" target="_blank">Dimensional Characterization and Pathway Modeling for Catastrophic AI Risks</a></h3>
                    <p><strong>Authors:</strong> Ze Shen Chin</p>
                    <p><strong>Published:</strong> 8/8/2025</p>
                    <p><strong>Categories:</strong> cs.CY, cs.AI, cs.LG</p>
                    <p><strong>Summary:</strong> Although discourse around the risks of Artificial Intelligence (AI) has grown, it often lacks a comprehensive, multidimensional framework, and concrete causal pathways mapping hazard to harm. This paper aims to bridge this gap by examining six commonly discussed AI catastrophic risks: CBRN, cyber offense, sudden loss of control, gradual loss of control, environmental risk, and geopolitical risk. First, we characterize these risks across seven key dimensions, namely intent, competency, entity, polarity, linearity, reach, and order. Next, we conduct risk pathway modeling by mapping step-by-step progressions from the initial hazard to the resulting harms. The dimensional approach supports systematic risk identification and generalizable mitigation strategies, while risk pathway models help identify scenario-specific interventions. Together, these methods offer a more structured and actionable foundation for managing catastrophic AI risks across the value chain.</p>
                
            
                
                    <h3><a href="http://dx.doi.org/10.1109/QCE60285.2024.10330" target="_blank">Quantum Annealing for the Set Splitting Problem</a></h3>
                    <p><strong>Authors:</strong> Sean Borneman</p>
                    <p><strong>Published:</strong> 8/8/2025</p>
                    <p><strong>Categories:</strong> quant-ph</p>
                    <p><strong>Summary:</strong> I present a novel use of quantum annealing to solve the Set Splitting Problem using (QUBO) problem formulation. The contribution of the work is in formulating penalty functions that ensure the ground state of the QUBO Hamiltonian corresponds to valid solutions that split the input subsets. This approach scales linearly in terms of the number of logical qubits relative to problem size. Empirical tests of the proposed solution show convergence to globally optimal solutions, with high accuracy rates over repeated trials. Hardware limitations of current quantum annealers lead to an exponential rise in required physical qubits, versus the theoretical linear increase, although this can improve with future developments. Further work is needed to enhance formulation robustness, reduce qubit requirements for embedded problems, and to conduct more extensive bench-marking. Quantum solutions to the Set-Splitting problem lead to reduced time complexity versus classical solutions, and may accelerate research in biology, cybersecurity, and other domains.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.06406v1" target="_blank">Blockchain-Enabled Federated Learning</a></h3>
                    <p><strong>Authors:</strong> Murtaza Rangwala, Venugopal K R, Rajkumar Buyya</p>
                    <p><strong>Published:</strong> 8/8/2025</p>
                    <p><strong>Categories:</strong> cs.DC, cs.LG</p>
                    <p><strong>Summary:</strong> Blockchain-enabled federated learning (BCFL) addresses fundamental challenges of trust, privacy, and coordination in collaborative AI systems. This chapter provides comprehensive architectural analysis of BCFL systems through a systematic four-dimensional taxonomy examining coordination structures, consensus mechanisms, storage architectures, and trust models. We analyze design patterns from blockchain-verified centralized coordination to fully decentralized peer-to-peer networks, evaluating trade-offs in scalability, security, and performance. Through detailed examination of consensus mechanisms designed for federated learning contexts, including Proof of Quality and Proof of Federated Learning, we demonstrate how computational work can be repurposed from arbitrary cryptographic puzzles to productive machine learning tasks. The chapter addresses critical storage challenges by examining multi-tier architectures that balance blockchains transaction constraints with neural networks large parameter requirements while maintaining cryptographic integrity. A technical case study of the TrustMesh framework illustrates practical implementation considerations in BCFL systems through distributed image classification training, demonstrating effective collaborative learning across IoT devices with highly non-IID data distributions while maintaining complete transparency and fault tolerance. Analysis of real-world deployments across healthcare consortiums, financial services, and IoT security applications validates the practical viability of BCFL systems, achieving performance comparable to centralized approaches while providing enhanced security guarantees and enabling new models of trustless collaborative intelligence.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.06401v1" target="_blank">A Systematic Literature Review of Retrieval-Augmented Generation: Techniques, Metrics, and Challenges</a></h3>
                    <p><strong>Authors:</strong> Andrew Brown, Muhammad Roman, Barry Devereux</p>
                    <p><strong>Published:</strong> 8/8/2025</p>
                    <p><strong>Categories:</strong> cs.DL, cs.AI, cs.CL, cs.IR</p>
                    <p><strong>Summary:</strong> This systematic review of the research literature on retrieval-augmented generation (RAG) provides a focused analysis of the most highly cited studies published between 2020 and May 2025. A total of 128 articles met our inclusion criteria. The records were retrieved from ACM Digital Library, IEEE Xplore, Scopus, ScienceDirect, and the Digital Bibliography and Library Project (DBLP). RAG couples a neural retriever with a generative language model, grounding output in up-to-date, non-parametric memory while retaining the semantic generalisation stored in model weights. Guided by the PRISMA 2020 framework, we (i) specify explicit inclusion and exclusion criteria based on citation count and research questions, (ii) catalogue datasets, architectures, and evaluation practices, and (iii) synthesise empirical evidence on the effectiveness and limitations of RAG. To mitigate citation-lag bias, we applied a lower citation-count threshold to papers published in 2025 so that emerging breakthroughs with naturally fewer citations were still captured. This review clarifies the current research landscape, highlights methodological gaps, and charts priority directions for future research.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.06394v1" target="_blank">When AIOps Become AI Oops: Subverting LLM-driven IT Operations via Telemetry Manipulation</a></h3>
                    <p><strong>Authors:</strong> Dario Pasquini, Evgenios M. Kornaropoulos, Giuseppe Ateniese, Omer Akgul, Athanasios Theocharis, Petros Efstathopoulos</p>
                    <p><strong>Published:</strong> 8/8/2025</p>
                    <p><strong>Categories:</strong> cs.CR</p>
                    <p><strong>Summary:</strong> AI for IT Operations (AIOps) is transforming how organizations manage complex software systems by automating anomaly detection, incident diagnosis, and remediation. Modern AIOps solutions increasingly rely on autonomous LLM-based agents to interpret telemetry data and take corrective actions with minimal human intervention, promising faster response times and operational cost savings. In this work, we perform the first security analysis of AIOps solutions, showing that, once again, AI-driven automation comes with a profound security cost. We demonstrate that adversaries can manipulate system telemetry to mislead AIOps agents into taking actions that compromise the integrity of the infrastructure they manage. We introduce techniques to reliably inject telemetry data using error-inducing requests that influence agent behavior through a form of adversarial reward-hacking; plausible but incorrect system error interpretations that steer the agents decision-making. Our attack methodology, AIOpsDoom, is fully automated--combining reconnaissance, fuzzing, and LLM-driven adversarial input generation--and operates without any prior knowledge of the target system. To counter this threat, we propose AIOpsShield, a defense mechanism that sanitizes telemetry data by exploiting its structured nature and the minimal role of user-generated content. Our experiments show that AIOpsShield reliably blocks telemetry-based attacks without affecting normal agent performance. Ultimately, this work exposes AIOps as an emerging attack vector for system compromise and underscores the urgent need for security-aware AIOps design.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.06388v1" target="_blank">LLMs vs. Chinese Anime Enthusiasts: A Comparative Study on Emotionally Supportive Role-Playing</a></h3>
                    <p><strong>Authors:</strong> Lanlan Qiu, Xiao Pu, Yeqi Feng, Tianxing He</p>
                    <p><strong>Published:</strong> 8/8/2025</p>
                    <p><strong>Categories:</strong> cs.CL</p>
                    <p><strong>Summary:</strong> Large Language Models (LLMs) have demonstrated impressive capabilities in role-playing conversations and providing emotional support as separate research directions. However, there remains a significant research gap in combining these capabilities to enable emotionally supportive interactions with virtual characters. To address this research gap, we focus on anime characters as a case study because of their well-defined personalities and large fan bases. This choice enables us to effectively evaluate how well LLMs can provide emotional support while maintaining specific character traits. We introduce ChatAnime, the first Emotionally Supportive Role-Playing (ESRP) dataset. We first thoughtfully select 20 top-tier characters from popular anime communities and design 60 emotion-centric real-world scenario questions. Then, we execute a nationwide selection process to identify 40 Chinese anime enthusiasts with profound knowledge of specific characters and extensive experience in role-playing. Next, we systematically collect two rounds of dialogue data from 10 LLMs and these 40 Chinese anime enthusiasts. To evaluate the ESRP performance of LLMs, we design a user experience-oriented evaluation system featuring 9 fine-grained metrics across three dimensions: basic dialogue, role-playing and emotional support, along with an overall metric for response diversity. In total, the dataset comprises 2,400 human-written and 24,000 LLM-generated answers, supported by over 132,000 human annotations. Experimental results show that top-performing LLMs surpass human fans in role-playing and emotional support, while humans still lead in response diversity. We hope this work can provide valuable resources and insights for future research on optimizing LLMs in ESRP. Our datasets are available at https://github.com/LanlanQiu/ChatAnime.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.06386v1" target="_blank">Bridging Farm Economics and Landscape Ecology for Global Sustainability through Hierarchical and Bayesian Optimization</a></h3>
                    <p><strong>Authors:</strong> Kevin Bradley Dsouza, Graham Alexander Watt, Yuri Leonenko, Juan Moreno-Cruz</p>
                    <p><strong>Published:</strong> 8/8/2025</p>
                    <p><strong>Categories:</strong> cs.CE</p>
                    <p><strong>Summary:</strong> Agricultural landscapes face the dual challenge of sustaining food production while reversing biodiversity loss. Agri-environmental policies often fall short of delivering ecological functions such as landscape connectivity, in part due to a persistent disconnect between farm-level economic decisions and landscape-scale spatial planning. We introduce a novel hierarchical optimization framework that bridges this gap. First, an Ecological Intensification (EI) model determines the economically optimal allocation of land to margin and habitat interventions at the individual farm level. These farm-specific intervention levels are then passed to an Ecological Connectivity (EC) model, which spatially arranges them across the landscape to maximize connectivity while preserving farm-level profitability. Finally, we introduce a Bayesian Optimization (BO) approach that translates these spatial outcomes into simple, cost effective, and scalable policy instruments, such as subsidies and eco-premiums, using non-spatial, farm-level policy parameters. Applying the framework to a Canadian agricultural landscape, we demonstrate how it enhances connectivity under real-world economic constraints. Our approach provides a globally relevant tool for aligning farm incentives with biodiversity goals, advancing the development of agri-environmental policies that are economically viable and ecologically effective.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.06385v1" target="_blank">Bayesian online collective anomaly and change point detection in fine-grained time series</a></h3>
                    <p><strong>Authors:</strong> Xian Chen, Weichi Wu</p>
                    <p><strong>Published:</strong> 8/8/2025</p>
                    <p><strong>Categories:</strong> stat.ME</p>
                    <p><strong>Summary:</strong> Fine-grained time series data are crucial for accurate and timely online change detection. While both collective anomalies and change points can coexist in such data, their joint online detection has received limited attention. In this research, we develop a Bayesian framework capturing time series with collective anomalies and change points, and introduce a recursive online inference algorithm to detect the most recent collective anomaly and change point jointly. For scaling, we further propose an algorithm enhanced with collective anomaly removal that effectively reduces the time and space complexity to linear. We demonstrate the effectiveness of our approach via extensive experiments on simulated data and two real-world applications.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.06382v1" target="_blank">Text as Any-Modality for Zero-Shot Classification by Consistent Prompt Tuning</a></h3>
                    <p><strong>Authors:</strong> Xiangyu Wu, Feng Yu, Yang Yang, Jianfeng Lu</p>
                    <p><strong>Published:</strong> 8/8/2025</p>
                    <p><strong>Categories:</strong> cs.CV</p>
                    <p><strong>Summary:</strong> The integration of prompt tuning with multimodal learning has shown significant generalization abilities for various downstream tasks. Despite advancements, existing methods heavily depend on massive modality-specific labeled data (e.g., video, audio, and image), or are customized for a single modality. In this study, we present Text as Any-Modality by Consistent Prompt Tuning (TaAM-CPT), a scalable approach for constructing a general representation model toward unlimited modalities using solely text data. TaAM-CPT comprises modality prompt pools, text construction, and modality-aligned text encoders from pre-trained models, which allows for extending new modalities by simply adding prompt pools and modality-aligned text encoders. To harmonize the learning across different modalities, TaAM-CPT designs intra- and inter-modal learning objectives, which can capture category details within modalities while maintaining semantic consistency across different modalities. Benefiting from its scalable architecture and pre-trained models, TaAM-CPT can be seamlessly extended to accommodate unlimited modalities. Remarkably, without any modality-specific labeled data, TaAM-CPT achieves leading results on diverse datasets spanning various modalities, including video classification, image classification, and audio classification. The code is available at https://github.com/Jinx630/TaAM-CPT.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.06374v1" target="_blank">Evaluating Style-Personalized Text Generation: Challenges and Directions</a></h3>
                    <p><strong>Authors:</strong> Anubhav Jangra, Bahareh Sarrafzadeh, Adrian de Wynter, Silviu Cucerzan, Sujay Kumar Jauhar</p>
                    <p><strong>Published:</strong> 8/8/2025</p>
                    <p><strong>Categories:</strong> cs.CL</p>
                    <p><strong>Summary:</strong> While prior research has built tools and benchmarks towards style personalized text generation, there has been limited exploration of evaluation in low-resource author style personalized text generation space. Through this work, we question the effectiveness of the widely adopted evaluation metrics like BLEU and ROUGE, and explore other evaluation paradigms such as style embeddings and LLM-as-judge to holistically evaluate the style personalized text generation task. We evaluate these metrics and their ensembles using our style discrimination benchmark, that spans eight writing tasks, and evaluates across three settings, domain discrimination, authorship attribution, and LLM personalized vs non-personalized discrimination. We provide conclusive evidence to adopt ensemble of diverse evaluation metrics to effectively evaluate style personalized text generation.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.06369v1" target="_blank">Conservation laws, a new class of group invariant solutions, and its applications for the Whitham Broer Kaup model</a></h3>
                    <p><strong>Authors:</strong> Sougata Mandal, Sukhendu Ghosh</p>
                    <p><strong>Published:</strong> 8/8/2025</p>
                    <p><strong>Categories:</strong> physics.flu-dyn</p>
                    <p><strong>Summary:</strong> The Whitham Broer Kaup (WBK) equations provide a fundamental framework for modeling shallow water wave dynamics, effectively capturing both nonlinear and dispersive effects. In this study, we construct a new class of analytical and numerical solutions for the WBK system using Lie symmetry analysis. By determining an optimal system of one-dimensional subalgebras, we obtain symmetry reductions that lead to new kinds of exact wave solutions expressed in hyperbolic, trigonometric, and rational forms. The influence of key physical parameters on wave structure is systematically explored, revealing their role in shaping the velocity and surface profiles of the waves. An important aspect of this work is the application of the WBK model to tsunami wave propagation, demonstrating its capability to simulate the generation, evolution, and spatial spreading of long surface waves in coastal regions. This highlights the practical relevance of the WBK equations in geophysical and oceanographic contexts. Additionally, employing the direct multiplier method, we derive a complete set of local conservation laws for the governing WBK model, ensuring the preservation of key physical properties. These findings enhance the understanding of shallow water wave behavior, unify existing research, and provide a framework for further exploration.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.06362v1" target="_blank">SQUID G.A.M.E.: Gamma, Atmospheric, and Mono-Energetic Neutron Effects on Quantum Devices</a></h3>
                    <p><strong>Authors:</strong> Gioele Casagranda, Elizabeth Auden, Carlo Cazzaniga, Maria Kastriotou, Christopher Frost, Marzio Vallero, Flavio Vella, Paolo Rech</p>
                    <p><strong>Published:</strong> 8/8/2025</p>
                    <p><strong>Categories:</strong> quant-ph</p>
                    <p><strong>Summary:</strong> Quantum devices are a promising solution to many research applications, including medical imaging, precision magnetic field measurements, condensed matter physics, and overcoming the limits of classical computing. Among the available implementations, the superconducting technology is the current focus of scientific research and industrial applications, excelling in performance and scalability. Despite this, superconducting quantum systems are extremely prone to decoherence, and in particular, they are highly sensitive to radiation events. In this paper, we analyze the response of a superconducting device (SQUID) to radiation. We expose the SQUID to beams of monoenergetic 14 MeV neutrons (NILE - ISIS), atmospheric 1-800 MeV neutrons (ChipIR - ISIS), and gamma rays with 1.25 MeV average energy (CALLIOPE - ENEA). These experiments show that the SQUID is sensitive to the two neutron fields, while gamma rays at 1.25 MeV leave it mostly unaffected. Following our experiments with neutrons, it is possible to characterize the SQUIDs response and even classify faults according to their shape and duration. We identify two categories: bursts (long lasting) and peaks (short lived). To investigate the different responses to neutrons and gamma rays, we employ Geant4 simulations, which highlight differences in the deposition spectra and the energy propagation, but likewise predict the vulnerability of the SQUID in both cases.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.06356v1" target="_blank">Use Cases for Voice Anonymization</a></h3>
                    <p><strong>Authors:</strong> Sarina Meyer, Ngoc Thang Vu</p>
                    <p><strong>Published:</strong> 8/8/2025</p>
                    <p><strong>Categories:</strong> eess.AS</p>
                    <p><strong>Summary:</strong> The performance of a voice anonymization system is typically measured according to its ability to hide the speakers identity and keep the datas utility for downstream tasks. This means that the requirements the anonymization should fulfill depend on the context in which it is used and may differ greatly between use cases. However, these use cases are rarely specified in research papers. In this paper, we study the implications of use case-specific requirements on the design of voice anonymization methods. We perform an extensive literature analysis and user study to collect possible use cases and to understand the expectations of the general public towards such tools. Based on these studies, we propose the first taxonomy of use cases for voice anonymization, and derive a set of requirements and design criteria for method development and evaluation. Using this scheme, we propose to focus more on use case-oriented research and development of voice anonymization systems.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.06354v1" target="_blank">Zombitron: towards a toolbox for repurposing obsolete smartphones into new interactive systems</a></h3>
                    <p><strong>Authors:</strong> Clara Rigaud</p>
                    <p><strong>Published:</strong> 8/8/2025</p>
                    <p><strong>Categories:</strong> cs.HC</p>
                    <p><strong>Summary:</strong> This article explores the possibilities of reusing obsolete smartphones and tablets to build new interactive systems. Taking the case of a musical instrument, I present my research into the design of a controller made from various of these obsolete smartphones. From the diagnostic stage to the creation of a new autonomous electronic object, I document the process, the barriers and the levers encountered. Based on these explorations and discussions with two professional musicians, I provide several insights into the software and hardware aspects, with a view to continuing this work, towards the creation of an open-source toolkit enabling anyone to build new interactive systems with old devices. I discuss the implication of how a high-level web-based approach could allow designers to enter the black box and foster permacomputing using smartphones.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.06352v1" target="_blank">From Explainable to Explanatory Artificial Intelligence: Toward a New Paradigm for Human-Centered Explanations through Generative AI</a></h3>
                    <p><strong>Authors:</strong> Christian Meske, Justin Brenne, Erdi Uenal, Sabahat Oelcer, Ayseguel Doganguen</p>
                    <p><strong>Published:</strong> 8/8/2025</p>
                    <p><strong>Categories:</strong> cs.AI, cs.HC</p>
                    <p><strong>Summary:</strong> Current explainable AI (XAI) approaches prioritize algorithmic transparency and present explanations in abstract, non-adaptive formats that often fail to support meaningful end-user understanding. This paper introduces Explanatory AI as a complementary paradigm that leverages generative AI capabilities to serve as explanatory partners for human understanding rather than providers of algorithmic transparency. While XAI reveals algorithmic decision processes for model validation, Explanatory AI addresses contextual reasoning to support human decision-making in sociotechnical contexts. We develop a definition and systematic eight-dimensional conceptual model distinguishing Explanatory AI through narrative communication, adaptive personalization, and progressive disclosure principles. Empirical validation through Rapid Contextual Design methodology with healthcare professionals demonstrates that users consistently prefer context-sensitive, multimodal explanations over technical transparency. Our findings reveal the practical urgency for AI systems designed for human comprehension rather than algorithmic introspection, establishing a comprehensive research agenda for advancing user-centered AI explanation approaches across diverse domains and cultural contexts.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.06350v1" target="_blank">Aligning Effective Tokens with Video Anomaly in Large Language Models</a></h3>
                    <p><strong>Authors:</strong> Yingxian Chen, Jiahui Liu, Ruifan Di, Yanwei Li, Chirui Chang, Shizhen Zhao, Wilton W. T. Fok, Xiaojuan Qi, Yik-Chung Wu</p>
                    <p><strong>Published:</strong> 8/8/2025</p>
                    <p><strong>Categories:</strong> cs.CV</p>
                    <p><strong>Summary:</strong> Understanding abnormal events in videos is a vital and challenging task that has garnered significant attention in a wide range of applications. Although current video understanding Multi-modal Large Language Models (MLLMs) are capable of analyzing general videos, they often struggle to handle anomalies due to the spatial and temporal sparsity of abnormal events, where the redundant information always leads to suboptimal outcomes. To address these challenges, exploiting the representation and generalization capabilities of Vison Language Models (VLMs) and Large Language Models (LLMs), we propose VA-GPT, a novel MLLM designed for summarizing and localizing abnormal events in various videos. Our approach efficiently aligns effective tokens between visual encoders and LLMs through two key proposed modules: Spatial Effective Token Selection (SETS) and Temporal Effective Token Generation (TETG). These modules enable our model to effectively capture and analyze both spatial and temporal information associated with abnormal events, resulting in more accurate responses and interactions. Furthermore, we construct an instruction-following dataset specifically for fine-tuning video-anomaly-aware MLLMs, and introduce a cross-domain evaluation benchmark based on XD-Violence dataset. Our proposed method outperforms existing state-of-the-art methods on various benchmarks.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.06348v1" target="_blank">AntiCheatPT: A Transformer-Based Approach to Cheat Detection in Competitive Computer Games</a></h3>
                    <p><strong>Authors:</strong> Mille Mei Zhen Loo, Gert Luzkov, Paolo Burelli</p>
                    <p><strong>Published:</strong> 8/8/2025</p>
                    <p><strong>Categories:</strong> cs.AI</p>
                    <p><strong>Summary:</strong> Cheating in online video games compromises the integrity of gaming experiences. Anti-cheat systems, such as VAC (Valve Anti-Cheat), face significant challenges in keeping pace with evolving cheating methods without imposing invasive measures on users systems. This paper presents AntiCheatPT\_256, a transformer-based machine learning model designed to detect cheating behaviour in Counter-Strike 2 using gameplay data. To support this, we introduce and publicly release CS2CD: A labelled dataset of 795 matches. Using this dataset, 90,707 context windows were created and subsequently augmented to address class imbalance. The transformer model, trained on these windows, achieved an accuracy of 89.17\% and an AUC of 93.36\% on an unaugmented test set. This approach emphasizes reproducibility and real-world applicability, offering a robust baseline for future research in data-driven cheat detection.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.06347v1" target="_blank">Structural Equation-VAE: Disentangled Latent Representations for Tabular Data</a></h3>
                    <p><strong>Authors:</strong> Ruiyu Zhang, Ce Zhao, Xin Zhao, Lin Nie, Wai-Fung Lam</p>
                    <p><strong>Published:</strong> 8/8/2025</p>
                    <p><strong>Categories:</strong> cs.LG, cs.AI, cs.NE</p>
                    <p><strong>Summary:</strong> Learning interpretable latent representations from tabular data remains a challenge in deep generative modeling. We introduce SE-VAE (Structural Equation-Variational Autoencoder), a novel architecture that embeds measurement structure directly into the design of a variational autoencoder. Inspired by structural equation modeling, SE-VAE aligns latent subspaces with known indicator groupings and introduces a global nuisance latent to isolate construct-specific confounding variation. This modular architecture enables disentanglement through design rather than through statistical regularizers alone. We evaluate SE-VAE on a suite of simulated tabular datasets and benchmark its performance against a series of leading baselines using standard disentanglement metrics. SE-VAE consistently outperforms alternatives in factor recovery, interpretability, and robustness to nuisance variation. Ablation results reveal that architectural structure, rather than regularization strength, is the key driver of performance. SE-VAE offers a principled framework for white-box generative modeling in scientific and social domains where latent constructs are theory-driven and measurement validity is essential.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.06342v1" target="_blank">Street View Sociability: Interpretable Analysis of Urban Social Behavior Across 15 Cities</a></h3>
                    <p><strong>Authors:</strong> Kieran Elrod, Katherine Flanigan, Mario Bergés</p>
                    <p><strong>Published:</strong> 8/8/2025</p>
                    <p><strong>Categories:</strong> cs.CV, cs.SI</p>
                    <p><strong>Summary:</strong> Designing socially active streets has long been a goal of urban planning, yet existing quantitative research largely measures pedestrian volume rather than the quality of social interactions. We hypothesize that street view imagery -- an inexpensive data source with global coverage -- contains latent social information that can be extracted and interpreted through established social science theory. As a proof of concept, we analyzed 2,998 street view images from 15 cities using a multimodal large language model guided by Mehtas taxonomy of passive, fleeting, and enduring sociability -- one illustrative example of a theory grounded in urban design that could be substituted or complemented by other sociological frameworks. We then used linear regression models, controlling for factors like weather, time of day, and pedestrian counts, to test whether the inferred sociability measures correlate with city-level place attachment scores from the World Values Survey and with environmental predictors (e.g., green, sky, and water view indices) derived from individual street view images. Results aligned with long-standing urban planning theory: the sky view index was associated with all three sociability types, the green view index predicted enduring sociability, and place attachment was positively associated with fleeting sociability. These results provide preliminary evidence that street view images can be used to infer relationships between specific types of social interactions and built environment variables. Further research could establish street view imagery as a scalable, privacy-preserving tool for studying urban sociability, enabling cross-cultural theory testing and evidence-based design of socially vibrant cities.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.06336v1" target="_blank">Unsupervised Partner Design Enables Robust Ad-hoc Teamwork</a></h3>
                    <p><strong>Authors:</strong> Constantin Ruhdorfer, Matteo Bortoletto, Victor Oei, Anna Penzkofer, Andreas Bulling</p>
                    <p><strong>Published:</strong> 8/8/2025</p>
                    <p><strong>Categories:</strong> cs.LG, cs.AI, cs.HC, cs.MA</p>
                    <p><strong>Summary:</strong> We introduce Unsupervised Partner Design (UPD) - a population-free, multi-agent reinforcement learning framework for robust ad-hoc teamwork that adaptively generates training partners without requiring pretrained partners or manual parameter tuning. UPD constructs diverse partners by stochastically mixing an ego agents policy with biased random behaviours and scores them using a variance-based learnability metric that prioritises partners near the ego agents current learning frontier. We show that UPD can be integrated with unsupervised environment design, resulting in the first method enabling fully unsupervised curricula over both level and partner distributions in a cooperative setting. Through extensive evaluations on Overcooked-AI and the Overcooked Generalisation Challenge, we demonstrate that this dynamic partner curriculum is highly effective: UPD consistently outperforms both population-based and population-free baselines as well as ablations. In a user study, we further show that UPD achieves higher returns than all baselines and was perceived as significantly more adaptive, more human-like, a better collaborator, and less frustrating.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.06334v1" target="_blank">Production of $π^{+}π^{-}$ pairs in diffractive photon-proton and in proton-proton collisions revisited, in particular concerning the Drell-Söding contribution</a></h3>
                    <p><strong>Authors:</strong> Piotr Lebiedowicz, Otto Nachtmann, Antoni Szczurek</p>
                    <p><strong>Published:</strong> 8/8/2025</p>
                    <p><strong>Categories:</strong> hep-ph, hep-ex</p>
                    <p><strong>Summary:</strong> We discuss the central exclusive photoproduction of $\pi^{+}\pi^{-}$ pairs in photon-proton and in proton-proton collisions at high energies. The $\rho^{0}$, $\omega$, $f_{2}(1270)$, and non-resonant (Drell-S\oding) contributions are considered. The calculation is based on the tensor-pomeron model that includes not only the dominant pomeron exchange but also reggeon and odderon exchanges. In the Drell-S\oding contribution we have different subenergies for the $\pi^{+}p$ and $\pi^{-}p$ systems. In the method which we propose now we take this into account. Respecting the gauge-invariance constraints is then a nontrivial problem for which, however, we present a solution here. In this way we improve the corresponding calculations presented in JHEP 01, 151 (2015) and in Phys. Rev. D 91, 074023 (2015). The revised model leads to enhanced cross sections and gives an increased skewing of the $\rho^{0}$ spectral shape. For the $pp\to pp\pi^{+}\pi^{-}$ reaction, we calculate differential cross sections as function of the two-pion invariant mass, pion transverse momentum and pion pseudorapidity. Predictions of proton-pion and proton-pion-pion invariant mass distributions and the distribution in the proton-proton four-momentum transfer squared are also presented. This research is relevant in the context of ALICE, ATLAS, CMS, and LHCb measurements in $pp$ collisions, even when the leading protons are not detected and instead only rapidity-gap conditions are checked experimentally. Our results can also serve as basis for the description of coherent $\pi^{+}\pi^{-}$ production in ultra-peripheral $p$A and AA collisions at the LHC. The formulas given in our paper can directly be used for the analysis of photoproduction and small-$Q^{2}$ electroproduction in $ep$ collisions at high energies. Such data exist from the HERA experiments and will be obtained in the future at the electron-ion colliders.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.06330v1" target="_blank">L2Calib: $SE(3)$-Manifold Reinforcement Learning for Robust Extrinsic Calibration with Degenerate Motion Resilience</a></h3>
                    <p><strong>Authors:</strong> Baorun Li, Chengrui Zhu, Siyi Du, Bingran Chen, Jie Ren, Wenfei Wang, Yong Liu, Jiajun Lv</p>
                    <p><strong>Published:</strong> 8/8/2025</p>
                    <p><strong>Categories:</strong> cs.RO</p>
                    <p><strong>Summary:</strong> Extrinsic calibration is essential for multi-sensor fusion, existing methods rely on structured targets or fully-excited data, limiting real-world applicability. Online calibration further suffers from weak excitation, leading to unreliable estimates. To address these limitations, we propose a reinforcement learning (RL)-based extrinsic calibration framework that formulates extrinsic calibration as a decision-making problem, directly optimizes $SE(3)$ extrinsics to enhance odometry accuracy. Our approach leverages a probabilistic Bingham distribution to model 3D rotations, ensuring stable optimization while inherently retaining quaternion symmetry. A trajectory alignment reward mechanism enables robust calibration without structured targets by quantitatively evaluating estimated tightly-coupled trajectory against a reference trajectory. Additionally, an automated data selection module filters uninformative samples, significantly improving efficiency and scalability for large-scale datasets. Extensive experiments on UAVs, UGVs, and handheld platforms demonstrate that our method outperforms traditional optimization-based approaches, achieving high-precision calibration even under weak excitation conditions. Our framework simplifies deployment on diverse robotic platforms by eliminating the need for high-quality initial extrinsics and enabling calibration from routine operating data. The code is available at https://github.com/APRIL-ZJU/learn-to-calibrate.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.06328v1" target="_blank">M2IO-R1: An Efficient RL-Enhanced Reasoning Framework for Multimodal Retrieval Augmented Multimodal Generation</a></h3>
                    <p><strong>Authors:</strong> Zhiyou Xiao, Qinhan Yu, Binghui Li, Geng Chen, Chong Chen, Wentao Zhang</p>
                    <p><strong>Published:</strong> 8/8/2025</p>
                    <p><strong>Categories:</strong> cs.IR</p>
                    <p><strong>Summary:</strong> Current research on Multimodal Retrieval-Augmented Generation (MRAG) enables diverse multimodal inputs but remains limited to single-modality outputs, restricting expressive capacity and practical utility. In contrast, real-world applications often demand both multimodal inputs and multimodal outputs for effective communication and grounded reasoning. Motivated by the recent success of Reinforcement Learning (RL) in complex reasoning tasks for Large Language Models (LLMs), we adopt RL as a principled and effective paradigm to address the multi-step, outcome-driven challenges inherent in multimodal output generation. Here, we introduce M2IO-R1, a novel framework for Multimodal Retrieval-Augmented Multimodal Generation (MRAMG) that supports both multimodal inputs and outputs. Central to our framework is an RL-based inserter, Inserter-R1-3B, trained with Group Relative Policy Optimization to guide image selection and placement in a controllable and semantically aligned manner. Empirical results show that our lightweight 3B inserter achieves strong reasoning capabilities with significantly reduced latency, outperforming baselines in both quality and efficiency.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.06327v1" target="_blank">Can Diffusion Models Bridge the Domain Gap in Cardiac MR Imaging?</a></h3>
                    <p><strong>Authors:</strong> Xin Ci Wong, Duygu Sarikaya, Kieran Zucker, Marc De Kamps, Nishant Ravikumar</p>
                    <p><strong>Published:</strong> 8/8/2025</p>
                    <p><strong>Categories:</strong> cs.CV</p>
                    <p><strong>Summary:</strong> Magnetic resonance (MR) imaging, including cardiac MR, is prone to domain shift due to variations in imaging devices and acquisition protocols. This challenge limits the deployment of trained AI models in real-world scenarios, where performance degrades on unseen domains. Traditional solutions involve increasing the size of the dataset through ad-hoc image augmentation or additional online training/transfer learning, which have several limitations. Synthetic data offers a promising alternative, but anatomical/structural consistency constraints limit the effectiveness of generative models in creating image-label pairs. To address this, we propose a diffusion model (DM) trained on a source domain that generates synthetic cardiac MR images that resemble a given reference. The synthetic data maintains spatial and structural fidelity, ensuring similarity to the source domain and compatibility with the segmentation mask. We assess the utility of our generative approach in multi-centre cardiac MR segmentation, using the 2D nnU-Net, 3D nnU-Net and vanilla U-Net segmentation networks. We explore domain generalisation, where, domain-invariant segmentation models are trained on synthetic source domain data, and domain adaptation, where, we shift target domain data towards the source domain using the DM. Both strategies significantly improved segmentation performance on data from an unseen target domain, in terms of surface-based metrics (Welchs t-test, p  0.01), compared to training segmentation models on real data alone. The proposed method ameliorates the need for transfer learning or online training to address domain shift challenges in cardiac MR image analysis, especially useful in data-scarce settings.</p>
                
            
                
                    <h3><a href="http://dx.doi.org/10.1007/978-3-032-03639-1_4" target="_blank">Social Welfare in Battery Charging Games</a></h3>
                    <p><strong>Authors:</strong> Simon Krogmann, Pascal Lenzner, Alexander Skopalik, Tobias Sträubig</p>
                    <p><strong>Published:</strong> 8/8/2025</p>
                    <p><strong>Categories:</strong> cs.GT</p>
                    <p><strong>Summary:</strong> The recent rise of renewable energy produced by many decentralized sources yields interesting market design challenges for electrical grids. Balancing supply and demand in such networks is both a temporal and spatial challenge due to capacity constraints. The recent surge in the number of household-owned batteries, especially in regions with rooftop solar adoption, offers mitigation potential but often acts misaligned with grid-level objectives. In fact, the decision to charge or discharge a household-owned battery is a strategic choice by each battery owner governed by selfish incentives. This calls for an analysis from a game-theoretic point of view. We initiate this timely research direction by considering a game-theoretic setting where selfish agents strategically charge or discharge their batteries to increase their profit. In particular, we study a Stackelberg-like market model where a third party introduces price incentives, aiming to optimize renewable energy utilization while preserving grid feasibility. For this, we study the existence and the quality of equilibria under various pricing strategies. We find that the existence of equilibria crucially depends on the chosen pricing and that the obtained social welfare varies widely. This calls for more sophisticated market models and pricing mechanisms and opens up a rich field for future research in Algorithmic Game Theory on incentives in renewable energy networks.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.06319v1" target="_blank">Towards Balanced Behavior Cloning from Imbalanced Datasets</a></h3>
                    <p><strong>Authors:</strong> Sagar Parekh, Heramb Nemlekar, Dylan P. Losey</p>
                    <p><strong>Published:</strong> 8/8/2025</p>
                    <p><strong>Categories:</strong> cs.RO</p>
                    <p><strong>Summary:</strong> Robots should be able to learn complex behaviors from human demonstrations. In practice, these human-provided datasets are inevitably imbalanced: i.e., the human demonstrates some subtasks more frequently than others. State-of-the-art methods default to treating each element of the humans dataset as equally important. So if -- for instance -- the majority of the humans data focuses on reaching a goal, and only a few state-action pairs move to avoid an obstacle, the learning algorithm will place greater emphasis on goal reaching. More generally, misalignment between the relative amounts of data and the importance of that data causes fundamental problems for imitation learning approaches. In this paper we analyze and develop learning methods that automatically account for mixed datasets. We formally prove that imbalanced data leads to imbalanced policies when each state-action pair is weighted equally; these policies emulate the most represented behaviors, and not the humans complex, multi-task demonstrations. We next explore algorithms that rebalance offline datasets (i.e., reweight the importance of different state-action pairs) without human oversight. Reweighting the dataset can enhance the overall policy performance. However, there is no free lunch: each method for autonomously rebalancing brings its own pros and cons. We formulate these advantages and disadvantages, helping other researchers identify when each type of approach is most appropriate. We conclude by introducing a novel meta-gradient rebalancing algorithm that addresses the primary limitations behind existing approaches. Our experiments show that dataset rebalancing leads to better downstream learning, improving the performance of general imitation learning algorithms without requiring additional data collection. See our project website: https://collab.me.vt.edu/data_curation/.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.06312v1" target="_blank">Chain-of-Alpha: Unleashing the Power of Large Language Models for Alpha Mining in Quantitative Trading</a></h3>
                    <p><strong>Authors:</strong> Lang Cao, Zekun Xi, Long Liao, Ziwei Yang, Zheng Cao</p>
                    <p><strong>Published:</strong> 8/8/2025</p>
                    <p><strong>Categories:</strong> cs.CE</p>
                    <p><strong>Summary:</strong> Alpha factor mining is a fundamental task in quantitative trading, aimed at discovering interpretable signals that can predict asset returns beyond systematic market risk. While traditional methods rely on manual formula design or heuristic search with machine learning, recent advances have leveraged Large Language Models (LLMs) for automated factor discovery. However, existing LLM-based alpha mining approaches remain limited in terms of automation, generality, and efficiency. In this paper, we propose Chain-of-Alpha, a novel, simple, yet effective and efficient LLM-based framework for fully automated formulaic alpha mining. Our method features a dual-chain architecture, consisting of a Factor Generation Chain and a Factor Optimization Chain, which iteratively generate, evaluate, and refine candidate alpha factors using only market data, while leveraging backtest feedback and prior optimization knowledge. The two chains work synergistically to enable high-quality alpha discovery without human intervention and offer strong scalability. Extensive experiments on real-world A-share benchmarks demonstrate that Chain-of-Alpha outperforms existing baselines across multiple metrics, presenting a promising direction for LLM-driven quantitative research.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.06306v1" target="_blank">Higher Order Regularization using Harmonic Eigenfunctions for Model-Based Reconstruction in Magnetic Particle Imaging</a></h3>
                    <p><strong>Authors:</strong> Thomas März, Vladyslav Gapyak, Andreas Weinmann</p>
                    <p><strong>Published:</strong> 8/8/2025</p>
                    <p><strong>Categories:</strong> math.NA, cs.NA, 65K10, 65R32, 65T40, 92C55</p>
                    <p><strong>Summary:</strong> Magnetic Particle Imaging (MPI) is a recent imaging modality where superparamagnetic nanoparticles are employed as tracers. The reconstruction task is to obtain the spatial particle distribution from a voltage signal induced by the particles. Generally, in computational imaging variational reconstruction techniques are common and rely on a mathematical model to describe the underlying physics. For the MPI reconstruction task we propose a model-based variational reconstruction technique which incorporates a higher order regularizer, where the regularizer is diagonalized by harmonic eigenfunctions. The proposed image reconstruction algorithm features two major stages: in the first stage, the core stage, the components of the MPI core response are reconstructed. This is the MPI-specific data approximation task which we formulate as a variational problem incorporating the higher order regularizer. The relationship between the particle distribution, the MPI core response and the measured data is given by a mathematical model which was introduced in our earlier research. According to this model the MPI core response is tied to the particle distribution by convolution. Therefore the outcome of the core stage yields the data for the second stage, the deconvolution stage, in which the final reconstructed image is produced by solving an ill-posed deconvolution problem in a robust way relying on earlier research. Interestingly, the quality of the final image depends significantly on the quality of the result of the core stage. A contribution is thus the enhancement of the core stage via higher order regularization. We provide a theoretical foundation for our approach and demonstrate its benefit with numerical examples.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.06300v1" target="_blank">Automatic Semantic Alignment of Flow Pattern Representations for Exploration with Large Language Models</a></h3>
                    <p><strong>Authors:</strong> Weihan Zhang, Jun Tao</p>
                    <p><strong>Published:</strong> 8/8/2025</p>
                    <p><strong>Categories:</strong> cs.HC</p>
                    <p><strong>Summary:</strong> Explorative flow visualization allows domain experts to analyze complex flow structures by interactively investigating flow patterns. However, traditional visual interfaces often rely on specialized graphical representations and interactions, which require additional effort to learn and use. Natural language interaction offers a more intuitive alternative, but teaching machines to recognize diverse scientific concepts and extract corresponding structures from flow data poses a significant challenge. In this paper, we introduce an automated framework that aligns flow pattern representations with the semantic space of large language models (LLMs), eliminating the need for manual labeling. Our approach encodes streamline segments using a denoising autoencoder and maps the generated flow pattern representations to LLM embeddings via a projector layer. This alignment empowers semantic matching between textual embeddings and flow representations through an attention mechanism, enabling the extraction of corresponding flow patterns based on textual descriptions. To enhance accessibility, we develop an interactive interface that allows users to query and visualize flow structures using natural language. Through case studies, we demonstrate the effectiveness of our framework in enabling intuitive and intelligent flow exploration.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.06297v1" target="_blank">KV Cache Compression for Inference Efficiency in LLMs: A Review</a></h3>
                    <p><strong>Authors:</strong> Yanyu Liu, Jingying Fu, Sixiang Liu, Yitian Zou, You Fu, Jiehan Zhou, Shouhua Zhang</p>
                    <p><strong>Published:</strong> 8/8/2025</p>
                    <p><strong>Categories:</strong> cs.DC</p>
                    <p><strong>Summary:</strong> Withtherapid advancement of large language models (LLMs), the context length for inference has been continuously increasing, leading to an exponential growth in the demand for Key-Value (KV) caching. This has resulted in a significant memory bottleneck, limiting the inference efficiency and scalability of the models. Therefore, optimizing the KV cache during inference is crucial for enhancing performance and efficiency. This review systematically examines current KV cache optimization techniques, including compression strategies such as selective token strategies, quantization, and attention compression. We evaluate the effectiveness, trade-offs, and application scenarios of these methods, providing a comprehensive analysis of their impact on memory usage and inference speed. We focus on identifying the limitations and challenges of existing methods, such as compatibility issues with different models and tasks. Additionally, this review highlights future research directions, including hybrid optimization techniques, adaptive dynamic strategies, and software-hardware co-design. These approaches aim to improve inference efficiency and promote the practical application of large language models.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.06296v1" target="_blank">LLM Robustness Leaderboard v1 --Technical report</a></h3>
                    <p><strong>Authors:</strong> Pierre Peigné - Lefebvre, Quentin Feuillade-Montixi, Tom David, Nicolas Miailhe</p>
                    <p><strong>Published:</strong> 8/8/2025</p>
                    <p><strong>Categories:</strong> cs.AI, cs.LG</p>
                    <p><strong>Summary:</strong> This technical report accompanies the LLM robustness leaderboard published by PRISM Eval for the Paris AI Action Summit. We introduce PRISM Eval Behavior Elicitation Tool (BET), an AI system performing automated red-teaming through Dynamic Adversarial Optimization that achieves 100% Attack Success Rate (ASR) against 37 of 41 state-of-the-art LLMs. Beyond binary success metrics, we propose a fine-grained robustness metric estimating the average number of attempts required to elicit harmful behaviors, revealing that attack difficulty varies by over 300-fold across models despite universal vulnerability. We introduce primitive-level vulnerability analysis to identify which jailbreaking techniques are most effective for specific hazard categories. Our collaborative evaluation with trusted third parties from the AI Safety Network demonstrates practical pathways for distributed robustness assessment across the community.</p>
                
            
                
                    <h3><a href="http://dx.doi.org/10.1109/ICIT64950.2025.11049178" target="_blank">Advanced Deep Learning Techniques for Accurate Lung Cancer Detection and Classification</a></h3>
                    <p><strong>Authors:</strong> Mobarak Abumohsen, Enrique Costa-Montenegro, Silvia García-Méndez, Amani Yousef Owda, Majdi Owda</p>
                    <p><strong>Published:</strong> 8/8/2025</p>
                    <p><strong>Categories:</strong> eess.IV, cs.AI, cs.CV</p>
                    <p><strong>Summary:</strong> Lung cancer (LC) ranks among the most frequently diagnosed cancers and is one of the most common causes of death for men and women worldwide. Computed Tomography (CT) images are the most preferred diagnosis method because of their low cost and their faster processing times. Many researchers have proposed various ways of identifying lung cancer using CT images. However, such techniques suffer from significant false positives, leading to low accuracy. The fundamental reason results from employing a small and imbalanced dataset. This paper introduces an innovative approach for LC detection and classification from CT images based on the DenseNet201 model. Our approach comprises several advanced methods such as Focal Loss, data augmentation, and regularization to overcome the imbalanced data issue and overfitting challenge. The findings show the appropriateness of the proposal, attaining a promising performance of 98.95% accuracy.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.06494v1" target="_blank">LightSwitch: Multi-view Relighting with Material-guided Diffusion</a></h3>
                    <p><strong>Authors:</strong> Yehonathan Litman, Fernando De la Torre, Shubham Tulsiani</p>
                    <p><strong>Published:</strong> 8/8/2025</p>
                    <p><strong>Categories:</strong> cs.CV</p>
                    <p><strong>Summary:</strong> Recent approaches for 3D relighting have shown promise in integrating 2D image relighting generative priors to alter the appearance of a 3D representation while preserving the underlying structure. Nevertheless, generative priors used for 2D relighting that directly relight from an input image do not take advantage of intrinsic properties of the subject that can be inferred or cannot consider multi-view data at scale, leading to subpar relighting. In this paper, we propose Lightswitch, a novel finetuned material-relighting diffusion framework that efficiently relights an arbitrary number of input images to a target lighting condition while incorporating cues from inferred intrinsic properties. By using multi-view and material information cues together with a scalable denoising scheme, our method consistently and efficiently relights dense multi-view data of objects with diverse material compositions. We show that our 2D relighting prediction quality exceeds previous state-of-the-art relighting priors that directly relight from images. We further demonstrate that LightSwitch matches or outperforms state-of-the-art diffusion inverse rendering methods in relighting synthetic and real objects in as little as 2 minutes.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.06493v1" target="_blank">Fate of an impurity strongly interacting with a thermal Bose gas</a></h3>
                    <p><strong>Authors:</strong> Jiří Etrych, Sebastian J. Morris, Simon M. Fischer, Gevorg Martirosyan, Christopher J. Ho, Moritz Drescher, Manfred Salmhofer, Zoran Hadzibabic, Tilman Enss, Christoph Eigen</p>
                    <p><strong>Published:</strong> 8/8/2025</p>
                    <p><strong>Categories:</strong> cond-mat.quant-gas, physics.atom-ph, quant-ph</p>
                    <p><strong>Summary:</strong> We spectroscopically study mobile impurities immersed in a homogeneous bosonic bath (a box-trapped Bose gas), varying the bath temperature and the strength of impurity-bath interactions. We compare our results to those for a quasipure Bose-Einstein condensate (BEC), and find that for strong impurity-bath interactions, the spectra narrow with increasing temperature, while the impurity energy shift is suppressed. Near the critical temperature for condensation, many-body effects still play an important role, and only for a nondegenerate bath, the system approaches the classical Boltzmann-gas behavior. The key spectral features are reproduced within the theory of an ideal Bose polaron.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.06492v1" target="_blank">Effective Training Data Synthesis for Improving MLLM Chart Understanding</a></h3>
                    <p><strong>Authors:</strong> Yuwei Yang, Zeyu Zhang, Yunzhong Hou, Zhuowan Li, Gaowen Liu, Ali Payani, Yuan-Sen Ting, Liang Zheng</p>
                    <p><strong>Published:</strong> 8/8/2025</p>
                    <p><strong>Categories:</strong> cs.CV, cs.CL</p>
                    <p><strong>Summary:</strong> Being able to effectively read scientific plots, or chart understanding, is a central part toward building effective agents for science. However, existing multimodal large language models (MLLMs), especially open-source ones, are still falling behind with a typical success rate of 30%-50% on challenging benchmarks. Previous studies on fine-tuning MLLMs with synthetic charts are often restricted by their inadequate similarity to the real charts, which could compromise model training and performance on complex real-world charts. In this study, we show that modularizing chart generation and diversifying visual details improves chart understanding capabilities. In particular, we design a five-step data synthesis pipeline, where we separate data and function creation for single plot generation, condition the generation of later subplots on earlier ones for multi-subplot figures, visually diversify the generated figures, filter out low quality data, and finally generate the question-answer (QA) pairs with GPT-4o. This approach allows us to streamline the generation of fine-tuning datasets and introduce the effective chart dataset (ECD), which contains 10k+ chart images and 300k+ QA pairs, covering 25 topics and featuring 250+ chart type combinations with high visual complexity. We show that ECD consistently improves the performance of various MLLMs on a range of real-world and synthetic test sets. Code, data and models are available at: https://github.com/yuweiyang-anu/ECD.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.06491v1" target="_blank">Computational Methods and Verification Theorem for Portfolio-Consumption Optimization under Exponential O-U Dynamics</a></h3>
                    <p><strong>Authors:</strong> Zhaoxiang Zhong, Haiming Song</p>
                    <p><strong>Published:</strong> 8/8/2025</p>
                    <p><strong>Categories:</strong> math.OC</p>
                    <p><strong>Summary:</strong> In this paper, we focus on the problem of optimal portfolio-consumption policies in a multi-asset financial market, where the n risky assets follow Exponential Ornstein-Uhlenbeck processes, along with one risk-free bond. The investors preferences are modeled using Constant Relative Risk Aversion utility with state-dependent stochastic discounting. The problem can be formulated as a high-dimensional stochastic optimal control problem, wherein the associated value function satisfies a Hamilton-Jacobi-Bellman (HJB) equation, which constitutes a necessary condition for optimality. We apply a variable separation technique to transform the HJB equation to a system of ordinary differential equations (ODEs). Then a class of hybrid numerical approaches that integrate exponential Rosenbrock-type methods with Runge-Kutta methods is proposed to solve the ODE system. More importantly, we establish a rigorous verification theorem that provides sufficient conditions for the existence of value function and admissible optimal control, which can be verified numerically. A series of experiments are performed, demonstrating that our proposed method outperforms the conventional grid-based method in both accuracy and computational cost. Furthermore, the numerically derived optimal policy achieves superior performance over all other considered admissible policies.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.06490v1" target="_blank">Multivariate Fields of Experts</a></h3>
                    <p><strong>Authors:</strong> Stanislas Ducotterd, Michael Unser</p>
                    <p><strong>Published:</strong> 8/8/2025</p>
                    <p><strong>Categories:</strong> eess.IV, cs.CV, cs.LG, eess.SP</p>
                    <p><strong>Summary:</strong> We introduce the multivariate fields of experts, a new framework for the learning of image priors. Our model generalizes existing fields of experts methods by incorporating multivariate potential functions constructed via Moreau envelopes of the $\ell_\infty$-norm. We demonstrate the effectiveness of our proposal across a range of inverse problems that include image denoising, deblurring, compressed-sensing magnetic-resonance imaging, and computed tomography. The proposed approach outperforms comparable univariate models and achieves performance close to that of deep-learning-based regularizers while being significantly faster, requiring fewer parameters, and being trained on substantially fewer data. In addition, our model retains a relatively high level of interpretability due to its structured design.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.06489v1" target="_blank">Voting-Based Semi-Parallel Proof-of-Work Protocol</a></h3>
                    <p><strong>Authors:</strong> Mustafa Doger, Sennur Ulukus</p>
                    <p><strong>Published:</strong> 8/8/2025</p>
                    <p><strong>Categories:</strong> cs.CR, cs.DC, cs.DM, cs.IT, math.IT, math.PR</p>
                    <p><strong>Summary:</strong> Parallel Proof-of-Work (PoW) protocols are suggested to improve the safety guarantees, transaction throughput and confirmation latencies of Nakamoto consensus. In this work, we first consider the existing parallel PoW protocols and develop hard-coded incentive attack structures. Our theoretical results and simulations show that the existing parallel PoW protocols are more vulnerable to incentive attacks than the Nakamoto consensus, e.g., attacks have smaller profitability threshold and they result in higher relative rewards. Next, we introduce a voting-based semi-parallel PoW protocol that outperforms both Nakamoto consensus and the existing parallel PoW protocols from most practical perspectives such as communication overheads, throughput, transaction conflicts, incentive compatibility of the protocol as well as a fair distribution of transaction fees among the voters and the leaders. We use state-of-the-art analysis to evaluate the consistency of the protocol and consider Markov decision process (MDP) models to substantiate our claims about the resilience of our protocol against incentive attacks.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.06486v1" target="_blank">Does block size matter in randomized block Krylov low-rank approximation?</a></h3>
                    <p><strong>Authors:</strong> Tyler Chen, Ethan N. Epperly, Raphael A. Meyer, Christopher Musco, Akash Rao</p>
                    <p><strong>Published:</strong> 8/8/2025</p>
                    <p><strong>Categories:</strong> cs.DS, cs.NA, math.NA, 65F55 65F15, G.1.3; F.2.1</p>
                    <p><strong>Summary:</strong> We study the problem of computing a rank-$k$ approximation of a matrix using randomized block Krylov iteration. Prior work has shown that, for block size $b = 1$ or $b = k$, a $(1 + \varepsilon)$-factor approximation to the best rank-$k$ approximation can be obtained after $\tilde O(k/\sqrt{\varepsilon})$ matrix-vector products with the target matrix. On the other hand, when $b$ is between $1$ and $k$, the best known bound on the number of matrix-vector products scales with $b(k-b)$, which could be as large as $O(k^2)$. Nevertheless, in practice, the performance of block Krylov methods is often optimized by choosing a block size $1 \ll b \ll k$. We resolve this theory-practice gap by proving that randomized block Krylov iteration produces a $(1 + \varepsilon)$-factor approximate rank-$k$ approximation using $\tilde O(k/\sqrt{\varepsilon})$ matrix-vector products for any block size $1\le b\le k$. Our analysis relies on new bounds for the minimum singular value of a random block Krylov matrix, which may be of independent interest. Similar bounds are central to recent breakthroughs on faster algorithms for sparse linear systems [Peng  Vempala, SODA 2021; Nie, STOC 2022].</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.06485v1" target="_blank">WGAST: Weakly-Supervised Generative Network for Daily 10 m Land Surface Temperature Estimation via Spatio-Temporal Fusion</a></h3>
                    <p><strong>Authors:</strong> Sofiane Bouaziz, Adel Hafiane, Raphael Canals, Rachid Nedjai</p>
                    <p><strong>Published:</strong> 8/8/2025</p>
                    <p><strong>Categories:</strong> cs.CV, cs.AI, cs.LG</p>
                    <p><strong>Summary:</strong> Urbanization, climate change, and agricultural stress are increasing the demand for precise and timely environmental monitoring. Land Surface Temperature (LST) is a key variable in this context and is retrieved from remote sensing satellites. However, these systems face a trade-off between spatial and temporal resolution. While spatio-temporal fusion methods offer promising solutions, few have addressed the estimation of daily LST at 10 m resolution. In this study, we present WGAST, a Weakly-Supervised Generative Network for Daily 10 m LST Estimation via Spatio-Temporal Fusion of Terra MODIS, Landsat 8, and Sentinel-2. WGAST is the first end-to-end deep learning framework designed for this task. It adopts a conditional generative adversarial architecture, with a generator composed of four stages: feature extraction, fusion, LST reconstruction, and noise suppression. The first stage employs a set of encoders to extract multi-level latent representations from the inputs, which are then fused in the second stage using cosine similarity, normalization, and temporal attention mechanisms. The third stage decodes the fused features into high-resolution LST, followed by a Gaussian filter to suppress high-frequency noise. Training follows a weakly supervised strategy based on physical averaging principles and reinforced by a PatchGAN discriminator. Experiments demonstrate that WGAST outperforms existing methods in both quantitative and qualitative evaluations. Compared to the best-performing baseline, on average, WGAST reduces RMSE by 17.18% and improves SSIM by 11.00%. Furthermore, WGAST is robust to cloud-induced LST and effectively captures fine-scale thermal patterns, as validated against 33 ground-based sensors. The code is available at https://github.com/Sofianebouaziz1/WGAST.git.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.06482v1" target="_blank">Post-training for Efficient Communication via Convention Formation</a></h3>
                    <p><strong>Authors:</strong> Yilun Hua, Evan Wang, Yoav Artzi</p>
                    <p><strong>Published:</strong> 8/8/2025</p>
                    <p><strong>Categories:</strong> cs.CL, cs.AI, cs.LG</p>
                    <p><strong>Summary:</strong> Humans communicate with increasing efficiency in multi-turn interactions, by adapting their language and forming ad-hoc conventions. In contrast, prior work shows that LLMs do not naturally show this behavior. We develop a post-training process to develop this ability through targeted fine-tuning on heuristically identified demonstrations of convention formation. We evaluate with two new benchmarks focused on this capability. First, we design a focused, cognitively-motivated interaction benchmark that consistently elicits strong convention formation trends in humans. Second, we create a new document-grounded reference completion task that reflects in-the-wild convention formation behavior. Our studies show significantly improved convention formation abilities in post-trained LLMs across the two evaluation methods.</p>
                
            
                
                    <h3><a href="http://dx.doi.org/10.1136/jme-2025-110972" target="_blank">The Problem of Atypicality in LLM-Powered Psychiatry</a></h3>
                    <p><strong>Authors:</strong> Bosco Garcia, Eugene Y. S. Chua, Harman Singh Brah</p>
                    <p><strong>Published:</strong> 8/8/2025</p>
                    <p><strong>Categories:</strong> cs.CY</p>
                    <p><strong>Summary:</strong> Large language models (LLMs) are increasingly proposed as scalable solutions to the global mental health crisis. But their deployment in psychiatric contexts raises a distinctive ethical concern: the problem of atypicality. Because LLMs generate outputs based on population-level statistical regularities, their responses -- while typically appropriate for general users -- may be dangerously inappropriate when interpreted by psychiatric patients, who often exhibit atypical cognitive or interpretive patterns. We argue that standard mitigation strategies, such as prompt engineering or fine-tuning, are insufficient to resolve this structural risk. Instead, we propose dynamic contextual certification (DCC): a staged, reversible and context-sensitive framework for deploying LLMs in psychiatry, inspired by clinical translation and dynamic safety models from artificial intelligence governance. DCC reframes chatbot deployment as an ongoing epistemic and ethical process that prioritises interpretive safety over static performance benchmarks. Atypicality, we argue, cannot be eliminated -- but it can, and must, be proactively managed.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.06478v1" target="_blank">On the Parallel Complexity of Identifying Groups and Quasigroups via Decompositions</a></h3>
                    <p><strong>Authors:</strong> Dan Johnson, Michael Levet, Petr Vojtěchovský, Brett Widholm</p>
                    <p><strong>Published:</strong> 8/8/2025</p>
                    <p><strong>Categories:</strong> cs.DS, cs.CC, math.GR</p>
                    <p><strong>Summary:</strong> In this paper, we investigate the computational complexity of isomorphism testing for finite groups and quasigroups, given by their multiplication tables. We crucially take advantage of their various decompositions to show the following: - We first consider the class $\mathcal{C}$ of groups that admit direct product decompositions, where each indecompsable factor is $O(1)$-generated, and either perfect or centerless. We show any group in $\mathcal{C}$ is identified by the $O(1)$-dimensional count-free Weisfeiler--Leman (WL) algorithm with $O(\log \log n)$ rounds, and the $O(1)$-dimensional counting WL algorithm with $O(1)$ rounds. Consequently, the isomorphism problem for $\mathcal{C}$ is in $\textsf{L}$. The previous upper bound for this class was $\textsf{TC}^{1}$, using $O(\log n)$ rounds of the $O(1)$-dimensional counting WL (Grochow and Levet, FCT 2023). - We next consider more generally, the class of groups where each indecomposable factor is $O(1)$-generated. We exhibit an $\textsf{AC}^{3}$ canonical labeling procedure for this class. Here, we accomplish this by showing that in the multiplication table model, the direct product decomposition can be computed in $\textsf{AC}^{3}$, parallelizing the work of Kayal and Nezhmetdinov (ICALP 2009). - Isomorphism testing between a central quasigroup $G$ and an arbitrary quasigroup $H$ is in $\textsf{NC}$. Here, we take advantage of the fact that central quasigroups admit an affine decomposition in terms of an underlying Abelian group. Only the trivial bound of $n^{\log(n)+O(1)}$-time was previously known for isomorphism testing of central quasigroups.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.06475v1" target="_blank">HapticLLaMA: A Multimodal Sensory Language Model for Haptic Captioning</a></h3>
                    <p><strong>Authors:</strong> Guimin Hu, Daniel Hershcovich, Hasti Seifi</p>
                    <p><strong>Published:</strong> 8/8/2025</p>
                    <p><strong>Categories:</strong> cs.CL</p>
                    <p><strong>Summary:</strong> Haptic captioning is the task of generating natural language descriptions from haptic signals, such as vibrations, for use in virtual reality, accessibility, and rehabilitation applications. While previous multimodal research has focused primarily on vision and audio, haptic signals for the sense of touch remain underexplored. To address this gap, we formalize the haptic captioning task and propose HapticLLaMA, a multimodal sensory language model that interprets vibration signals into descriptions in a given sensory, emotional, or associative category. We investigate two types of haptic tokenizers, a frequency-based tokenizer and an EnCodec-based tokenizer, that convert haptic signals into sequences of discrete units, enabling their integration with the LLaMA model. HapticLLaMA is trained in two stages: (1) supervised fine-tuning using the LLaMA architecture with LoRA-based adaptation, and (2) fine-tuning via reinforcement learning from human feedback (RLHF). We assess HapticLLaMAs captioning performance using both automated n-gram metrics and human evaluation. HapticLLaMA demonstrates strong capability in interpreting haptic vibration signals, achieving a METEOR score of 59.98 and a BLEU-4 score of 32.06 respectively. Additionally, over 61% of the generated captions received human ratings above 3.5 on a 7-point scale, with RLHF yielding a 10% improvement in the overall rating distribution, indicating stronger alignment with human haptic perception. These findings highlight the potential of large language models to process and adapt to sensory data.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.06474v1" target="_blank">Exploring the feasibility of probabilistic and deterministic quantum gates between T centers in silicon</a></h3>
                    <p><strong>Authors:</strong> Shahrzad Taherizadegan, Faezeh Kimiaee Asadi, Jia-Wei Ji, Daniel Higginbottom, Christoph Simon</p>
                    <p><strong>Published:</strong> 8/8/2025</p>
                    <p><strong>Categories:</strong> quant-ph, physics.optics</p>
                    <p><strong>Summary:</strong> T center defects in silicon provide an attractive platform for quantum technologies due to their unique spin properties and compatibility with mature silicon technologies. We investigate several gate protocols between single T centers, including two probabilistic photon interference-based schemes, a near-deterministic photon scattering gate, and a deterministic magnetic dipole-based scheme. In particular, we study a photon interference-based scheme with feedback which can achieve success probabilities above 50%, and use the photon-count decomposition method to perform the first analytical calculations of its entanglement fidelity and efficiency while accounting for imperfections. We also calculate the fidelity and efficiency of the other schemes. Finally, we compare the performance of all the schemes, considering current and near-future experimental capabilities. In particular, we find that the photon interference-based scheme with feedback has the potential to achieve competitive efficiency and fidelity, making it interesting to explore experimentally.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.06471v1" target="_blank">GLM-4.5: Agentic, Reasoning, and Coding (ARC) Foundation Models</a></h3>
                    <p><strong>Authors:</strong> GLM-4. 5 Team, :, Aohan Zeng, Xin Lv, Qinkai Zheng, Zhenyu Hou, Bin Chen, Chengxing Xie, Cunxiang Wang, Da Yin, Hao Zeng, Jiajie Zhang, Kedong Wang, Lucen Zhong, Mingdao Liu, Rui Lu, Shulin Cao, Xiaohan Zhang, Xuancheng Huang, Yao Wei, Yean Cheng, Yifan An, Yilin Niu, Yuanhao Wen, Yushi Bai, Zhengxiao Du, Zihan Wang, Zilin Zhu, Bohan Zhang, Bosi Wen, Bowen Wu, Bowen Xu, Can Huang, Casey Zhao, Changpeng Cai, Chao Yu, Chen Li, Chendi Ge, Chenghua Huang, Chenhui Zhang, Chenxi Xu, Chenzheng Zhu, Chuang Li, Congfeng Yin, Daoyan Lin, Dayong Yang, Dazhi Jiang, Ding Ai, Erle Zhu, Fei Wang, Gengzheng Pan, Guo Wang, Hailong Sun, Haitao Li, Haiyang Li, Haiyi Hu, Hanyu Zhang, Hao Peng, Hao Tai, Haoke Zhang, Haoran Wang, Haoyu Yang, He Liu, He Zhao, Hongwei Liu, Hongxi Yan, Huan Liu, Huilong Chen, Ji Li, Jiajing Zhao, Jiamin Ren, Jian Jiao, Jiani Zhao, Jianyang Yan, Jiaqi Wang, Jiayi Gui, Jiayue Zhao, Jie Liu, Jijie Li, Jing Li, Jing Lu, Jingsen Wang, Jingwei Yuan, Jingxuan Li, Jingzhao Du, Jinhua Du, Jinxin Liu, Junkai Zhi, Junli Gao, Ke Wang, Lekang Yang, Liang Xu, Lin Fan, Lindong Wu, Lintao Ding, Lu Wang, Man Zhang, Minghao Li, Minghuan Xu, Mingming Zhao, Mingshu Zhai, Pengfan Du, Qian Dong, Shangde Lei, Shangqing Tu, Shangtong Yang, Shaoyou Lu, Shijie Li, Shuang Li, Shuang-Li, Shuxun Yang, Sibo Yi, Tianshu Yu, Wei Tian, Weihan Wang, Wenbo Yu, Weng Lam Tam, Wenjie Liang, Wentao Liu, Xiao Wang, Xiaohan Jia, Xiaotao Gu, Xiaoying Ling, Xin Wang, Xing Fan, Xingru Pan, Xinyuan Zhang, Xinze Zhang, Xiuqing Fu, Xunkai Zhang, Yabo Xu, Yandong Wu, Yida Lu, Yidong Wang, Yilin Zhou, Yiming Pan, Ying Zhang, Yingli Wang, Yingru Li, Yinpei Su, Yipeng Geng, Yitong Zhu, Yongkun Yang, Yuhang Li, Yuhao Wu, Yujiang Li, Yunan Liu, Yunqing Wang, Yuntao Li, Yuxuan Zhang, Zezhen Liu, Zhen Yang, Zhengda Zhou, Zhongpei Qiao, Zhuoer Feng, Zhuorui Liu, Zichen Zhang, Zihan Wang, Zijun Yao, Zikang Wang, Ziqiang Liu, Ziwei Chai, Zixuan Li, Zuodong Zhao, Wenguang Chen, Jidong Zhai, Bin Xu, Minlie Huang, Hongning Wang, Juanzi Li, Yuxiao Dong, Jie Tang</p>
                    <p><strong>Published:</strong> 8/8/2025</p>
                    <p><strong>Categories:</strong> cs.CL</p>
                    <p><strong>Summary:</strong> We present GLM-4.5, an open-source Mixture-of-Experts (MoE) large language model with 355B total parameters and 32B activated parameters, featuring a hybrid reasoning method that supports both thinking and direct response modes. Through multi-stage training on 23T tokens and comprehensive post-training with expert model iteration and reinforcement learning, GLM-4.5 achieves strong performance across agentic, reasoning, and coding (ARC) tasks, scoring 70.1% on TAU-Bench, 91.0% on AIME 24, and 64.2% on SWE-bench Verified. With much fewer parameters than several competitors, GLM-4.5 ranks 3rd overall among all evaluated models and 2nd on agentic benchmarks. We release both GLM-4.5 (355B parameters) and a compact version, GLM-4.5-Air (106B parameters), to advance research in reasoning and agentic AI systems. Code, models, and more information are available at https://github.com/zai-org/GLM-4.5.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.06470v1" target="_blank">Generative AI and the Future of the Digital Commons: Five Open Questions and Knowledge Gaps</a></h3>
                    <p><strong>Authors:</strong> Arman Noroozian, Lorena Aldana, Marta Arisi, Hadi Asghari, Renata Avila, Pietro Giovanni Bizzaro, Ramya Chandrasekhar, Cristian Consonni, Deborah De Angelis, Francesca De Chiara, Maria del Rio-Chanona, Melanie Dulong de Rosnay, Maria Eriksson, Frederic Font, Emilia Gomez, Valérian Guillier, Lisa Gutermuth, David Hartmann, Lucie-Aimée Kaffee, Paul Keller, Felix Stalder, Joao Vinagre, Denny Vrandečić, Amanda Wasielewski</p>
                    <p><strong>Published:</strong> 8/8/2025</p>
                    <p><strong>Categories:</strong> cs.CY, K.4.1; K.4.2; I.2.0</p>
                    <p><strong>Summary:</strong> The rapid advancement of Generative AI (GenAI) relies heavily on the digital commons, a vast collection of free and open online content that is created, shared, and maintained by communities. However, this relationship is becoming increasingly strained due to financial burdens, decreased contributions, and misalignment between AI models and community norms. As we move deeper into the GenAI era, it is essential to examine the interdependent relationship between GenAI, the long-term sustainability of the digital commons, and the equity of current AI development practices. We highlight five critical questions that require urgent attention: 1. How can we prevent the digital commons from being threatened by undersupply as individuals cease contributing to the commons and turn to Generative AI for information? 2. How can we mitigate the risk of the open web closing due to restrictions on access to curb AI crawlers? 3. How can technical standards and legal frameworks be updated to reflect the evolving needs of organizations hosting common content? 4. What are the effects of increased synthetic content in open knowledge databases, and how can we ensure their integrity? 5. How can we account for and distribute the infrastructural and environmental costs of providing data for AI training? We emphasize the need for more responsible practices in AI development, recognizing the digital commons not only as content but as a collaborative and decentralized form of knowledge governance, which relies on the practice of commoning - making, maintaining, and protecting shared and open resources. Ultimately, our goal is to stimulate discussion and research on the intersection of Generative AI and the digital commons, with the aim of developing an AI commons and public infrastructures for AI development that support the long-term health of the digital commons.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.06469v1" target="_blank">A Geometric Analysis of Gains from Trade</a></h3>
                    <p><strong>Authors:</strong> Jason Hartline, Kangning Wang</p>
                    <p><strong>Published:</strong> 8/8/2025</p>
                    <p><strong>Categories:</strong> cs.GT, econ.TH</p>
                    <p><strong>Summary:</strong> We provide a geometric proof that the random proposer mechanism is a $4$-approximation to the first-best gains from trade in bilateral exchange. We then refine this geometric analysis to recover the state-of-the-art approximation ratio of $3.15$.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.06466v1" target="_blank">Simulating Floquet non-Abelian topological insulator with photonic quantum walks</a></h3>
                    <p><strong>Authors:</strong> Quan Lin, Tianyu Li, Haiping Hu, Wei Yi, Peng Xue</p>
                    <p><strong>Published:</strong> 8/8/2025</p>
                    <p><strong>Categories:</strong> cond-mat.mes-hall, cond-mat.quant-gas, physics.optics, quant-ph</p>
                    <p><strong>Summary:</strong> Floquet non-Abelian topological phases emerge in periodically driven systems and exhibit properties that are absent in their Abelian or static counterparts. Dubbed the Floquet non-Abelian topological insulators (FNATIs), they are characterized by non-Abelian topological charges and feature multifold bulk-boundary correspondence, making their experimental observation challenging. Here we simulate the FNATI using a higher-dimensional photonic quantum walk and develop dynamic measurement schemes to demonstrate key signatures of the FNATI. Importantly, combining a direct bulk-dynamic detection for the underlying quaternion topological charge, and a spatially-resolved injection spectroscopy for the edge states, we experimentally establish the multifold bulk-boundary correspondence, and, in particular, identify the anomalous non-Abelian phase where edge states appear in all band gaps, despite the presence of a trivial topological charge. Our experiment marks the first experimental characterization of the FNATI, providing general insight into the non-Abelian topological phases.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.06465v1" target="_blank">Vacuum polarization in the horizonless Bardeen metric</a></h3>
                    <p><strong>Authors:</strong> Andrés Boasso, Francisco D. Mazzitelli</p>
                    <p><strong>Published:</strong> 8/8/2025</p>
                    <p><strong>Categories:</strong> hep-th, gr-qc</p>
                    <p><strong>Summary:</strong> We compute the renormalized stress-energy tensor for a massless quantum scalar field in the background of the horizonless Bardeen spacetime. Within the weak-field approximation, we show that the vacuum fluctuations differ significantly between conformally and non-conformally coupled fields, both in magnitude and in their behavior at short and intermediate distances. At large distances, we recover the universal asymptotic behavior previously observed in black hole and Newtonian star backgrounds. Going beyond the weak-field regime, we find that, for certain parameter ranges, the modes of the field can develop imaginary frequencies, leading to instabilities and an exponential growth of vacuum fluctuations. We also discuss critically the applicability of the anomaly-induced effective action for computing the renormalized stress-energy tensor in the conformally coupled case.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.06460v1" target="_blank">A Simple PTAS for Weighted $k$-means and Sensor Coverage</a></h3>
                    <p><strong>Authors:</strong> Akash Pareek, Supratim Shit</p>
                    <p><strong>Published:</strong> 8/8/2025</p>
                    <p><strong>Categories:</strong> cs.DS</p>
                    <p><strong>Summary:</strong> Clustering is a fundamental technique in data analysis, with the $k$-means being one of the widely studied objectives due to its simplicity and broad applicability. In many practical scenarios, data points come with associated weights that reflect their importance, frequency, or confidence. Given a weighted point set $P \subset R^d$, where each point $p \in P$ has a positive weight $w_p$, the goal is to compute a set of $k$ centers $C = \{ c_1, c_2, \ldots, c_k \} \subset R^d$ that minimizes the weighted clustering cost: $\Delta_w(P,C) = \sum_{p \in P} w_p \cdot d(p,C)^2$, where $d(p,C)$ denotes the Euclidean distance from $p$ to its nearest center in $C$. Although most existing coreset-based algorithms for $k$-means extend naturally to the weighted setting and provide a PTAS, no prior work has offered a simple, coreset-free PTAS designed specifically for the weighted $k$-means problem. In this paper, we present a simple PTAS for weighted $k$-means that does not rely on coresets. Building upon the framework of Jaiswal, Kumar, and Sen (2012) for the unweighted case, we extend the result to the weighted setting by using the weighted $D^2$-sampling technique. Our algorithm runs in time $n d \cdot 2^{O\left(\frac{k^2}{\epsilon}\right)}$ and outputs a set of $k$ centers whose total clustering cost is within a $(1 + \epsilon)$-factor of the optimal cost. As a key application of the weighted $k$-means, we obtain a PTAS for the sensor coverage problem, which can also be viewed as a continuous locational optimization problem. For this problem, the best-known result prior to our work was an $O(\log k)$-approximation by Deshpande (2014), whereas our algorithm guarantees a $(1 + \epsilon)$-approximation to the optimal coverage cost even before applying refinement steps like Lloyd desent.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.06459v1" target="_blank">A literature-derived dataset of migration barriers for quantifying ionic transport in battery materials</a></h3>
                    <p><strong>Authors:</strong> Reshma Devi, Avaneesh Balasubramanian, Keith T. Butler, Gopalakrishnan Sai Gautam</p>
                    <p><strong>Published:</strong> 8/8/2025</p>
                    <p><strong>Categories:</strong> cond-mat.mtrl-sci</p>
                    <p><strong>Summary:</strong> The rate performance of any electrode or solid electrolyte material used in a battery is critically dependent on the migration barrier ($E_m$) governing the motion of the intercalant ion, which is a difficult-to-estimate quantity both experimentally and computationally. The foundation for constructing and validating accurate machine learning (ML) models that are capable of predicting $E_m$, and hence accelerating the discovery of novel electrodes and solid electrolytes, lies in the availability of high-quality dataset(s) containing $E_m$. Addressing this critical requirement, we present a comprehensive dataset comprising 619 distinct literature-reported $E_m$ values calculated using density functional theory based nudged elastic band computations, across 443 compositions and 27 structural groups consisting of various compounds that have been explored as electrodes or solid electrolytes in batteries. Our dataset includes compositions that correspond to fully charged and/or discharged states of electrode materials, with intermediate compositions incorporated in select instances. Crucially, for each compound, our dataset provides structural information, including the initial and final positions of the migrating ion, along with its corresponding $E_m$ in easy-to-use .xlsx and JSON formats. We envision our dataset to be a highly useful resource for the scientific community, facilitating the development of advanced ML models that can predict $E_m$ precisely and accelerate materials discovery.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.06457v1" target="_blank">ScamAgents: How AI Agents Can Simulate Human-Level Scam Calls</a></h3>
                    <p><strong>Authors:</strong> Sanket Badhe</p>
                    <p><strong>Published:</strong> 8/8/2025</p>
                    <p><strong>Categories:</strong> cs.CR, cs.AI, cs.CL, cs.MA</p>
                    <p><strong>Summary:</strong> Large Language Models (LLMs) have demonstrated impressive fluency and reasoning capabilities, but their potential for misuse has raised growing concern. In this paper, we present ScamAgent, an autonomous multi-turn agent built on top of LLMs, capable of generating highly realistic scam call scripts that simulate real-world fraud scenarios. Unlike prior work focused on single-shot prompt misuse, ScamAgent maintains dialogue memory, adapts dynamically to simulated user responses, and employs deceptive persuasion strategies across conversational turns. We show that current LLM safety guardrails, including refusal mechanisms and content filters, are ineffective against such agent-based threats. Even models with strong prompt-level safeguards can be bypassed when prompts are decomposed, disguised, or delivered incrementally within an agent framework. We further demonstrate the transformation of scam scripts into lifelike voice calls using modern text-to-speech systems, completing a fully automated scam pipeline. Our findings highlight an urgent need for multi-turn safety auditing, agent-level control frameworks, and new methods to detect and disrupt conversational deception powered by generative AI.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.06456v1" target="_blank">Comparative study of ensemble-based uncertainty quantification methods for neural network interatomic potentials</a></h3>
                    <p><strong>Authors:</strong> Yonatan Kurniawan, Mingjian Wen, Ellad B. Tadmor, Mark K. Transtrum</p>
                    <p><strong>Published:</strong> 8/8/2025</p>
                    <p><strong>Categories:</strong> cond-mat.mtrl-sci, physics.data-an</p>
                    <p><strong>Summary:</strong> Machine learning interatomic potentials (MLIPs) enable atomistic simulations with near first-principles accuracy at substantially reduced computational cost, making them powerful tools for large-scale materials modeling. The accuracy of MLIPs is typically validated on a held-out dataset of \emph{ab initio} energies and atomic forces. However, accuracy on these small-scale properties does not guarantee reliability for emergent, system-level behavior -- precisely the regime where atomistic simulations are most needed, but for which direct validation is often computationally prohibitive. As a practical heuristic, predictive precision -- quantified as inverse uncertainty -- is commonly used as a proxy for accuracy, but its reliability remains poorly understood, particularly for system-level predictions. In this work, we systematically assess the relationship between predictive precision and accuracy in both in-distribution (ID) and out-of-distribution (OOD) regimes, focusing on ensemble-based uncertainty quantification methods for neural network potentials, including bootstrap, dropout, random initialization, and snapshot ensembles. We use held-out cross-validation for ID assessment and calculate cold curve energies and phonon dispersion relations for OOD testing. These evaluations are performed across various carbon allotropes as representative test systems. We find that uncertainty estimates can behave counterintuitively in OOD settings, often plateauing or even decreasing as predictive errors grow. These results highlight fundamental limitations of current uncertainty quantification approaches and underscore the need for caution when using predictive precision as a stand-in for accuracy in large-scale, extrapolative applications.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.06455v1" target="_blank">Maximum Impact with Fewer Features: Efficient Feature Selection for Cold-Start Recommenders through Collaborative Importance Weighting</a></h3>
                    <p><strong>Authors:</strong> Nikita Sukhorukov, Danil Gusak, Evgeny Frolov</p>
                    <p><strong>Published:</strong> 8/8/2025</p>
                    <p><strong>Categories:</strong> cs.IR, cs.LG</p>
                    <p><strong>Summary:</strong> Cold-start challenges in recommender systems necessitate leveraging auxiliary features beyond user-item interactions. However, the presence of irrelevant or noisy features can degrade predictive performance, whereas an excessive number of features increases computational demands, leading to higher memory consumption and prolonged training times. To address this, we propose a feature selection strategy that prioritizes the user behavioral information. Our method enhances the feature representation by incorporating correlations from collaborative behavior data using a hybrid matrix factorization technique and then ranks features using a mechanism based on the maximum volume algorithm. This approach identifies the most influential features, striking a balance between recommendation accuracy and computational efficiency. We conduct an extensive evaluation across various datasets and hybrid recommendation models, demonstrating that our method excels in cold-start scenarios by selecting minimal yet highly effective feature subsets. Even under strict feature reduction, our approach surpasses existing feature selection techniques while maintaining superior efficiency.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.06454v1" target="_blank">What Voting Rules Actually Do: A Data-Driven Analysis of Multi-Winner Voting</a></h3>
                    <p><strong>Authors:</strong> Joshua Caiata, Ben Armstrong, Kate Larson</p>
                    <p><strong>Published:</strong> 8/8/2025</p>
                    <p><strong>Categories:</strong> cs.AI, cs.GT</p>
                    <p><strong>Summary:</strong> Committee-selection problems arise in many contexts and applications, and there has been increasing interest within the social choice research community on identifying which properties are satisfied by different multi-winner voting rules. In this work, we propose a data-driven framework to evaluate how frequently voting rules violate axioms across diverse preference distributions in practice, shifting away from the binary perspective of axiom satisfaction given by worst-case analysis. Using this framework, we analyze the relationship between multi-winner voting rules and their axiomatic performance under several preference distributions. We then show that neural networks, acting as voting rules, can outperform traditional rules in minimizing axiom violations. Our results suggest that data-driven approaches to social choice can inform the design of new voting systems and support the continuation of data-driven research in social choice.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.06453v1" target="_blank">Text Embedded Swin-UMamba for DeepLesion Segmentation</a></h3>
                    <p><strong>Authors:</strong> Ruida Cheng, Tejas Sudharshan Mathai, Pritam Mukherjee, Benjamin Hou, Qingqing Zhu, Zhiyong Lu, Matthew McAuliffe, Ronald M. Summers</p>
                    <p><strong>Published:</strong> 8/8/2025</p>
                    <p><strong>Categories:</strong> cs.CV, cs.AI</p>
                    <p><strong>Summary:</strong> Segmentation of lesions on CT enables automatic measurement for clinical assessment of chronic diseases (e.g., lymphoma). Integrating large language models (LLMs) into the lesion segmentation workflow offers the potential to combine imaging features with descriptions of lesion characteristics from the radiology reports. In this study, we investigate the feasibility of integrating text into the Swin-UMamba architecture for the task of lesion segmentation. The publicly available ULS23 DeepLesion dataset was used along with short-form descriptions of the findings from the reports. On the test dataset, a high Dice Score of 82% and low Hausdorff distance of 6.58 (pixels) was obtained for lesion segmentation. The proposed Text-Swin-UMamba model outperformed prior approaches: 37% improvement over the LLM-driven LanGuideMedSeg model (p  0.001),and surpassed the purely image-based xLSTM-UNet and nnUNet models by 1.74% and 0.22%, respectively. The dataset and code can be accessed at https://github.com/ruida/LLM-Swin-UMamba</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.06452v1" target="_blank">TRUST: Leveraging Text Robustness for Unsupervised Domain Adaptation</a></h3>
                    <p><strong>Authors:</strong> Mattia Litrico, Mario Valerio Giuffrida, Sebastiano Battiato, Devis Tuia</p>
                    <p><strong>Published:</strong> 8/8/2025</p>
                    <p><strong>Categories:</strong> cs.CV, cs.LG</p>
                    <p><strong>Summary:</strong> Recent unsupervised domain adaptation (UDA) methods have shown great success in addressing classical domain shifts (e.g., synthetic-to-real), but they still suffer under complex shifts (e.g. geographical shift), where both the background and object appearances differ significantly across domains. Prior works showed that the language modality can help in the adaptation process, exhibiting more robustness to such complex shifts. In this paper, we introduce TRUST, a novel UDA approach that exploits the robustness of the language modality to guide the adaptation of a vision model. TRUST generates pseudo-labels for target samples from their captions and introduces a novel uncertainty estimation strategy that uses normalised CLIP similarity scores to estimate the uncertainty of the generated pseudo-labels. Such estimated uncertainty is then used to reweight the classification loss, mitigating the adverse effects of wrong pseudo-labels obtained from low-quality captions. To further increase the robustness of the vision model, we propose a multimodal soft-contrastive learning loss that aligns the vision and language feature spaces, by leveraging captions to guide the contrastive training of the vision model on target images. In our contrastive loss, each pair of images acts as both a positive and a negative pair and their feature representations are attracted and repulsed with a strength proportional to the similarity of their captions. This solution avoids the need for hardly determining positive and negative pairs, which is critical in the UDA setting. Our approach outperforms previous methods, setting the new state-of-the-art on classical (DomainNet) and complex (GeoNet) domain shifts. The code will be available upon acceptance.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.06448v1" target="_blank">Can a Quantum Computer Simulate Nuclear Magnetic Resonance Spectra Better than a Classical One?</a></h3>
                    <p><strong>Authors:</strong> Keith R. Fratus, Nicklas Enenkel, Sebastian Zanker, Jan-Michael Reiner, Michael Marthaler, Peter Schmitteckert</p>
                    <p><strong>Published:</strong> 8/8/2025</p>
                    <p><strong>Categories:</strong> quant-ph, physics.chem-ph</p>
                    <p><strong>Summary:</strong> The simulation of the spectra measured in nuclear magnetic resonance (NMR) spectroscopy experiments is a computationally non-trivial problem, and as such, it represents a problem for which a quantum computer may provide some practical advantage over traditional computing methods. In order to understand the extent to which such problems may provide examples of useful quantum advantage, it is important to understand the limitations of existing classical simulation methods. In this work, we benchmark our classical solver designed to solve such problems. We find that it performs well, even beyond the common experimental parameter regimes, except for a specific molecule with certain unusual features. We discuss what implications this may have for future efforts to demonstrate quantum advantage in the context of NMR.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.06447v1" target="_blank">SlimInfer: Accelerating Long-Context LLM Inference via Dynamic Token Pruning</a></h3>
                    <p><strong>Authors:</strong> Lingkun Long, Rubing Yang, Yushi Huang, Desheng Hui, Ao Zhou, Jianlei Yang</p>
                    <p><strong>Published:</strong> 8/8/2025</p>
                    <p><strong>Categories:</strong> cs.CL</p>
                    <p><strong>Summary:</strong> Long-context inference for Large Language Models (LLMs) is heavily limited by high computational demands. While several existing methods optimize attention computation, they still process the full set of hidden states at each layer, limiting overall efficiency. In this work, we propose SlimInfer, an innovative framework that aims to accelerate inference by directly pruning less critical prompt tokens during the forward pass. Our key insight is an information diffusion phenomenon: As information from critical tokens propagates through layers, it becomes distributed across the entire sequence. This diffusion process suggests that LLMs can maintain their semantic integrity when excessive tokens, even including these critical ones, are pruned in hidden states. Motivated by this, SlimInfer introduces a dynamic fine-grained pruning mechanism that accurately removes redundant tokens of hidden state at intermediate layers. This layer-wise pruning naturally enables an asynchronous KV cache manager that prefetches required token blocks without complex predictors, reducing both memory usage and I/O costs. Extensive experiments show that SlimInfer can achieve up to $\mathbf{2.53\times}$ time-to-first-token (TTFT) speedup and $\mathbf{1.88\times}$ end-to-end latency reduction for LLaMA3.1-8B-Instruct on a single RTX 4090, without sacrificing performance on LongBench. Our code will be released upon acceptance.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.06445v1" target="_blank">Echoes of Automation: The Increasing Use of LLMs in Newsmaking</a></h3>
                    <p><strong>Authors:</strong> Abolfazl Ansari, Delvin Ce Zhang, Nafis Irtiza Tripto, Dongwon Lee</p>
                    <p><strong>Published:</strong> 8/8/2025</p>
                    <p><strong>Categories:</strong> cs.CL, cs.AI</p>
                    <p><strong>Summary:</strong> The rapid rise of Generative AI (GenAI), particularly LLMs, poses concerns for journalistic integrity and authorship. This study examines AI-generated content across over 40,000 news articles from major, local, and college news media, in various media formats. Using three advanced AI-text detectors (e.g., Binoculars, Fast-Detect GPT, and GPTZero), we find substantial increase of GenAI use in recent years, especially in local and college news. Sentence-level analysis reveals LLMs are often used in the introduction of news, while conclusions usually written manually. Linguistic analysis shows GenAI boosts word richness and readability but lowers formality, leading to more uniform writing styles, particularly in local media.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.06444v1" target="_blank">Nonreciprocal and Geometric Frustration in Dissipative Quantum Spins</a></h3>
                    <p><strong>Authors:</strong> Guitao Lyu, Myung-Joong Hwang</p>
                    <p><strong>Published:</strong> 8/8/2025</p>
                    <p><strong>Categories:</strong> quant-ph, cond-mat.quant-gas</p>
                    <p><strong>Summary:</strong> Nonreciprocal interactions often create conflicting dynamical objectives that cannot be simultaneously satisfied, leading to nonreciprocal frustration. On the other hand, geometric frustration arises when conflicting static objectives in energy minimization cannot be satisfied. In this work, we show that nonreciprocal interaction among three collective quantum spins, mediated by a damped cavity, induces not only nonreciprocal frustration, intrinsic to nonreciprocity, but also geometric frustration with a remarkable robustness against disorder. It therefore ensures that the accidental degeneracy for steady states remains intact even when the system is perturbed away from a fine-tuned point of enhanced symmetry, in sharp contrast to the equilibrium case. Leveraging this finding, we identify a nonreciprocal phase transition driven by both geometric and nonreciprocal frustration. It gives rise to a time-dependent state, which shows a chiral dynamics along a geometry shaped by the geometric frustration and dynamically restores the broken discrete symmetries. Moreover, it constitutes a time-crystalline order, with multiple harmonics set by an emergent time scale that exhibits critical slowing down. Our predictions have important physical implications for a three-component spinor BEC-cavity system, which manifest as a geometric frustration in the structural phase transition and chiral dynamics of the frustrated self-organized BECs. We demonstrate the feasibility of experimental observation despite the presence of disorder in the spin-cavity coupling strengths.</p>
                
            
                
                    <h3><a href="http://dx.doi.org/10.1017/cfl.2025.8" target="_blank">The Fair Game: Auditing  Debiasing AI Algorithms Over Time</a></h3>
                    <p><strong>Authors:</strong> Debabrota Basu, Udvas Das</p>
                    <p><strong>Published:</strong> 8/8/2025</p>
                    <p><strong>Categories:</strong> cs.AI, cs.CY, cs.ET, cs.GT</p>
                    <p><strong>Summary:</strong> An emerging field of AI, namely Fair Machine Learning (ML), aims to quantify different types of bias (also known as unfairness) exhibited in the predictions of ML algorithms, and to design new algorithms to mitigate them. Often, the definitions of bias used in the literature are observational, i.e. they use the input and output of a pre-trained algorithm to quantify a bias under concern. In reality,these definitions are often conflicting in nature and can only be deployed if either the ground truth is known or only in retrospect after deploying the algorithm. Thus,there is a gap between what we want Fair ML to achieve and what it does in a dynamic social environment. Hence, we propose an alternative dynamic mechanism,Fair Game,to assure fairness in the predictions of an ML algorithm and to adapt its predictions as the society interacts with the algorithm over time. Fair Game puts together an Auditor and a Debiasing algorithm in a loop around an ML algorithm. The Fair Game puts these two components in a loop by leveraging Reinforcement Learning (RL). RL algorithms interact with an environment to take decisions, which yields new observations (also known as data/feedback) from the environment and in turn, adapts future decisions. RL is already used in algorithms with pre-fixed long-term fairness goals. Fair Game provides a unique framework where the fairness goals can be adapted over time by only modifying the auditor and the different biases it quantifies. Thus,Fair Game aims to simulate the evolution of ethical and legal frameworks in the society by creating an auditor which sends feedback to a debiasing algorithm deployed around an ML system. This allows us to develop a flexible and adaptive-over-time framework to build Fair ML systems pre- and post-deployment.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.06442v1" target="_blank">Magnetization-induced reordering of ground states phase diagram in a two-component Bose-Hubbard model</a></h3>
                    <p><strong>Authors:</strong> Oskar Stachowiak, Hubert Dunikowski, Emilia Witkowska</p>
                    <p><strong>Published:</strong> 8/8/2025</p>
                    <p><strong>Categories:</strong> cond-mat.quant-gas, quant-ph</p>
                    <p><strong>Summary:</strong> We investigate the influence of non-zero magnetization on the ground-state phase diagram of the two-component Bose-Hubbard model. Employing a mean-field theoretical framework, both analytically and numerically, we demonstrate that positions and sizes of specific phases on the diagram are magnetization dependent. In particular, non-zero magnetization introduces different Mott insulator phase boundaries for each of the two components. This effect leads to the emergence of a hybrid phase characterized by the coexistence of superfluid in one of the components and Mott insulator in the another one. Our findings highlight the important role of a conserved quantities, which is magnetization here, in reshaping the phase landscape, significantly influencing the stability and emergence of distinct quantum phases.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.06441v1" target="_blank">Accelerating Quantum Monte Carlo Calculations with Set-Equivariant Architectures and Transfer Learning</a></h3>
                    <p><strong>Authors:</strong> Manuel Gallego, Sebastián Roca-Jerat, David Zueco, Jesús Carrete</p>
                    <p><strong>Published:</strong> 8/8/2025</p>
                    <p><strong>Categories:</strong> quant-ph</p>
                    <p><strong>Summary:</strong> Machine-learning (ML) ans\atze have greatly expanded the accuracy and reach of variational quantum Monte Carlo (QMC) calculations, in particular when exploring the manifold quantum phenomena exhibited by spin systems. However, the scalability of QMC is still compromised by several other bottlenecks, and specifically those related to the actual evaluation of observables based on random deviates that lies at the core of the approach. Here we show how the set-transformer architecture can be used to dramatically accelerate or even bypass that step, especially for time-consuming operators such as powers of the magnetization. We illustrate the procedure with a range of examples of increasing complexity, from the classical Ising model to quantum systems with long-range interactions, and comprising both regressions (to predict observables) and classifications (to detect phase transitions). Moreover, we show how transfer learning can be leveraged to reduce the training cost by reusing knowledge from different systems and smaller system sizes.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.06439v1" target="_blank">Schwinger--DeWitt expansion for the heat kernel of nonminimal operators in causal theories</a></h3>
                    <p><strong>Authors:</strong> Andrei O. Barvinsky, Alexey E. Kalugin, Władysław Wachowski</p>
                    <p><strong>Published:</strong> 8/8/2025</p>
                    <p><strong>Categories:</strong> hep-th, gr-qc</p>
                    <p><strong>Summary:</strong> We suggest a systematic calculational scheme for heat kernels of covariant nonminimal operators in causal theories whose characteristic surfaces are null with respect to a generic metric. The calculational formalism is based on a pseudodifferential operator calculus which allows one to build a linear operator map from the heat kernel of the minimal operator to the nonminimal one. This map is realized as a local expansion in powers of spacetime curvature, dimensional background fields, and their covariant derivatives with the coefficients -- the functions of the Synge world function and its derivatives. Finiteness of these functions, determined by multiple proper time integrals, is achieved by a special subtraction procedure which is an important part of the calculational scheme. We illustrate this technique on the examples of the vector Proca model and the vector field operator with a nondegenerate principal symbol. We also discuss smoothness properties of heat kernels of nonminimal operators in connection with the nondegenerate nature of their operator symbols.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.06435v1" target="_blank">Learning the Topic, Not the Language: How LLMs Classify Online Immigration Discourse Across Languages</a></h3>
                    <p><strong>Authors:</strong> Andrea Nasuto, Stefano Maria Iacus, Francisco Rowe, Devika Jain</p>
                    <p><strong>Published:</strong> 8/8/2025</p>
                    <p><strong>Categories:</strong> cs.CL, cs.AI</p>
                    <p><strong>Summary:</strong> Large language models (LLMs) are transforming social-science research by enabling scalable, precise analysis. Their adaptability raises the question of whether knowledge acquired through fine-tuning in a few languages can transfer to unseen languages that only appeared during pre-training. To examine this, we fine-tune lightweight LLaMA 3.2-3B models on monolingual, bilingual, or multilingual data sets to classify immigration-related tweets from X/Twitter across 13 languages, a domain characterised by polarised, culturally specific discourse. We evaluate whether minimal language-specific fine-tuning enables cross-lingual topic detection and whether adding targeted languages corrects pre-training biases. Results show that LLMs fine-tuned in one or two languages can reliably classify immigration-related content in unseen languages. However, identifying whether a tweet expresses a pro- or anti-immigration stance benefits from multilingual fine-tuning. Pre-training bias favours dominant languages, but even minimal exposure to under-represented languages during fine-tuning (as little as $9.62\times10^{-11}$ of the original pre-training token volume) yields significant gains. These findings challenge the assumption that cross-lingual mastery requires extensive multilingual training: limited language coverage suffices for topic-level generalisation, and structural biases can be corrected with lightweight interventions. By releasing 4-bit-quantised, LoRA fine-tuned models, we provide an open-source, reproducible alternative to proprietary LLMs that delivers 35 times faster inference at just 0.00000989% of the dollar cost of the OpenAI GPT-4o model, enabling scalable, inclusive research.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.06434v1" target="_blank">CLIPin: A Non-contrastive Plug-in to CLIP for Multimodal Semantic Alignment</a></h3>
                    <p><strong>Authors:</strong> Shengzhu Yang, Jiawei Du, Shuai Lu, Weihang Zhang, Ningli Wang, Huiqi Li</p>
                    <p><strong>Published:</strong> 8/8/2025</p>
                    <p><strong>Categories:</strong> cs.CV, cs.AI</p>
                    <p><strong>Summary:</strong> Large-scale natural image-text datasets, especially those automatically collected from the web, often suffer from loose semantic alignment due to weak supervision, while medical datasets tend to have high cross-modal correlation but low content diversity. These properties pose a common challenge for contrastive language-image pretraining (CLIP): they hinder the models ability to learn robust and generalizable representations. In this work, we propose CLIPin, a unified non-contrastive plug-in that can be seamlessly integrated into CLIP-style architectures to improve multimodal semantic alignment, providing stronger supervision and enhancing alignment robustness. Furthermore, two shared pre-projectors are designed for image and text modalities respectively to facilitate the integration of contrastive and non-contrastive learning in a parameter-compromise manner. Extensive experiments on diverse downstream tasks demonstrate the effectiveness and generality of CLIPin as a plug-and-play component compatible with various contrastive frameworks. Code is available at https://github.com/T6Yang/CLIPin.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.06433v1" target="_blank">Memp: Exploring Agent Procedural Memory</a></h3>
                    <p><strong>Authors:</strong> Runnan Fang, Yuan Liang, Xiaobin Wang, Jialong Wu, Shuofei Qiao, Pengjun Xie, Fei Huang, Huajun Chen, Ningyu Zhang</p>
                    <p><strong>Published:</strong> 8/8/2025</p>
                    <p><strong>Categories:</strong> cs.CL, cs.AI, cs.LG, cs.MA</p>
                    <p><strong>Summary:</strong> Large Language Models (LLMs) based agents excel at diverse tasks, yet they suffer from brittle procedural memory that is manually engineered or entangled in static parameters. In this work, we investigate strategies to endow agents with a learnable, updatable, and lifelong procedural memory. We propose Memp that distills past agent trajectories into both fine-grained, step-by-step instructions and higher-level, script-like abstractions, and explore the impact of different strategies for Build, Retrieval, and Update of procedural memory. Coupled with a dynamic regimen that continuously updates, corrects, and deprecates its contents, this repository evolves in lockstep with new experience. Empirical evaluation on TravelPlanner and ALFWorld shows that as the memory repository is refined, agents achieve steadily higher success rates and greater efficiency on analogous tasks. Moreover, procedural memory built from a stronger model retains its value: migrating the procedural memory to a weaker model yields substantial performance gains.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.06431v1" target="_blank">Nonparametric Learning Non-Gaussian Quantum States of Continuous Variable Systems</a></h3>
                    <p><strong>Authors:</strong> Liubov A. Markovich, Xiaoyu Liu, Jordi Tura</p>
                    <p><strong>Published:</strong> 8/8/2025</p>
                    <p><strong>Categories:</strong> quant-ph, stat.OT</p>
                    <p><strong>Summary:</strong> Continuous-variable quantum systems are foundational to quantum computation, communication, and sensing. While traditional representations using wave functions or density matrices are often impractical, the tomographic picture of quantum mechanics provides an accessible alternative by associating quantum states with classical probability distribution functions called tomograms. Despite its advantages, including compatibility with classical statistical methods, tomographic method remain underutilized due to a lack of robust estimation techniques. This work addresses this gap by introducing a non-parametric \emph{kernel quantum state estimation} (KQSE) framework for reconstructing quantum states and their trace characteristics from noisy data, without prior knowledge of the state. In contrast to existing methods, KQSE yields estimates of the density matrix in various bases, as well as trace quantities such as purity, higher moments, overlap, and trace distance, with a near-optimal convergence rate of $\tilde{O}\bigl(T^{-1}\bigr)$, where $T$ is the total number of measurements. KQSE is robust for multimodal, non-Gaussian states, making it particularly well suited for characterizing states essential for quantum science.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.06430v1" target="_blank">MotionSwap</a></h3>
                    <p><strong>Authors:</strong> Om Patil, Jinesh Modi, Suryabha Mukhopadhyay, Meghaditya Giri, Chhavi Malhotra</p>
                    <p><strong>Published:</strong> 8/8/2025</p>
                    <p><strong>Categories:</strong> cs.CV</p>
                    <p><strong>Summary:</strong> Face swapping technology has gained significant attention in both academic research and commercial applications. This paper presents our implementation and enhancement of SimSwap, an efficient framework for high fidelity face swapping. We introduce several improvements to the original model, including the integration of self and cross-attention mechanisms in the generator architecture, dynamic loss weighting, and cosine annealing learning rate scheduling. These enhancements lead to significant improvements in identity preservation, attribute consistency, and overall visual quality. Our experimental results, spanning 400,000 training iterations, demonstrate progressive improvements in generator and discriminator performance. The enhanced model achieves better identity similarity, lower FID scores, and visibly superior qualitative results compared to the baseline. Ablation studies confirm the importance of each architectural and training improvement. We conclude by identifying key future directions, such as integrating StyleGAN3, improving lip synchronization, incorporating 3D facial modeling, and introducing temporal consistency for video-based applications.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.06429v1" target="_blank">SPARSE Data, Rich Results: Few-Shot Semi-Supervised Learning via Class-Conditioned Image Translation</a></h3>
                    <p><strong>Authors:</strong> Guido Manni, Clemente Lauretti, Loredana Zollo, Paolo Soda</p>
                    <p><strong>Published:</strong> 8/8/2025</p>
                    <p><strong>Categories:</strong> cs.CV, cs.AI</p>
                    <p><strong>Summary:</strong> Deep learning has revolutionized medical imaging, but its effectiveness is severely limited by insufficient labeled training data. This paper introduces a novel GAN-based semi-supervised learning framework specifically designed for low labeled-data regimes, evaluated across settings with 5 to 50 labeled samples per class. Our approach integrates three specialized neural networks -- a generator for class-conditioned image translation, a discriminator for authenticity assessment and classification, and a dedicated classifier -- within a three-phase training framework. The method alternates between supervised training on limited labeled data and unsupervised learning that leverages abundant unlabeled images through image-to-image translation rather than generation from noise. We employ ensemble-based pseudo-labeling that combines confidence-weighted predictions from the discriminator and classifier with temporal consistency through exponential moving averaging, enabling reliable label estimation for unlabeled data. Comprehensive evaluation across eleven MedMNIST datasets demonstrates that our approach achieves statistically significant improvements over six state-of-the-art GAN-based semi-supervised methods, with particularly strong performance in the extreme 5-shot setting where the scarcity of labeled data is most challenging. The framework maintains its superiority across all evaluated settings (5, 10, 20, and 50 shots per class). Our approach offers a practical solution for medical imaging applications where annotation costs are prohibitive, enabling robust classification performance even with minimal labeled data. Code is available at https://github.com/GuidoManni/SPARSE.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.06426v1" target="_blank">Shortcut Learning in Generalist Robot Policies: The Role of Dataset Diversity and Fragmentation</a></h3>
                    <p><strong>Authors:</strong> Youguang Xing, Xu Luo, Junlin Xie, Lianli Gao, Hengtao Shen, Jingkuan Song</p>
                    <p><strong>Published:</strong> 8/8/2025</p>
                    <p><strong>Categories:</strong> cs.RO, cs.AI, cs.CV</p>
                    <p><strong>Summary:</strong> Generalist robot policies trained on large-scale datasets such as Open X-Embodiment (OXE) demonstrate strong performance across a wide range of tasks. However, they often struggle to generalize beyond the distribution of their training data. In this paper, we investigate the underlying cause of this limited generalization capability. We identify shortcut learning -- the reliance on task-irrelevant features -- as a key impediment to generalization. Through comprehensive theoretical and empirical analysis, we uncover two primary contributors to shortcut learning: (1) limited diversity within individual sub-datasets, and (2) significant distributional disparities across sub-datasets, leading to dataset fragmentation. These issues arise from the inherent structure of large-scale datasets like OXE, which are typically composed of multiple sub-datasets collected independently across varied environments and embodiments. Our findings provide critical insights into dataset collection strategies that can reduce shortcut learning and enhance the generalization ability of generalist robot policies. Moreover, in scenarios where acquiring new large-scale data is impractical, we demonstrate that carefully selected robotic data augmentation strategies can effectively reduce shortcut learning in existing offline datasets, thereby improving generalization capabilities of generalist robot policies, e.g., $\pi_0$, in both simulation and real-world environments. More information at https://lucky-light-sun.github.io/proj/shortcut-learning-in-grps/.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.06424v1" target="_blank">Single photon emission from lithographically-positioned engineered nanodiamonds for cryogenic applications</a></h3>
                    <p><strong>Authors:</strong> Vivekanand Tiwari, Zhaojin Liu, Hao-Cheng Weng, Krishna C Balram, John G Rarity, Soumen Mandal, Oliver A Williams, Gavin W Morley, Joe A Smith</p>
                    <p><strong>Published:</strong> 8/8/2025</p>
                    <p><strong>Categories:</strong> quant-ph</p>
                    <p><strong>Summary:</strong> Nitrogen-vacancy centres in nanodiamonds (NDs) provide a promising resource for quantum photonic systems. However, developing a technology beyond proof-of-principle physics requires optimally engineering its component parts. In this work, we present a hybrid materials platform by photolithographically positioning ball-milled isotopically-enriched NDs on broadband metal reflectors. The structure enhances the photonic collection efficiency, enabling cryogenic characterisation despite the limited numerical aperture imposed by our cryostat. Our device, with SiO$_2$ above a silver reflector, allows us to perform spectroscopic characterisation at 16 K and measure autocorrelation functions confirming single-photon emission (g$^2$(0)0.5). Through comparative studies of similar hybrid device configurations, we can move towards optimally engineered techniques for building and analysing quantum emitters in wafer-scale photonic environments.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.06422v1" target="_blank">Improving the atomic modelling for solar UV radiative transfer calculations</a></h3>
                    <p><strong>Authors:</strong> R. P. Dufresne, G. Del Zanna, C. M. J. Osborne</p>
                    <p><strong>Published:</strong> 8/8/2025</p>
                    <p><strong>Categories:</strong> astro-ph.SR, astro-ph.IM, physics.atom-ph</p>
                    <p><strong>Summary:</strong> Radiative transfer calculations have been produced over the years for many lines and continua in the UV wavelength range of solar and cool stellar atmospheres for a variety of conditions. Despite significant improvements in computing power and availability of atomic data over time, atomic models are often still limited in size and rely on approximations for data. There have also been inconsistencies in the way photo-ionisation and radiative recombination have been treated. Here, we incorporate into the Lightweaver radiative transfer code new data and updated modelling of atomic processes for the low charge states of C, Si and S. Data are taken from the CHIANTI database and other widely-available sources for the relevant elements. We show the significant impact this has on the UV continua in the 1100-1700{\AA} region, especially for Si. The results are in much better agreement with averaged, quiet Sun observations, and remove the need to invoke missing opacity to resolve discrepancies. The present treatment has important implications for radiative transfer calculations and the model atmospheres used as inputs.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.06420v1" target="_blank">Feature-Space Oversampling for Addressing Class Imbalance in SAR Ship Classification</a></h3>
                    <p><strong>Authors:</strong> Ch Muhammad Awais, Marco Reggiannini, Davide Moroni, Oktay Karakus</p>
                    <p><strong>Published:</strong> 8/8/2025</p>
                    <p><strong>Categories:</strong> cs.CV</p>
                    <p><strong>Summary:</strong> SAR ship classification faces the challenge of long-tailed datasets, which complicates the classification of underrepresented classes. Oversampling methods have proven effective in addressing class imbalance in optical data. In this paper, we evaluated the effect of oversampling in the feature space for SAR ship classification. We propose two novel algorithms inspired by the Major-to-minor (M2m) method M2m$_f$, M2m$_u$. The algorithms are tested on two public datasets, OpenSARShip (6 classes) and FuSARShip (9 classes), using three state-of-the-art models as feature extractors: ViT, VGG16, and ResNet50. Additionally, we also analyzed the impact of oversampling methods on different class sizes. The results demonstrated the effectiveness of our novel methods over the original M2m and baselines, with an average F1-score increase of 8.82% for FuSARShip and 4.44% for OpenSARShip.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.06419v1" target="_blank">Dynamics of Protonated Oxalate from Machine-Learned Simulations and Experiment: Infrared Signatures, Proton Transfer Dynamics and Tunneling Splittings</a></h3>
                    <p><strong>Authors:</strong> Valerii Andreichev, Silvan Käser, Erica L. Bocanegra, Madeeha Salik, Mark A. Johnson, Markus Meuwly</p>
                    <p><strong>Published:</strong> 8/8/2025</p>
                    <p><strong>Categories:</strong> physics.chem-ph</p>
                    <p><strong>Summary:</strong> The infrared spectroscopy and proton transfer dynamics together with the associated tunneling splittings for H/D-transfer in oxalate are investigated using a machine learning-based potential energy surface (PES) of CCSD(T) quality, calibrated against the results of new spectroscopic measurements. Second order vibrational perturbation calculations (VPT2) very successfully describe both the framework and H-transfer modes compared with the experiments. In particular, a new low-intensity signature at 1666 cm$^{-1}$ was correctly predicted from the VPT2 calculations. An unstructured band centered at 2940 cm$^{-1}$ superimposed on a broad background extending from 2600 to 3200 cm$^{-1}$ is assigned to the H-transfer motion. The broad background involves a multitude of combination bands but a major role is played by the COH-bend. For the deuterated species, VPT2 and molecular dynamics simulations provide equally convincing assignments, in particular for the framework modes. Finally, based on the new PES the tunneling splitting for H-transfer is predicted as $\Delta_{\rm H} = 35.0$ cm$^{-1}$ from ring polymer instanton calculations using higher-order corrections. This provides an experimentally accessible benchmark to validate the computations, in particular the quality of the machine-learned PES.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.06418v1" target="_blank">Quantifying Conversation Drift in MCP via Latent Polytope</a></h3>
                    <p><strong>Authors:</strong> Haoran Shi, Hongwei Yao, Shuo Shao, Shaopeng Jiao, Ziqi Peng, Zhan Qin, Cong Wang</p>
                    <p><strong>Published:</strong> 8/8/2025</p>
                    <p><strong>Categories:</strong> cs.CL</p>
                    <p><strong>Summary:</strong> The Model Context Protocol (MCP) enhances large language models (LLMs) by integrating external tools, enabling dynamic aggregation of real-time data to improve task execution. However, its non-isolated execution context introduces critical security and privacy risks. In particular, adversarially crafted content can induce tool poisoning or indirect prompt injection, leading to conversation hijacking, misinformation propagation, or data exfiltration. Existing defenses, such as rule-based filters or LLM-driven detection, remain inadequate due to their reliance on static signatures, computational inefficiency, and inability to quantify conversational hijacking. To address these limitations, we propose SecMCP, a secure framework that detects and quantifies conversation drift, deviations in latent space trajectories induced by adversarial external knowledge. By modeling LLM activation vectors within a latent polytope space, SecMCP identifies anomalous shifts in conversational dynamics, enabling proactive detection of hijacking, misleading, and data exfiltration. We evaluate SecMCP on three state-of-the-art LLMs (Llama3, Vicuna, Mistral) across benchmark datasets (MS MARCO, HotpotQA, FinQA), demonstrating robust detection with AUROC scores exceeding 0.915 while maintaining system usability. Our contributions include a systematic categorization of MCP security threats, a novel latent polytope-based methodology for quantifying conversation drift, and empirical validation of SecMCPs efficacy.</p>
                
            
                
                    <h3><a href="http://dx.doi.org/10.1093/mnras/staf1285" target="_blank">Dynamical neutron-star tides: The signature of a mode resonance</a></h3>
                    <p><strong>Authors:</strong> P. Pnigouras, N. Andersson, F. Gittins, A. R. Counsell</p>
                    <p><strong>Published:</strong> 8/8/2025</p>
                    <p><strong>Categories:</strong> gr-qc, astro-ph.HE, astro-ph.SR</p>
                    <p><strong>Summary:</strong> Motivated by future opportunities in gravitational-wave astronomy and the ongoing effort to constrain physics under extreme conditions, we consider the signature of individual mode resonances excited during the inspiral of binary systems involving neutron stars. Specifically, we quantify how each resonant mode contributes to the effective (frequency-dependent) tidal deformability. The resonant solution is shown to be accurately represented by a new closed-form approximation, which sheds light on the involved phenomenology, and which should be useful for the development of precise waveform models and future parameter extraction efforts.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.06412v1" target="_blank">Sample-efficient LLM Optimization with Reset Replay</a></h3>
                    <p><strong>Authors:</strong> Zichuan Liu, Jinyu Wang, Lei Song, Jiang Bian</p>
                    <p><strong>Published:</strong> 8/8/2025</p>
                    <p><strong>Categories:</strong> cs.LG, cs.CL</p>
                    <p><strong>Summary:</strong> Recent advancements in post-training Large Language Models (LLMs), particularly through Reinforcement Learning (RL) and preference optimization methods, are key drivers for enhancing their reasoning capabilities. However, these methods are often plagued by low sample efficiency and a susceptibility to primacy bias, where overfitting to initial experiences degrades policy quality and damages the learning process. To address these challenges, we introduce LLM optimization with Reset Replay (LoRR), a general and powerful plugin designed to enhance sample efficiency in any preference-based optimization framework. LoRR core mechanism enables training at a high replay number, maximizing the utility of each collected data batch. To counteract the risk of overfitting inherent in high-replay training, LoRR incorporates a periodic reset strategy with reusing initial data, which preserves network plasticity. Furthermore, it leverages a hybrid optimization objective, combining supervised fine-tuning (SFT) and preference-based losses to further bolster data exploitation. Our extensive experiments demonstrate that LoRR significantly boosts the performance of various preference optimization methods on both mathematical and general reasoning benchmarks. Notably, an iterative DPO approach augmented with LoRR achieves comparable performance on challenging math tasks, outperforming some complex and computationally intensive RL-based algorithms. These findings highlight that LoRR offers a practical, sample-efficient, and highly effective paradigm for LLM finetuning, unlocking greater performance from limited data.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.06411v1" target="_blank">Dimensional Characterization and Pathway Modeling for Catastrophic AI Risks</a></h3>
                    <p><strong>Authors:</strong> Ze Shen Chin</p>
                    <p><strong>Published:</strong> 8/8/2025</p>
                    <p><strong>Categories:</strong> cs.CY, cs.AI, cs.LG</p>
                    <p><strong>Summary:</strong> Although discourse around the risks of Artificial Intelligence (AI) has grown, it often lacks a comprehensive, multidimensional framework, and concrete causal pathways mapping hazard to harm. This paper aims to bridge this gap by examining six commonly discussed AI catastrophic risks: CBRN, cyber offense, sudden loss of control, gradual loss of control, environmental risk, and geopolitical risk. First, we characterize these risks across seven key dimensions, namely intent, competency, entity, polarity, linearity, reach, and order. Next, we conduct risk pathway modeling by mapping step-by-step progressions from the initial hazard to the resulting harms. The dimensional approach supports systematic risk identification and generalizable mitigation strategies, while risk pathway models help identify scenario-specific interventions. Together, these methods offer a more structured and actionable foundation for managing catastrophic AI risks across the value chain.</p>
                
            
                
                    <h3><a href="http://dx.doi.org/10.1109/QCE60285.2024.10330" target="_blank">Quantum Annealing for the Set Splitting Problem</a></h3>
                    <p><strong>Authors:</strong> Sean Borneman</p>
                    <p><strong>Published:</strong> 8/8/2025</p>
                    <p><strong>Categories:</strong> quant-ph</p>
                    <p><strong>Summary:</strong> I present a novel use of quantum annealing to solve the Set Splitting Problem using (QUBO) problem formulation. The contribution of the work is in formulating penalty functions that ensure the ground state of the QUBO Hamiltonian corresponds to valid solutions that split the input subsets. This approach scales linearly in terms of the number of logical qubits relative to problem size. Empirical tests of the proposed solution show convergence to globally optimal solutions, with high accuracy rates over repeated trials. Hardware limitations of current quantum annealers lead to an exponential rise in required physical qubits, versus the theoretical linear increase, although this can improve with future developments. Further work is needed to enhance formulation robustness, reduce qubit requirements for embedded problems, and to conduct more extensive bench-marking. Quantum solutions to the Set-Splitting problem lead to reduced time complexity versus classical solutions, and may accelerate research in biology, cybersecurity, and other domains.</p>
                
            
        
        
            <p><em>Generated by AI News Agent</em></p>
        
    

