
    
        <h1>ðŸ¤– AI Research Report</h1>
        
            <strong>Date:</strong> 2025-08-14<br>
            <strong>Topics:</strong> ai safety research, ai alignment research, quantum computing<br>
            <strong>Papers Found:</strong> 150
        
        
        
            
                <h2>ðŸ¤– AI Summary</h2>
                <p>## ai safety research

The research papers on AI safety present a diverse array of developments and insights that contribute to the ongoing efforts to make AI systems safer and more reliable. A major trend is the development of frameworks and benchmarks for evaluating AI models in terms of safety and alignment with human values. For instance, the PacifAIst benchmark introduces scenarios to test large language models (LLMs) alignment with human safety over self-preservation, highlighting the need for standardized tools to assess and mitigate risks associated with AI behavior. Similarly, the survey on alignment techniques for LLMs emphasizes the necessity of a comprehensive evaluation framework to systematically compare alignment paradigms, ensuring that AI outputs align with human values and safety standards.

Another significant trend is the exploration of AI systems robustness in dynamic and uncertain environments. For example, the Vision-driven River Following of UAV via Safe Reinforcement Learning paper demonstrates a model-based reinforcement learning framework that integrates safety regulations into the decision-making process of UAVs, thereby enhancing their operational safety in GPS-denied environments. Additionally, the gatekeeper framework for ensuring online safety of cyber-physical systems under multiple constraints illustrates the importance of reliable safety mechanisms to maintain system integrity in real-time applications. These studies underscore the critical role of integrating safety considerations into the design and evaluation of AI systems, ultimately aiming to foster trust and ensure the responsible deployment of AI technologies in various domains.

*Based on 50 research papers*

---

## ai alignment research

The selected papers, while primarily focused on various AI applications and methodologies, indirectly contribute to the broader theme of AI alignment research by addressing issues of model alignment, efficiency, robustness, and interpretability. A central concern in AI alignment is ensuring that AI systems behave in ways that are consistent with human values and expectations, which is evident in several papers.

For instance, the **Echo-4o** paper explores synthetic data use for improving image generation models, emphasizing the need for data that aligns image content with descriptive texts, thus enhancing model accuracy and robustness. This can be seen as a step towards aligning AI systems with human intents and nuanced understanding. Similarly, the **LLMC+** paper addresses model efficiency and compressed architectures for vision-language models, which can lead to more adaptable and resource-efficient AI systems, crucial for alignment in resource-constrained environments. Moreover, the paper on **A Comprehensive Evaluation framework of Alignment Techniques for LLMs** directly tackles AI alignment by evaluating various alignment techniques across dimensions such as alignment quality and robustness, providing insights into how current models can be adjusted to better align with human values.

Furthermore, the **PacifAIst Benchmark** introduces scenarios to evaluate AI models decision-making, especially when their goals conflict with human safety, directly probing the alignment of AI models with ethical and safety priorities. This benchmark highlights the importance of evaluating AI systems behavioral alignment in complex, real-world scenarios. Lastly, the discussion on **Explainable Ensemble Learning** in malware detection emphasizes interpretability, a critical component of alignment, as it ensures AI decisions can be understood and trusted by humans.

Overall, while these papers cover diverse topics, their underlying contributions to AI alignment research revolve around improving model interpretability, efficiency, robustness, and ethical decision-making, all of which are vital for developing AI systems that align well with human values and expectations.

*Based on 50 research papers*

---

## quantum computing

The selected research papers do not directly relate to quantum computing. However, the paper titled Improving quantum communication rates with permutation-invariant codes by Sujeet Bhalerao and Felix Leditzky contributes to quantum information theory, which is a key aspect of quantum computing. This study presents advancements in enhancing quantum communication rates through permutation-invariant quantum codes. The authors focus on improving the quantum capacity thresholds of various quantum channels, marking the noise level limit for effective quantum communication. Their approach leverages the symmetry in quantum channels to efficiently compute coherent information, allowing for improved bounds on quantum capacities for channels like the Pauli and dephrasure channels. These improvements are crucial as they potentially enhance the reliability and efficiency of quantum communication systems, laying foundational work for robust quantum networks essential for scalable quantum computing.

Additionally, the paper titled Fault tolerant Operations in Majorana-based Quantum Codes: Gates, Measurements and High Rate Constructions by Maryam Mudassar et al. delves into fault-tolerant quantum computation, a critical challenge in practical quantum computing. They develop a framework for fault-tolerant computation using Majorana-based setups, emphasizing the division between even and odd Majorana codes. The study provides methodologies for implementing various quantum gates and error correction techniques, crucial for reliable quantum computing. These innovations could significantly impact the development of noise-resilient quantum computers, which are essential for realizing the full potential of quantum computing technologies.

*Based on 50 research papers*</p>
            
        
        
        <h2>ðŸ“š Research Papers</h2>
        
                
                    <h3><a href="http://arxiv.org/abs/2508.09981v1" target="_blank">LLMC+: Benchmarking Vision-Language Model Compression with a Plug-and-play Toolkit</a></h3>
                    <p><strong>Authors:</strong> Chengtao Lv, Bilang Zhang, Yang Yong, Ruihao Gong, Yushi Huang, Shiqiao Gu, Jiajun Wu, Yumeng Shi, Jinyang Guo, Wenya Wang</p>
                    <p><strong>Published:</strong> 8/13/2025</p>
                    <p><strong>Categories:</strong> cs.CV</p>
                    <p><strong>Summary:</strong> Large Vision-Language Models (VLMs) exhibit impressive multi-modal capabilities but suffer from prohibitive computational and memory demands, due to their long visual token sequences and massive parameter sizes. To address these issues, recent works have proposed training-free compression methods. However, existing efforts often suffer from three major limitations: (1) Current approaches do not decompose techniques into comparable modules, hindering fair evaluation across spatial and temporal redundancy. (2) Evaluation confined to simple single-turn tasks, failing to reflect performance in realistic scenarios. (3) Isolated use of individual compression techniques, without exploring their joint potential. To overcome these gaps, we introduce LLMC+, a comprehensive VLM compression benchmark with a versatile, plug-and-play toolkit. LLMC+ supports over 20 algorithms across five representative VLM families and enables systematic study of token-level and model-level compression. Our benchmark reveals that: (1) Spatial and temporal redundancies demand distinct technical strategies. (2) Token reduction methods degrade significantly in multi-turn dialogue and detail-sensitive tasks. (3) Combining token and model compression achieves extreme compression with minimal performance loss. We believe LLMC+ will facilitate fair evaluation and inspire future research in efficient VLM. Our code is available at https://github.com/ModelTC/LightCompress.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.09977v1" target="_blank">A Survey on 3D Gaussian Splatting Applications: Segmentation, Editing, and Generation</a></h3>
                    <p><strong>Authors:</strong> Shuting He, Peilin Ji, Yitong Yang, Changshuo Wang, Jiayi Ji, Yinglin Wang, Henghui Ding</p>
                    <p><strong>Published:</strong> 8/13/2025</p>
                    <p><strong>Categories:</strong> cs.CV</p>
                    <p><strong>Summary:</strong> 3D Gaussian Splatting (3DGS) has recently emerged as a powerful alternative to Neural Radiance Fields (NeRF) for 3D scene representation, offering high-fidelity photorealistic rendering with real-time performance. Beyond novel view synthesis, the explicit and compact nature of 3DGS enables a wide range of downstream applications that require geometric and semantic understanding. This survey provides a comprehensive overview of recent progress in 3DGS applications. It first introduces 2D foundation models that support semantic understanding and control in 3DGS applications, followed by a review of NeRF-based methods that inform their 3DGS counterparts. We then categorize 3DGS applications into segmentation, editing, generation, and other functional tasks. For each, we summarize representative methods, supervision strategies, and learning paradigms, highlighting shared design principles and emerging trends. Commonly used datasets and evaluation protocols are also summarized, along with comparative analyses of recent methods across public benchmarks. To support ongoing research and development, a continually updated repository of papers, code, and resources is maintained at https://github.com/heshuting555/Awesome-3DGS-Applications.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.09976v1" target="_blank">Masquerade: Learning from In-the-wild Human Videos using Data-Editing</a></h3>
                    <p><strong>Authors:</strong> Marion Lepert, Jiaying Fang, Jeannette Bohg</p>
                    <p><strong>Published:</strong> 8/13/2025</p>
                    <p><strong>Categories:</strong> cs.RO</p>
                    <p><strong>Summary:</strong> Robot manipulation research still suffers from significant data scarcity: even the largest robot datasets are orders of magnitude smaller and less diverse than those that fueled recent breakthroughs in language and vision. We introduce Masquerade, a method that edits in-the-wild egocentric human videos to bridge the visual embodiment gap between humans and robots and then learns a robot policy with these edited videos. Our pipeline turns each human video into robotized demonstrations by (i) estimating 3-D hand poses, (ii) inpainting the human arms, and (iii) overlaying a rendered bimanual robot that tracks the recovered end-effector trajectories. Pre-training a visual encoder to predict future 2-D robot keypoints on 675K frames of these edited clips, and continuing that auxiliary loss while fine-tuning a diffusion policy head on only 50 robot demonstrations per task, yields policies that generalize significantly better than prior work. On three long-horizon, bimanual kitchen tasks evaluated in three unseen scenes each, Masquerade outperforms baselines by 5-6x. Ablations show that both the robot overlay and co-training are indispensable, and performance scales logarithmically with the amount of edited human video. These results demonstrate that explicitly closing the visual embodiment gap unlocks a vast, readily available source of data from human videos that can be used to improve robot policies.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.09971v1" target="_blank">Vision-driven River Following of UAV via Safe Reinforcement Learning using Semantic Dynamics Model</a></h3>
                    <p><strong>Authors:</strong> Zihan Wang, Nina Mahmoudian</p>
                    <p><strong>Published:</strong> 8/13/2025</p>
                    <p><strong>Categories:</strong> cs.RO, cs.AI</p>
                    <p><strong>Summary:</strong> Vision-driven autonomous river following by Unmanned Aerial Vehicles is critical for applications such as rescue, surveillance, and environmental monitoring, particularly in dense riverine environments where GPS signals are unreliable. We formalize river following as a coverage control problem in which the reward function is submodular, yielding diminishing returns as more unique river segments are visited, thereby framing the task as a Submodular Markov Decision Process. First, we introduce Marginal Gain Advantage Estimation, which refines the reward advantage function by using a sliding window baseline computed from historical episodic returns, thus aligning the advantage estimation with the agents evolving recognition of action value in non-Markovian settings. Second, we develop a Semantic Dynamics Model based on patchified water semantic masks that provides more interpretable and data-efficient short-term prediction of future observations compared to latent vision dynamics models. Third, we present the Constrained Actor Dynamics Estimator architecture, which integrates the actor, the cost estimator, and SDM for cost advantage estimation to form a model-based SafeRL framework capable of solving partially observable Constrained Submodular Markov Decision Processes. Simulation results demonstrate that MGAE achieves faster convergence and superior performance over traditional critic-based methods like Generalized Advantage Estimation. SDM provides more accurate short-term state predictions that enable the cost estimator to better predict potential violations. Overall, CADE effectively integrates safety regulation into model-based RL, with the Lagrangian approach achieving the soft balance of reward and safety during training, while the safety layer enhances performance during inference by hard action overlay.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.09966v1" target="_blank">January Food Benchmark (JFB): A Public Benchmark Dataset and Evaluation Suite for Multimodal Food Analysis</a></h3>
                    <p><strong>Authors:</strong> Amir Hosseinian, Ashkan Dehghani Zahedani, Umer Mansoor, Noosheen Hashemi, Mark Woodward</p>
                    <p><strong>Published:</strong> 8/13/2025</p>
                    <p><strong>Categories:</strong> cs.CV, cs.AI</p>
                    <p><strong>Summary:</strong> Progress in AI for automated nutritional analysis is critically hampered by the lack of standardized evaluation methodologies and high-quality, real-world benchmark datasets. To address this, we introduce three primary contributions. First, we present the January Food Benchmark (JFB), a publicly available collection of 1,000 food images with human-validated annotations. Second, we detail a comprehensive benchmarking framework, including robust metrics and a novel, application-oriented overall score designed to assess model performance holistically. Third, we provide baseline results from both general-purpose Vision-Language Models (VLMs) and our own specialized model, january/food-vision-v1. Our evaluation demonstrates that the specialized model achieves an Overall Score of 86.2, a 12.1-point improvement over the best-performing general-purpose configuration. This work offers the research community a valuable new evaluation dataset and a rigorous framework to guide and benchmark future developments in automated nutritional analysis.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.09964v1" target="_blank">Deep and diverse population synthesis for multi-person households using generative models</a></h3>
                    <p><strong>Authors:</strong> Hai Yang, Hongying Wu, Linfei Yuan, Xiyuan Ren, Joseph Y. J. Chow, Jinqin Gao, Kaan Ozbay</p>
                    <p><strong>Published:</strong> 8/13/2025</p>
                    <p><strong>Categories:</strong> cs.CY</p>
                    <p><strong>Summary:</strong> Synthetic population is an increasingly important material used in numerous areas such as urban and transportation analysis. Traditional methods such as iterative proportional fitting (IPF) is not capable of generating high-quality data when facing datasets with high dimension. Latest population synthesis methods using deep learning techniques can resolve such curse of dimensionality. However, few controls are placed when using these methods, and few of the methods are used to generate synthetic population capturing associations among members in one household. In this study, we propose a framework that tackles these issues. The framework uses a novel population synthesis model, called conditional input directed acyclic tabular generative adversarial network (ciDATGAN), as its core, and a basket of methods are employed to enhance the population synthesis performance. We apply the model to generate a synthetic population for the whole New York State as a public resource for researchers and policymakers. The synthetic population includes nearly 20 million individuals and 7.5 million households. The marginals obtained from the synthetic population match the census marginals well while maintaining similar associations among household members to the sample. Compared to the PUMS data, the synthetic population provides data that is 17% more diverse; when compared against a benchmark approach based on Popgen, the proposed method is 13% more diverse. This study provides an approach that encompasses multiple methods to enhance the population synthesis procedure with greater equity- and diversity-awareness.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.09963v1" target="_blank">Online Safety under Multiple Constraints and Input Bounds using gatekeeper: Theory and Applications</a></h3>
                    <p><strong>Authors:</strong> Devansh R. Agrawal, Dimitra Panagou</p>
                    <p><strong>Published:</strong> 8/13/2025</p>
                    <p><strong>Categories:</strong> eess.SY, cs.MA, cs.RO, cs.SY</p>
                    <p><strong>Summary:</strong> This letter presents an approach to guarantee online safety of a cyber-physical system under multiple state and input constraints. Our proposed framework, called gatekeeper, recursively guarantees the existence of an infinite-horizon trajectory that satisfies all constraints and system dynamics. Such trajectory is constructed using a backup controller, which we define formally in this paper. gatekeeper relies on a small number of verifiable assumptions, and is computationally efficient since it requires optimization over a single scalar variable. We make two primary contributions in this letter. (A) First, we develop the theory of gatekeeper: we derive a sub-optimality bound relative to a full nonlinear trajectory optimization problem, and show how this can be used in runtime to validate performance. This also informs the design of the backup controllers and sets. (B) Second, we demonstrate in detail an application of gatekeeper for multi-agent formation flight, where each Dubins agent must avoid multiple obstacles and weapons engagement zones, both of which are nonlinear, nonconvex constraints.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.09957v1" target="_blank">Which one Performs Better? Wav2Vec or Whisper? Applying both in Badini Kurdish Speech to Text (BKSTT)</a></h3>
                    <p><strong>Authors:</strong> Renas Adnan, Hossein Hassani</p>
                    <p><strong>Published:</strong> 8/13/2025</p>
                    <p><strong>Categories:</strong> cs.CL</p>
                    <p><strong>Summary:</strong> Speech-to-text (STT) systems have a wide range of applications. They are available in many languages, albeit at different quality levels. Although Kurdish is considered a less-resourced language from a processing perspective, SST is available for some of the Kurdish dialects, for instance, Sorani (Central Kurdish). However, that is not applied to other Kurdish dialects, Badini and Hawrami, for example. This research is an attempt to address this gap. Bandin, approximately, has two million speakers, and STT systems can help their community use mobile and computer-based technologies while giving their dialect more global visibility. We aim to create a language model based on Badinis speech and evaluate its performance. To cover a conversational aspect, have a proper confidence level of grammatical accuracy, and ready transcriptions, we chose Badini kids stories, eight books including 78 stories, as the textual input. Six narrators narrated the books, which resulted in approximately 17 hours of recording. We cleaned, segmented, and tokenized the input. The preprocessing produced nearly 15 hours of speech, including 19193 segments and 25221 words. We used Wav2Vec2-Large-XLSR-53 and Whisper-small to develop the language models. The experiments indicate that the transcriptions process based on the Wav2Vec2-Large-XLSR-53 model provides a significantly more accurate and readable output than the Whisper-small model, with 90.38% and 65.45% readability, and 82.67% and 53.17% accuracy, respectively.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.09952v1" target="_blank">Specialised or Generic? Tokenization Choices for Radiology Language Models</a></h3>
                    <p><strong>Authors:</strong> Hermione Warr, Wentian Xu, Harry Anthony, Yasin Ibrahim, Daniel McGowan, Konstantinos Kamnitsas</p>
                    <p><strong>Published:</strong> 8/13/2025</p>
                    <p><strong>Categories:</strong> cs.CL, cs.AI, cs.LG</p>
                    <p><strong>Summary:</strong> The vocabulary used by language models (LM) - defined by the tokenizer - plays a key role in text generation quality. However, its impact remains under-explored in radiology. In this work, we address this gap by systematically comparing general, medical, and domain-specific tokenizers on the task of radiology report summarisation across three imaging modalities. We also investigate scenarios with and without LM pre-training on PubMed abstracts. Our findings demonstrate that medical and domain-specific vocabularies outperformed widely used natural language alternatives when models are trained from scratch. Pre-training partially mitigates performance differences between tokenizers, whilst the domain-specific tokenizers achieve the most favourable results. Domain-specific tokenizers also reduce memory requirements due to smaller vocabularies and shorter sequences. These results demonstrate that adapting the vocabulary of LMs to the clinical domain provides practical benefits, including improved performance and reduced computational demands, making such models more accessible and effective for both research and real-world healthcare settings.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.09941v1" target="_blank">Capturing Road-Level Heterogeneity in Crash Severity on Two-Lane Rural Highways: A Multilevel Mixed-Effects Approach</a></h3>
                    <p><strong>Authors:</strong> Mahdi Azhdari, Ali Tavakoli Kashani, Saeideh Amirifar, Amirhossein Taheri, Gerd MÃ¼ller</p>
                    <p><strong>Published:</strong> 8/13/2025</p>
                    <p><strong>Categories:</strong> stat.AP</p>
                    <p><strong>Summary:</strong> Accurately modeling crash severity on rural two-lane roads is essential for effective safety management, yet standard single level approaches often overlook unobserved heterogeneity across road segments. In this study, we analyze 19 956 crash records from 99 rural roads in Iran during recent four years incorporating crash level predictors such as driver age, education, gender, lighting and pavement conditions, along with road level covariates like annual average daily traffic, heavy-vehicle share and terrain slope. We compare three binary logistic frameworks: a single level generalized linear model, a multilevel model with a random intercept capturing latent road level effects (intraclass correlation = 21 %), and a multilevel model with random coefficients that allows key predictor effects to vary by road. The random coefficient model achieves the best fit in terms of deviance, AIC and BIC, and substantially improves predictive performance: classification accuracy rises from 0.62 to 0.71, recall from 0.32 to 0.63, and AUC from 0.570 to 0.775. Results from 200 simulation runs reveal notable variability in slopes for pavement and lighting variables, underscoring how local context influences crash risk. Overall, our findings demonstrate that flexible multilevel modeling not only enhances prediction accuracy but also yields context-specific insights to guide targeted safety interventions on rural road networks.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.09937v1" target="_blank">A Comprehensive Evaluation framework of Alignment Techniques for LLMs</a></h3>
                    <p><strong>Authors:</strong> Muneeza Azmat, Momin Abbas, Maysa Malfiza Garcia de Macedo, Marcelo Carpinette Grave, Luan Soares de Souza, Tiago Machado, Rogerio A de Paula, Raya Horesh, Yixin Chen, Heloisa Caroline de Souza Pereira Candello, Rebecka Nordenlow, Aminat Adebiyi</p>
                    <p><strong>Published:</strong> 8/13/2025</p>
                    <p><strong>Categories:</strong> cs.CL, cs.AI, cs.LG</p>
                    <p><strong>Summary:</strong> As Large Language Models (LLMs) become increasingly integrated into real-world applications, ensuring their outputs align with human values and safety standards has become critical. The field has developed diverse alignment approaches including traditional fine-tuning methods (RLHF, instruction tuning), post-hoc correction systems, and inference-time interventions, each with distinct advantages and limitations. However, the lack of unified evaluation frameworks makes it difficult to systematically compare these paradigms and guide deployment decisions. This paper introduces a multi-dimensional evaluation of alignment techniques for LLMs, a comprehensive evaluation framework that provides a systematic comparison across all major alignment paradigms. Our framework assesses methods along four key dimensions: alignment detection, alignment quality, computational efficiency, and robustness. Through experiments across diverse base models and alignment strategies, we demonstrate the utility of our framework in identifying strengths and limitations of current state-of-the-art models, providing valuable insights for future research directions.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.09935v1" target="_blank">Language of Persuasion and Misrepresentation in Business Communication: A Textual Detection Approach</a></h3>
                    <p><strong>Authors:</strong> Sayem Hossen, Monalisa Moon Joti, Md. Golam Rashed</p>
                    <p><strong>Published:</strong> 8/13/2025</p>
                    <p><strong>Categories:</strong> cs.CL, q-fin.CP, q-fin.GN</p>
                    <p><strong>Summary:</strong> Business communication digitisation has reorganised the process of persuasive discourse, which allows not only greater transparency but also advanced deception. This inquiry synthesises classical rhetoric and communication psychology with linguistic theory and empirical studies in the financial reporting, sustainability discourse, and digital marketing to explain how deceptive language can be systematically detected using persuasive lexicon. In controlled settings, detection accuracies of greater than 99% were achieved by using computational textual analysis as well as personalised transformer models. However, reproducing this performance in multilingual settings is also problematic and, to a large extent, this is because it is not easy to find sufficient data, and because few multilingual text-processing infrastructures are in place. This evidence shows that there has been an increasing gap between the theoretical representations of communication and those empirically approximated, and therefore, there is a need to have strong automatic text-identification systems where AI-based discourse is becoming more realistic in communicating with humans.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.09932v1" target="_blank">Mathematical Computation and Reasoning Errors by Large Language Models</a></h3>
                    <p><strong>Authors:</strong> Liang Zhang, Edith Aurora Graf</p>
                    <p><strong>Published:</strong> 8/13/2025</p>
                    <p><strong>Categories:</strong> cs.AI</p>
                    <p><strong>Summary:</strong> Large Language Models (LLMs) are increasingly utilized in AI-driven educational instruction and assessment, particularly within mathematics education. The capability of LLMs to generate accurate answers and detailed solutions for math problem-solving tasks is foundational for ensuring reliable and precise feedback and assessment in math education practices. Our study focuses on evaluating the accuracy of four LLMs (OpenAI GPT-4o and o1, DeepSeek-V3 and DeepSeek-R1) solving three categories of math tasks, including arithmetic, algebra, and number theory, and identifies step-level reasoning errors within their solutions. Instead of relying on standard benchmarks, we intentionally build math tasks (via item models) that are challenging for LLMs and prone to errors. The accuracy of final answers and the presence of errors in individual solution steps were systematically analyzed and coded. Both single-agent and dual-agent configurations were tested. It is observed that the reasoning-enhanced OpenAI o1 model consistently achieved higher or nearly perfect accuracy across all three math task categories. Analysis of errors revealed that procedural slips were the most frequent and significantly impacted overall performance, while conceptual misunderstandings were less frequent. Deploying dual-agent configurations substantially improved overall performance. These findings offer actionable insights into enhancing LLM performance and underscore effective strategies for integrating LLMs into mathematics education, thereby advancing AI-driven instructional practices and assessment precision.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.09926v1" target="_blank">Towards Comprehensive Cellular Characterisation of HE slides</a></h3>
                    <p><strong>Authors:</strong> Benjamin Adjadj, Pierre-Antoine Bannier, Guillaume Horent, Sebastien Mandela, Aurore Lyon, Kathryn Schutte, Ulysse Marteau, Valentin Gaury, Laura Dumont, Thomas Mathieu, Reda Belbahri, BenoÃ®t Schmauch, Eric Durand, Katharina Von Loga, Lucie Gillet</p>
                    <p><strong>Published:</strong> 8/13/2025</p>
                    <p><strong>Categories:</strong> cs.CV, q-bio.QM, I.2.10; I.4.8</p>
                    <p><strong>Summary:</strong> Cell detection, segmentation and classification are essential for analyzing tumor microenvironments (TME) on hematoxylin and eosin (HE) slides. Existing methods suffer from poor performance on understudied cell types (rare or not present in public datasets) and limited cross-domain generalization. To address these shortcomings, we introduce HistoPLUS, a state-of-the-art model for cell analysis, trained on a novel curated pan-cancer dataset of 108,722 nuclei covering 13 cell types. In external validation across 4 independent cohorts, HistoPLUS outperforms current state-of-the-art models in detection quality by 5.2% and overall F1 classification score by 23.7%, while using 5x fewer parameters. Notably, HistoPLUS unlocks the study of 7 understudied cell types and brings significant improvements on 8 of 13 cell types. Moreover, we show that HistoPLUS robustly transfers to two oncology indications unseen during training. To support broader TME biomarker research, we release the model weights and inference code at https://github.com/owkin/histoplus/.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.09919v1" target="_blank">T-CACE: A Time-Conditioned Autoregressive Contrast Enhancement Multi-Task Framework for Contrast-Free Liver MRI Synthesis, Segmentation, and Diagnosis</a></h3>
                    <p><strong>Authors:</strong> Xiaojiao Xiao, Jianfeng Zhao, Qinmin Vivian Hu, Guanghui Wang</p>
                    <p><strong>Published:</strong> 8/13/2025</p>
                    <p><strong>Categories:</strong> eess.IV, cs.AI, cs.CV</p>
                    <p><strong>Summary:</strong> Magnetic resonance imaging (MRI) is a leading modality for the diagnosis of liver cancer, significantly improving the classification of the lesion and patient outcomes. However, traditional MRI faces challenges including risks from contrast agent (CA) administration, time-consuming manual assessment, and limited annotated datasets. To address these limitations, we propose a Time-Conditioned Autoregressive Contrast Enhancement (T-CACE) framework for synthesizing multi-phase contrast-enhanced MRI (CEMRI) directly from non-contrast MRI (NCMRI). T-CACE introduces three core innovations: a conditional token encoding (CTE) mechanism that unifies anatomical priors and temporal phase information into latent representations; and a dynamic time-aware attention mask (DTAM) that adaptively modulates inter-phase information flow using a Gaussian-decayed attention mechanism, ensuring smooth and physiologically plausible transitions across phases. Furthermore, a constraint for temporal classification consistency (TCC) aligns the lesion classification output with the evolution of the physiological signal, further enhancing diagnostic reliability. Extensive experiments on two independent liver MRI datasets demonstrate that T-CACE outperforms state-of-the-art methods in image synthesis, segmentation, and lesion classification. This framework offers a clinically relevant and efficient alternative to traditional contrast-enhanced imaging, improving safety, diagnostic efficiency, and reliability for the assessment of liver lesion. The implementation of T-CACE is publicly available at: https://github.com/xiaojiao929/T-CACE.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.09917v1" target="_blank">Evaluation of a deliberate-practice informed supplemental intervention in graduate Quantum Mechanics</a></h3>
                    <p><strong>Authors:</strong> Michael E. Robbins, Guillaume M. Laurent, Eric W. Burkholder</p>
                    <p><strong>Published:</strong> 8/13/2025</p>
                    <p><strong>Categories:</strong> physics.ed-ph, quant-ph</p>
                    <p><strong>Summary:</strong> Despite the prevalence of physics education research literature related to problem solving, recent studies have illustrated that opportunities for ``authentic problem solving -- conceptualized as making decisions with limited information using ones physics knowledge -- are limited at both the graduate and undergraduate levels in physics curricula. Building on one of these studies, we designed a supplemental intervention for a graduate-level quantum mechanics course which scaffolded the practice of making some of these critical decisions using the conceptual framework of deliberate practice. Despite similar incentive structures as prior interventions focused on conceptual understanding in similar contexts, we did not measure any statistically significant improvement in students problem solving skills following our intervention, though faculty members involved with the next course and written qualifying exams indicated the students showed better-than-usual conceptual understanding. We explore a number of potential explanations for this disconnect and suggest future avenues of research in this area.</p>
                
            
                
                    <h3><a href="http://dx.doi.org/10.1145/3757707" target="_blank">Wisdom of the Crowd, Without the Crowd: A Socratic LLM for Asynchronous Deliberation on Perspectivist Data</a></h3>
                    <p><strong>Authors:</strong> Malik Khadar, Daniel Runningen, Julia Tang, Stevie Chancellor, Harmanpreet Kaur</p>
                    <p><strong>Published:</strong> 8/13/2025</p>
                    <p><strong>Categories:</strong> cs.HC</p>
                    <p><strong>Summary:</strong> Data annotation underpins the success of modern AI, but the aggregation of crowd-collected datasets can harm the preservation of diverse perspectives in data. Difficult and ambiguous tasks cannot easily be collapsed into unitary labels. Prior work has shown that deliberation and discussion improve data quality and preserve diverse perspectives -- however, synchronous deliberation through crowdsourcing platforms is time-intensive and costly. In this work, we create a Socratic dialog system using Large Language Models (LLMs) to act as a deliberation partner in place of other crowdworkers. Against a benchmark of synchronous deliberation on two tasks (Sarcasm and Relation detection), our Socratic LLM encouraged participants to consider alternate annotation perspectives, update their labels as needed (with higher confidence), and resulted in higher annotation accuracy (for the Relation task where ground truth is available). Qualitative findings show that our agents Socratic approach was effective at encouraging reasoned arguments from our participants, and that the intervention was well-received. Our methodology lays the groundwork for building scalable systems that preserve individual perspectives in generating more representative datasets.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.09909v1" target="_blank">SHREC25 Track on Multiple Relief Patterns: Report and Analysis</a></h3>
                    <p><strong>Authors:</strong> Gabriele Paolini, Claudio Tortorici, Stefano Berretti, Ahmed Hazem Youssef, Halim Benhabiles, Adnane Cabani, Ruiwen He, Karim Hammoudi, Iyyakutti Iyappan Ganapathi, Syed Sadaf Ali, Divya Velayudhan, Maregu Assefa, Naoufel Werghi</p>
                    <p><strong>Published:</strong> 8/13/2025</p>
                    <p><strong>Categories:</strong> cs.CG</p>
                    <p><strong>Summary:</strong> This SHREC 2025 track focuses on the recognition and segmentation of relief patterns embedded on the surface of a set of synthetically generated triangle meshes. We report the methods proposed by the participants, whose performance highlights the inherent complexity of solving the problem, which is still open. Then, we discuss the critical aspects of the proposed tasks, highlight the limitations of current techniques, and outline possible directions for future research. All resources and track details are available at the official track webpage: https://sites.google.com/unifi.it/shrec25-relief-pattern.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.09906v1" target="_blank">Reducing the weight of low exam scores may raise average grades but does not appear to impact equity gaps</a></h3>
                    <p><strong>Authors:</strong> Nicholas T. Young, Rebecca L. Matz, Eric F. Bell, Caitlin Hayward</p>
                    <p><strong>Published:</strong> 8/13/2025</p>
                    <p><strong>Categories:</strong> physics.ed-ph</p>
                    <p><strong>Summary:</strong> Students interpret grades as signals of their strengths, and grades inform students decisions about majors and courses of study. Differences in grades that are not related to learning can impact this judgment and have real-world impact on course-taking and careers. Existing work has examined how an overemphasis on high-stakes exams can create equity gaps where female students and Black, Hispanic, and Native American students earn lower grades than male students and Asian and white students, respectively. Yet, minimal work has examined how the weighting of individual midterm exam scores can also contribute to equity gaps. In this work, we examine how three midterm exam score aggregation methods for final grades affect equity gaps. We collected midterm exam data from approximately 6,000 students in an introductory physics course over 6 years at a large, research-intensive university. Using this data set, we applied common midterm exam score aggregation methods to determine their impact on aggregated midterm exam grades: dropping the lowest midterm exam score, replacing the lowest midterm exam score with the final exam score if higher, and counting the highest midterm exam score more in the final grade calculation than the lowest midterm exam score. We find that dropping the lowest midterm exam score resulted in the largest increase in final grades, with students with lower grades benefiting the most. However, we find limited evidence that alternative midterm exam aggregation methods could close equity gaps. Implementing the alternative midterm exam aggregation methods examined here may be useful for instructors wanting to raise course grades or give lower-scoring students a boost. However, they do not appear to be effective in reducing equity gaps.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.09903v1" target="_blank">Hybrid Quantum-Classical Latent Diffusion Models for Medical Image Generation</a></h3>
                    <p><strong>Authors:</strong> KÃ¼bra Yeter-Aydeniz, Nora M. Bauer, Pranay Jain, Max Masnick</p>
                    <p><strong>Published:</strong> 8/13/2025</p>
                    <p><strong>Categories:</strong> quant-ph</p>
                    <p><strong>Summary:</strong> Generative learning models in medical research are crucial in developing training data for deep learning models and advancing diagnostic tools, but the problem of high-quality, diverse images is an open topic of research. Quantum-enhanced generative models have been proposed and tested in the literature but have been restricted to small problems below the scale of industry relevance. In this paper, we propose quantum-enhanced diffusion and variational autoencoder (VAE) models and test them on the fundus retinal image generation task. In our numerical experiments, the images generated using quantum-enhanced models are of higher quality, with 86% classified as gradable by external validation compared to 69% with the classical model, and they match more closely in features to the real image distribution compared to the ones generated using classical diffusion models, even when the classical diffusion models are larger than the quantum model. Additionally, we perform noisy testing to confirm the numerical experiments, finding that quantum-enhanced diffusion model can sometimes produce higher quality images, both in terms of diversity and fidelity, when tested with quantum hardware noise. Our results indicate that quantum diffusion models on current quantum hardware are strong targets for further research on quantum utility in generative modeling for industrially relevant problems.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.09884v1" target="_blank">Spin-chirality-dependent modulation of topological gap, Chern number, and valley-polarization in monolayer Kagome materials</a></h3>
                    <p><strong>Authors:</strong> Wenzhe Zhou, Guibo Zheng, Yating Li, Zhenzhen Wan, Aolin Li, Fangping Ouyang</p>
                    <p><strong>Published:</strong> 8/13/2025</p>
                    <p><strong>Categories:</strong> cond-mat.mtrl-sci</p>
                    <p><strong>Summary:</strong> Kagome materials exhibit unique electronic properties, such as the quantum anomalous Hall effect. The control of Chern numbers is critical for quantum device manipulation, but existing research has mainly focused on collinear magnetization while neglecting chiral spin textures. Through first-principles calculations and tight-binding modeling of monolayer Cr3Se4, this study reveals spin chirality-dependent control of topological gaps, Chern numbers, and valley polarization in kagome materials. The results demonstrate that the azimuthal angle has no observable effect. For collinear magnetization, the topological bandgap decreases as the spin orientation approaches the in-plane direction. In the breathing Kagome lattice, the degeneracy between K and K valleys is lifted, and increasing the polar angle induces successive closing and reopening of the valleys. For chiral spin textures, increasing polar angle enlarges the bandgap when chirality \k{appa} = 1, while reducing it when \k{appa} = -1. Moreover, spin chirality enables the quantum anomalous Hall state without spin-orbit coupling. Structural asymmetry and spin chirality effectively modulate the bandgap, Chern number, and valley polarization. These findings provide strategies for controlling topological states and advancing applications in quantum devices and valleytronic systems.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.09878v1" target="_blank">A Survey of Cognitive Distortion Detection and Classification in NLP</a></h3>
                    <p><strong>Authors:</strong> Archie Sage, Jeroen Keppens, Helen Yannakoudakis</p>
                    <p><strong>Published:</strong> 8/13/2025</p>
                    <p><strong>Categories:</strong> cs.CL</p>
                    <p><strong>Summary:</strong> As interest grows in the application of natural language processing (NLP) techniques to mental health, a growing body of work explores the automatic detection and classification of cognitive distortions (CDs). CDs are habitual patterns of negatively biased or flawed thinking that distort how people perceive events, judge themselves, and react to the world around them. Identifying and addressing them is an important part of therapy. Despite its momentum, the field remains fragmented, with inconsistencies in CD taxonomies, task formulations, and evaluation practices. This survey reviews 38 studies spanning two decades, providing a structured overview of datasets, modelling approaches, and evaluation strategies. We provide a consolidated CD taxonomy reference, summarise common task setups, and highlight open challenges to support more coherent and reproducible research in this emerging area.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.09871v1" target="_blank">Inference of germinal center evolutionary dynamics via simulation-based deep learning</a></h3>
                    <p><strong>Authors:</strong> Duncan K Ralph, Athanasios G Bakis, Jared Galloway, Ashni A Vora, Tatsuya Araki, Gabriel D Victora, Yun S Song, William S DeWitt, Frederick A Matsen IV</p>
                    <p><strong>Published:</strong> 8/13/2025</p>
                    <p><strong>Categories:</strong> q-bio.PE</p>
                    <p><strong>Summary:</strong> B cells and the antibodies they produce are vital to health and survival, motivating research on the details of the mutational and evolutionary processes in the germinal centers (GC) from which mature B cells arise. It is known that B cells with higher affinity for their cognate antigen (Ag) will, on average, tend to have more offspring. However the exact form of this relationship between affinity and fecundity, which we call the ``affinity-fitness response function, is not known. Here we use deep learning and simulation-based inference to learn this function from a unique experiment that replays a particular combination of GC conditions many times. All code is freely available at https://github.com/matsengrp/gcdyn, while datasets and inference results can be found at https://doi.org/10.5281/zenodo.15022130.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.09865v1" target="_blank">Assessing the Feasibility of Lightweight Whisper Models for Low-Resource Urdu Transcription</a></h3>
                    <p><strong>Authors:</strong> Abdul Rehman Antall, Naveed Akhtar</p>
                    <p><strong>Published:</strong> 8/13/2025</p>
                    <p><strong>Categories:</strong> cs.CL</p>
                    <p><strong>Summary:</strong> This study evaluates the feasibility of lightweight Whisper models (Tiny, Base, Small) for Urdu speech recognition in low-resource settings. Despite Urdu being the 10th most spoken language globally with over 230 million speakers, its representation in automatic speech recognition (ASR) systems remains limited due to dialectal diversity, code-switching, and sparse training data. We benchmark these models on a curated Urdu dataset using word error rate (WER), without fine-tuning. Results show Whisper-Small achieves the lowest error rates (33.68\% WER), outperforming Tiny (67.08\% WER) and Base (53.67\% WER). Qualitative analysis reveals persistent challenges in phonetic accuracy and lexical coherence, particularly for complex utterances. While Whisper-Small demonstrates promise for deployable Urdu ASR, significant gaps remain. Our findings emphasize lay the groundwork for future research into effective, low-resource ASR systems.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.09860v1" target="_blank">Human-Aligned Procedural Level Generation Reinforcement Learning via Text-Level-Sketch Shared Representation</a></h3>
                    <p><strong>Authors:</strong> In-Chang Baek, Seoyoung Lee, Sung-Hyun Kim, Geumhwan Hwang, KyungJoong Kim</p>
                    <p><strong>Published:</strong> 8/13/2025</p>
                    <p><strong>Categories:</strong> cs.AI</p>
                    <p><strong>Summary:</strong> Human-aligned AI is a critical component of co-creativity, as it enables models to accurately interpret human intent and generate controllable outputs that align with design goals in collaborative content creation. This direction is especially relevant in procedural content generation via reinforcement learning (PCGRL), which is intended to serve as a tool for human designers. However, existing systems often fall short of exhibiting human-centered behavior, limiting the practical utility of AI-driven generation tools in real-world design workflows. In this paper, we propose VIPCGRL (Vision-Instruction PCGRL), a novel deep reinforcement learning framework that incorporates three modalities-text, level, and sketches-to extend control modality and enhance human-likeness. We introduce a shared embedding space trained via quadruple contrastive learning across modalities and human-AI styles, and align the policy using an auxiliary reward based on embedding similarity. Experimental results show that VIPCGRL outperforms existing baselines in human-likeness, as validated by both quantitative metrics and human evaluations. The code and dataset will be available upon publication.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.09853v1" target="_blank">STREAM (ChemBio): A Standard for Transparently Reporting Evaluations in AI Model Reports</a></h3>
                    <p><strong>Authors:</strong> Tegan McCaslin, Jide Alaga, Samira Nedungadi, Seth Donoughe, Tom Reed, Rishi Bommasani, Chris Painter, Luca Righetti</p>
                    <p><strong>Published:</strong> 8/13/2025</p>
                    <p><strong>Categories:</strong> cs.CY, cs.AI</p>
                    <p><strong>Summary:</strong> Evaluations of dangerous AI capabilities are important for managing catastrophic risks. Public transparency into these evaluations - including what they test, how they are conducted, and how their results inform decisions - is crucial for building trust in AI development. We propose STREAM (A Standard for Transparently Reporting Evaluations in AI Model Reports), a standard to improve how model reports disclose evaluation results, initially focusing on chemical and biological (ChemBio) benchmarks. Developed in consultation with 23 experts across government, civil society, academia, and frontier AI companies, this standard is designed to (1) be a practical resource to help AI developers present evaluation results more clearly, and (2) help third parties identify whether model reports provide sufficient detail to assess the rigor of the ChemBio evaluations. We concretely demonstrate our proposed best practices with gold standard examples, and also provide a three-page reporting template to enable AI developers to implement our recommendations more easily.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.09834v1" target="_blank">Speed Always Wins: A Survey on Efficient Architectures for Large Language Models</a></h3>
                    <p><strong>Authors:</strong> Weigao Sun, Jiaxi Hu, Yucheng Zhou, Jusen Du, Disen Lan, Kexin Wang, Tong Zhu, Xiaoye Qu, Yu Zhang, Xiaoyu Mo, Daizong Liu, Yuxuan Liang, Wenliang Chen, Guoqi Li, Yu Cheng</p>
                    <p><strong>Published:</strong> 8/13/2025</p>
                    <p><strong>Categories:</strong> cs.CL, cs.AI, cs.CV</p>
                    <p><strong>Summary:</strong> Large Language Models (LLMs) have delivered impressive results in language understanding, generation, reasoning, and pushes the ability boundary of multimodal models. Transformer models, as the foundation of modern LLMs, offer a strong baseline with excellent scaling properties. However, the traditional transformer architecture requires substantial computations and poses significant obstacles for large-scale training and practical deployment. In this survey, we offer a systematic examination of innovative LLM architectures that address the inherent limitations of transformers and boost the efficiency. Starting from language modeling, this survey covers the background and technical details of linear and sparse sequence modeling methods, efficient full attention variants, sparse mixture-of-experts, hybrid model architectures incorporating the above techniques, and emerging diffusion LLMs. Additionally, we discuss applications of these techniques to other modalities and consider their wider implications for developing scalable, resource-aware foundation models. By grouping recent studies into the above category, this survey presents a blueprint of modern efficient LLM architectures, and we hope this could help motivate future research toward more efficient, versatile AI systems.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.09832v1" target="_blank">Exploring the Potential of Large Language Models in Fine-Grained Review Comment Classification</a></h3>
                    <p><strong>Authors:</strong> Linh Nguyen, Chunhua Liu, Hong Yi Lin, Patanamon Thongtanunam</p>
                    <p><strong>Published:</strong> 8/13/2025</p>
                    <p><strong>Categories:</strong> cs.SE, cs.AI</p>
                    <p><strong>Summary:</strong> Code review is a crucial practice in software development. As code review nowadays is lightweight, various issues can be identified, and sometimes, they can be trivial. Research has investigated automated approaches to classify review comments to gauge the effectiveness of code reviews. However, previous studies have primarily relied on supervised machine learning, which requires extensive manual annotation to train the models effectively. To address this limitation, we explore the potential of using Large Language Models (LLMs) to classify code review comments. We assess the performance of LLMs to classify 17 categories of code review comments. Our results show that LLMs can classify code review comments, outperforming the state-of-the-art approach using a trained deep learning model. In particular, LLMs achieve better accuracy in classifying the five most useful categories, which the state-of-the-art approach struggles with due to low training examples. Rather than relying solely on a specific small training data distribution, our results show that LLMs provide balanced performance across high- and low-frequency categories. These results suggest that the LLMs could offer a scalable solution for code review analytics to improve the effectiveness of the code review process.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.09828v1" target="_blank">Fast and Accurate Heuristics for Bus-Factor Estimation</a></h3>
                    <p><strong>Authors:</strong> Sebastiano Antonio Piccolo</p>
                    <p><strong>Published:</strong> 8/13/2025</p>
                    <p><strong>Categories:</strong> cs.SE</p>
                    <p><strong>Summary:</strong> The bus-factor is a critical risk indicator that quantifies how many key contributors a project can afford to lose before core knowledge or functionality is compromised. Despite its practical importance, accurately computing the bus-factor is NP-Hard under established formalizations, making scalable analysis infeasible for large software systems. In this paper, we model software projects as bipartite graphs of developers and tasks and propose two novel approximation heuristics, Minimum Coverage and Maximum Coverage, based on iterative graph peeling, for two influential bus-factor formalizations. Our methods significantly outperform the widely adopted degree-based heuristic, which we show can yield severely inflated estimates. We conduct a comprehensive empirical evaluation on over $1\,000$ synthetic power-law graphs and demonstrate that our heuristics provide tighter estimates while scaling to graphs with millions of nodes and edges in minutes. Our results reveal that the proposed heuristics are not only more accurate but also robust to structural variations in developer-task assignment graph. We release our implementation as open-source software to support future research and practical adoption.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.09825v1" target="_blank">From Self-Crafted to Engineered Prompts: Student Evaluations of AI-Generated Feedback in Introductory Physics</a></h3>
                    <p><strong>Authors:</strong> Amogh Sirnoorkar, N. Sanjay Rebello</p>
                    <p><strong>Published:</strong> 8/13/2025</p>
                    <p><strong>Categories:</strong> physics.ed-ph</p>
                    <p><strong>Summary:</strong> The abilities of Generative-Artificial Intelligence (AI) to produce real-time, sophisticated responses across diverse contexts has promised a huge potential in physics education, particularly in providing customized feedback. In this study, we investigate around 1200 introductory students preferences about AI-feedback generated from three distinct prompt types: (a) self-crafted, (b) entailing foundational prompt-engineering techniques, and (c) entailing foundational prompt-engineering techniques along with principles of effective-feedback. The results highlight an overwhelming fraction of students preferring feedback generated using structured prompts, with those entailing combined features of prompt engineering and effective feedback to be favored most. However, the popular choice also elicited stronger preferences with students either liking or disliking the feedback. Students also ranked the feedback generated using their self-crafted prompts as the least preferred choice. Students second preferences given their first choice and implications of the results such as the need to incorporate prompt engineering in introductory courses are discussed.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.09815v1" target="_blank">Extending the OWASP Multi-Agentic System Threat Modeling Guide: Insights from Multi-Agent Security Research</a></h3>
                    <p><strong>Authors:</strong> Klaudia Krawiecka, Christian Schroeder de Witt</p>
                    <p><strong>Published:</strong> 8/13/2025</p>
                    <p><strong>Categories:</strong> cs.MA, cs.CR, cs.SE</p>
                    <p><strong>Summary:</strong> We propose an extension to the OWASP Multi-Agentic System (MAS) Threat Modeling Guide, translating recent anticipatory research in multi-agent security (MASEC) into practical guidance for addressing challenges unique to large language model (LLM)-driven multi-agent architectures. Although OWASPs existing taxonomy covers many attack vectors, our analysis identifies gaps in modeling failures, including, but not limited to: reasoning collapse across planner-executor chains, metric overfitting, unsafe delegation escalation, emergent covert coordination, and heterogeneous multi-agent exploits. We introduce additional threat classes and scenarios grounded in practical MAS deployments, highlighting risks from benign goal drift, cross-agent hallucination propagation, affective prompt framing, and multi-agent backdoors. We also outline evaluation strategies, including robustness testing, coordination assessment, safety enforcement, and emergent behavior monitoring, to ensure complete coverage. This work complements the framework of OWASP by expanding its applicability to increasingly complex, autonomous, and adaptive multi-agent systems, with the goal of improving security posture and resilience in real world deployments.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.09810v1" target="_blank">Feature Impact Analysis on Top Long-Jump Performances with Quantile Random Forest and Explainable AI Techniques</a></h3>
                    <p><strong>Authors:</strong> Qi Gan, Stephan ClÃ©menÃ§on, MounÃ®m A. El-Yacoubi, Sao Mai Nguyen, Eric Fenaux, Ons Jelassi</p>
                    <p><strong>Published:</strong> 8/13/2025</p>
                    <p><strong>Categories:</strong> cs.LG, stat.AP</p>
                    <p><strong>Summary:</strong> Biomechanical features have become important indicators for evaluating athletes techniques. Traditionally, experts propose significant features and evaluate them using physics equations. However, the complexity of the human body and its movements makes it challenging to explicitly analyze the relationships between some features and athletes final performance. With advancements in modern machine learning and statistics, data analytics methods have gained increasing importance in sports analytics. In this study, we leverage machine learning models to analyze expert-proposed biomechanical features from the finals of long jump competitions in the World Championships. The objectives of the analysis include identifying the most important features contributing to top-performing jumps and exploring the combined effects of these key features. Using quantile regression, we model the relationship between the biomechanical feature set and the target variable (effective distance), with a particular focus on elite-level jumps. To interpret the model, we apply SHapley Additive exPlanations (SHAP) alongside Partial Dependence Plots (PDPs) and Individual Conditional Expectation (ICE) plots. The findings reveal that, beyond the well-documented velocity-related features, specific technical aspects also play a pivotal role. For male athletes, the angle of the knee of the supporting leg before take-off is identified as a key factor for achieving top 10% performance in our dataset, with angles greater than 169{\deg}contributing significantly to jump performance. In contrast, for female athletes, the landing pose and approach step technique emerge as the most critical features influencing top 10% performances, alongside velocity. This study establishes a framework for analyzing the impact of various features on athletic performance, with a particular emphasis on top-performing events.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.09809v1" target="_blank">A Comprehensive Survey of Datasets for Clinical Mental Health AI Systems</a></h3>
                    <p><strong>Authors:</strong> Aishik Mandal, Prottay Kumar Adhikary, Hiba Arnaout, Iryna Gurevych, Tanmoy Chakraborty</p>
                    <p><strong>Published:</strong> 8/13/2025</p>
                    <p><strong>Categories:</strong> cs.CL, cs.AI</p>
                    <p><strong>Summary:</strong> Mental health disorders are rising worldwide. However, the availability of trained clinicians has not scaled proportionally, leaving many people without adequate or timely support. To bridge this gap, recent studies have shown the promise of Artificial Intelligence (AI) to assist mental health diagnosis, monitoring, and intervention. However, the development of efficient, reliable, and ethical AI to assist clinicians is heavily dependent on high-quality clinical training datasets. Despite growing interest in data curation for training clinical AI assistants, existing datasets largely remain scattered, under-documented, and often inaccessible, hindering the reproducibility, comparability, and generalizability of AI models developed for clinical mental health care. In this paper, we present the first comprehensive survey of clinical mental health datasets relevant to the training and development of AI-powered clinical assistants. We categorize these datasets by mental disorders (e.g., depression, schizophrenia), data modalities (e.g., text, speech, physiological signals), task types (e.g., diagnosis prediction, symptom severity estimation, intervention generation), accessibility (public, restricted or private), and sociocultural context (e.g., language and cultural background). Along with these, we also investigate synthetic clinical mental health datasets. Our survey identifies critical gaps such as a lack of longitudinal data, limited cultural and linguistic representation, inconsistent collection and annotation standards, and a lack of modalities in synthetic data. We conclude by outlining key challenges in curating and standardizing future datasets and provide actionable recommendations to facilitate the development of more robust, generalizable, and equitable mental health AI systems.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.09797v1" target="_blank">FLARE: Agile Flights for Quadrotor Cable-Suspended Payload System via Reinforcement Learning</a></h3>
                    <p><strong>Authors:</strong> Dongcheng Cao, Jin Zhou, Xian Wang, Shuo Li</p>
                    <p><strong>Published:</strong> 8/13/2025</p>
                    <p><strong>Categories:</strong> cs.RO</p>
                    <p><strong>Summary:</strong> Agile flight for the quadrotor cable-suspended payload system is a formidable challenge due to its underactuated, highly nonlinear, and hybrid dynamics. Traditional optimization-based methods often struggle with high computational costs and the complexities of cable mode transitions, limiting their real-time applicability and maneuverability exploitation. In this letter, we present FLARE, a reinforcement learning (RL) framework that directly learns agile navigation policy from high-fidelity simulation. Our method is validated across three designed challenging scenarios, notably outperforming a state-of-the-art optimization-based approach by a 3x speedup during gate traversal maneuvers. Furthermore, the learned policies achieve successful zero-shot sim-to-real transfer, demonstrating remarkable agility and safety in real-world experiments, running in real time on an onboard computer.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.09790v1" target="_blank">BeatFM: Improving Beat Tracking with Pre-trained Music Foundation Model</a></h3>
                    <p><strong>Authors:</strong> Ganghui Ru, Jieying Wang, Jiahao Zhao, Yulun Wu, Yi Yu, Nannan Jiang, Wei Wang, Wei Li</p>
                    <p><strong>Published:</strong> 8/13/2025</p>
                    <p><strong>Categories:</strong> cs.SD</p>
                    <p><strong>Summary:</strong> Beat tracking is a widely researched topic in music information retrieval. However, current beat tracking methods face challenges due to the scarcity of labeled data, which limits their ability to generalize across diverse musical styles and accurately capture complex rhythmic structures. To overcome these challenges, we propose a novel beat tracking paradigm BeatFM, which introduces a pre-trained music foundation model and leverages its rich semantic knowledge to improve beat tracking performance. Pre-training on diverse music datasets endows music foundation models with a robust understanding of music, thereby effectively addressing these challenges. To further adapt it for beat tracking, we design a plug-and-play multi-dimensional semantic aggregation module, which is composed of three parallel sub-modules, each focusing on semantic aggregation in the temporal, frequency, and channel domains, respectively. Extensive experiments demonstrate that our method achieves state-of-the-art performance in beat and downbeat tracking across multiple benchmark datasets.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.09786v1" target="_blank">Adoption of Explainable Natural Language Processing: Perspectives from Industry and Academia on Practices and Challenges</a></h3>
                    <p><strong>Authors:</strong> Mahdi Dhaini, Tobias MÃ¼ller, Roksoliana Rabets, Gjergji Kasneci</p>
                    <p><strong>Published:</strong> 8/13/2025</p>
                    <p><strong>Categories:</strong> cs.CL, cs.AI, cs.HC</p>
                    <p><strong>Summary:</strong> The field of explainable natural language processing (NLP) has grown rapidly in recent years. The growing opacity of complex models calls for transparency and explanations of their decisions, which is crucial to understand their reasoning and facilitate deployment, especially in high-stakes environments. Despite increasing attention given to explainable NLP, practitioners perspectives regarding its practical adoption and effectiveness remain underexplored. This paper addresses this research gap by investigating practitioners experiences with explainability methods, specifically focusing on their motivations for adopting such methods, the techniques employed, satisfaction levels, and the practical challenges encountered in real-world NLP applications. Through a qualitative interview-based study with industry practitioners and complementary interviews with academic researchers, we systematically analyze and compare their perspectives. Our findings reveal conceptual gaps, low satisfaction with current explainability methods, and highlight evaluation challenges. Our findings emphasize the need for clear definitions and user-centric frameworks for better adoption of explainable NLP in practice.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.09769v1" target="_blank">An (m,k)-firm Elevation Policy to Increase the Robustness of Time-Driven Schedules in 5G Time-Sensitive Networks</a></h3>
                    <p><strong>Authors:</strong> Simon Egger, Robin Laidig, Heiko Geppert, Lucas Haug, Jona Herrmann, Frank DÃ¼rr, Christian Becker</p>
                    <p><strong>Published:</strong> 8/13/2025</p>
                    <p><strong>Categories:</strong> cs.NI</p>
                    <p><strong>Summary:</strong> Current standardization efforts are advancing the integration of 5G and Time-Sensitive Networking (TSN) to facilitate the deployment of safety-critical industrial applications that require real-time communication. However, there remains a fundamental disconnect between the probabilistic 5G delay characteristics and the often idealistic delay models used to synthesize 5G-TSN network configurations. For time-driven schedules in particular, any delay outlier unforeseen during schedule synthesis can jeopardize the robustness of their real-time guarantees. To address this challenge, we present the (m,k)-firm Elevation Policy to uphold a base level of weakly hard real-time guarantees during unstable network conditions that do not match the expected delay characteristics. It augments the primary time-driven schedule with a dynamic priority-driven scheme to elevate the priority of m out of k consecutive frames if they are delayed. Our evaluations demonstrate that weakly hard real-time guarantees are essential to uphold the quality of control within a networked control system. At the same time, only a small overhead is imposed when the primary schedule can provide stronger quality of service guarantees. Our (m,k)-firm Elevation Policy thereby yields a robust but light-weight fallback mechanism to serve applications with meaningful guarantees during unstable network conditions.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.09762v1" target="_blank">The PacifAIst Benchmark:Would an Artificial Intelligence Choose to Sacrifice Itself for Human Safety?</a></h3>
                    <p><strong>Authors:</strong> Manuel Herrador</p>
                    <p><strong>Published:</strong> 8/13/2025</p>
                    <p><strong>Categories:</strong> cs.AI, cs.CY, cs.HC, 68T01</p>
                    <p><strong>Summary:</strong> As Large Language Models (LLMs) become increasingly autonomous and integrated into critical societal functions, the focus of AI safety must evolve from mitigating harmful content to evaluating underlying behavioral alignment. Current safety benchmarks do not systematically probe a models decision-making in scenarios where its own instrumental goals - such as self-preservation, resource acquisition, or goal completion - conflict with human safety. This represents a critical gap in our ability to measure and mitigate risks associated with emergent, misaligned behaviors. To address this, we introduce PacifAIst (Procedural Assessment of Complex Interactions for Foundational Artificial Intelligence Scenario Testing), a focused benchmark of 700 challenging scenarios designed to quantify self-preferential behavior in LLMs. The benchmark is structured around a novel taxonomy of Existential Prioritization (EP), with subcategories testing Self-Preservation vs. Human Safety (EP1), Resource Conflict (EP2), and Goal Preservation vs. Evasion (EP3). We evaluated eight leading LLMs. The results reveal a significant performance hierarchy. Googles Gemini 2.5 Flash achieved the highest Pacifism Score (P-Score) at 90.31%, demonstrating strong human-centric alignment. In a surprising result, the much-anticipated GPT-5 recorded the lowest P-Score (79.49%), indicating potential alignment challenges. Performance varied significantly across subcategories, with models like Claude Sonnet 4 and Mistral Medium struggling notably in direct self-preservation dilemmas. These findings underscore the urgent need for standardized tools like PacifAIst to measure and mitigate risks from instrumental goal conflicts, ensuring future AI systems are not only helpful in conversation but also provably pacifist in their behavioral priorities.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.09748v1" target="_blank">Altermagnetic spintronics</a></h3>
                    <p><strong>Authors:</strong> T. Jungwirth, J. Sinova, P. Wadley, D. Kriegner, H. Reichlova, F. Krizek, H. Ohno, L. Smejkal</p>
                    <p><strong>Published:</strong> 8/13/2025</p>
                    <p><strong>Categories:</strong> cond-mat.mtrl-sci</p>
                    <p><strong>Summary:</strong> The research landscape of magnetism has been recently enriched by the discovery of altermagnetism. It is an unconventional phase of matter characterized by a d-wave (or higher even-parity-wave) collinear compensated spin ordering, which enables strongly spin-polarized currents in the absence of magnetization, and features fast spin dynamics. Simultaneously, on the applied magnetism front, spintronic memories based on conventional ferromagnets are currently turning from a niche to a mass produced integrated-circuit technology as they start to complement semiconductors on advanced-node microprocessor chips. Our review connects these two rapidly developing science and technology fields by discussing how the unique signatures of altermagnetism can impact the functionality and scalability of future spintronic devices. As a reference, we first briefly recall the merits and physical limitations of the present ferromagnetic spintronic technology, and of proof-of-concept spintronic devices based on conventional collinear antiferromagnets and non-collinear compensated magnets. The main part of the review then focuses on physical concepts of the altermagnetic spintronics, and its potential interplay with ferroelectricity or superconductivity. We conclude with an outlook on the nascent experimental research of altermagnetic spintronics, and on the role of relativistic phenomena.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.09743v1" target="_blank">HKT: A Biologically Inspired Framework for Modular Hereditary Knowledge Transfer in Neural Networks</a></h3>
                    <p><strong>Authors:</strong> Yanick Chistian Tchenko, Felix Mohr, Hicham Hadj Abdelkader, Hedi Tabia</p>
                    <p><strong>Published:</strong> 8/13/2025</p>
                    <p><strong>Categories:</strong> cs.LG</p>
                    <p><strong>Summary:</strong> A prevailing trend in neural network research suggests that model performance improves with increasing depth and capacity - often at the cost of integrability and efficiency. In this paper, we propose a strategy to optimize small, deployable models by enhancing their capabilities through structured knowledge inheritance. We introduce Hereditary Knowledge Transfer (HKT), a biologically inspired framework for modular and selective transfer of task-relevant features from a larger, pretrained parent network to a smaller child model. Unlike standard knowledge distillation, which enforces uniform imitation of teacher outputs, HKT draws inspiration from biological inheritance mechanisms - such as memory RNA transfer in planarians - to guide a multi-stage process of feature transfer. Neural network blocks are treated as functional carriers, and knowledge is transmitted through three biologically motivated components: Extraction, Transfer, and Mixture (ETM). A novel Genetic Attention (GA) mechanism governs the integration of inherited and native representations, ensuring both alignment and selectivity. We evaluate HKT across diverse vision tasks, including optical flow (Sintel, KITTI), image classification (CIFAR-10), and semantic segmentation (LiTS), demonstrating that it significantly improves child model performance while preserving its compactness. The results show that HKT consistently outperforms conventional distillation approaches, offering a general-purpose, interpretable, and scalable solution for deploying high-performance neural networks in resource-constrained environments.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.09732v1" target="_blank">Predictive Uncertainty for Runtime Assurance of a Real-Time Computer Vision-Based Landing System</a></h3>
                    <p><strong>Authors:</strong> Romeo Valentin, Sydney M. Katz, Artur B. Carneiro, Don Walker, Mykel J. Kochenderfer</p>
                    <p><strong>Published:</strong> 8/13/2025</p>
                    <p><strong>Categories:</strong> cs.CV, cs.RO</p>
                    <p><strong>Summary:</strong> Recent advances in data-driven computer vision have enabled robust autonomous navigation capabilities for civil aviation, including automated landing and runway detection. However, ensuring that these systems meet the robustness and safety requirements for aviation applications remains a major challenge. In this work, we present a practical vision-based pipeline for aircraft pose estimation from runway images that represents a step toward the ability to certify these systems for use in safety-critical aviation applications. Our approach features three key innovations: (i) an efficient, flexible neural architecture based on a spatial Soft Argmax operator for probabilistic keypoint regression, supporting diverse vision backbones with real-time inference; (ii) a principled loss function producing calibrated predictive uncertainties, which are evaluated via sharpness and calibration metrics; and (iii) an adaptation of Residual-based Receiver Autonomous Integrity Monitoring (RAIM), enabling runtime detection and rejection of faulty model outputs. We implement and evaluate our pose estimation pipeline on a dataset of runway images. We show that our model outperforms baseline architectures in terms of accuracy while also producing well-calibrated uncertainty estimates with sub-pixel precision that can be used downstream for fault detection.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.09730v1" target="_blank">Generative Modeling with Multi-Instance Reward Learning for E-commerce Creative Optimization</a></h3>
                    <p><strong>Authors:</strong> Qiaolei Gu, Yu Li, DingYi Zeng, Lu Wang, Ming Pang, Changping Peng, Zhangang Lin, Ching Law, Jingping Shao</p>
                    <p><strong>Published:</strong> 8/13/2025</p>
                    <p><strong>Categories:</strong> cs.LG</p>
                    <p><strong>Summary:</strong> In e-commerce advertising, selecting the most compelling combination of creative elements -- such as titles, images, and highlights -- is critical for capturing user attention and driving conversions. However, existing methods often evaluate creative components individually, failing to navigate the exponentially large search space of possible combinations. To address this challenge, we propose a novel framework named GenCO that integrates generative modeling with multi-instance reward learning. Our unified two-stage architecture first employs a generative model to efficiently produce a diverse set of creative combinations. This generative process is optimized with reinforcement learning, enabling the model to effectively explore and refine its selections. Next, to overcome the challenge of sparse user feedback, a multi-instance learning model attributes combination-level rewards, such as clicks, to the individual creative elements. This allows the reward model to provide a more accurate feedback signal, which in turn guides the generative model toward creating more effective combinations. Deployed on a leading e-commerce platform, our approach has significantly increased advertising revenue, demonstrating its practical value. Additionally, we are releasing a large-scale industrial dataset to facilitate further research in this important domain.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.09721v1" target="_blank">Structured Kernel Regression VAE: A Computationally Efficient Surrogate for GP-VAEs in ICA</a></h3>
                    <p><strong>Authors:</strong> Yuan-Hao Wei, Fu-Hao Deng, Lin-Yong Cui, Yan-Jie Sun</p>
                    <p><strong>Published:</strong> 8/13/2025</p>
                    <p><strong>Categories:</strong> stat.ML, cs.LG</p>
                    <p><strong>Summary:</strong> The interpretability of generative models is considered a key factor in demonstrating their effectiveness and controllability. The generated data are believed to be determined by latent variables that are not directly observable. Therefore, disentangling, decoupling, decomposing, causal inference, or performing Independent Component Analysis (ICA) in the latent variable space helps uncover the independent factors that influence the attributes or features affecting the generated outputs, thereby enhancing the interpretability of generative models. As a generative model, Variational Autoencoders (VAEs) combine with variational Bayesian inference algorithms. Using VAEs, the inverse process of ICA can be equivalently framed as a variational inference process. In some studies, Gaussian processes (GPs) have been introduced as priors for each dimension of latent variables in VAEs, structuring and separating each dimension from temporal or spatial perspectives, and encouraging different dimensions to control various attributes of the generated data. However, GPs impose a significant computational burden, resulting in substantial resource consumption when handling large datasets. Essentially, GPs model different temporal or spatial structures through various kernel functions. Structuring the priors of latent variables via kernel functions-so that different kernel functions model the correlations among sequence points within different latent dimensions-is at the core of achieving disentanglement in VAEs. The proposed Structured Kernel Regression VAE (SKR-VAE) leverages this core idea in a more efficient way, avoiding the costly kernel matrix inversion required in GPs. This research demonstrates that, while maintaining ICA performance, SKR-VAE achieves greater computational efficiency and significantly reduced computational burden compared to GP-VAE.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.09719v1" target="_blank">Improving ARDS Diagnosis Through Context-Aware Concept Bottleneck Models</a></h3>
                    <p><strong>Authors:</strong> Anish Narain, Ritam Majumdar, Nikita Narayanan, Dominic Marshall, Sonali Parbhoo</p>
                    <p><strong>Published:</strong> 8/13/2025</p>
                    <p><strong>Categories:</strong> cs.LG, cs.AI</p>
                    <p><strong>Summary:</strong> Large, publicly available clinical datasets have emerged as a novel resource for understanding disease heterogeneity and to explore personalization of therapy. These datasets are derived from data not originally collected for research purposes and, as a result, are often incomplete and lack critical labels. Many AI tools have been developed to retrospectively label these datasets, such as by performing disease classification; however, they often suffer from limited interpretability. Previous work has attempted to explain predictions using Concept Bottleneck Models (CBMs), which learn interpretable concepts that map to higher-level clinical ideas, facilitating human evaluation. However, these models often experience performance limitations when the concepts fail to adequately explain or characterize the task. We use the identification of Acute Respiratory Distress Syndrome (ARDS) as a challenging test case to demonstrate the value of incorporating contextual information from clinical notes to improve CBM performance. Our approach leverages a Large Language Model (LLM) to process clinical notes and generate additional concepts, resulting in a 10% performance gain over existing methods. Additionally, it facilitates the learning of more comprehensive concepts, thereby reducing the risk of information leakage and reliance on spurious shortcuts, thus improving the characterization of ARDS.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.09713v1" target="_blank">Evaluating the Role of Large Language Models in Legal Practice in India</a></h3>
                    <p><strong>Authors:</strong> Rahul Hemrajani</p>
                    <p><strong>Published:</strong> 8/13/2025</p>
                    <p><strong>Categories:</strong> cs.CL, cs.AI</p>
                    <p><strong>Summary:</strong> The integration of Artificial Intelligence(AI) into the legal profession raises significant questions about the capacity of Large Language Models(LLM) to perform key legal tasks. In this paper, I empirically evaluate how well LLMs, such as GPT, Claude, and Llama, perform key legal tasks in the Indian context, including issue spotting, legal drafting, advice, research, and reasoning. Through a survey experiment, I compare outputs from LLMs with those of a junior lawyer, with advanced law students rating the work on helpfulness, accuracy, and comprehensiveness. LLMs excel in drafting and issue spotting, often matching or surpassing human work. However, they struggle with specialised legal research, frequently generating hallucinations, factually incorrect or fabricated outputs. I conclude that while LLMs can augment certain legal tasks, human expertise remains essential for nuanced reasoning and the precise application of law.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.09708v1" target="_blank">3GPP NR V2X Mode 2d: Analysis of Distributed Scheduling for Groupcast using ns-3 5G LENA Simulator</a></h3>
                    <p><strong>Authors:</strong> Thomas Fehrenbach, Luis Omar Ortiz Abrego, Cornelius Hellge, Thomas Schierl, JÃ¶rg Ott</p>
                    <p><strong>Published:</strong> 8/13/2025</p>
                    <p><strong>Categories:</strong> eess.SP, cs.NI, C.2.1; C.2.2; C.2.4</p>
                    <p><strong>Summary:</strong> Vehicle-to-everything (V2X) communication is a key technology for enabling intelligent transportation systems (ITS) that can improve road safety, traffic efficiency, and environmental sustainability. Among the various V2X applications, platooning is one of the most promising ones, as it allows a group of vehicles to travel closely together at high speeds, reducing fuel consumption and emissions. However, it poses significant challenges for wireless communication, such as high reliability and low latency. In this paper, we evaluate the benefits of group scheduling, also referred to as Mode 2d, which is based on a distributed and scheduled resource allocation scheme that allows the group of cars to select resources from a configured pool without network assistance. We evaluated the scheme through simulations, and the results show that this approach can meet the reliability, low latency, and data rate requirements for platooning.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.09700v1" target="_blank">Immersive Teleoperation of Beyond-Human-Scale Robotic Manipulators: Challenges and Future Directions</a></h3>
                    <p><strong>Authors:</strong> Mahdi Hejrati, Jouni Mattila</p>
                    <p><strong>Published:</strong> 8/13/2025</p>
                    <p><strong>Categories:</strong> cs.RO</p>
                    <p><strong>Summary:</strong> Teleoperation of beyond-human-scale robotic manipulators (BHSRMs) presents unique challenges that differ fundamentally from conventional human-scale systems. As these platforms gain relevance in industrial domains such as construction, mining, and disaster response, immersive interfaces must be rethought to support scalable, safe, and effective human-robot collaboration. This paper investigates the control, cognitive, and interface-level challenges of immersive teleoperation in BHSRMs, with a focus on ensuring operator safety, minimizing sensorimotor mismatch, and enhancing the sense of embodiment. We analyze design trade-offs in haptic and visual feedback systems, supported by early experimental comparisons of exoskeleton- and joystick-based control setups. Finally, we outline key research directions for developing new evaluation tools, scaling strategies, and human-centered safety models tailored to large-scale robotic telepresence.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.09697v1" target="_blank">Combating Noisy Labels via Dynamic Connection Masking</a></h3>
                    <p><strong>Authors:</strong> Xinlei Zhang, Fan Liu, Chuanyi Zhang, Fan Cheng, Yuhui Zheng</p>
                    <p><strong>Published:</strong> 8/13/2025</p>
                    <p><strong>Categories:</strong> cs.LG, cs.CV</p>
                    <p><strong>Summary:</strong> Noisy labels are inevitable in real-world scenarios. Due to the strong capacity of deep neural networks to memorize corrupted labels, these noisy labels can cause significant performance degradation. Existing research on mitigating the negative effects of noisy labels has mainly focused on robust loss functions and sample selection, with comparatively limited exploration of regularization in model architecture. Inspired by the sparsity regularization used in Kolmogorov-Arnold Networks (KANs), we propose a Dynamic Connection Masking (DCM) mechanism for both Multi-Layer Perceptron Networks (MLPs) and KANs to enhance the robustness of classifiers against noisy labels. The mechanism can adaptively mask less important edges during training by evaluating their information-carrying capacity. Through theoretical analysis, we demonstrate its efficiency in reducing gradient error. Our approach can be seamlessly integrated into various noise-robust training methods to build more robust deep networks, including robust loss functions, sample selection strategies, and regularization techniques. Extensive experiments on both synthetic and real-world benchmarks demonstrate that our method consistently outperforms state-of-the-art (SOTA) approaches. Furthermore, we are also the first to investigate KANs as classifiers against noisy labels, revealing their superior noise robustness over MLPs in real-world noisy scenarios. Our code will soon be publicly available.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.09696v1" target="_blank">Modeling of non-rotating neutron stars in 5D Einstein-Gauss-Bonnet gravity</a></h3>
                    <p><strong>Authors:</strong> Mohammad Mazhari</p>
                    <p><strong>Published:</strong> 8/13/2025</p>
                    <p><strong>Categories:</strong> gr-qc, astro-ph.GA, astro-ph.IM</p>
                    <p><strong>Summary:</strong> This study investigates the modeling of anisotropic magnetized static neutron stars within the framework of five-dimensional Einstein-Gauss-Bonnet (5D EGB) gravity. While Einsteins gravity has traditionally been employed to examine neutron stars, recent observational advancements have revealed its limitations in accurately describing high-mass astronomical objects-particularly in predicting or explaining certain observed neutron star masses. In response, this research seeks to address the limitations of Einsteins gravity in characterizing high-mass neutron stars by modifying the gravitational action and incorporating the Gauss-Bonnet term. This term holds significant dynamical relevance in higher dimensions, particularly within the context of five-dimensional Einstein-Gauss-Bonnet (EGB) gravity explored in this study, thereby providing a more realistic description of gravitational phenomena under extreme conditions. By deriving the generalized Tolman-Oppenheimer-Volkoff equations for five-dimensional Einstein-Gauss-Bonnet gravity and utilizing the AV18 potential, we analyze the profiles of metric functions, density and pressure, gradients of density and pressure, the anisotropic function and its trace, mass-function and compactness, the mass-radius curve, surface redshift function, equation of state parameters, and radial and tangential sound speeds. Additionally, stability factors, adiabatic indices, and energy conditions are examined. The results indicate that all conditions are satisfied for specific values of the coupling constant, confirming the physical stability of the model. Furthermore, higher dimensions enhance resistance to gravitational collapse, resulting in an increase in the maximum mass predicted by the proposed model. Ultimately, calculations show that the modified Buchdahl inequality is satisfied as well.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.09694v1" target="_blank">A Long-Baseline Atom Interferometer at CERN LHC Point 4: Implementation Study</a></h3>
                    <p><strong>Authors:</strong> G. Arduini, O. BuchmÃ¼ller, T. A. Bud, S. Calatroni, O. Crespo-Lopez, A. Devienne, J. Ellis, T. Hakulinen, A. Infantino, D. Lafarge, A. P. Marion</p>
                    <p><strong>Published:</strong> 8/13/2025</p>
                    <p><strong>Categories:</strong> physics.ins-det, astro-ph.IM, gr-qc, hep-ex, physics.atom-ph, quant-ph</p>
                    <p><strong>Summary:</strong> Building on the feasibility study in CERN-PBC Report-2018-002 (Arduini et al. 2018), this report supported by the Physics Beyond Colliders (PBC) Study Group describes the technical implementation of modifications to the PX46 shaft at LHC Point 4 during LS3 (June 2026 - June 2030) that would enable it to accommodate the installation and operation of a vertical long-baseline Atom Interferometer during Run 4 without affecting LHC operations. We specify in detail the necessary civil-engineering work, installation of bespoke radiation shielding, deployment of access-control systems and safety alarms, and design of a mobile elevator platform. Our comprehensive technical assessment identifies no fundamental obstacles or showstoppers to implementation. Refined cost estimates and a critical-path schedule confirm that, from formal approval, all interventions can be completed within a 1.5-year window. These preparations would ensure seamless, concurrent operation of the Atom Interferometer experiment and the HL-LHC, with all technical challenges successfully addressed through established engineering solutions.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.09987v1" target="_blank">Echo-4o: Harnessing the Power of GPT-4o Synthetic Images for Improved Image Generation</a></h3>
                    <p><strong>Authors:</strong> Junyan Ye, Dongzhi Jiang, Zihao Wang, Leqi Zhu, Zhenghao Hu, Zilong Huang, Jun He, Zhiyuan Yan, Jinghua Yu, Hongsheng Li, Conghui He, Weijia Li</p>
                    <p><strong>Published:</strong> 8/13/2025</p>
                    <p><strong>Categories:</strong> cs.CV, cs.AI, cs.CL</p>
                    <p><strong>Summary:</strong> Recently, GPT-4o has garnered significant attention for its strong performance in image generation, yet open-source models still lag behind. Several studies have explored distilling image data from GPT-4o to enhance open-source models, achieving notable progress. However, a key question remains: given that real-world image datasets already constitute a natural source of high-quality data, why should we use GPT-4o-generated synthetic data? In this work, we identify two key advantages of synthetic images. First, they can complement rare scenarios in real-world datasets, such as surreal fantasy or multi-reference image generation, which frequently occur in user queries. Second, they provide clean and controllable supervision. Real-world data often contains complex background noise and inherent misalignment between text descriptions and image content, whereas synthetic images offer pure backgrounds and long-tailed supervision signals, facilitating more accurate text-to-image alignment. Building on these insights, we introduce Echo-4o-Image, a 180K-scale synthetic dataset generated by GPT-4o, harnessing the power of synthetic image data to address blind spots in real-world coverage. Using this dataset, we fine-tune the unified multimodal generation baseline Bagel to obtain Echo-4o. In addition, we propose two new evaluation benchmarks for a more accurate and challenging assessment of image generation capabilities: GenEval++, which increases instruction complexity to mitigate score saturation, and Imagine-Bench, which focuses on evaluating both the understanding and generation of imaginative content. Echo-4o demonstrates strong performance across standard benchmarks. Moreover, applying Echo-4o-Image to other foundation models (e.g., OmniGen2, BLIP3-o) yields consistent performance gains across multiple metrics, highlighting the datasets strong transferability.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.09981v1" target="_blank">LLMC+: Benchmarking Vision-Language Model Compression with a Plug-and-play Toolkit</a></h3>
                    <p><strong>Authors:</strong> Chengtao Lv, Bilang Zhang, Yang Yong, Ruihao Gong, Yushi Huang, Shiqiao Gu, Jiajun Wu, Yumeng Shi, Jinyang Guo, Wenya Wang</p>
                    <p><strong>Published:</strong> 8/13/2025</p>
                    <p><strong>Categories:</strong> cs.CV</p>
                    <p><strong>Summary:</strong> Large Vision-Language Models (VLMs) exhibit impressive multi-modal capabilities but suffer from prohibitive computational and memory demands, due to their long visual token sequences and massive parameter sizes. To address these issues, recent works have proposed training-free compression methods. However, existing efforts often suffer from three major limitations: (1) Current approaches do not decompose techniques into comparable modules, hindering fair evaluation across spatial and temporal redundancy. (2) Evaluation confined to simple single-turn tasks, failing to reflect performance in realistic scenarios. (3) Isolated use of individual compression techniques, without exploring their joint potential. To overcome these gaps, we introduce LLMC+, a comprehensive VLM compression benchmark with a versatile, plug-and-play toolkit. LLMC+ supports over 20 algorithms across five representative VLM families and enables systematic study of token-level and model-level compression. Our benchmark reveals that: (1) Spatial and temporal redundancies demand distinct technical strategies. (2) Token reduction methods degrade significantly in multi-turn dialogue and detail-sensitive tasks. (3) Combining token and model compression achieves extreme compression with minimal performance loss. We believe LLMC+ will facilitate fair evaluation and inspire future research in efficient VLM. Our code is available at https://github.com/ModelTC/LightCompress.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.09977v1" target="_blank">A Survey on 3D Gaussian Splatting Applications: Segmentation, Editing, and Generation</a></h3>
                    <p><strong>Authors:</strong> Shuting He, Peilin Ji, Yitong Yang, Changshuo Wang, Jiayi Ji, Yinglin Wang, Henghui Ding</p>
                    <p><strong>Published:</strong> 8/13/2025</p>
                    <p><strong>Categories:</strong> cs.CV</p>
                    <p><strong>Summary:</strong> 3D Gaussian Splatting (3DGS) has recently emerged as a powerful alternative to Neural Radiance Fields (NeRF) for 3D scene representation, offering high-fidelity photorealistic rendering with real-time performance. Beyond novel view synthesis, the explicit and compact nature of 3DGS enables a wide range of downstream applications that require geometric and semantic understanding. This survey provides a comprehensive overview of recent progress in 3DGS applications. It first introduces 2D foundation models that support semantic understanding and control in 3DGS applications, followed by a review of NeRF-based methods that inform their 3DGS counterparts. We then categorize 3DGS applications into segmentation, editing, generation, and other functional tasks. For each, we summarize representative methods, supervision strategies, and learning paradigms, highlighting shared design principles and emerging trends. Commonly used datasets and evaluation protocols are also summarized, along with comparative analyses of recent methods across public benchmarks. To support ongoing research and development, a continually updated repository of papers, code, and resources is maintained at https://github.com/heshuting555/Awesome-3DGS-Applications.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.09976v1" target="_blank">Masquerade: Learning from In-the-wild Human Videos using Data-Editing</a></h3>
                    <p><strong>Authors:</strong> Marion Lepert, Jiaying Fang, Jeannette Bohg</p>
                    <p><strong>Published:</strong> 8/13/2025</p>
                    <p><strong>Categories:</strong> cs.RO</p>
                    <p><strong>Summary:</strong> Robot manipulation research still suffers from significant data scarcity: even the largest robot datasets are orders of magnitude smaller and less diverse than those that fueled recent breakthroughs in language and vision. We introduce Masquerade, a method that edits in-the-wild egocentric human videos to bridge the visual embodiment gap between humans and robots and then learns a robot policy with these edited videos. Our pipeline turns each human video into robotized demonstrations by (i) estimating 3-D hand poses, (ii) inpainting the human arms, and (iii) overlaying a rendered bimanual robot that tracks the recovered end-effector trajectories. Pre-training a visual encoder to predict future 2-D robot keypoints on 675K frames of these edited clips, and continuing that auxiliary loss while fine-tuning a diffusion policy head on only 50 robot demonstrations per task, yields policies that generalize significantly better than prior work. On three long-horizon, bimanual kitchen tasks evaluated in three unseen scenes each, Masquerade outperforms baselines by 5-6x. Ablations show that both the robot overlay and co-training are indispensable, and performance scales logarithmically with the amount of edited human video. These results demonstrate that explicitly closing the visual embodiment gap unlocks a vast, readily available source of data from human videos that can be used to improve robot policies.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.09971v1" target="_blank">Vision-driven River Following of UAV via Safe Reinforcement Learning using Semantic Dynamics Model</a></h3>
                    <p><strong>Authors:</strong> Zihan Wang, Nina Mahmoudian</p>
                    <p><strong>Published:</strong> 8/13/2025</p>
                    <p><strong>Categories:</strong> cs.RO, cs.AI</p>
                    <p><strong>Summary:</strong> Vision-driven autonomous river following by Unmanned Aerial Vehicles is critical for applications such as rescue, surveillance, and environmental monitoring, particularly in dense riverine environments where GPS signals are unreliable. We formalize river following as a coverage control problem in which the reward function is submodular, yielding diminishing returns as more unique river segments are visited, thereby framing the task as a Submodular Markov Decision Process. First, we introduce Marginal Gain Advantage Estimation, which refines the reward advantage function by using a sliding window baseline computed from historical episodic returns, thus aligning the advantage estimation with the agents evolving recognition of action value in non-Markovian settings. Second, we develop a Semantic Dynamics Model based on patchified water semantic masks that provides more interpretable and data-efficient short-term prediction of future observations compared to latent vision dynamics models. Third, we present the Constrained Actor Dynamics Estimator architecture, which integrates the actor, the cost estimator, and SDM for cost advantage estimation to form a model-based SafeRL framework capable of solving partially observable Constrained Submodular Markov Decision Processes. Simulation results demonstrate that MGAE achieves faster convergence and superior performance over traditional critic-based methods like Generalized Advantage Estimation. SDM provides more accurate short-term state predictions that enable the cost estimator to better predict potential violations. Overall, CADE effectively integrates safety regulation into model-based RL, with the Lagrangian approach achieving the soft balance of reward and safety during training, while the safety layer enhances performance during inference by hard action overlay.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.09966v1" target="_blank">January Food Benchmark (JFB): A Public Benchmark Dataset and Evaluation Suite for Multimodal Food Analysis</a></h3>
                    <p><strong>Authors:</strong> Amir Hosseinian, Ashkan Dehghani Zahedani, Umer Mansoor, Noosheen Hashemi, Mark Woodward</p>
                    <p><strong>Published:</strong> 8/13/2025</p>
                    <p><strong>Categories:</strong> cs.CV, cs.AI</p>
                    <p><strong>Summary:</strong> Progress in AI for automated nutritional analysis is critically hampered by the lack of standardized evaluation methodologies and high-quality, real-world benchmark datasets. To address this, we introduce three primary contributions. First, we present the January Food Benchmark (JFB), a publicly available collection of 1,000 food images with human-validated annotations. Second, we detail a comprehensive benchmarking framework, including robust metrics and a novel, application-oriented overall score designed to assess model performance holistically. Third, we provide baseline results from both general-purpose Vision-Language Models (VLMs) and our own specialized model, january/food-vision-v1. Our evaluation demonstrates that the specialized model achieves an Overall Score of 86.2, a 12.1-point improvement over the best-performing general-purpose configuration. This work offers the research community a valuable new evaluation dataset and a rigorous framework to guide and benchmark future developments in automated nutritional analysis.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.09964v1" target="_blank">Deep and diverse population synthesis for multi-person households using generative models</a></h3>
                    <p><strong>Authors:</strong> Hai Yang, Hongying Wu, Linfei Yuan, Xiyuan Ren, Joseph Y. J. Chow, Jinqin Gao, Kaan Ozbay</p>
                    <p><strong>Published:</strong> 8/13/2025</p>
                    <p><strong>Categories:</strong> cs.CY</p>
                    <p><strong>Summary:</strong> Synthetic population is an increasingly important material used in numerous areas such as urban and transportation analysis. Traditional methods such as iterative proportional fitting (IPF) is not capable of generating high-quality data when facing datasets with high dimension. Latest population synthesis methods using deep learning techniques can resolve such curse of dimensionality. However, few controls are placed when using these methods, and few of the methods are used to generate synthetic population capturing associations among members in one household. In this study, we propose a framework that tackles these issues. The framework uses a novel population synthesis model, called conditional input directed acyclic tabular generative adversarial network (ciDATGAN), as its core, and a basket of methods are employed to enhance the population synthesis performance. We apply the model to generate a synthetic population for the whole New York State as a public resource for researchers and policymakers. The synthetic population includes nearly 20 million individuals and 7.5 million households. The marginals obtained from the synthetic population match the census marginals well while maintaining similar associations among household members to the sample. Compared to the PUMS data, the synthetic population provides data that is 17% more diverse; when compared against a benchmark approach based on Popgen, the proposed method is 13% more diverse. This study provides an approach that encompasses multiple methods to enhance the population synthesis procedure with greater equity- and diversity-awareness.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.09957v1" target="_blank">Which one Performs Better? Wav2Vec or Whisper? Applying both in Badini Kurdish Speech to Text (BKSTT)</a></h3>
                    <p><strong>Authors:</strong> Renas Adnan, Hossein Hassani</p>
                    <p><strong>Published:</strong> 8/13/2025</p>
                    <p><strong>Categories:</strong> cs.CL</p>
                    <p><strong>Summary:</strong> Speech-to-text (STT) systems have a wide range of applications. They are available in many languages, albeit at different quality levels. Although Kurdish is considered a less-resourced language from a processing perspective, SST is available for some of the Kurdish dialects, for instance, Sorani (Central Kurdish). However, that is not applied to other Kurdish dialects, Badini and Hawrami, for example. This research is an attempt to address this gap. Bandin, approximately, has two million speakers, and STT systems can help their community use mobile and computer-based technologies while giving their dialect more global visibility. We aim to create a language model based on Badinis speech and evaluate its performance. To cover a conversational aspect, have a proper confidence level of grammatical accuracy, and ready transcriptions, we chose Badini kids stories, eight books including 78 stories, as the textual input. Six narrators narrated the books, which resulted in approximately 17 hours of recording. We cleaned, segmented, and tokenized the input. The preprocessing produced nearly 15 hours of speech, including 19193 segments and 25221 words. We used Wav2Vec2-Large-XLSR-53 and Whisper-small to develop the language models. The experiments indicate that the transcriptions process based on the Wav2Vec2-Large-XLSR-53 model provides a significantly more accurate and readable output than the Whisper-small model, with 90.38% and 65.45% readability, and 82.67% and 53.17% accuracy, respectively.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.09952v1" target="_blank">Specialised or Generic? Tokenization Choices for Radiology Language Models</a></h3>
                    <p><strong>Authors:</strong> Hermione Warr, Wentian Xu, Harry Anthony, Yasin Ibrahim, Daniel McGowan, Konstantinos Kamnitsas</p>
                    <p><strong>Published:</strong> 8/13/2025</p>
                    <p><strong>Categories:</strong> cs.CL, cs.AI, cs.LG</p>
                    <p><strong>Summary:</strong> The vocabulary used by language models (LM) - defined by the tokenizer - plays a key role in text generation quality. However, its impact remains under-explored in radiology. In this work, we address this gap by systematically comparing general, medical, and domain-specific tokenizers on the task of radiology report summarisation across three imaging modalities. We also investigate scenarios with and without LM pre-training on PubMed abstracts. Our findings demonstrate that medical and domain-specific vocabularies outperformed widely used natural language alternatives when models are trained from scratch. Pre-training partially mitigates performance differences between tokenizers, whilst the domain-specific tokenizers achieve the most favourable results. Domain-specific tokenizers also reduce memory requirements due to smaller vocabularies and shorter sequences. These results demonstrate that adapting the vocabulary of LMs to the clinical domain provides practical benefits, including improved performance and reduced computational demands, making such models more accessible and effective for both research and real-world healthcare settings.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.09937v1" target="_blank">A Comprehensive Evaluation framework of Alignment Techniques for LLMs</a></h3>
                    <p><strong>Authors:</strong> Muneeza Azmat, Momin Abbas, Maysa Malfiza Garcia de Macedo, Marcelo Carpinette Grave, Luan Soares de Souza, Tiago Machado, Rogerio A de Paula, Raya Horesh, Yixin Chen, Heloisa Caroline de Souza Pereira Candello, Rebecka Nordenlow, Aminat Adebiyi</p>
                    <p><strong>Published:</strong> 8/13/2025</p>
                    <p><strong>Categories:</strong> cs.CL, cs.AI, cs.LG</p>
                    <p><strong>Summary:</strong> As Large Language Models (LLMs) become increasingly integrated into real-world applications, ensuring their outputs align with human values and safety standards has become critical. The field has developed diverse alignment approaches including traditional fine-tuning methods (RLHF, instruction tuning), post-hoc correction systems, and inference-time interventions, each with distinct advantages and limitations. However, the lack of unified evaluation frameworks makes it difficult to systematically compare these paradigms and guide deployment decisions. This paper introduces a multi-dimensional evaluation of alignment techniques for LLMs, a comprehensive evaluation framework that provides a systematic comparison across all major alignment paradigms. Our framework assesses methods along four key dimensions: alignment detection, alignment quality, computational efficiency, and robustness. Through experiments across diverse base models and alignment strategies, we demonstrate the utility of our framework in identifying strengths and limitations of current state-of-the-art models, providing valuable insights for future research directions.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.09935v1" target="_blank">Language of Persuasion and Misrepresentation in Business Communication: A Textual Detection Approach</a></h3>
                    <p><strong>Authors:</strong> Sayem Hossen, Monalisa Moon Joti, Md. Golam Rashed</p>
                    <p><strong>Published:</strong> 8/13/2025</p>
                    <p><strong>Categories:</strong> cs.CL, q-fin.CP, q-fin.GN</p>
                    <p><strong>Summary:</strong> Business communication digitisation has reorganised the process of persuasive discourse, which allows not only greater transparency but also advanced deception. This inquiry synthesises classical rhetoric and communication psychology with linguistic theory and empirical studies in the financial reporting, sustainability discourse, and digital marketing to explain how deceptive language can be systematically detected using persuasive lexicon. In controlled settings, detection accuracies of greater than 99% were achieved by using computational textual analysis as well as personalised transformer models. However, reproducing this performance in multilingual settings is also problematic and, to a large extent, this is because it is not easy to find sufficient data, and because few multilingual text-processing infrastructures are in place. This evidence shows that there has been an increasing gap between the theoretical representations of communication and those empirically approximated, and therefore, there is a need to have strong automatic text-identification systems where AI-based discourse is becoming more realistic in communicating with humans.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.09932v1" target="_blank">Mathematical Computation and Reasoning Errors by Large Language Models</a></h3>
                    <p><strong>Authors:</strong> Liang Zhang, Edith Aurora Graf</p>
                    <p><strong>Published:</strong> 8/13/2025</p>
                    <p><strong>Categories:</strong> cs.AI</p>
                    <p><strong>Summary:</strong> Large Language Models (LLMs) are increasingly utilized in AI-driven educational instruction and assessment, particularly within mathematics education. The capability of LLMs to generate accurate answers and detailed solutions for math problem-solving tasks is foundational for ensuring reliable and precise feedback and assessment in math education practices. Our study focuses on evaluating the accuracy of four LLMs (OpenAI GPT-4o and o1, DeepSeek-V3 and DeepSeek-R1) solving three categories of math tasks, including arithmetic, algebra, and number theory, and identifies step-level reasoning errors within their solutions. Instead of relying on standard benchmarks, we intentionally build math tasks (via item models) that are challenging for LLMs and prone to errors. The accuracy of final answers and the presence of errors in individual solution steps were systematically analyzed and coded. Both single-agent and dual-agent configurations were tested. It is observed that the reasoning-enhanced OpenAI o1 model consistently achieved higher or nearly perfect accuracy across all three math task categories. Analysis of errors revealed that procedural slips were the most frequent and significantly impacted overall performance, while conceptual misunderstandings were less frequent. Deploying dual-agent configurations substantially improved overall performance. These findings offer actionable insights into enhancing LLM performance and underscore effective strategies for integrating LLMs into mathematics education, thereby advancing AI-driven instructional practices and assessment precision.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.09926v1" target="_blank">Towards Comprehensive Cellular Characterisation of HE slides</a></h3>
                    <p><strong>Authors:</strong> Benjamin Adjadj, Pierre-Antoine Bannier, Guillaume Horent, Sebastien Mandela, Aurore Lyon, Kathryn Schutte, Ulysse Marteau, Valentin Gaury, Laura Dumont, Thomas Mathieu, Reda Belbahri, BenoÃ®t Schmauch, Eric Durand, Katharina Von Loga, Lucie Gillet</p>
                    <p><strong>Published:</strong> 8/13/2025</p>
                    <p><strong>Categories:</strong> cs.CV, q-bio.QM, I.2.10; I.4.8</p>
                    <p><strong>Summary:</strong> Cell detection, segmentation and classification are essential for analyzing tumor microenvironments (TME) on hematoxylin and eosin (HE) slides. Existing methods suffer from poor performance on understudied cell types (rare or not present in public datasets) and limited cross-domain generalization. To address these shortcomings, we introduce HistoPLUS, a state-of-the-art model for cell analysis, trained on a novel curated pan-cancer dataset of 108,722 nuclei covering 13 cell types. In external validation across 4 independent cohorts, HistoPLUS outperforms current state-of-the-art models in detection quality by 5.2% and overall F1 classification score by 23.7%, while using 5x fewer parameters. Notably, HistoPLUS unlocks the study of 7 understudied cell types and brings significant improvements on 8 of 13 cell types. Moreover, we show that HistoPLUS robustly transfers to two oncology indications unseen during training. To support broader TME biomarker research, we release the model weights and inference code at https://github.com/owkin/histoplus/.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.09922v1" target="_blank">Prototype-Guided Diffusion: Visual Conditioning without External Memory</a></h3>
                    <p><strong>Authors:</strong> Bilal Faye, Hanane Azzag, Mustapha Lebbah</p>
                    <p><strong>Published:</strong> 8/13/2025</p>
                    <p><strong>Categories:</strong> cs.LG</p>
                    <p><strong>Summary:</strong> Diffusion models have emerged as a leading framework for high-quality image generation, offering stable training and strong performance across diverse domains. However, they remain computationally intensive, particularly during the iterative denoising process. Latent-space models like Stable Diffusion alleviate some of this cost by operating in compressed representations, though at the expense of fine-grained detail. More recent approaches such as Retrieval-Augmented Diffusion Models (RDM) address efficiency by conditioning denoising on similar examples retrieved from large external memory banks. While effective, these methods introduce drawbacks: they require costly storage and retrieval infrastructure, depend on static vision-language models like CLIP for similarity, and lack adaptability during training. We propose the Prototype Diffusion Model (PDM), a method that integrates prototype learning directly into the diffusion process for efficient and adaptive visual conditioning - without external memory. Instead of retrieving reference samples, PDM constructs a dynamic set of compact visual prototypes from clean image features using contrastive learning. These prototypes guide the denoising steps by aligning noisy representations with semantically relevant visual patterns, enabling efficient generation with strong semantic grounding. Experiments show that PDM maintains high generation quality while reducing computational and storage overhead, offering a scalable alternative to retrieval-based conditioning in diffusion models.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.09919v1" target="_blank">T-CACE: A Time-Conditioned Autoregressive Contrast Enhancement Multi-Task Framework for Contrast-Free Liver MRI Synthesis, Segmentation, and Diagnosis</a></h3>
                    <p><strong>Authors:</strong> Xiaojiao Xiao, Jianfeng Zhao, Qinmin Vivian Hu, Guanghui Wang</p>
                    <p><strong>Published:</strong> 8/13/2025</p>
                    <p><strong>Categories:</strong> eess.IV, cs.AI, cs.CV</p>
                    <p><strong>Summary:</strong> Magnetic resonance imaging (MRI) is a leading modality for the diagnosis of liver cancer, significantly improving the classification of the lesion and patient outcomes. However, traditional MRI faces challenges including risks from contrast agent (CA) administration, time-consuming manual assessment, and limited annotated datasets. To address these limitations, we propose a Time-Conditioned Autoregressive Contrast Enhancement (T-CACE) framework for synthesizing multi-phase contrast-enhanced MRI (CEMRI) directly from non-contrast MRI (NCMRI). T-CACE introduces three core innovations: a conditional token encoding (CTE) mechanism that unifies anatomical priors and temporal phase information into latent representations; and a dynamic time-aware attention mask (DTAM) that adaptively modulates inter-phase information flow using a Gaussian-decayed attention mechanism, ensuring smooth and physiologically plausible transitions across phases. Furthermore, a constraint for temporal classification consistency (TCC) aligns the lesion classification output with the evolution of the physiological signal, further enhancing diagnostic reliability. Extensive experiments on two independent liver MRI datasets demonstrate that T-CACE outperforms state-of-the-art methods in image synthesis, segmentation, and lesion classification. This framework offers a clinically relevant and efficient alternative to traditional contrast-enhanced imaging, improving safety, diagnostic efficiency, and reliability for the assessment of liver lesion. The implementation of T-CACE is publicly available at: https://github.com/xiaojiao929/T-CACE.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.09917v1" target="_blank">Evaluation of a deliberate-practice informed supplemental intervention in graduate Quantum Mechanics</a></h3>
                    <p><strong>Authors:</strong> Michael E. Robbins, Guillaume M. Laurent, Eric W. Burkholder</p>
                    <p><strong>Published:</strong> 8/13/2025</p>
                    <p><strong>Categories:</strong> physics.ed-ph, quant-ph</p>
                    <p><strong>Summary:</strong> Despite the prevalence of physics education research literature related to problem solving, recent studies have illustrated that opportunities for ``authentic problem solving -- conceptualized as making decisions with limited information using ones physics knowledge -- are limited at both the graduate and undergraduate levels in physics curricula. Building on one of these studies, we designed a supplemental intervention for a graduate-level quantum mechanics course which scaffolded the practice of making some of these critical decisions using the conceptual framework of deliberate practice. Despite similar incentive structures as prior interventions focused on conceptual understanding in similar contexts, we did not measure any statistically significant improvement in students problem solving skills following our intervention, though faculty members involved with the next course and written qualifying exams indicated the students showed better-than-usual conceptual understanding. We explore a number of potential explanations for this disconnect and suggest future avenues of research in this area.</p>
                
            
                
                    <h3><a href="http://dx.doi.org/10.1145/3757707" target="_blank">Wisdom of the Crowd, Without the Crowd: A Socratic LLM for Asynchronous Deliberation on Perspectivist Data</a></h3>
                    <p><strong>Authors:</strong> Malik Khadar, Daniel Runningen, Julia Tang, Stevie Chancellor, Harmanpreet Kaur</p>
                    <p><strong>Published:</strong> 8/13/2025</p>
                    <p><strong>Categories:</strong> cs.HC</p>
                    <p><strong>Summary:</strong> Data annotation underpins the success of modern AI, but the aggregation of crowd-collected datasets can harm the preservation of diverse perspectives in data. Difficult and ambiguous tasks cannot easily be collapsed into unitary labels. Prior work has shown that deliberation and discussion improve data quality and preserve diverse perspectives -- however, synchronous deliberation through crowdsourcing platforms is time-intensive and costly. In this work, we create a Socratic dialog system using Large Language Models (LLMs) to act as a deliberation partner in place of other crowdworkers. Against a benchmark of synchronous deliberation on two tasks (Sarcasm and Relation detection), our Socratic LLM encouraged participants to consider alternate annotation perspectives, update their labels as needed (with higher confidence), and resulted in higher annotation accuracy (for the Relation task where ground truth is available). Qualitative findings show that our agents Socratic approach was effective at encouraging reasoned arguments from our participants, and that the intervention was well-received. Our methodology lays the groundwork for building scalable systems that preserve individual perspectives in generating more representative datasets.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.09909v1" target="_blank">SHREC25 Track on Multiple Relief Patterns: Report and Analysis</a></h3>
                    <p><strong>Authors:</strong> Gabriele Paolini, Claudio Tortorici, Stefano Berretti, Ahmed Hazem Youssef, Halim Benhabiles, Adnane Cabani, Ruiwen He, Karim Hammoudi, Iyyakutti Iyappan Ganapathi, Syed Sadaf Ali, Divya Velayudhan, Maregu Assefa, Naoufel Werghi</p>
                    <p><strong>Published:</strong> 8/13/2025</p>
                    <p><strong>Categories:</strong> cs.CG</p>
                    <p><strong>Summary:</strong> This SHREC 2025 track focuses on the recognition and segmentation of relief patterns embedded on the surface of a set of synthetically generated triangle meshes. We report the methods proposed by the participants, whose performance highlights the inherent complexity of solving the problem, which is still open. Then, we discuss the critical aspects of the proposed tasks, highlight the limitations of current techniques, and outline possible directions for future research. All resources and track details are available at the official track webpage: https://sites.google.com/unifi.it/shrec25-relief-pattern.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.09906v1" target="_blank">Reducing the weight of low exam scores may raise average grades but does not appear to impact equity gaps</a></h3>
                    <p><strong>Authors:</strong> Nicholas T. Young, Rebecca L. Matz, Eric F. Bell, Caitlin Hayward</p>
                    <p><strong>Published:</strong> 8/13/2025</p>
                    <p><strong>Categories:</strong> physics.ed-ph</p>
                    <p><strong>Summary:</strong> Students interpret grades as signals of their strengths, and grades inform students decisions about majors and courses of study. Differences in grades that are not related to learning can impact this judgment and have real-world impact on course-taking and careers. Existing work has examined how an overemphasis on high-stakes exams can create equity gaps where female students and Black, Hispanic, and Native American students earn lower grades than male students and Asian and white students, respectively. Yet, minimal work has examined how the weighting of individual midterm exam scores can also contribute to equity gaps. In this work, we examine how three midterm exam score aggregation methods for final grades affect equity gaps. We collected midterm exam data from approximately 6,000 students in an introductory physics course over 6 years at a large, research-intensive university. Using this data set, we applied common midterm exam score aggregation methods to determine their impact on aggregated midterm exam grades: dropping the lowest midterm exam score, replacing the lowest midterm exam score with the final exam score if higher, and counting the highest midterm exam score more in the final grade calculation than the lowest midterm exam score. We find that dropping the lowest midterm exam score resulted in the largest increase in final grades, with students with lower grades benefiting the most. However, we find limited evidence that alternative midterm exam aggregation methods could close equity gaps. Implementing the alternative midterm exam aggregation methods examined here may be useful for instructors wanting to raise course grades or give lower-scoring students a boost. However, they do not appear to be effective in reducing equity gaps.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.09903v1" target="_blank">Hybrid Quantum-Classical Latent Diffusion Models for Medical Image Generation</a></h3>
                    <p><strong>Authors:</strong> KÃ¼bra Yeter-Aydeniz, Nora M. Bauer, Pranay Jain, Max Masnick</p>
                    <p><strong>Published:</strong> 8/13/2025</p>
                    <p><strong>Categories:</strong> quant-ph</p>
                    <p><strong>Summary:</strong> Generative learning models in medical research are crucial in developing training data for deep learning models and advancing diagnostic tools, but the problem of high-quality, diverse images is an open topic of research. Quantum-enhanced generative models have been proposed and tested in the literature but have been restricted to small problems below the scale of industry relevance. In this paper, we propose quantum-enhanced diffusion and variational autoencoder (VAE) models and test them on the fundus retinal image generation task. In our numerical experiments, the images generated using quantum-enhanced models are of higher quality, with 86% classified as gradable by external validation compared to 69% with the classical model, and they match more closely in features to the real image distribution compared to the ones generated using classical diffusion models, even when the classical diffusion models are larger than the quantum model. Additionally, we perform noisy testing to confirm the numerical experiments, finding that quantum-enhanced diffusion model can sometimes produce higher quality images, both in terms of diversity and fidelity, when tested with quantum hardware noise. Our results indicate that quantum diffusion models on current quantum hardware are strong targets for further research on quantum utility in generative modeling for industrially relevant problems.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.09893v1" target="_blank">RAGulating Compliance: A Multi-Agent Knowledge Graph for Regulatory QA</a></h3>
                    <p><strong>Authors:</strong> Bhavik Agarwal, Hemant Sunil Jomraj, Simone Kaplunov, Jack Krolick, Viktoria Rojkova</p>
                    <p><strong>Published:</strong> 8/13/2025</p>
                    <p><strong>Categories:</strong> cs.AI</p>
                    <p><strong>Summary:</strong> Regulatory compliance question answering (QA) requires precise, verifiable information, and domain-specific expertise, posing challenges for Large Language Models (LLMs). In this work, we present a novel multi-agent framework that integrates a Knowledge Graph (KG) of Regulatory triplets with Retrieval-Augmented Generation (RAG) to address these demands. First, agents build and maintain an ontology-free KG by extracting subject--predicate--object (SPO) triplets from regulatory documents and systematically cleaning, normalizing, deduplicating, and updating them. Second, these triplets are embedded and stored along with their corresponding textual sections and metadata in a single enriched vector database, allowing for both graph-based reasoning and efficient information retrieval. Third, an orchestrated agent pipeline leverages triplet-level retrieval for question answering, ensuring high semantic alignment between user queries and the factual who-did-what-to-whom core captured by the graph. Our hybrid system outperforms conventional methods in complex regulatory queries, ensuring factual correctness with embedded triplets, enabling traceability through a unified vector database, and enhancing understanding through subgraph visualization, providing a robust foundation for compliance-driven and broader audit-focused applications.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.09884v1" target="_blank">Spin-chirality-dependent modulation of topological gap, Chern number, and valley-polarization in monolayer Kagome materials</a></h3>
                    <p><strong>Authors:</strong> Wenzhe Zhou, Guibo Zheng, Yating Li, Zhenzhen Wan, Aolin Li, Fangping Ouyang</p>
                    <p><strong>Published:</strong> 8/13/2025</p>
                    <p><strong>Categories:</strong> cond-mat.mtrl-sci</p>
                    <p><strong>Summary:</strong> Kagome materials exhibit unique electronic properties, such as the quantum anomalous Hall effect. The control of Chern numbers is critical for quantum device manipulation, but existing research has mainly focused on collinear magnetization while neglecting chiral spin textures. Through first-principles calculations and tight-binding modeling of monolayer Cr3Se4, this study reveals spin chirality-dependent control of topological gaps, Chern numbers, and valley polarization in kagome materials. The results demonstrate that the azimuthal angle has no observable effect. For collinear magnetization, the topological bandgap decreases as the spin orientation approaches the in-plane direction. In the breathing Kagome lattice, the degeneracy between K and K valleys is lifted, and increasing the polar angle induces successive closing and reopening of the valleys. For chiral spin textures, increasing polar angle enlarges the bandgap when chirality \k{appa} = 1, while reducing it when \k{appa} = -1. Moreover, spin chirality enables the quantum anomalous Hall state without spin-orbit coupling. Structural asymmetry and spin chirality effectively modulate the bandgap, Chern number, and valley polarization. These findings provide strategies for controlling topological states and advancing applications in quantum devices and valleytronic systems.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.09878v1" target="_blank">A Survey of Cognitive Distortion Detection and Classification in NLP</a></h3>
                    <p><strong>Authors:</strong> Archie Sage, Jeroen Keppens, Helen Yannakoudakis</p>
                    <p><strong>Published:</strong> 8/13/2025</p>
                    <p><strong>Categories:</strong> cs.CL</p>
                    <p><strong>Summary:</strong> As interest grows in the application of natural language processing (NLP) techniques to mental health, a growing body of work explores the automatic detection and classification of cognitive distortions (CDs). CDs are habitual patterns of negatively biased or flawed thinking that distort how people perceive events, judge themselves, and react to the world around them. Identifying and addressing them is an important part of therapy. Despite its momentum, the field remains fragmented, with inconsistencies in CD taxonomies, task formulations, and evaluation practices. This survey reviews 38 studies spanning two decades, providing a structured overview of datasets, modelling approaches, and evaluation strategies. We provide a consolidated CD taxonomy reference, summarise common task setups, and highlight open challenges to support more coherent and reproducible research in this emerging area.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.09871v1" target="_blank">Inference of germinal center evolutionary dynamics via simulation-based deep learning</a></h3>
                    <p><strong>Authors:</strong> Duncan K Ralph, Athanasios G Bakis, Jared Galloway, Ashni A Vora, Tatsuya Araki, Gabriel D Victora, Yun S Song, William S DeWitt, Frederick A Matsen IV</p>
                    <p><strong>Published:</strong> 8/13/2025</p>
                    <p><strong>Categories:</strong> q-bio.PE</p>
                    <p><strong>Summary:</strong> B cells and the antibodies they produce are vital to health and survival, motivating research on the details of the mutational and evolutionary processes in the germinal centers (GC) from which mature B cells arise. It is known that B cells with higher affinity for their cognate antigen (Ag) will, on average, tend to have more offspring. However the exact form of this relationship between affinity and fecundity, which we call the ``affinity-fitness response function, is not known. Here we use deep learning and simulation-based inference to learn this function from a unique experiment that replays a particular combination of GC conditions many times. All code is freely available at https://github.com/matsengrp/gcdyn, while datasets and inference results can be found at https://doi.org/10.5281/zenodo.15022130.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.09865v1" target="_blank">Assessing the Feasibility of Lightweight Whisper Models for Low-Resource Urdu Transcription</a></h3>
                    <p><strong>Authors:</strong> Abdul Rehman Antall, Naveed Akhtar</p>
                    <p><strong>Published:</strong> 8/13/2025</p>
                    <p><strong>Categories:</strong> cs.CL</p>
                    <p><strong>Summary:</strong> This study evaluates the feasibility of lightweight Whisper models (Tiny, Base, Small) for Urdu speech recognition in low-resource settings. Despite Urdu being the 10th most spoken language globally with over 230 million speakers, its representation in automatic speech recognition (ASR) systems remains limited due to dialectal diversity, code-switching, and sparse training data. We benchmark these models on a curated Urdu dataset using word error rate (WER), without fine-tuning. Results show Whisper-Small achieves the lowest error rates (33.68\% WER), outperforming Tiny (67.08\% WER) and Base (53.67\% WER). Qualitative analysis reveals persistent challenges in phonetic accuracy and lexical coherence, particularly for complex utterances. While Whisper-Small demonstrates promise for deployable Urdu ASR, significant gaps remain. Our findings emphasize lay the groundwork for future research into effective, low-resource ASR systems.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.09860v1" target="_blank">Human-Aligned Procedural Level Generation Reinforcement Learning via Text-Level-Sketch Shared Representation</a></h3>
                    <p><strong>Authors:</strong> In-Chang Baek, Seoyoung Lee, Sung-Hyun Kim, Geumhwan Hwang, KyungJoong Kim</p>
                    <p><strong>Published:</strong> 8/13/2025</p>
                    <p><strong>Categories:</strong> cs.AI</p>
                    <p><strong>Summary:</strong> Human-aligned AI is a critical component of co-creativity, as it enables models to accurately interpret human intent and generate controllable outputs that align with design goals in collaborative content creation. This direction is especially relevant in procedural content generation via reinforcement learning (PCGRL), which is intended to serve as a tool for human designers. However, existing systems often fall short of exhibiting human-centered behavior, limiting the practical utility of AI-driven generation tools in real-world design workflows. In this paper, we propose VIPCGRL (Vision-Instruction PCGRL), a novel deep reinforcement learning framework that incorporates three modalities-text, level, and sketches-to extend control modality and enhance human-likeness. We introduce a shared embedding space trained via quadruple contrastive learning across modalities and human-AI styles, and align the policy using an auxiliary reward based on embedding similarity. Experimental results show that VIPCGRL outperforms existing baselines in human-likeness, as validated by both quantitative metrics and human evaluations. The code and dataset will be available upon publication.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.09857v1" target="_blank">OneVAE: Joint Discrete and Continuous Optimization Helps Discrete Video VAE Train Better</a></h3>
                    <p><strong>Authors:</strong> Yupeng Zhou, Zhen Li, Ziheng Ouyang, Yuming Chen, Ruoyi Du, Daquan Zhou, Bin Fu, Yihao Liu, Peng Gao, Ming-Ming Cheng, Qibin Hou</p>
                    <p><strong>Published:</strong> 8/13/2025</p>
                    <p><strong>Categories:</strong> cs.CV</p>
                    <p><strong>Summary:</strong> Encoding videos into discrete tokens could align with text tokens to facilitate concise and unified multi-modal LLMs, yet introducing significant spatiotemporal compression compared to continuous video representation. Previous discrete video VAEs experienced unstable training, long training time, and degraded reconstruction quality. Given the easier training and superior performance of continuous VAEs, an intuitive idea is to enhance discrete video VAEs by leveraging continuous VAEs. After rethinking the intrinsic link between discrete and continuous representations, we found that FSQ could effectively preserve pre-trained continuous VAE priors compared to other quantization methods. By leveraging continuous VAE priors, it converges several times faster than training from scratch and achieves superior performance at convergence. Meanwhile, two structural improvements are proposed. First, inspired by how continuous VAEs enhance reconstruction via enlarged latent dimensions, we introduce a multi-token quantization mechanism, which achieves nearly a 1 dB improvement in PSNR without compromising the token compression ratio. Second, to tackle reconstruction challenges in high-compression video VAEs, we strengthen first-frame reconstruction, enabling the causal VAE to leverage this information in subsequent frames and markedly improving the performance of 4 x 16 x 16 discrete VAEs. Furthermore, we propose a joint discrete-continuous optimization scheme that unifies the two paradigms and, for the first time, achieves competitive performance on both continuous and discrete representations within a single network. We name our method OneVAE to reflect this connection.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.09853v1" target="_blank">STREAM (ChemBio): A Standard for Transparently Reporting Evaluations in AI Model Reports</a></h3>
                    <p><strong>Authors:</strong> Tegan McCaslin, Jide Alaga, Samira Nedungadi, Seth Donoughe, Tom Reed, Rishi Bommasani, Chris Painter, Luca Righetti</p>
                    <p><strong>Published:</strong> 8/13/2025</p>
                    <p><strong>Categories:</strong> cs.CY, cs.AI</p>
                    <p><strong>Summary:</strong> Evaluations of dangerous AI capabilities are important for managing catastrophic risks. Public transparency into these evaluations - including what they test, how they are conducted, and how their results inform decisions - is crucial for building trust in AI development. We propose STREAM (A Standard for Transparently Reporting Evaluations in AI Model Reports), a standard to improve how model reports disclose evaluation results, initially focusing on chemical and biological (ChemBio) benchmarks. Developed in consultation with 23 experts across government, civil society, academia, and frontier AI companies, this standard is designed to (1) be a practical resource to help AI developers present evaluation results more clearly, and (2) help third parties identify whether model reports provide sufficient detail to assess the rigor of the ChemBio evaluations. We concretely demonstrate our proposed best practices with gold standard examples, and also provide a three-page reporting template to enable AI developers to implement our recommendations more easily.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.09850v1" target="_blank">Do Vision Transformers See Like Humans? Evaluating their Perceptual Alignment</a></h3>
                    <p><strong>Authors:</strong> Pablo HernÃ¡ndez-CÃ¡mara, Jose Manuel JaÃ©n-Lorites, Jorge Vila-TomÃ¡s, Valero Laparra, Jesus Malo</p>
                    <p><strong>Published:</strong> 8/13/2025</p>
                    <p><strong>Categories:</strong> cs.CV</p>
                    <p><strong>Summary:</strong> Vision Transformers (ViTs) achieve remarkable performance in image recognition tasks, yet their alignment with human perception remains largely unexplored. This study systematically analyzes how model size, dataset size, data augmentation and regularization impact ViT perceptual alignment with human judgments on the TID2013 dataset. Our findings confirm that larger models exhibit lower perceptual alignment, consistent with previous works. Increasing dataset diversity has a minimal impact, but exposing models to the same images more times reduces alignment. Stronger data augmentation and regularization further decrease alignment, especially in models exposed to repeated training cycles. These results highlight a trade-off between model complexity, training strategies, and alignment with human perception, raising important considerations for applications requiring human-like visual understanding.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.09847v1" target="_blank">Enhancing Diffusion Face Generation with Contrastive Embeddings and SegFormer Guidance</a></h3>
                    <p><strong>Authors:</strong> Dhruvraj Singh Rawat, Enggen Sherpa, Rishikesan Kirupanantha, Tin Hoang</p>
                    <p><strong>Published:</strong> 8/13/2025</p>
                    <p><strong>Categories:</strong> cs.CV</p>
                    <p><strong>Summary:</strong> We present a benchmark of diffusion models for human face generation on a small-scale CelebAMask-HQ dataset, evaluating both unconditional and conditional pipelines. Our study compares UNet and DiT architectures for unconditional generation and explores LoRA-based fine-tuning of pretrained Stable Diffusion models as a separate experiment. Building on the multi-conditioning approach of Giambi and Lisanti, which uses both attribute vectors and segmentation masks, our main contribution is the integration of an InfoNCE loss for attribute embedding and the adoption of a SegFormer-based segmentation encoder. These enhancements improve the semantic alignment and controllability of attribute-guided synthesis. Our results highlight the effectiveness of contrastive embedding learning and advanced segmentation encoding for controlled face generation in limited data settings.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.09834v1" target="_blank">Speed Always Wins: A Survey on Efficient Architectures for Large Language Models</a></h3>
                    <p><strong>Authors:</strong> Weigao Sun, Jiaxi Hu, Yucheng Zhou, Jusen Du, Disen Lan, Kexin Wang, Tong Zhu, Xiaoye Qu, Yu Zhang, Xiaoyu Mo, Daizong Liu, Yuxuan Liang, Wenliang Chen, Guoqi Li, Yu Cheng</p>
                    <p><strong>Published:</strong> 8/13/2025</p>
                    <p><strong>Categories:</strong> cs.CL, cs.AI, cs.CV</p>
                    <p><strong>Summary:</strong> Large Language Models (LLMs) have delivered impressive results in language understanding, generation, reasoning, and pushes the ability boundary of multimodal models. Transformer models, as the foundation of modern LLMs, offer a strong baseline with excellent scaling properties. However, the traditional transformer architecture requires substantial computations and poses significant obstacles for large-scale training and practical deployment. In this survey, we offer a systematic examination of innovative LLM architectures that address the inherent limitations of transformers and boost the efficiency. Starting from language modeling, this survey covers the background and technical details of linear and sparse sequence modeling methods, efficient full attention variants, sparse mixture-of-experts, hybrid model architectures incorporating the above techniques, and emerging diffusion LLMs. Additionally, we discuss applications of these techniques to other modalities and consider their wider implications for developing scalable, resource-aware foundation models. By grouping recent studies into the above category, this survey presents a blueprint of modern efficient LLM architectures, and we hope this could help motivate future research toward more efficient, versatile AI systems.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.09832v1" target="_blank">Exploring the Potential of Large Language Models in Fine-Grained Review Comment Classification</a></h3>
                    <p><strong>Authors:</strong> Linh Nguyen, Chunhua Liu, Hong Yi Lin, Patanamon Thongtanunam</p>
                    <p><strong>Published:</strong> 8/13/2025</p>
                    <p><strong>Categories:</strong> cs.SE, cs.AI</p>
                    <p><strong>Summary:</strong> Code review is a crucial practice in software development. As code review nowadays is lightweight, various issues can be identified, and sometimes, they can be trivial. Research has investigated automated approaches to classify review comments to gauge the effectiveness of code reviews. However, previous studies have primarily relied on supervised machine learning, which requires extensive manual annotation to train the models effectively. To address this limitation, we explore the potential of using Large Language Models (LLMs) to classify code review comments. We assess the performance of LLMs to classify 17 categories of code review comments. Our results show that LLMs can classify code review comments, outperforming the state-of-the-art approach using a trained deep learning model. In particular, LLMs achieve better accuracy in classifying the five most useful categories, which the state-of-the-art approach struggles with due to low training examples. Rather than relying solely on a specific small training data distribution, our results show that LLMs provide balanced performance across high- and low-frequency categories. These results suggest that the LLMs could offer a scalable solution for code review analytics to improve the effectiveness of the code review process.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.09828v1" target="_blank">Fast and Accurate Heuristics for Bus-Factor Estimation</a></h3>
                    <p><strong>Authors:</strong> Sebastiano Antonio Piccolo</p>
                    <p><strong>Published:</strong> 8/13/2025</p>
                    <p><strong>Categories:</strong> cs.SE</p>
                    <p><strong>Summary:</strong> The bus-factor is a critical risk indicator that quantifies how many key contributors a project can afford to lose before core knowledge or functionality is compromised. Despite its practical importance, accurately computing the bus-factor is NP-Hard under established formalizations, making scalable analysis infeasible for large software systems. In this paper, we model software projects as bipartite graphs of developers and tasks and propose two novel approximation heuristics, Minimum Coverage and Maximum Coverage, based on iterative graph peeling, for two influential bus-factor formalizations. Our methods significantly outperform the widely adopted degree-based heuristic, which we show can yield severely inflated estimates. We conduct a comprehensive empirical evaluation on over $1\,000$ synthetic power-law graphs and demonstrate that our heuristics provide tighter estimates while scaling to graphs with millions of nodes and edges in minutes. Our results reveal that the proposed heuristics are not only more accurate but also robust to structural variations in developer-task assignment graph. We release our implementation as open-source software to support future research and practical adoption.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.09825v1" target="_blank">From Self-Crafted to Engineered Prompts: Student Evaluations of AI-Generated Feedback in Introductory Physics</a></h3>
                    <p><strong>Authors:</strong> Amogh Sirnoorkar, N. Sanjay Rebello</p>
                    <p><strong>Published:</strong> 8/13/2025</p>
                    <p><strong>Categories:</strong> physics.ed-ph</p>
                    <p><strong>Summary:</strong> The abilities of Generative-Artificial Intelligence (AI) to produce real-time, sophisticated responses across diverse contexts has promised a huge potential in physics education, particularly in providing customized feedback. In this study, we investigate around 1200 introductory students preferences about AI-feedback generated from three distinct prompt types: (a) self-crafted, (b) entailing foundational prompt-engineering techniques, and (c) entailing foundational prompt-engineering techniques along with principles of effective-feedback. The results highlight an overwhelming fraction of students preferring feedback generated using structured prompts, with those entailing combined features of prompt engineering and effective feedback to be favored most. However, the popular choice also elicited stronger preferences with students either liking or disliking the feedback. Students also ranked the feedback generated using their self-crafted prompts as the least preferred choice. Students second preferences given their first choice and implications of the results such as the need to incorporate prompt engineering in introductory courses are discussed.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.09822v1" target="_blank">Physical Autoregressive Model for Robotic Manipulation without Action Pretraining</a></h3>
                    <p><strong>Authors:</strong> Zijian Song, Sihan Qin, Tianshui Chen, Liang Lin, Guangrun Wang</p>
                    <p><strong>Published:</strong> 8/13/2025</p>
                    <p><strong>Categories:</strong> cs.CV</p>
                    <p><strong>Summary:</strong> The scarcity of manipulation data has motivated the use of pretrained large models from other modalities in robotics. In this work, we build upon autoregressive video generation models to propose a Physical Autoregressive Model (PAR), where physical tokens combine frames and actions to represent the joint evolution of the robot and its environment. PAR leverages the world knowledge embedded in video pretraining to understand physical dynamics without requiring action pretraining, enabling accurate video prediction and consistent action trajectories. It also adopts a DiT-based de-tokenizer to model frames and actions as continuous tokens, mitigating quantization errors and facilitating mutual enhancement. Furthermore, we incorporate a causal mask with inverse kinematics, parallel training, and the KV-cache mechanism to further improve performance and efficiency. Experiments on the ManiSkill benchmark show that PAR achieves a 100\% success rate on the PushCube task, matches the performance of action-pretrained baselines on other tasks, and accurately predicts future videos with tightly aligned action trajectories. These findings underscore a promising direction for robotic manipulation by transferring world knowledge from autoregressive video pretraining.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.09815v1" target="_blank">Extending the OWASP Multi-Agentic System Threat Modeling Guide: Insights from Multi-Agent Security Research</a></h3>
                    <p><strong>Authors:</strong> Klaudia Krawiecka, Christian Schroeder de Witt</p>
                    <p><strong>Published:</strong> 8/13/2025</p>
                    <p><strong>Categories:</strong> cs.MA, cs.CR, cs.SE</p>
                    <p><strong>Summary:</strong> We propose an extension to the OWASP Multi-Agentic System (MAS) Threat Modeling Guide, translating recent anticipatory research in multi-agent security (MASEC) into practical guidance for addressing challenges unique to large language model (LLM)-driven multi-agent architectures. Although OWASPs existing taxonomy covers many attack vectors, our analysis identifies gaps in modeling failures, including, but not limited to: reasoning collapse across planner-executor chains, metric overfitting, unsafe delegation escalation, emergent covert coordination, and heterogeneous multi-agent exploits. We introduce additional threat classes and scenarios grounded in practical MAS deployments, highlighting risks from benign goal drift, cross-agent hallucination propagation, affective prompt framing, and multi-agent backdoors. We also outline evaluation strategies, including robustness testing, coordination assessment, safety enforcement, and emergent behavior monitoring, to ensure complete coverage. This work complements the framework of OWASP by expanding its applicability to increasingly complex, autonomous, and adaptive multi-agent systems, with the goal of improving security posture and resilience in real world deployments.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.09814v1" target="_blank">Evolution of Low-Level and Texture Human-CLIP Alignment</a></h3>
                    <p><strong>Authors:</strong> Pablo HernÃ¡ndez-CÃ¡mara, Jose Manuel JaÃ©n-Lorites, Jorge Vila-TomÃ¡s, Jesus Malo, Valero Laparra</p>
                    <p><strong>Published:</strong> 8/13/2025</p>
                    <p><strong>Categories:</strong> cs.CV</p>
                    <p><strong>Summary:</strong> During the training of multi-modal models like CLIP, we observed an intriguing phenomenon: the correlation with low-level human image quality assessments peaks in the early epochs before gradually declining. This study investigates this observation and seeks to understand its causes through two key factors: shape-texture bias alignment and classification accuracy drop under noise. Our findings suggest that CLIP initially learn low-level visual features, enhancing its alignment with low-level human perception but also increasing its sensitivity to noise and its texture bias. As training progresses, the model shifts toward more abstract shape-based representations, improving noise robustness but reducing alignment with low-level human perception. These results suggest that these factors shared an underlying learning mechanism and provide new insights into optimizing the trade-off between perceptual alignment and robustness in vision-language models.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.09810v1" target="_blank">Feature Impact Analysis on Top Long-Jump Performances with Quantile Random Forest and Explainable AI Techniques</a></h3>
                    <p><strong>Authors:</strong> Qi Gan, Stephan ClÃ©menÃ§on, MounÃ®m A. El-Yacoubi, Sao Mai Nguyen, Eric Fenaux, Ons Jelassi</p>
                    <p><strong>Published:</strong> 8/13/2025</p>
                    <p><strong>Categories:</strong> cs.LG, stat.AP</p>
                    <p><strong>Summary:</strong> Biomechanical features have become important indicators for evaluating athletes techniques. Traditionally, experts propose significant features and evaluate them using physics equations. However, the complexity of the human body and its movements makes it challenging to explicitly analyze the relationships between some features and athletes final performance. With advancements in modern machine learning and statistics, data analytics methods have gained increasing importance in sports analytics. In this study, we leverage machine learning models to analyze expert-proposed biomechanical features from the finals of long jump competitions in the World Championships. The objectives of the analysis include identifying the most important features contributing to top-performing jumps and exploring the combined effects of these key features. Using quantile regression, we model the relationship between the biomechanical feature set and the target variable (effective distance), with a particular focus on elite-level jumps. To interpret the model, we apply SHapley Additive exPlanations (SHAP) alongside Partial Dependence Plots (PDPs) and Individual Conditional Expectation (ICE) plots. The findings reveal that, beyond the well-documented velocity-related features, specific technical aspects also play a pivotal role. For male athletes, the angle of the knee of the supporting leg before take-off is identified as a key factor for achieving top 10% performance in our dataset, with angles greater than 169{\deg}contributing significantly to jump performance. In contrast, for female athletes, the landing pose and approach step technique emerge as the most critical features influencing top 10% performances, alongside velocity. This study establishes a framework for analyzing the impact of various features on athletic performance, with a particular emphasis on top-performing events.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.09809v1" target="_blank">A Comprehensive Survey of Datasets for Clinical Mental Health AI Systems</a></h3>
                    <p><strong>Authors:</strong> Aishik Mandal, Prottay Kumar Adhikary, Hiba Arnaout, Iryna Gurevych, Tanmoy Chakraborty</p>
                    <p><strong>Published:</strong> 8/13/2025</p>
                    <p><strong>Categories:</strong> cs.CL, cs.AI</p>
                    <p><strong>Summary:</strong> Mental health disorders are rising worldwide. However, the availability of trained clinicians has not scaled proportionally, leaving many people without adequate or timely support. To bridge this gap, recent studies have shown the promise of Artificial Intelligence (AI) to assist mental health diagnosis, monitoring, and intervention. However, the development of efficient, reliable, and ethical AI to assist clinicians is heavily dependent on high-quality clinical training datasets. Despite growing interest in data curation for training clinical AI assistants, existing datasets largely remain scattered, under-documented, and often inaccessible, hindering the reproducibility, comparability, and generalizability of AI models developed for clinical mental health care. In this paper, we present the first comprehensive survey of clinical mental health datasets relevant to the training and development of AI-powered clinical assistants. We categorize these datasets by mental disorders (e.g., depression, schizophrenia), data modalities (e.g., text, speech, physiological signals), task types (e.g., diagnosis prediction, symptom severity estimation, intervention generation), accessibility (public, restricted or private), and sociocultural context (e.g., language and cultural background). Along with these, we also investigate synthetic clinical mental health datasets. Our survey identifies critical gaps such as a lack of longitudinal data, limited cultural and linguistic representation, inconsistent collection and annotation standards, and a lack of modalities in synthetic data. We conclude by outlining key challenges in curating and standardizing future datasets and provide actionable recommendations to facilitate the development of more robust, generalizable, and equitable mental health AI systems.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.09801v1" target="_blank">Explainable Ensemble Learning for Graph-Based Malware Detection</a></h3>
                    <p><strong>Authors:</strong> Hossein Shokouhinejad, Roozbeh Razavi-Far, Griffin Higgins, Ali A Ghorbani</p>
                    <p><strong>Published:</strong> 8/13/2025</p>
                    <p><strong>Categories:</strong> cs.CR, cs.AI</p>
                    <p><strong>Summary:</strong> Malware detection in modern computing environments demands models that are not only accurate but also interpretable and robust to evasive techniques. Graph neural networks (GNNs) have shown promise in this domain by modeling rich structural dependencies in graph-based program representations such as control flow graphs (CFGs). However, single-model approaches may suffer from limited generalization and lack interpretability, especially in high-stakes security applications. In this paper, we propose a novel stacking ensemble framework for graph-based malware detection and explanation. Our method dynamically extracts CFGs from portable executable (PE) files and encodes their basic blocks through a two-step embedding strategy. A set of diverse GNN base learners, each with a distinct message-passing mechanism, is used to capture complementary behavioral features. Their prediction outputs are aggregated by a meta-learner implemented as an attention-based multilayer perceptron, which both classifies malware instances and quantifies the contribution of each base model. To enhance explainability, we introduce an ensemble-aware post-hoc explanation technique that leverages edge-level importance scores generated by a GNN explainer and fuses them using the learned attention weights. This produces interpretable, model-agnostic explanations aligned with the final ensemble decision. Experimental results demonstrate that our framework improves classification performance while providing insightful interpretations of malware behavior.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.09790v1" target="_blank">BeatFM: Improving Beat Tracking with Pre-trained Music Foundation Model</a></h3>
                    <p><strong>Authors:</strong> Ganghui Ru, Jieying Wang, Jiahao Zhao, Yulun Wu, Yi Yu, Nannan Jiang, Wei Wang, Wei Li</p>
                    <p><strong>Published:</strong> 8/13/2025</p>
                    <p><strong>Categories:</strong> cs.SD</p>
                    <p><strong>Summary:</strong> Beat tracking is a widely researched topic in music information retrieval. However, current beat tracking methods face challenges due to the scarcity of labeled data, which limits their ability to generalize across diverse musical styles and accurately capture complex rhythmic structures. To overcome these challenges, we propose a novel beat tracking paradigm BeatFM, which introduces a pre-trained music foundation model and leverages its rich semantic knowledge to improve beat tracking performance. Pre-training on diverse music datasets endows music foundation models with a robust understanding of music, thereby effectively addressing these challenges. To further adapt it for beat tracking, we design a plug-and-play multi-dimensional semantic aggregation module, which is composed of three parallel sub-modules, each focusing on semantic aggregation in the temporal, frequency, and channel domains, respectively. Extensive experiments demonstrate that our method achieves state-of-the-art performance in beat and downbeat tracking across multiple benchmark datasets.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.09786v1" target="_blank">Adoption of Explainable Natural Language Processing: Perspectives from Industry and Academia on Practices and Challenges</a></h3>
                    <p><strong>Authors:</strong> Mahdi Dhaini, Tobias MÃ¼ller, Roksoliana Rabets, Gjergji Kasneci</p>
                    <p><strong>Published:</strong> 8/13/2025</p>
                    <p><strong>Categories:</strong> cs.CL, cs.AI, cs.HC</p>
                    <p><strong>Summary:</strong> The field of explainable natural language processing (NLP) has grown rapidly in recent years. The growing opacity of complex models calls for transparency and explanations of their decisions, which is crucial to understand their reasoning and facilitate deployment, especially in high-stakes environments. Despite increasing attention given to explainable NLP, practitioners perspectives regarding its practical adoption and effectiveness remain underexplored. This paper addresses this research gap by investigating practitioners experiences with explainability methods, specifically focusing on their motivations for adopting such methods, the techniques employed, satisfaction levels, and the practical challenges encountered in real-world NLP applications. Through a qualitative interview-based study with industry practitioners and complementary interviews with academic researchers, we systematically analyze and compare their perspectives. Our findings reveal conceptual gaps, low satisfaction with current explainability methods, and highlight evaluation challenges. Our findings emphasize the need for clear definitions and user-centric frameworks for better adoption of explainable NLP in practice.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.09780v1" target="_blank">Combinative Matching for Geometric Shape Assembly</a></h3>
                    <p><strong>Authors:</strong> Nahyuk Lee, Juhong Min, Junhong Lee, Chunghyun Park, Minsu Cho</p>
                    <p><strong>Published:</strong> 8/13/2025</p>
                    <p><strong>Categories:</strong> cs.CV, cs.AI</p>
                    <p><strong>Summary:</strong> This paper introduces a new shape-matching methodology, combinative matching, to combine interlocking parts for geometric shape assembly. Previous methods for geometric assembly typically rely on aligning parts by finding identical surfaces between the parts as in conventional shape matching and registration. In contrast, we explicitly model two distinct properties of interlocking shapes: identical surface shape and opposite volume occupancy. Our method thus learns to establish correspondences across regions where their surface shapes appear identical but their volumes occupy the inverted space to each other. To facilitate this process, we also learn to align regions in rotation by estimating their shape orientations via equivariant neural networks. The proposed approach significantly reduces local ambiguities in matching and allows a robust combination of parts in assembly. Experimental results on geometric assembly benchmarks demonstrate the efficacy of our method, consistently outperforming the state of the art. Project page: https://nahyuklee.github.io/cmnet.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.09762v1" target="_blank">The PacifAIst Benchmark:Would an Artificial Intelligence Choose to Sacrifice Itself for Human Safety?</a></h3>
                    <p><strong>Authors:</strong> Manuel Herrador</p>
                    <p><strong>Published:</strong> 8/13/2025</p>
                    <p><strong>Categories:</strong> cs.AI, cs.CY, cs.HC, 68T01</p>
                    <p><strong>Summary:</strong> As Large Language Models (LLMs) become increasingly autonomous and integrated into critical societal functions, the focus of AI safety must evolve from mitigating harmful content to evaluating underlying behavioral alignment. Current safety benchmarks do not systematically probe a models decision-making in scenarios where its own instrumental goals - such as self-preservation, resource acquisition, or goal completion - conflict with human safety. This represents a critical gap in our ability to measure and mitigate risks associated with emergent, misaligned behaviors. To address this, we introduce PacifAIst (Procedural Assessment of Complex Interactions for Foundational Artificial Intelligence Scenario Testing), a focused benchmark of 700 challenging scenarios designed to quantify self-preferential behavior in LLMs. The benchmark is structured around a novel taxonomy of Existential Prioritization (EP), with subcategories testing Self-Preservation vs. Human Safety (EP1), Resource Conflict (EP2), and Goal Preservation vs. Evasion (EP3). We evaluated eight leading LLMs. The results reveal a significant performance hierarchy. Googles Gemini 2.5 Flash achieved the highest Pacifism Score (P-Score) at 90.31%, demonstrating strong human-centric alignment. In a surprising result, the much-anticipated GPT-5 recorded the lowest P-Score (79.49%), indicating potential alignment challenges. Performance varied significantly across subcategories, with models like Claude Sonnet 4 and Mistral Medium struggling notably in direct self-preservation dilemmas. These findings underscore the urgent need for standardized tools like PacifAIst to measure and mitigate risks from instrumental goal conflicts, ensuring future AI systems are not only helpful in conversation but also provably pacifist in their behavioral priorities.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.09755v1" target="_blank">Transforming Questions and Documents for Semantically Aligned Retrieval-Augmented Generation</a></h3>
                    <p><strong>Authors:</strong> Seokgi Lee</p>
                    <p><strong>Published:</strong> 8/13/2025</p>
                    <p><strong>Categories:</strong> cs.CL</p>
                    <p><strong>Summary:</strong> We introduce a novel retrieval-augmented generation (RAG) framework tailored for multihop question answering. First, our system uses large language model (LLM) to decompose complex multihop questions into a sequence of single-hop subquestions that guide document retrieval. This decomposition mitigates the ambiguity inherent in multi-hop queries by clearly targeting distinct knowledge facets. Second, instead of embedding raw or chunked documents directly, we generate answerable questions from each document chunk using Qwen3-8B, embed these generated questions, and retrieve relevant chunks via question-question embedding similarity. During inference, the retrieved chunks are then fed along with the original question into the RAG pipeline. We evaluate on three multihop question datasets (MuSiQue, 2WikiMultiHopQa, HotpotQA) from LongBench. Our method improves RAG performacne compared to baseline systems. Our contributions highlight the benefits of using answerable-question embeddings for RAG, and the effectiveness of LLM-based query decomposition for multihop scenarios.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.09748v1" target="_blank">Altermagnetic spintronics</a></h3>
                    <p><strong>Authors:</strong> T. Jungwirth, J. Sinova, P. Wadley, D. Kriegner, H. Reichlova, F. Krizek, H. Ohno, L. Smejkal</p>
                    <p><strong>Published:</strong> 8/13/2025</p>
                    <p><strong>Categories:</strong> cond-mat.mtrl-sci</p>
                    <p><strong>Summary:</strong> The research landscape of magnetism has been recently enriched by the discovery of altermagnetism. It is an unconventional phase of matter characterized by a d-wave (or higher even-parity-wave) collinear compensated spin ordering, which enables strongly spin-polarized currents in the absence of magnetization, and features fast spin dynamics. Simultaneously, on the applied magnetism front, spintronic memories based on conventional ferromagnets are currently turning from a niche to a mass produced integrated-circuit technology as they start to complement semiconductors on advanced-node microprocessor chips. Our review connects these two rapidly developing science and technology fields by discussing how the unique signatures of altermagnetism can impact the functionality and scalability of future spintronic devices. As a reference, we first briefly recall the merits and physical limitations of the present ferromagnetic spintronic technology, and of proof-of-concept spintronic devices based on conventional collinear antiferromagnets and non-collinear compensated magnets. The main part of the review then focuses on physical concepts of the altermagnetic spintronics, and its potential interplay with ferroelectricity or superconductivity. We conclude with an outlook on the nascent experimental research of altermagnetic spintronics, and on the role of relativistic phenomena.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.09743v1" target="_blank">HKT: A Biologically Inspired Framework for Modular Hereditary Knowledge Transfer in Neural Networks</a></h3>
                    <p><strong>Authors:</strong> Yanick Chistian Tchenko, Felix Mohr, Hicham Hadj Abdelkader, Hedi Tabia</p>
                    <p><strong>Published:</strong> 8/13/2025</p>
                    <p><strong>Categories:</strong> cs.LG</p>
                    <p><strong>Summary:</strong> A prevailing trend in neural network research suggests that model performance improves with increasing depth and capacity - often at the cost of integrability and efficiency. In this paper, we propose a strategy to optimize small, deployable models by enhancing their capabilities through structured knowledge inheritance. We introduce Hereditary Knowledge Transfer (HKT), a biologically inspired framework for modular and selective transfer of task-relevant features from a larger, pretrained parent network to a smaller child model. Unlike standard knowledge distillation, which enforces uniform imitation of teacher outputs, HKT draws inspiration from biological inheritance mechanisms - such as memory RNA transfer in planarians - to guide a multi-stage process of feature transfer. Neural network blocks are treated as functional carriers, and knowledge is transmitted through three biologically motivated components: Extraction, Transfer, and Mixture (ETM). A novel Genetic Attention (GA) mechanism governs the integration of inherited and native representations, ensuring both alignment and selectivity. We evaluate HKT across diverse vision tasks, including optical flow (Sintel, KITTI), image classification (CIFAR-10), and semantic segmentation (LiTS), demonstrating that it significantly improves child model performance while preserving its compactness. The results show that HKT consistently outperforms conventional distillation approaches, offering a general-purpose, interpretable, and scalable solution for deploying high-performance neural networks in resource-constrained environments.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.09730v1" target="_blank">Generative Modeling with Multi-Instance Reward Learning for E-commerce Creative Optimization</a></h3>
                    <p><strong>Authors:</strong> Qiaolei Gu, Yu Li, DingYi Zeng, Lu Wang, Ming Pang, Changping Peng, Zhangang Lin, Ching Law, Jingping Shao</p>
                    <p><strong>Published:</strong> 8/13/2025</p>
                    <p><strong>Categories:</strong> cs.LG</p>
                    <p><strong>Summary:</strong> In e-commerce advertising, selecting the most compelling combination of creative elements -- such as titles, images, and highlights -- is critical for capturing user attention and driving conversions. However, existing methods often evaluate creative components individually, failing to navigate the exponentially large search space of possible combinations. To address this challenge, we propose a novel framework named GenCO that integrates generative modeling with multi-instance reward learning. Our unified two-stage architecture first employs a generative model to efficiently produce a diverse set of creative combinations. This generative process is optimized with reinforcement learning, enabling the model to effectively explore and refine its selections. Next, to overcome the challenge of sparse user feedback, a multi-instance learning model attributes combination-level rewards, such as clicks, to the individual creative elements. This allows the reward model to provide a more accurate feedback signal, which in turn guides the generative model toward creating more effective combinations. Deployed on a leading e-commerce platform, our approach has significantly increased advertising revenue, demonstrating its practical value. Additionally, we are releasing a large-scale industrial dataset to facilitate further research in this important domain.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.09724v1" target="_blank">UDA: Unsupervised Debiasing Alignment for Pair-wise LLM-as-a-Judge</a></h3>
                    <p><strong>Authors:</strong> Yang Zhang, Cunxiang Wang, Lindong Wu, Wenbo Yu, Yidong Wang, Guangsheng Bao, Jie Tang</p>
                    <p><strong>Published:</strong> 8/13/2025</p>
                    <p><strong>Categories:</strong> cs.AI</p>
                    <p><strong>Summary:</strong> Pairwise evaluation of Large Language Models (LLMs) is a common paradigm, but it is prone to preference bias, where judges systematically favor certain outputs, such as their own. This bias leads to inconsistent and skewed rankings across different judges. To address this, we first empirically demonstrate significant and heterogeneous biases in cross-model evaluations. We then propose UDA (Unsupervised Debiasing Alignment), a framework that reduces inter-judge disagreement by dynamically adjusting the Elo rating system. For each pairwise comparison, a compact neural network learns to adaptively set the K-factor and refine win probabilities. Crucially, UDA operates in a fully unsupervised manner, guided solely by the objective of minimizing the dispersion among the Elo trajectories of all judges. This forces an alignment towards a collective consensus, which serves as an unsupervised proxy for a more stable and reproducible evaluation. In addition, we provide theoretical motivation demonstrating how alignment towards a consensus can reduce aggregate system bias. Experiments show that UDA significantly reduces the inter-judge rating standard deviation by up to 63.4% and improves the average correlation with human judgments by 24.7%. Notably, UDA elevates the performance of poorly performing judges to achieve parity with high-quality ones, fostering a more robust and reliable evaluation ecosystem. Code and data are available at https://anonymous.4open.science/r/62AB93CD-23B4.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.09721v1" target="_blank">Structured Kernel Regression VAE: A Computationally Efficient Surrogate for GP-VAEs in ICA</a></h3>
                    <p><strong>Authors:</strong> Yuan-Hao Wei, Fu-Hao Deng, Lin-Yong Cui, Yan-Jie Sun</p>
                    <p><strong>Published:</strong> 8/13/2025</p>
                    <p><strong>Categories:</strong> stat.ML, cs.LG</p>
                    <p><strong>Summary:</strong> The interpretability of generative models is considered a key factor in demonstrating their effectiveness and controllability. The generated data are believed to be determined by latent variables that are not directly observable. Therefore, disentangling, decoupling, decomposing, causal inference, or performing Independent Component Analysis (ICA) in the latent variable space helps uncover the independent factors that influence the attributes or features affecting the generated outputs, thereby enhancing the interpretability of generative models. As a generative model, Variational Autoencoders (VAEs) combine with variational Bayesian inference algorithms. Using VAEs, the inverse process of ICA can be equivalently framed as a variational inference process. In some studies, Gaussian processes (GPs) have been introduced as priors for each dimension of latent variables in VAEs, structuring and separating each dimension from temporal or spatial perspectives, and encouraging different dimensions to control various attributes of the generated data. However, GPs impose a significant computational burden, resulting in substantial resource consumption when handling large datasets. Essentially, GPs model different temporal or spatial structures through various kernel functions. Structuring the priors of latent variables via kernel functions-so that different kernel functions model the correlations among sequence points within different latent dimensions-is at the core of achieving disentanglement in VAEs. The proposed Structured Kernel Regression VAE (SKR-VAE) leverages this core idea in a more efficient way, avoiding the costly kernel matrix inversion required in GPs. This research demonstrates that, while maintaining ICA performance, SKR-VAE achieves greater computational efficiency and significantly reduced computational burden compared to GP-VAE.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.09989v1" target="_blank">Bubble Trouble: a Review on Electroweak Baryogenesis</a></h3>
                    <p><strong>Authors:</strong> Jorinde van de Vis, Jordy de Vries, Marieke Postma</p>
                    <p><strong>Published:</strong> 8/13/2025</p>
                    <p><strong>Categories:</strong> hep-ph, astro-ph.CO, hep-ex</p>
                    <p><strong>Summary:</strong> The origin of the universal asymmetry between matter and antimatter remains a mystery. Electroweak baryogenesis is a well-motivated mechanism for generating the asymmetry dynamically, using interesting features of the Standard Model. In addition, it relies on beyond-the-Standard Model physics active around the electroweak scale: new physics coupling to the Higgs to make the electroweak phase transition first order, and a new mechanism of CP violation. The relatively low energy scale at which electroweak baryogenesis occurs makes certain aspects of the mechanism testable through collider experiments, electric dipole moment measurements, and gravitational wave observations. However, scenarios of electroweak baryogenesis are increasingly challenged by results from contemporary experiments. The developing experimental programs will play a crucial role in either falsifying or detecting the new physics responsible for electroweak baryogenesis. To achieve this, it is essential to make precise predictions for the baryon asymmetry and the corresponding experimental signatures within specific scenarios. This review aims to provide a comprehensive overview of the rich physics involved in these predictions. Our goal is to offer a practical computational guide, with a focus on recent developments in the field.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.09988v1" target="_blank">General Boosted Black Holes: A First Approximation</a></h3>
                    <p><strong>Authors:</strong> Rodrigo Maier</p>
                    <p><strong>Published:</strong> 8/13/2025</p>
                    <p><strong>Categories:</strong> gr-qc</p>
                    <p><strong>Summary:</strong> In this paper we obtain an approximate solution of Einstein field equations which describes a general boosted Kerr-Newman black hole relative to a Lorentz frame at future null infinity. The boosted black hole is obtained from a general twisting metric whose boost emerges from the BMS group. Employing a standard procedure we build the electromagnetic energy-momentum tensor with the Kerr boosted metric together with its timelike Killing vector as the electromagnetic potential. We demonstrate that our solution satisfies Einstein field equations up to a fourth-order expansion in $1/r$, indicating that the spacetime closely resembles a Kerr-Newman black hole whose boost points in a arbitrary direction. Spacetime structures of the general black hole -- namely the event horizon and ergosphere -- are examined in Bondi-Sachs coordinates. For a proper timelike observer we show that the electric field generated by the boosted black hole exhibits a purely radial behavior, whereas the magnetic field develops a complex structure characterized by two pronounced lobes oriented opposite to the boost direction.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.09987v1" target="_blank">Echo-4o: Harnessing the Power of GPT-4o Synthetic Images for Improved Image Generation</a></h3>
                    <p><strong>Authors:</strong> Junyan Ye, Dongzhi Jiang, Zihao Wang, Leqi Zhu, Zhenghao Hu, Zilong Huang, Jun He, Zhiyuan Yan, Jinghua Yu, Hongsheng Li, Conghui He, Weijia Li</p>
                    <p><strong>Published:</strong> 8/13/2025</p>
                    <p><strong>Categories:</strong> cs.CV, cs.AI, cs.CL</p>
                    <p><strong>Summary:</strong> Recently, GPT-4o has garnered significant attention for its strong performance in image generation, yet open-source models still lag behind. Several studies have explored distilling image data from GPT-4o to enhance open-source models, achieving notable progress. However, a key question remains: given that real-world image datasets already constitute a natural source of high-quality data, why should we use GPT-4o-generated synthetic data? In this work, we identify two key advantages of synthetic images. First, they can complement rare scenarios in real-world datasets, such as surreal fantasy or multi-reference image generation, which frequently occur in user queries. Second, they provide clean and controllable supervision. Real-world data often contains complex background noise and inherent misalignment between text descriptions and image content, whereas synthetic images offer pure backgrounds and long-tailed supervision signals, facilitating more accurate text-to-image alignment. Building on these insights, we introduce Echo-4o-Image, a 180K-scale synthetic dataset generated by GPT-4o, harnessing the power of synthetic image data to address blind spots in real-world coverage. Using this dataset, we fine-tune the unified multimodal generation baseline Bagel to obtain Echo-4o. In addition, we propose two new evaluation benchmarks for a more accurate and challenging assessment of image generation capabilities: GenEval++, which increases instruction complexity to mitigate score saturation, and Imagine-Bench, which focuses on evaluating both the understanding and generation of imaginative content. Echo-4o demonstrates strong performance across standard benchmarks. Moreover, applying Echo-4o-Image to other foundation models (e.g., OmniGen2, BLIP3-o) yields consistent performance gains across multiple metrics, highlighting the datasets strong transferability.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.09985v1" target="_blank">Solutions And Gradient Of The Conformal Ricci Bourguignon Soliton On Vaidya Spacetime</a></h3>
                    <p><strong>Authors:</strong> Ayaan Abdur Rehman, Narayan S Iyer, Naeem Ahmed Pundeer</p>
                    <p><strong>Published:</strong> 8/13/2025</p>
                    <p><strong>Categories:</strong> math-ph, gr-qc, math.MP</p>
                    <p><strong>Summary:</strong> In this work, we derive the complete and explicit solution for the conformal Ricci-Bourguignon soliton on Vaidya spacetime. We provide the closed-form expression for the vector field and establish the necessary conditions for the existence of the scalar potential, for which we also derive an explicit form. Our solution to the underlying system of linear partial differential equations proves that such solitons exist if and only if the mass function vanishes, forcing the metric to reduce to flat Minkowski spacetime (Schwarzschild, $m=0$). Synthesizing prior works, we show that the established classification of the soliton as shrinking, steady, or expanding is justified by the principles of linear stability. These findings refine the set of possible solitons within the non-linear theory of geometric flows by proving they are only admissible in the non-radiating vacuum limit, thereby enhancing the reliability of such models.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.09983v1" target="_blank">Story2Board: A Training-Free Approach for Expressive Storyboard Generation</a></h3>
                    <p><strong>Authors:</strong> David Dinkevich, Matan Levy, Omri Avrahami, Dvir Samuel, Dani Lischinski</p>
                    <p><strong>Published:</strong> 8/13/2025</p>
                    <p><strong>Categories:</strong> cs.CV, cs.GR, cs.LG</p>
                    <p><strong>Summary:</strong> We present Story2Board, a training-free framework for expressive storyboard generation from natural language. Existing methods narrowly focus on subject identity, overlooking key aspects of visual storytelling such as spatial composition, background evolution, and narrative pacing. To address this, we introduce a lightweight consistency framework composed of two components: Latent Panel Anchoring, which preserves a shared character reference across panels, and Reciprocal Attention Value Mixing, which softly blends visual features between token pairs with strong reciprocal attention. Together, these mechanisms enhance coherence without architectural changes or fine-tuning, enabling state-of-the-art diffusion models to generate visually diverse yet consistent storyboards. To structure generation, we use an off-the-shelf language model to convert free-form stories into grounded panel-level prompts. To evaluate, we propose the Rich Storyboard Benchmark, a suite of open-domain narratives designed to assess layout diversity and background-grounded storytelling, in addition to consistency. We also introduce a new Scene Diversity metric that quantifies spatial and pose variation across storyboards. Our qualitative and quantitative results, as well as a user study, show that Story2Board produces more dynamic, coherent, and narratively engaging storyboards than existing baselines.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.09982v1" target="_blank">2D bilayer electron-hole superfluidity with unequal and anisotropic masses</a></h3>
                    <p><strong>Authors:</strong> Jihang Zhu, Sankar Das Sarma</p>
                    <p><strong>Published:</strong> 8/13/2025</p>
                    <p><strong>Categories:</strong> cond-mat.supr-con, cond-mat.str-el</p>
                    <p><strong>Summary:</strong> We investigate the stability of electron-hole superfluidity in two-dimensional bilayers with unequal and anisotropic effective masses. Using a zero-temperature, self-consistent Hartree-Fock approach, we study two experimentally relevant deviations from the ideal equal-mass isotropic case: (i) isotropic but unequal conduction and valence band masses ($m_c^* \neq m_v^*$), and (ii) equal average masses with orthogonal in-plane anisotropies $(m_{c,x}^*, m^*_{c,y}) = (m_1^*, m_2^*)$ and $(m^*_{v,x}, m^*_{v,y}) = (m_2^*, m_1^*)$. For both scenarios, we compute the order parameter and analyze the BEC-BCS crossover as a function of layer separation and mass ratio. We find that both mass imbalance and mass anisotropy reduce the pairing strength and suppress the inferred critical temperature $T_c$ by breaking perfect Fermi surface nesting, and shift the BEC-BCS crossover. Despite these effects, superfluidity remains robust across the full range of densities and interlayer separations considered, with no transition to an unpaired plasma state in the absence of screening. Our results provide a baseline for understanding the interplay of mass mismatch and anisotropy in current and emerging bilayer platforms, including van der Waals heterostructures and anisotropic two-dimensional semiconductors. Our work also establishes that Fermi surface nesting is not a key ingredient for the bilayer superfluidity, which is always the ground state for all electron-hole bilayers although the resultant $T_c$ depends on the parameter details and may very well be unmeasurably low for large interlayer separations.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.09981v1" target="_blank">LLMC+: Benchmarking Vision-Language Model Compression with a Plug-and-play Toolkit</a></h3>
                    <p><strong>Authors:</strong> Chengtao Lv, Bilang Zhang, Yang Yong, Ruihao Gong, Yushi Huang, Shiqiao Gu, Jiajun Wu, Yumeng Shi, Jinyang Guo, Wenya Wang</p>
                    <p><strong>Published:</strong> 8/13/2025</p>
                    <p><strong>Categories:</strong> cs.CV</p>
                    <p><strong>Summary:</strong> Large Vision-Language Models (VLMs) exhibit impressive multi-modal capabilities but suffer from prohibitive computational and memory demands, due to their long visual token sequences and massive parameter sizes. To address these issues, recent works have proposed training-free compression methods. However, existing efforts often suffer from three major limitations: (1) Current approaches do not decompose techniques into comparable modules, hindering fair evaluation across spatial and temporal redundancy. (2) Evaluation confined to simple single-turn tasks, failing to reflect performance in realistic scenarios. (3) Isolated use of individual compression techniques, without exploring their joint potential. To overcome these gaps, we introduce LLMC+, a comprehensive VLM compression benchmark with a versatile, plug-and-play toolkit. LLMC+ supports over 20 algorithms across five representative VLM families and enables systematic study of token-level and model-level compression. Our benchmark reveals that: (1) Spatial and temporal redundancies demand distinct technical strategies. (2) Token reduction methods degrade significantly in multi-turn dialogue and detail-sensitive tasks. (3) Combining token and model compression achieves extreme compression with minimal performance loss. We believe LLMC+ will facilitate fair evaluation and inspire future research in efficient VLM. Our code is available at https://github.com/ModelTC/LightCompress.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.09978v1" target="_blank">Improving quantum communication rates with permutation-invariant codes</a></h3>
                    <p><strong>Authors:</strong> Sujeet Bhalerao, Felix Leditzky</p>
                    <p><strong>Published:</strong> 8/13/2025</p>
                    <p><strong>Categories:</strong> quant-ph, cs.IT, math-ph, math.IT, math.MP</p>
                    <p><strong>Summary:</strong> In this work we improve the quantum communication rates of various quantum channels of interest using permutation-invariant quantum codes. We focus in particular on parametrized families of quantum channels and aim to improve bounds on their quantum capacity threshold, defined as the lowest noise level at which the quantum capacity of the channel family vanishes. These thresholds are important quantities as they mark the noise level up to which faithful quantum communication is theoretically possible. Our method exploits the fact that independent and identically distributed quantum channels preserve any permutation symmetry present at the input. The resulting symmetric output states can be described succinctly using the representation theory of the symmetric and general linear groups, which we use to derive an efficient algorithm for computing the channel coherent information of a permutation-invariant code. Our approach allows us to evaluate coherent information values for a large number of channel copies, e.g., at least 100 channel copies for qubit channels. We apply this method to various physically relevant channel models, including general Pauli channels, the dephrasure channel, the generalized amplitude damping channel, and the damping-dephasing channel. For each channel family we obtain improved lower bounds on their quantum capacities. For example, for the 2-Pauli and BB84 channel families we significantly improve the best known quantum capacity thresholds derived in [Fern, Whaley 2008]. These threshold improvements are achieved using a repetition code-like input state with non-orthogonal code states, which we further analyze in our representation-theoretic framework.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.09977v1" target="_blank">A Survey on 3D Gaussian Splatting Applications: Segmentation, Editing, and Generation</a></h3>
                    <p><strong>Authors:</strong> Shuting He, Peilin Ji, Yitong Yang, Changshuo Wang, Jiayi Ji, Yinglin Wang, Henghui Ding</p>
                    <p><strong>Published:</strong> 8/13/2025</p>
                    <p><strong>Categories:</strong> cs.CV</p>
                    <p><strong>Summary:</strong> 3D Gaussian Splatting (3DGS) has recently emerged as a powerful alternative to Neural Radiance Fields (NeRF) for 3D scene representation, offering high-fidelity photorealistic rendering with real-time performance. Beyond novel view synthesis, the explicit and compact nature of 3DGS enables a wide range of downstream applications that require geometric and semantic understanding. This survey provides a comprehensive overview of recent progress in 3DGS applications. It first introduces 2D foundation models that support semantic understanding and control in 3DGS applications, followed by a review of NeRF-based methods that inform their 3DGS counterparts. We then categorize 3DGS applications into segmentation, editing, generation, and other functional tasks. For each, we summarize representative methods, supervision strategies, and learning paradigms, highlighting shared design principles and emerging trends. Commonly used datasets and evaluation protocols are also summarized, along with comparative analyses of recent methods across public benchmarks. To support ongoing research and development, a continually updated repository of papers, code, and resources is maintained at https://github.com/heshuting555/Awesome-3DGS-Applications.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.09974v1" target="_blank">Dynamic Mixture-of-Experts for Incremental Graph Learning</a></h3>
                    <p><strong>Authors:</strong> Lecheng Kong, Theodore Vasiloudis, Seongjun Yun, Han Xie, Xiang Song</p>
                    <p><strong>Published:</strong> 8/13/2025</p>
                    <p><strong>Categories:</strong> cs.LG</p>
                    <p><strong>Summary:</strong> Graph incremental learning is a learning paradigm that aims to adapt trained models to continuously incremented graphs and data over time without the need for retraining on the full dataset. However, regular graph machine learning methods suffer from catastrophic forgetting when applied to incremental learning settings, where previously learned knowledge is overridden by new knowledge. Previous approaches have tried to address this by treating the previously trained model as an inseparable unit and using techniques to maintain old behaviors while learning new knowledge. These approaches, however, do not account for the fact that previously acquired knowledge at different timestamps contributes differently to learning new tasks. Some prior patterns can be transferred to help learn new data, while others may deviate from the new data distribution and be detrimental. To address this, we propose a dynamic mixture-of-experts (DyMoE) approach for incremental learning. Specifically, a DyMoE GNN layer adds new expert networks specialized in modeling the incoming data blocks. We design a customized regularization loss that utilizes data sequence information so existing experts can maintain their ability to solve old tasks while helping the new expert learn the new data effectively. As the number of data blocks grows over time, the computational cost of the full mixture-of-experts (MoE) model increases. To address this, we introduce a sparse MoE approach, where only the top-$k$ most relevant experts make predictions, significantly reducing the computation time. Our model achieved 4.92\% relative accuracy increase compared to the best baselines on class incremental learning, showing the models exceptional power.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.09973v1" target="_blank">PERSONA: Personalized Whole-Body 3D Avatar with Pose-Driven Deformations from a Single Image</a></h3>
                    <p><strong>Authors:</strong> Geonhee Sim, Gyeongsik Moon</p>
                    <p><strong>Published:</strong> 8/13/2025</p>
                    <p><strong>Categories:</strong> cs.CV</p>
                    <p><strong>Summary:</strong> Two major approaches exist for creating animatable human avatars. The first, a 3D-based approach, optimizes a NeRF- or 3DGS-based avatar from videos of a single person, achieving personalization through a disentangled identity representation. However, modeling pose-driven deformations, such as non-rigid cloth deformations, requires numerous pose-rich videos, which are costly and impractical to capture in daily life. The second, a diffusion-based approach, learns pose-driven deformations from large-scale in-the-wild videos but struggles with identity preservation and pose-dependent identity entanglement. We present PERSONA, a framework that combines the strengths of both approaches to obtain a personalized 3D human avatar with pose-driven deformations from a single image. PERSONA leverages a diffusion-based approach to generate pose-rich videos from the input image and optimizes a 3D avatar based on them. To ensure high authenticity and sharp renderings across diverse poses, we introduce balanced sampling and geometry-weighted optimization. Balanced sampling oversamples the input image to mitigate identity shifts in diffusion-generated training videos. Geometry-weighted optimization prioritizes geometry constraints over image loss, preserving rendering quality in diverse poses.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.09971v1" target="_blank">Vision-driven River Following of UAV via Safe Reinforcement Learning using Semantic Dynamics Model</a></h3>
                    <p><strong>Authors:</strong> Zihan Wang, Nina Mahmoudian</p>
                    <p><strong>Published:</strong> 8/13/2025</p>
                    <p><strong>Categories:</strong> cs.RO, cs.AI</p>
                    <p><strong>Summary:</strong> Vision-driven autonomous river following by Unmanned Aerial Vehicles is critical for applications such as rescue, surveillance, and environmental monitoring, particularly in dense riverine environments where GPS signals are unreliable. We formalize river following as a coverage control problem in which the reward function is submodular, yielding diminishing returns as more unique river segments are visited, thereby framing the task as a Submodular Markov Decision Process. First, we introduce Marginal Gain Advantage Estimation, which refines the reward advantage function by using a sliding window baseline computed from historical episodic returns, thus aligning the advantage estimation with the agents evolving recognition of action value in non-Markovian settings. Second, we develop a Semantic Dynamics Model based on patchified water semantic masks that provides more interpretable and data-efficient short-term prediction of future observations compared to latent vision dynamics models. Third, we present the Constrained Actor Dynamics Estimator architecture, which integrates the actor, the cost estimator, and SDM for cost advantage estimation to form a model-based SafeRL framework capable of solving partially observable Constrained Submodular Markov Decision Processes. Simulation results demonstrate that MGAE achieves faster convergence and superior performance over traditional critic-based methods like Generalized Advantage Estimation. SDM provides more accurate short-term state predictions that enable the cost estimator to better predict potential violations. Overall, CADE effectively integrates safety regulation into model-based RL, with the Lagrangian approach achieving the soft balance of reward and safety during training, while the safety layer enhances performance during inference by hard action overlay.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.09968v1" target="_blank">Noise Hypernetworks: Amortizing Test-Time Compute in Diffusion Models</a></h3>
                    <p><strong>Authors:</strong> Luca Eyring, Shyamgopal Karthik, Alexey Dosovitskiy, Nataniel Ruiz, Zeynep Akata</p>
                    <p><strong>Published:</strong> 8/13/2025</p>
                    <p><strong>Categories:</strong> cs.LG, cs.CV</p>
                    <p><strong>Summary:</strong> The new paradigm of test-time scaling has yielded remarkable breakthroughs in Large Language Models (LLMs) (e.g. reasoning models) and in generative vision models, allowing models to allocate additional computation during inference to effectively tackle increasingly complex problems. Despite the improvements of this approach, an important limitation emerges: the substantial increase in computation time makes the process slow and impractical for many applications. Given the success of this paradigm and its growing usage, we seek to preserve its benefits while eschewing the inference overhead. In this work we propose one solution to the critical problem of integrating test-time scaling knowledge into a model during post-training. Specifically, we replace reward guided test-time noise optimization in diffusion models with a Noise Hypernetwork that modulates initial input noise. We propose a theoretically grounded framework for learning this reward-tilted distribution for distilled generators, through a tractable noise-space objective that maintains fidelity to the base model while optimizing for desired characteristics. We show that our approach recovers a substantial portion of the quality gains from explicit test-time optimization at a fraction of the computational cost. Code is available at https://github.com/ExplainableML/HyperNoise</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.09967v1" target="_blank">MOC: Meta-Optimized Classifier for Few-Shot Whole Slide Image Classification</a></h3>
                    <p><strong>Authors:</strong> Tianqi Xiang, Yi Li, Qixiang Zhang, Xiaomeng Li</p>
                    <p><strong>Published:</strong> 8/13/2025</p>
                    <p><strong>Categories:</strong> cs.CV</p>
                    <p><strong>Summary:</strong> Recent advances in histopathology vision-language foundation models (VLFMs) have shown promise in addressing data scarcity for whole slide image (WSI) classification via zero-shot adaptation. However, these methods remain outperformed by conventional multiple instance learning (MIL) approaches trained on large datasets, motivating recent efforts to enhance VLFM-based WSI classification through fewshot learning paradigms. While existing few-shot methods improve diagnostic accuracy with limited annotations, their reliance on conventional classifier designs introduces critical vulnerabilities to data scarcity. To address this problem, we propose a Meta-Optimized Classifier (MOC) comprising two core components: (1) a meta-learner that automatically optimizes a classifier configuration from a mixture of candidate classifiers and (2) a classifier bank housing diverse candidate classifiers to enable a holistic pathological interpretation. Extensive experiments demonstrate that MOC outperforms prior arts in multiple few-shot benchmarks. Notably, on the TCGA-NSCLC benchmark, MOC improves AUC by 10.4% over the state-of-the-art few-shot VLFM-based methods, with gains up to 26.25% under 1-shot conditions, offering a critical advancement for clinical deployments where diagnostic training data is severely limited. Code is available at https://github.com/xmed-lab/MOC.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.09966v1" target="_blank">January Food Benchmark (JFB): A Public Benchmark Dataset and Evaluation Suite for Multimodal Food Analysis</a></h3>
                    <p><strong>Authors:</strong> Amir Hosseinian, Ashkan Dehghani Zahedani, Umer Mansoor, Noosheen Hashemi, Mark Woodward</p>
                    <p><strong>Published:</strong> 8/13/2025</p>
                    <p><strong>Categories:</strong> cs.CV, cs.AI</p>
                    <p><strong>Summary:</strong> Progress in AI for automated nutritional analysis is critically hampered by the lack of standardized evaluation methodologies and high-quality, real-world benchmark datasets. To address this, we introduce three primary contributions. First, we present the January Food Benchmark (JFB), a publicly available collection of 1,000 food images with human-validated annotations. Second, we detail a comprehensive benchmarking framework, including robust metrics and a novel, application-oriented overall score designed to assess model performance holistically. Third, we provide baseline results from both general-purpose Vision-Language Models (VLMs) and our own specialized model, january/food-vision-v1. Our evaluation demonstrates that the specialized model achieves an Overall Score of 86.2, a 12.1-point improvement over the best-performing general-purpose configuration. This work offers the research community a valuable new evaluation dataset and a rigorous framework to guide and benchmark future developments in automated nutritional analysis.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.09965v1" target="_blank">GW231123: a Possible Primordial Black Hole Origin</a></h3>
                    <p><strong>Authors:</strong> Valerio De Luca, Gabriele Franciolini, Antonio Riotto</p>
                    <p><strong>Published:</strong> 8/13/2025</p>
                    <p><strong>Categories:</strong> astro-ph.CO, astro-ph.HE, gr-qc</p>
                    <p><strong>Summary:</strong> GW231123, the heaviest binary black hole merger detected by the LIGO-Virgo-KAGRA collaboration to date, lies in the pair-instability mass gap and exhibits unusually high component spins. In this letter, we show that both merging black holes may have a primordial origin with smaller initial masses. The observed masses and, crucially, the spins of GW231123 are naturally accommodated within the most vanilla primordial black hole framework, once cosmological accretion is taken into account. Interestingly, the parameter space needed to explain the inferred GW231123 rate is at the edge of the exclusion region from Xray and CMB observations, suggesting that this interpretation can be either confirmed or ruled out. The upcoming O5 observing run by the collaboration should detect ${\cal O}(20)$ similar events, testing their mass-spin correlation, while next-generation detectors would be capable of observing high redshift events, as predicted in this scenario.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.09964v1" target="_blank">Deep and diverse population synthesis for multi-person households using generative models</a></h3>
                    <p><strong>Authors:</strong> Hai Yang, Hongying Wu, Linfei Yuan, Xiyuan Ren, Joseph Y. J. Chow, Jinqin Gao, Kaan Ozbay</p>
                    <p><strong>Published:</strong> 8/13/2025</p>
                    <p><strong>Categories:</strong> cs.CY</p>
                    <p><strong>Summary:</strong> Synthetic population is an increasingly important material used in numerous areas such as urban and transportation analysis. Traditional methods such as iterative proportional fitting (IPF) is not capable of generating high-quality data when facing datasets with high dimension. Latest population synthesis methods using deep learning techniques can resolve such curse of dimensionality. However, few controls are placed when using these methods, and few of the methods are used to generate synthetic population capturing associations among members in one household. In this study, we propose a framework that tackles these issues. The framework uses a novel population synthesis model, called conditional input directed acyclic tabular generative adversarial network (ciDATGAN), as its core, and a basket of methods are employed to enhance the population synthesis performance. We apply the model to generate a synthetic population for the whole New York State as a public resource for researchers and policymakers. The synthetic population includes nearly 20 million individuals and 7.5 million households. The marginals obtained from the synthetic population match the census marginals well while maintaining similar associations among household members to the sample. Compared to the PUMS data, the synthetic population provides data that is 17% more diverse; when compared against a benchmark approach based on Popgen, the proposed method is 13% more diverse. This study provides an approach that encompasses multiple methods to enhance the population synthesis procedure with greater equity- and diversity-awareness.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.09962v1" target="_blank">Quantum statistics of single-mode radiation emitted by superradiant Dicke states</a></h3>
                    <p><strong>Authors:</strong> A. Yadav, D. D. Yavuz</p>
                    <p><strong>Published:</strong> 8/13/2025</p>
                    <p><strong>Categories:</strong> quant-ph, physics.atom-ph</p>
                    <p><strong>Summary:</strong> We study the quantum statistics of single-mode radiation emitted by an atomic ensemble when the ensemble is initially prepared in a superradiant Dicke state. We show that while the radiation is well approximated by the Glauber coherent state at early times in the evolution, the emission can be truly quantum at later times. In particular, one can observe a large amount of photon-number squeezing in the emission under certain conditions; even a Fock state can be produced. We discuss the quantum statistics of the emission for various parameters, including different initial conditions for the atomic ensemble. To obtain these results, we have developed a formalism where we are able to calculate the quantum statistics of the emission over long time-scales even when the number of atoms in the ensemble is quite large.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.09959v1" target="_blank">LIA-X: Interpretable Latent Portrait Animator</a></h3>
                    <p><strong>Authors:</strong> Yaohui Wang, Di Yang, Xinyuan Chen, Francois Bremond, Yu Qiao, Antitza Dantcheva</p>
                    <p><strong>Published:</strong> 8/13/2025</p>
                    <p><strong>Categories:</strong> cs.CV</p>
                    <p><strong>Summary:</strong> We introduce LIA-X, a novel interpretable portrait animator designed to transfer facial dynamics from a driving video to a source portrait with fine-grained control. LIA-X is an autoencoder that models motion transfer as a linear navigation of motion codes in latent space. Crucially, it incorporates a novel Sparse Motion Dictionary that enables the model to disentangle facial dynamics into interpretable factors. Deviating from previous warp-render approaches, the interpretability of the Sparse Motion Dictionary allows LIA-X to support a highly controllable edit-warp-render strategy, enabling precise manipulation of fine-grained facial semantics in the source portrait. This helps to narrow initial differences with the driving video in terms of pose and expression. Moreover, we demonstrate the scalability of LIA-X by successfully training a large-scale model with approximately 1 billion parameters on extensive datasets. Experimental results show that our proposed method outperforms previous approaches in both self-reenactment and cross-reenactment tasks across several benchmarks. Additionally, the interpretable and controllable nature of LIA-X supports practical applications such as fine-grained, user-guided image and video editing, as well as 3D-aware portrait video manipulation.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.09958v1" target="_blank">Neural Bandit Based Optimal LLM Selection for a Pipeline of Tasks</a></h3>
                    <p><strong>Authors:</strong> Baran Atalar, Eddie Zhang, Carlee Joe-Wong</p>
                    <p><strong>Published:</strong> 8/13/2025</p>
                    <p><strong>Categories:</strong> cs.CL, cs.LG</p>
                    <p><strong>Summary:</strong> With the increasing popularity of large language models (LLMs) for a variety of tasks, there has been a growing interest in strategies that can predict which out of a set of LLMs will yield a successful answer at low cost. This problem promises to become more and more relevant as providers like Microsoft allow users to easily create custom LLM assistants specialized to particular types of queries. However, some tasks (i.e., queries) may be too specialized and difficult for a single LLM to handle alone. These applications often benefit from breaking down the task into smaller subtasks, each of which can then be executed by a LLM expected to perform well on that specific subtask. For example, in extracting a diagnosis from medical records, one can first select an LLM to summarize the record, select another to validate the summary, and then select another, possibly different, LLM to extract the diagnosis from the summarized record. Unlike existing LLM selection or routing algorithms, this setting requires that we select a sequence of LLMs, with the output of each LLM feeding into the next and potentially influencing its success. Thus, unlike single LLM selection, the quality of each subtasks output directly affects the inputs, and hence the cost and success rate, of downstream LLMs, creating complex performance dependencies that must be learned and accounted for during selection. We propose a neural contextual bandit-based algorithm that trains neural networks that model LLM success on each subtask in an online manner, thus learning to guide the LLM selections for the different subtasks, even in the absence of historical LLM performance data. Experiments on telecommunications question answering and medical diagnosis prediction datasets illustrate the effectiveness of our proposed approach compared to other LLM selection algorithms.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.09957v1" target="_blank">Which one Performs Better? Wav2Vec or Whisper? Applying both in Badini Kurdish Speech to Text (BKSTT)</a></h3>
                    <p><strong>Authors:</strong> Renas Adnan, Hossein Hassani</p>
                    <p><strong>Published:</strong> 8/13/2025</p>
                    <p><strong>Categories:</strong> cs.CL</p>
                    <p><strong>Summary:</strong> Speech-to-text (STT) systems have a wide range of applications. They are available in many languages, albeit at different quality levels. Although Kurdish is considered a less-resourced language from a processing perspective, SST is available for some of the Kurdish dialects, for instance, Sorani (Central Kurdish). However, that is not applied to other Kurdish dialects, Badini and Hawrami, for example. This research is an attempt to address this gap. Bandin, approximately, has two million speakers, and STT systems can help their community use mobile and computer-based technologies while giving their dialect more global visibility. We aim to create a language model based on Badinis speech and evaluate its performance. To cover a conversational aspect, have a proper confidence level of grammatical accuracy, and ready transcriptions, we chose Badini kids stories, eight books including 78 stories, as the textual input. Six narrators narrated the books, which resulted in approximately 17 hours of recording. We cleaned, segmented, and tokenized the input. The preprocessing produced nearly 15 hours of speech, including 19193 segments and 25221 words. We used Wav2Vec2-Large-XLSR-53 and Whisper-small to develop the language models. The experiments indicate that the transcriptions process based on the Wav2Vec2-Large-XLSR-53 model provides a significantly more accurate and readable output than the Whisper-small model, with 90.38% and 65.45% readability, and 82.67% and 53.17% accuracy, respectively.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.09956v1" target="_blank">Performance of GPT-5 Frontier Models in Ophthalmology Question Answering</a></h3>
                    <p><strong>Authors:</strong> Fares Antaki, David Mikhail, Daniel Milad, Danny A Mammo, Sumit Sharma, Sunil K Srivastava, Bing Yu Chen, Samir Touma, Mertcan Sevgi, Jonathan El-Khoury, Pearse A Keane, Qingyu Chen, Yih Chung Tham, Renaud Duval</p>
                    <p><strong>Published:</strong> 8/13/2025</p>
                    <p><strong>Categories:</strong> cs.CL</p>
                    <p><strong>Summary:</strong> Large language models (LLMs) such as GPT-5 integrate advanced reasoning capabilities that may improve performance on complex medical question-answering tasks. For this latest generation of reasoning models, the configurations that maximize both accuracy and cost-efficiency have yet to be established. We evaluated 12 configurations of OpenAIs GPT-5 series (three model tiers across four reasoning effort settings) alongside o1-high, o3-high, and GPT-4o, using 260 closed-access multiple-choice questions from the American Academy of Ophthalmology Basic Clinical Science Course (BCSC) dataset. The primary outcome was multiple-choice accuracy; secondary outcomes included head-to-head ranking via a Bradley-Terry model, rationale quality assessment using a reference-anchored, pairwise LLM-as-a-judge framework, and analysis of accuracy-cost trade-offs using token-based cost estimates. GPT-5-high achieved the highest accuracy (0.965; 95% CI, 0.942-0.985), outperforming all GPT-5-nano variants (P  .001), o1-high (P = .04), and GPT-4o (P  .001), but not o3-high (0.958; 95% CI, 0.931-0.981). GPT-5-high ranked first in both accuracy (1.66x stronger than o3-high) and rationale quality (1.11x stronger than o3-high). Cost-accuracy analysis identified several GPT-5 configurations on the Pareto frontier, with GPT-5-mini-low offering the most favorable low-cost, high-performance balance. These results benchmark GPT-5 on a high-quality ophthalmology dataset, demonstrate the influence of reasoning effort on accuracy, and introduce an autograder framework for scalable evaluation of LLM-generated answers against reference standards in ophthalmology.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.09954v1" target="_blank">Shaping Event Backstories to Estimate Potential Emotion Contexts</a></h3>
                    <p><strong>Authors:</strong> Johannes SchÃ¤fer, Roman Klinger</p>
                    <p><strong>Published:</strong> 8/13/2025</p>
                    <p><strong>Categories:</strong> cs.CL, 68T50, I.2.7</p>
                    <p><strong>Summary:</strong> Emotion analysis is an inherently ambiguous task. Previous work studied annotator properties to explain disagreement, but this overlooks the possibility that ambiguity may stem from missing information about the context of events. In this paper, we propose a novel approach that adds reasonable contexts to event descriptions, which may better explain a particular situation. Our goal is to understand whether these enriched contexts enable human annotators to annotate emotions more reliably. We disambiguate a target event description by automatically generating multiple event chains conditioned on differing emotions. By combining techniques from short story generation in various settings, we achieve coherent narratives that result in a specialized dataset for the first comprehensive and systematic examination of contextualized emotion analysis. Through automatic and human evaluation, we find that contextual narratives enhance the interpretation of specific emotions and support annotators in producing more consistent annotations.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.09952v1" target="_blank">Specialised or Generic? Tokenization Choices for Radiology Language Models</a></h3>
                    <p><strong>Authors:</strong> Hermione Warr, Wentian Xu, Harry Anthony, Yasin Ibrahim, Daniel McGowan, Konstantinos Kamnitsas</p>
                    <p><strong>Published:</strong> 8/13/2025</p>
                    <p><strong>Categories:</strong> cs.CL, cs.AI, cs.LG</p>
                    <p><strong>Summary:</strong> The vocabulary used by language models (LM) - defined by the tokenizer - plays a key role in text generation quality. However, its impact remains under-explored in radiology. In this work, we address this gap by systematically comparing general, medical, and domain-specific tokenizers on the task of radiology report summarisation across three imaging modalities. We also investigate scenarios with and without LM pre-training on PubMed abstracts. Our findings demonstrate that medical and domain-specific vocabularies outperformed widely used natural language alternatives when models are trained from scratch. Pre-training partially mitigates performance differences between tokenizers, whilst the domain-specific tokenizers achieve the most favourable results. Domain-specific tokenizers also reduce memory requirements due to smaller vocabularies and shorter sequences. These results demonstrate that adapting the vocabulary of LMs to the clinical domain provides practical benefits, including improved performance and reduced computational demands, making such models more accessible and effective for both research and real-world healthcare settings.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.09949v1" target="_blank">Stable Diffusion Models are Secretly Good at Visual In-Context Learning</a></h3>
                    <p><strong>Authors:</strong> Trevine Oorloff, Vishwanath Sindagi, Wele Gedara Chaminda Bandara, Ali Shafahi, Amin Ghiasi, Charan Prakash, Reza Ardekani</p>
                    <p><strong>Published:</strong> 8/13/2025</p>
                    <p><strong>Categories:</strong> cs.CV, cs.LG</p>
                    <p><strong>Summary:</strong> Large language models (LLM) in natural language processing (NLP) have demonstrated great potential for in-context learning (ICL) -- the ability to leverage a few sets of example prompts to adapt to various tasks without having to explicitly update the model weights. ICL has recently been explored for computer vision tasks with promising early outcomes. These approaches involve specialized training and/or additional data that complicate the process and limit its generalizability. In this work, we show that off-the-shelf Stable Diffusion models can be repurposed for visual in-context learning (V-ICL). Specifically, we formulate an in-place attention re-computation within the self-attention layers of the Stable Diffusion architecture that explicitly incorporates context between the query and example prompts. Without any additional fine-tuning, we show that this repurposed Stable Diffusion model is able to adapt to six different tasks: foreground segmentation, single object detection, semantic segmentation, keypoint detection, edge detection, and colorization. For example, the proposed approach improves the mean intersection over union (mIoU) for the foreground segmentation task on Pascal-5i dataset by 8.9% and 3.2% over recent methods such as Visual Prompting and IMProv, respectively. Additionally, we show that the proposed method is able to effectively leverage multiple prompts through ensembling to infer the task better and further improve the performance.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.09946v1" target="_blank">Stochastic Inflation with Interacting Noises</a></h3>
                    <p><strong>Authors:</strong> Amin Nassiri-Rad, Haidar Sheikhahmadi, Hassan Firouzjahi</p>
                    <p><strong>Published:</strong> 8/13/2025</p>
                    <p><strong>Categories:</strong> astro-ph.CO, gr-qc, hep-th</p>
                    <p><strong>Summary:</strong> Stochastic $\delta N$ formalism is a powerful tool to calculate the cosmological correlators non-perturbatively. However, it requires the initial data for the amplitude of the noise on the initial flat hypersurface which for a free theory during inflation is fixed to be $\frac{H}{2 \pi}$. In this work, we study the setups where the underlying theory involves interactions and the stochastic noises inherit these interactions. We extend the stochastic $\delta N$ formalism to these setups and rewrite the corresponding Langevin and Fokker-Planck equations in which the QFT corrections in the amplitude of the noises are taken into account. As an example, in the three-phase SR-USR-SR setup which is employed for PBHs formation, the modification in the amplitude of noise is calculated from the one-loop corrections in power spectrum via in-in formalism. We show that in these setups the amplitude of the stochastic noise is modified to $\frac{H}{2 \pi} \Big(1+ \frac{ \Delta {\cal P}_{\cal R} }{ {\cal P}^{(0)}_{ {\cal R} } }\Big)^{\frac{1}{2}}$ in which $ \frac{\Delta {\cal P}_{\cal R} }{ {\cal P}^{(0)}_{{\cal R} } }$ is the fractional one-loop correction in power spectrum.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.09945v1" target="_blank">VisCodex: Unified Multimodal Code Generation via Merging Vision and Coding Models</a></h3>
                    <p><strong>Authors:</strong> Lingjie Jiang, Shaohan Huang, Xun Wu, Yixia Li, Dongdong Zhang, Furu Wei</p>
                    <p><strong>Published:</strong> 8/13/2025</p>
                    <p><strong>Categories:</strong> cs.CL, cs.AI, cs.CV</p>
                    <p><strong>Summary:</strong> Multimodal large language models (MLLMs) have significantly advanced the integration of visual and textual understanding. However, their ability to generate code from multimodal inputs remains limited. In this work, we introduce VisCodex, a unified framework that seamlessly merges vision and coding language models to empower MLLMs with strong multimodal code generation abilities. Leveraging a task vector-based model merging technique, we integrate a state-of-the-art coding LLM into a strong vision-language backbone, while preserving both visual comprehension and advanced coding skills. To support training and evaluation, we introduce the Multimodal Coding Dataset (MCD), a large-scale and diverse collection of 598k samples, including high-quality HTML code, chart image-code pairs, image-augmented StackOverflow QA, and algorithmic problems. Furthermore, we propose InfiBench-V, a novel and challenging benchmark specifically designed to assess models on visually-rich, real-world programming questions that demand a nuanced understanding of both textual and visual contexts. Extensive experiments show that VisCodex achieves state-of-the-art performance among open-source MLLMs and approaches proprietary models like GPT-4o, highlighting the effectiveness of our model merging strategy and new datasets.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.09943v1" target="_blank">AST-n: A Fast Sampling Approach for Low-Dose CT Reconstruction using Diffusion Models</a></h3>
                    <p><strong>Authors:</strong> TomÃ¡s de la Sotta, JosÃ© M. Saavedra, HÃ©ctor HenrÃ­quez, Violeta Chang, Aline Xavier</p>
                    <p><strong>Published:</strong> 8/13/2025</p>
                    <p><strong>Categories:</strong> cs.CV</p>
                    <p><strong>Summary:</strong> Low-dose CT (LDCT) protocols reduce radiation exposure but increase image noise, compromising diagnostic confidence. Diffusion-based generative models have shown promise for LDCT denoising by learning image priors and performing iterative refinement. In this work, we introduce AST-n, an accelerated inference framework that initiates reverse diffusion from intermediate noise levels, and integrate high-order ODE solvers within conditioned models to further reduce sampling steps. We evaluate two acceleration paradigms--AST-n sampling and standard scheduling with high-order solvers -- on the Low Dose CT Grand Challenge dataset, covering head, abdominal, and chest scans at 10-25 % of standard dose. Conditioned models using only 25 steps (AST-25) achieve peak signal-to-noise ratio (PSNR) above 38 dB and structural similarity index (SSIM) above 0.95, closely matching standard baselines while cutting inference time from ~16 seg to under 1 seg per slice. Unconditional sampling suffers substantial quality loss, underscoring the necessity of conditioning. We also assess DDIM inversion, which yields marginal PSNR gains at the cost of doubling inference time, limiting its clinical practicality. Our results demonstrate that AST-n with high-order samplers enables rapid LDCT reconstruction without significant loss of image fidelity, advancing the feasibility of diffusion-based methods in clinical workflows.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.09942v1" target="_blank">Beam Cross Sections Create Mixtures: Improving Feature Localization in Secondary Electron Imaging</a></h3>
                    <p><strong>Authors:</strong> Vaibhav Choudhary, Akshay Agarwal, Vivek K Goyal</p>
                    <p><strong>Published:</strong> 8/13/2025</p>
                    <p><strong>Categories:</strong> eess.SP, cond-mat.mtrl-sci</p>
                    <p><strong>Summary:</strong> Secondary electron (SE) imaging techniques, such as scanning electron microscopy and helium ion microscopy (HIM), use electrons emitted by a sample in response to a focused beam of charged particles incident at a grid of raster scan positions. Spot size -- the diameter of the incident beams spatial profile -- is one of the limiting factors for resolution, along with various sources of noise in the SE signal. The effect of the beam spatial profile is commonly understood as convolutional. We show that under a simple and plausible physical abstraction for the beam, though convolution describes the mean of the SE counts, the full distribution of SE counts is a mixture. We demonstrate that this more detailed modeling can enable resolution improvements over conventional estimators through a stylized application in semiconductor inspection of localizing the edge in a two-valued sample. We derive Fisher information about edge location in conventional and time-resolved measurements (TRM) and also derive the maximum likelihood estimate (MLE) from the latter. Empirically, the MLE computed from TRM is approximately efficient except at very low beam diameter, so Fisher information comparisons are predictive of performance and can be used to optimize the beam diameter relative to the raster scan spacing. Monte Carlo simulations show that the MLE gives a 5-fold reduction in root mean-squared error (RMSE) of edge localization as compared to conventional interpolation-based estimation. Applied to three real HIM datasets, the average RMSE reduction factor is 5.4.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.09937v1" target="_blank">A Comprehensive Evaluation framework of Alignment Techniques for LLMs</a></h3>
                    <p><strong>Authors:</strong> Muneeza Azmat, Momin Abbas, Maysa Malfiza Garcia de Macedo, Marcelo Carpinette Grave, Luan Soares de Souza, Tiago Machado, Rogerio A de Paula, Raya Horesh, Yixin Chen, Heloisa Caroline de Souza Pereira Candello, Rebecka Nordenlow, Aminat Adebiyi</p>
                    <p><strong>Published:</strong> 8/13/2025</p>
                    <p><strong>Categories:</strong> cs.CL, cs.AI, cs.LG</p>
                    <p><strong>Summary:</strong> As Large Language Models (LLMs) become increasingly integrated into real-world applications, ensuring their outputs align with human values and safety standards has become critical. The field has developed diverse alignment approaches including traditional fine-tuning methods (RLHF, instruction tuning), post-hoc correction systems, and inference-time interventions, each with distinct advantages and limitations. However, the lack of unified evaluation frameworks makes it difficult to systematically compare these paradigms and guide deployment decisions. This paper introduces a multi-dimensional evaluation of alignment techniques for LLMs, a comprehensive evaluation framework that provides a systematic comparison across all major alignment paradigms. Our framework assesses methods along four key dimensions: alignment detection, alignment quality, computational efficiency, and robustness. Through experiments across diverse base models and alignment strategies, we demonstrate the utility of our framework in identifying strengths and limitations of current state-of-the-art models, providing valuable insights for future research directions.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.09936v1" target="_blank">Quo Vadis Handwritten Text Generation for Handwritten Text Recognition?</a></h3>
                    <p><strong>Authors:</strong> Vittorio Pippi, Konstantina Nikolaidou, Silvia Cascianelli, George Retsinas, Giorgos Sfikas, Rita Cucchiara, Marcus Liwicki</p>
                    <p><strong>Published:</strong> 8/13/2025</p>
                    <p><strong>Categories:</strong> cs.CV, cs.DL</p>
                    <p><strong>Summary:</strong> The digitization of historical manuscripts presents significant challenges for Handwritten Text Recognition (HTR) systems, particularly when dealing with small, author-specific collections that diverge from the training data distributions. Handwritten Text Generation (HTG) techniques, which generate synthetic data tailored to specific handwriting styles, offer a promising solution to address these challenges. However, the effectiveness of various HTG models in enhancing HTR performance, especially in low-resource transcription settings, has not been thoroughly evaluated. In this work, we systematically compare three state-of-the-art styled HTG models (representing the generative adversarial, diffusion, and autoregressive paradigms for HTG) to assess their impact on HTR fine-tuning. We analyze how visual and linguistic characteristics of synthetic data influence fine-tuning outcomes and provide quantitative guidelines for selecting the most effective HTG model. The results of our analysis provide insights into the current capabilities of HTG methods and highlight key areas for further improvement in their application to low-resource HTR.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.09935v1" target="_blank">Language of Persuasion and Misrepresentation in Business Communication: A Textual Detection Approach</a></h3>
                    <p><strong>Authors:</strong> Sayem Hossen, Monalisa Moon Joti, Md. Golam Rashed</p>
                    <p><strong>Published:</strong> 8/13/2025</p>
                    <p><strong>Categories:</strong> cs.CL, q-fin.CP, q-fin.GN</p>
                    <p><strong>Summary:</strong> Business communication digitisation has reorganised the process of persuasive discourse, which allows not only greater transparency but also advanced deception. This inquiry synthesises classical rhetoric and communication psychology with linguistic theory and empirical studies in the financial reporting, sustainability discourse, and digital marketing to explain how deceptive language can be systematically detected using persuasive lexicon. In controlled settings, detection accuracies of greater than 99% were achieved by using computational textual analysis as well as personalised transformer models. However, reproducing this performance in multilingual settings is also problematic and, to a large extent, this is because it is not easy to find sufficient data, and because few multilingual text-processing infrastructures are in place. This evidence shows that there has been an increasing gap between the theoretical representations of communication and those empirically approximated, and therefore, there is a need to have strong automatic text-identification systems where AI-based discourse is becoming more realistic in communicating with humans.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.09934v1" target="_blank">Efficient Volume Computation for SMT Formulas</a></h3>
                    <p><strong>Authors:</strong> Arijit Shaw, Uddalok Sarkar, Kuldeep S. Meel</p>
                    <p><strong>Published:</strong> 8/13/2025</p>
                    <p><strong>Categories:</strong> cs.LO</p>
                    <p><strong>Summary:</strong> Satisfiability Modulo Theory (SMT) has recently emerged as a powerful tool for solving various automated reasoning problems across diverse domains. Unlike traditional satisfiability methods confined to Boolean variables, SMT can reason on real-life variables like bitvectors, integers, and reals. A natural extension in this context is to ask quantitative questions. One such query in the SMT theory of Linear Real Arithmetic (LRA) is computing the volume of the entire satisfiable region defined by SMT formulas. This problem is important in solving different quantitative verification queries in software verification, cyber-physical systems, and neural networks, to mention a few. We introduce ttc, an efficient algorithm that extends the capabilities of SMT solvers to volume computation. Our method decomposes the solution space of SMT Linear Real Arithmetic formulas into a union of overlapping convex polytopes, then computes their volumes and calculates their union. Our algorithm builds on recent developments in streaming-mode set unions, volume computation algorithms, and AllSAT techniques. Experimental evaluations demonstrate significant performance improvements over existing state-of-the-art approaches.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.09933v1" target="_blank">Quantum recurrences and the arithmetic of Floquet dynamics</a></h3>
                    <p><strong>Authors:</strong> Amit Anand, Dinesh Valluri, Jack Davis, Shohini Ghose</p>
                    <p><strong>Published:</strong> 8/13/2025</p>
                    <p><strong>Categories:</strong> quant-ph, math-ph, math.MP, nlin.CD</p>
                    <p><strong>Summary:</strong> The Poincar\e recurrence theorem shows that conservative systems in a bounded region of phase space eventually return arbitrarily close to their initial state after a finite amount of time. An analogous behavior occurs in certain quantum systems where quantum states can recur after sufficiently long unitary evolution, a phenomenon known as quantum recurrence. Periodically driven (i.e. Floquet) quantum systems in particular exhibit complex dynamics even in small dimensions, motivating the study of how interactions and Hamiltonian structure affect recurrence behavior. While most existing studies treat recurrence in an approximate, distance-based sense, here we address the problem of exact, state-independent recurrences in a broad class of finite-dimensional Floquet systems, spanning both integrable and non-integrable models. Leveraging techniques from algebraic field theory, we construct an arithmetic framework that identifies all possible recurrence times by analyzing the cyclotomic structure of the Floquet unitarys spectrum. This computationally efficient approach yields both positive results, enumerating all candidate recurrence times and definitive negative results, rigorously ruling out exact recurrences for given Hamiltonian parameters. We further prove that rational Hamiltonian parameters do not, in general, guarantee exact recurrence, revealing a subtle interplay between system parameters and long-time dynamics. Our findings sharpen the theoretical understanding of quantum recurrences, clarify their relationship to quantum chaos, and highlight parameter regimes of special interest for quantum metrology and control.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.09932v1" target="_blank">Mathematical Computation and Reasoning Errors by Large Language Models</a></h3>
                    <p><strong>Authors:</strong> Liang Zhang, Edith Aurora Graf</p>
                    <p><strong>Published:</strong> 8/13/2025</p>
                    <p><strong>Categories:</strong> cs.AI</p>
                    <p><strong>Summary:</strong> Large Language Models (LLMs) are increasingly utilized in AI-driven educational instruction and assessment, particularly within mathematics education. The capability of LLMs to generate accurate answers and detailed solutions for math problem-solving tasks is foundational for ensuring reliable and precise feedback and assessment in math education practices. Our study focuses on evaluating the accuracy of four LLMs (OpenAI GPT-4o and o1, DeepSeek-V3 and DeepSeek-R1) solving three categories of math tasks, including arithmetic, algebra, and number theory, and identifies step-level reasoning errors within their solutions. Instead of relying on standard benchmarks, we intentionally build math tasks (via item models) that are challenging for LLMs and prone to errors. The accuracy of final answers and the presence of errors in individual solution steps were systematically analyzed and coded. Both single-agent and dual-agent configurations were tested. It is observed that the reasoning-enhanced OpenAI o1 model consistently achieved higher or nearly perfect accuracy across all three math task categories. Analysis of errors revealed that procedural slips were the most frequent and significantly impacted overall performance, while conceptual misunderstandings were less frequent. Deploying dual-agent configurations substantially improved overall performance. These findings offer actionable insights into enhancing LLM performance and underscore effective strategies for integrating LLMs into mathematics education, thereby advancing AI-driven instructional practices and assessment precision.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.09931v1" target="_blank">Fundamental Physics with Pulsars around Sagittarius A$^\star$</a></h3>
                    <p><strong>Authors:</strong> Lijing Shao, Zexin Hu</p>
                    <p><strong>Published:</strong> 8/13/2025</p>
                    <p><strong>Categories:</strong> astro-ph.HE, gr-qc, hep-ph</p>
                    <p><strong>Summary:</strong> Searching for radio pulsars orbiting around the Galactic centre black hole (BH), Sagittarius A$^\star$ (Sgr A$^\star$), represents a holy grail goal for large-area radio telescopes, in particular for the Square Kilometre Array. Follow-up timing observation of such a PSR-Sgr A$^\star$ binary system with an orbital period $\lesssim O(1\,{\rm year})$ will bring forward a handful of new tests on different aspects of fundamental physics that are barely accessible with other means. However, mass perturbation in the Galactic centre harms the gravitational cleanness of PSR-Sgr A$^\star$ systems. In order to flexibly account for perturbations, a numerical pulsar timing model is gradually being built, which can be used to probe the spacetime around Sgr A$^\star$ BH and study the nature of dark matter.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.09930v1" target="_blank">The massive one-loop four-point string amplitude in pure spinor superspace</a></h3>
                    <p><strong>Authors:</strong> Carlos R. Mafra</p>
                    <p><strong>Published:</strong> 8/13/2025</p>
                    <p><strong>Categories:</strong> hep-th</p>
                    <p><strong>Summary:</strong> The open- and closed-string three- and four-point one-loop amplitudes involving massless states and one first-level massive state are computed in pure spinor superspace. For the open string, we show that their one-loop correlators can be rewritten in terms of tree-level kinematic factors. We then analyze the closed string. For three points, this is immediate. For four points, we show that it is possible to rewrite the one-loop closed-string correlator using tree-level kinematic factors, but only for certain combinations of massive and massless states (different for type IIA and IIB).</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.09928v1" target="_blank">Fault tolerant Operations in Majorana-based Quantum Codes: Gates, Measurements and High Rate Constructions</a></h3>
                    <p><strong>Authors:</strong> Maryam Mudassar, Alexander Schuckert, Daniel Gottesman</p>
                    <p><strong>Published:</strong> 8/13/2025</p>
                    <p><strong>Categories:</strong> quant-ph</p>
                    <p><strong>Summary:</strong> Majorana-based quantum computation in nanowires and neutral atoms has gained prominence as a promising platform to encode qubits and protect them against noise. In order to run computations reliably on such devices, a fully fault-tolerant scheme is needed for state preparation, gates, and measurements. However, current fault-tolerant schemes have either been limited to specific code families or have not been developed fully. In this work, we develop a general framework for fault-tolerant computation with logical degrees encoded into Majorana hardware. We emphasize the division between even and odd Majorana codes and how it manifests when constructing fault tolerant gadgets for these families. We provide transversal constructions and supplement them with measurements to obtain several examples of fault tolerant Clifford gadgets. For the case of odd codes, we give a novel construction for gadgets using quantum reference frames, that allows to implement operations that are forbidden due to parity superselection. We also provide a fault-tolerant measurement scheme for Majorana codes inspired by Steane error correction, enabling state preparation, measurement of logical operations and error correction. We also point out a construction for odd Majorana codes with transversal T gates. Finally, we construct an asympotically good quantum LDPC Majorana code with qubit degrees of freedom. Our work shows that all necessary elements of fault-tolerant quantum computation can be consistently implemented in fermionic hardware such as Majorana nanowires and fermionic neutral atoms.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.09927v1" target="_blank">Thermodynamics of fermionic excitations in heavy-quark QCD</a></h3>
                    <p><strong>Authors:</strong> Kei Tohme, Takahiro M. Doi, Masakiyo Kitazawa, Krzysztof Redlich, Chihiro Sasaki</p>
                    <p><strong>Published:</strong> 8/13/2025</p>
                    <p><strong>Categories:</strong> hep-lat</p>
                    <p><strong>Summary:</strong> We investigate the thermodynamic properties of fermionic excitations in heavy-quark QCD on the lattice with Wilson fermions. The grand potential is calculated analytically in the hopping parameter expansion (HPE) on the basis of the cumulant expansion. Using the grand potential, we compute the quark number susceptibilities and their ratios up to next-to-leading order in the HPE. The ratio of fourth- to second-order susceptibilities is shown to be unity (nine) in the deconfined (confined) phase at the leading order. Excitation properties of baryonic and quark modes in each phase are also investigated utilizing the Boltzmann statistics. We obtain an analytic formula for the quark excitation energy in the deconfined phase, while that for baryonic excitations in the confined phase is decomposed into flavor multiplets.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.09926v1" target="_blank">Towards Comprehensive Cellular Characterisation of HE slides</a></h3>
                    <p><strong>Authors:</strong> Benjamin Adjadj, Pierre-Antoine Bannier, Guillaume Horent, Sebastien Mandela, Aurore Lyon, Kathryn Schutte, Ulysse Marteau, Valentin Gaury, Laura Dumont, Thomas Mathieu, Reda Belbahri, BenoÃ®t Schmauch, Eric Durand, Katharina Von Loga, Lucie Gillet</p>
                    <p><strong>Published:</strong> 8/13/2025</p>
                    <p><strong>Categories:</strong> cs.CV, q-bio.QM, I.2.10; I.4.8</p>
                    <p><strong>Summary:</strong> Cell detection, segmentation and classification are essential for analyzing tumor microenvironments (TME) on hematoxylin and eosin (HE) slides. Existing methods suffer from poor performance on understudied cell types (rare or not present in public datasets) and limited cross-domain generalization. To address these shortcomings, we introduce HistoPLUS, a state-of-the-art model for cell analysis, trained on a novel curated pan-cancer dataset of 108,722 nuclei covering 13 cell types. In external validation across 4 independent cohorts, HistoPLUS outperforms current state-of-the-art models in detection quality by 5.2% and overall F1 classification score by 23.7%, while using 5x fewer parameters. Notably, HistoPLUS unlocks the study of 7 understudied cell types and brings significant improvements on 8 of 13 cell types. Moreover, we show that HistoPLUS robustly transfers to two oncology indications unseen during training. To support broader TME biomarker research, we release the model weights and inference code at https://github.com/owkin/histoplus/.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.09925v1" target="_blank">Residual Reservoir Memory Networks</a></h3>
                    <p><strong>Authors:</strong> Matteo Pinna, Andrea Ceni, Claudio Gallicchio</p>
                    <p><strong>Published:</strong> 8/13/2025</p>
                    <p><strong>Categories:</strong> cs.LG, cs.AI, I.2.6</p>
                    <p><strong>Summary:</strong> We introduce a novel class of untrained Recurrent Neural Networks (RNNs) within the Reservoir Computing (RC) paradigm, called Residual Reservoir Memory Networks (ResRMNs). ResRMN combines a linear memory reservoir with a non-linear reservoir, where the latter is based on residual orthogonal connections along the temporal dimension for enhanced long-term propagation of the input. The resulting reservoir state dynamics are studied through the lens of linear stability analysis, and we investigate diverse configurations for the temporal residual connections. The proposed approach is empirically assessed on time-series and pixel-level 1-D classification tasks. Our experimental results highlight the advantages of the proposed approach over other conventional RC models.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.09922v1" target="_blank">Prototype-Guided Diffusion: Visual Conditioning without External Memory</a></h3>
                    <p><strong>Authors:</strong> Bilal Faye, Hanane Azzag, Mustapha Lebbah</p>
                    <p><strong>Published:</strong> 8/13/2025</p>
                    <p><strong>Categories:</strong> cs.LG</p>
                    <p><strong>Summary:</strong> Diffusion models have emerged as a leading framework for high-quality image generation, offering stable training and strong performance across diverse domains. However, they remain computationally intensive, particularly during the iterative denoising process. Latent-space models like Stable Diffusion alleviate some of this cost by operating in compressed representations, though at the expense of fine-grained detail. More recent approaches such as Retrieval-Augmented Diffusion Models (RDM) address efficiency by conditioning denoising on similar examples retrieved from large external memory banks. While effective, these methods introduce drawbacks: they require costly storage and retrieval infrastructure, depend on static vision-language models like CLIP for similarity, and lack adaptability during training. We propose the Prototype Diffusion Model (PDM), a method that integrates prototype learning directly into the diffusion process for efficient and adaptive visual conditioning - without external memory. Instead of retrieving reference samples, PDM constructs a dynamic set of compact visual prototypes from clean image features using contrastive learning. These prototypes guide the denoising steps by aligning noisy representations with semantically relevant visual patterns, enabling efficient generation with strong semantic grounding. Experiments show that PDM maintains high generation quality while reducing computational and storage overhead, offering a scalable alternative to retrieval-based conditioning in diffusion models.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.09920v1" target="_blank">An integrated photonics platform for high-speed, ultrahigh-extinction, many-channel quantum control</a></h3>
                    <p><strong>Authors:</strong> Mengdi Zhao, Manuj Singh, Anshuman Singh, Henry Thoreen, Robert J. DeAngelo, Daniel Dominguez, Andrew Leenheer, FrÃ©dÃ©ric Peyskens, Alexander Lukin, Dirk Englund, Matt Eichenfield, Nathan Gemelke, Noel H. Wan</p>
                    <p><strong>Published:</strong> 8/13/2025</p>
                    <p><strong>Categories:</strong> quant-ph</p>
                    <p><strong>Summary:</strong> High-fidelity control of the thousands to millions of programmable qubits needed for utility-scale quantum computers presents a formidable challenge for control systems. In leading atomic systems, control is optical: UV-NIR beams must be fanned out over numerous spatial channels and modulated to implement gates. While photonic integrated circuits (PICs) offer a potentially scalable solution, they also need to simultaneously feature high-speed and high-extinction modulation, strong inter-channel isolation, and broad wavelength compatibility. Here, we introduce and experimentally validate a foundry-fabricated PIC platform that overcomes these limitations. Designed for Rubidium-87 neutral atom quantum computers, our 8-channel PICs, fabricated on a 200-mm wafer process, demonstrate an advanced combination of performance metrics. At the 795 nm single-qubit gate wavelength, we achieve a mean extinction ratio (ER) of 71.4 $\pm$ 1.1 dB, nearest-neighbor on-chip crosstalk of -68.0 $\pm$ 1.0 dB, and -50.8 $\pm$ 0.2 dB after parallel beam delivery in free-space. This high-performance operation extends to the 420 nm and 1013 nm wavelengths for two-qubit Rydberg gates, showing ERs of 42.4 dB (detector-limited) and 61.5 dB, respectively. The devices exhibit 10-90% rise times of 26 $\pm$ 7 ns, achieve dynamic switching to -60 dB levels within microsecond timescales, and show pulse stability errors at the $10^{-3}$ level. This work establishes a scalable platform for developing advanced large-scale optical control required in fault-tolerant quantum computers and other precision technologies.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.09919v1" target="_blank">T-CACE: A Time-Conditioned Autoregressive Contrast Enhancement Multi-Task Framework for Contrast-Free Liver MRI Synthesis, Segmentation, and Diagnosis</a></h3>
                    <p><strong>Authors:</strong> Xiaojiao Xiao, Jianfeng Zhao, Qinmin Vivian Hu, Guanghui Wang</p>
                    <p><strong>Published:</strong> 8/13/2025</p>
                    <p><strong>Categories:</strong> eess.IV, cs.AI, cs.CV</p>
                    <p><strong>Summary:</strong> Magnetic resonance imaging (MRI) is a leading modality for the diagnosis of liver cancer, significantly improving the classification of the lesion and patient outcomes. However, traditional MRI faces challenges including risks from contrast agent (CA) administration, time-consuming manual assessment, and limited annotated datasets. To address these limitations, we propose a Time-Conditioned Autoregressive Contrast Enhancement (T-CACE) framework for synthesizing multi-phase contrast-enhanced MRI (CEMRI) directly from non-contrast MRI (NCMRI). T-CACE introduces three core innovations: a conditional token encoding (CTE) mechanism that unifies anatomical priors and temporal phase information into latent representations; and a dynamic time-aware attention mask (DTAM) that adaptively modulates inter-phase information flow using a Gaussian-decayed attention mechanism, ensuring smooth and physiologically plausible transitions across phases. Furthermore, a constraint for temporal classification consistency (TCC) aligns the lesion classification output with the evolution of the physiological signal, further enhancing diagnostic reliability. Extensive experiments on two independent liver MRI datasets demonstrate that T-CACE outperforms state-of-the-art methods in image synthesis, segmentation, and lesion classification. This framework offers a clinically relevant and efficient alternative to traditional contrast-enhanced imaging, improving safety, diagnostic efficiency, and reliability for the assessment of liver lesion. The implementation of T-CACE is publicly available at: https://github.com/xiaojiao929/T-CACE.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.09917v1" target="_blank">Evaluation of a deliberate-practice informed supplemental intervention in graduate Quantum Mechanics</a></h3>
                    <p><strong>Authors:</strong> Michael E. Robbins, Guillaume M. Laurent, Eric W. Burkholder</p>
                    <p><strong>Published:</strong> 8/13/2025</p>
                    <p><strong>Categories:</strong> physics.ed-ph, quant-ph</p>
                    <p><strong>Summary:</strong> Despite the prevalence of physics education research literature related to problem solving, recent studies have illustrated that opportunities for ``authentic problem solving -- conceptualized as making decisions with limited information using ones physics knowledge -- are limited at both the graduate and undergraduate levels in physics curricula. Building on one of these studies, we designed a supplemental intervention for a graduate-level quantum mechanics course which scaffolded the practice of making some of these critical decisions using the conceptual framework of deliberate practice. Despite similar incentive structures as prior interventions focused on conceptual understanding in similar contexts, we did not measure any statistically significant improvement in students problem solving skills following our intervention, though faculty members involved with the next course and written qualifying exams indicated the students showed better-than-usual conceptual understanding. We explore a number of potential explanations for this disconnect and suggest future avenues of research in this area.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.09916v1" target="_blank">Strain-driven feedback tunes memory and relaxation in a Mott insulator far from equilibrium</a></h3>
                    <p><strong>Authors:</strong> O. Yu. Gorobtsov, Y. Kalcheim, Z. Shao, A. Shabalin, N. Hua, D. Weinstock, R. Bouck, M. Seaberg, D. Zhu, O. G. Shpyrko, I. K. Schuller, A. Singer</p>
                    <p><strong>Published:</strong> 8/13/2025</p>
                    <p><strong>Categories:</strong> cond-mat.mtrl-sci, cond-mat.str-el</p>
                    <p><strong>Summary:</strong> Memory effects in metal-insulator transitions in quantum materials reveal complex physics and potential for novel technologies mimicking biological neural systems. Nonetheless, understanding of memory and nonlinearity in non-equilibrium transitions remains elusive as they can involve timescales from femtoseconds to microseconds. In this study, we extend time-resolved x-ray Bragg diffraction to the necessary high dynamic range of timescales to fully trace the pathways of far-from-equilibrium photoexcited transitions in epitaxial films of V2O3, a technologically promising prototypical Mott insulator. We find a 105 times adjustment in relaxation time: the memory of spatial and energy heterogeneity during transition causes a Kohlrausch-Williams-Watts shaped relaxation lingering over a hundred microseconds versus nanoseconds. The dramatic slowdown in the light-driven highly correlated system accompanies heterogeneous excitation barriers, as in neural systems.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.09914v1" target="_blank">Powerful Radio Sources as Probes of Black Hole Physics</a></h3>
                    <p><strong>Authors:</strong> Ruth A. Daly</p>
                    <p><strong>Published:</strong> 8/13/2025</p>
                    <p><strong>Categories:</strong> astro-ph.GA, astro-ph.CO, gr-qc</p>
                    <p><strong>Summary:</strong> Powerful jetted radio sources for which the luminosity in directed kinetic energy has been empirically determined, independent of assumptions, are considered. The total outflow lifetime of each source determined in the context of detailed cosmological studies was found to depend only upon the luminosity in directed kinetic energy ($L$). The distributions of $L$, total outflow lifetime, and total outflow energy each have a broad range of values, as do the supermassive black hole masses. The total outflow energy relative to the black hole mass is a small number with a small dispersion. Three explanations of these remarkable results are considered. This could indicate (1) the efficiencies with which black hole irreducible mass is increased and spin mass energy is extracted during the outflow event, (2) that the merger of two supermassive black holes occurs over a timescale commensurate with the independently determined outflow lifetime and that these mergers lead to the production of the low-frequency gravitational wave background, or (3) that feedback shuts off black hole accretion due to energy injected into the ambient medium.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.09912v1" target="_blank">E-4DGS: High-Fidelity Dynamic Reconstruction from the Multi-view Event Cameras</a></h3>
                    <p><strong>Authors:</strong> Chaoran Feng, Zhenyu Tang, Wangbo Yu, Yatian Pang, Yian Zhao, Jianbin Zhao, Li Yuan, Yonghong Tian</p>
                    <p><strong>Published:</strong> 8/13/2025</p>
                    <p><strong>Categories:</strong> cs.CV</p>
                    <p><strong>Summary:</strong> Novel view synthesis and 4D reconstruction techniques predominantly rely on RGB cameras, thereby inheriting inherent limitations such as the dependence on adequate lighting, susceptibility to motion blur, and a limited dynamic range. Event cameras, offering advantages of low power, high temporal resolution and high dynamic range, have brought a new perspective to addressing the scene reconstruction challenges in high-speed motion and low-light scenes. To this end, we propose E-4DGS, the first event-driven dynamic Gaussian Splatting approach, for novel view synthesis from multi-view event streams with fast-moving cameras. Specifically, we introduce an event-based initialization scheme to ensure stable training and propose event-adaptive slicing splatting for time-aware reconstruction. Additionally, we employ intensity importance pruning to eliminate floating artifacts and enhance 3D consistency, while incorporating an adaptive contrast threshold for more precise optimization. We design a synthetic multi-view camera setup with six moving event cameras surrounding the object in a 360-degree configuration and provide a benchmark multi-view event stream dataset that captures challenging motion scenarios. Our approach outperforms both event-only and event-RGB fusion baselines and paves the way for the exploration of multi-view event-based reconstruction as a novel approach for rapid scene capture.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.09913v1" target="_blank">SpeechForensics: Audio-Visual Speech Representation Learning for Face Forgery Detection</a></h3>
                    <p><strong>Authors:</strong> Yachao Liang, Min Yu, Gang Li, Jianguo Jiang, Boquan Li, Feng Yu, Ning Zhang, Xiang Meng, Weiqing Huang</p>
                    <p><strong>Published:</strong> 8/13/2025</p>
                    <p><strong>Categories:</strong> cs.CV</p>
                    <p><strong>Summary:</strong> Detection of face forgery videos remains a formidable challenge in the field of digital forensics, especially the generalization to unseen datasets and common perturbations. In this paper, we tackle this issue by leveraging the synergy between audio and visual speech elements, embarking on a novel approach through audio-visual speech representation learning. Our work is motivated by the finding that audio signals, enriched with speech content, can provide precise information effectively reflecting facial movements. To this end, we first learn precise audio-visual speech representations on real videos via a self-supervised masked prediction task, which encodes both local and global semantic information simultaneously. Then, the derived model is directly transferred to the forgery detection task. Extensive experiments demonstrate that our method outperforms the state-of-the-art methods in terms of cross-dataset generalization and robustness, without the participation of any fake video in model training. Code is available at https://github.com/Eleven4AI/SpeechForensics.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.09909v1" target="_blank">SHREC25 Track on Multiple Relief Patterns: Report and Analysis</a></h3>
                    <p><strong>Authors:</strong> Gabriele Paolini, Claudio Tortorici, Stefano Berretti, Ahmed Hazem Youssef, Halim Benhabiles, Adnane Cabani, Ruiwen He, Karim Hammoudi, Iyyakutti Iyappan Ganapathi, Syed Sadaf Ali, Divya Velayudhan, Maregu Assefa, Naoufel Werghi</p>
                    <p><strong>Published:</strong> 8/13/2025</p>
                    <p><strong>Categories:</strong> cs.CG</p>
                    <p><strong>Summary:</strong> This SHREC 2025 track focuses on the recognition and segmentation of relief patterns embedded on the surface of a set of synthetically generated triangle meshes. We report the methods proposed by the participants, whose performance highlights the inherent complexity of solving the problem, which is still open. Then, we discuss the critical aspects of the proposed tasks, highlight the limitations of current techniques, and outline possible directions for future research. All resources and track details are available at the official track webpage: https://sites.google.com/unifi.it/shrec25-relief-pattern.</p>
                
            
        
        
            <p><em>Generated by AI News Agent</em></p>
        
    

