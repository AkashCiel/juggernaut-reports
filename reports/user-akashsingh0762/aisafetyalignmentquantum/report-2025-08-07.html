
    
        <h1>ðŸ¤– AI Research Report</h1>
        
            <strong>Date:</strong> 2025-08-07<br>
            <strong>Topics:</strong> ai safety research, ai alignment research, quantum computing<br>
            <strong>Papers Found:</strong> 150
        
        
        
            
                <h2>ðŸ¤– AI Summary</h2>
                <p>## ai safety research

The recent research papers on AI safety highlight a diverse range of studies focusing on improving model robustness, understanding model failures, and ensuring ethical AI deployment. A significant trend is the exploration of reasoning model limitations, as seen in the study Hop, Skip, and Overthink, which delves into why AI models struggle with multi-hop reasoning tasks. This research underscores the importance of understanding cognitive inefficiencies in AI systems, which can lead to hallucinations and errors, thereby informing future improvements in AI reasoning robustness and transparency.

Another critical area of focus is the personalization and alignment of AI systems, as detailed in FaST: Feature-aware Sampling and Tuning, which addresses the challenge of aligning language models with individual user preferences despite limited data availability. This trend towards personalization is crucial for enhancing user trust and acceptance of AI technologies. Additionally, the paper From MAS to MARS highlights the complexities and safety challenges in deploying multi-agent robotic systems in real-world scenarios, particularly in healthcare. This research emphasizes the need for robust coordination and edge-case testing to ensure system reliability and safety.

The papers collectively underscore a growing emphasis on AI systems transparency, robustness, and alignment, which are essential for safe AI deployment. The integration of human-centered design and the development of tools for understanding and correcting AI model errors are pivotal in advancing AI safety. These insights are crucial as AI systems become more integrated into critical areas such as healthcare, finance, and personal assistance, where safety and ethical considerations are paramount.

*Based on 50 research papers*

---

## ai alignment research

AI alignment research focuses on ensuring that AI systems act according to human values and intentions. Recent papers in this domain highlight several trends and breakthroughs that are crucial for advancing this field.

One significant trend is the exploration of reasoning and cognitive limitations in AI models, as seen in Yadav et al.s study on reasoning models errors during multi-hop analysis. This work provides insights into the hallucination issues of language models, proposing error categorization frameworks that could guide future enhancements in reasoning fidelity and transparency. Another important development is personalized preference alignment in AI, as discussed by Thonet et al. They introduce practical datasets and methods for aligning AI models with user preferences despite limited data, highlighting the growing demand for personalization in AI systems.

Additionally, the integration of AI with real-world systems is being explored through frameworks like VirtLab, which simulates team dynamics using AI agents, and Sculptor, which enhances LLMs performance by managing context proactively. These efforts aim to bridge the gap between AI capabilities and practical applications, emphasizing the need for robust, context-aware AI systems.

Moreover, studies like From MAS to MARS by Bai et al. focus on multi-agent coordination in complex environments, underscoring the importance of aligning AI agents to work collaboratively in real-world scenarios. Collectively, these works emphasize the need for AI systems to understand, reason, and act in ways that align with human values, ensuring they are safe and beneficial as they are increasingly integrated into society.

*Based on 50 research papers*</p>
            
        
        
        <h2>ðŸ“š Research Papers</h2>
        
                
                    <h3><a href="http://arxiv.org/abs/2508.04699v1" target="_blank">Hop, Skip, and Overthink: Diagnosing Why Reasoning Models Fumble during Multi-Hop Analysis</a></h3>
                    <p><strong>Authors:</strong> Anushka Yadav, Isha Nalawade, Srujana Pillarichety, Yashwanth Babu, Reshmi Ghosh, Samyadeep Basu, Wenlong Zhao, Ali Nasaeh, Sriram Balasubramanian, Soundararajan Srinivasan</p>
                    <p><strong>Published:</strong> 8/6/2025</p>
                    <p><strong>Categories:</strong> cs.CL, cs.AI</p>
                    <p><strong>Summary:</strong> The emergence of reasoning models and their integration into practical AI chat bots has led to breakthroughs in solving advanced math, deep search, and extractive question answering problems that requires a complex and multi-step thought process. Yet, a complete understanding of why these models hallucinate more than general purpose language models is missing. In this investigative study, we systematicallyexplore reasoning failures of contemporary language models on multi-hop question answering tasks. We introduce a novel, nuanced error categorization framework that examines failures across three critical dimensions: the diversity and uniqueness of source documents involved (hops), completeness in capturing relevant information (coverage), and cognitive inefficiency (overthinking). Through rigorous hu-man annotation, supported by complementary automated metrics, our exploration uncovers intricate error patterns often hidden by accuracy-centric evaluations. This investigative approach provides deeper insights into the cognitive limitations of current models and offers actionable guidance toward enhancing reasoning fidelity, transparency, and robustness in future language modeling efforts.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.04698v1" target="_blank">FaST: Feature-aware Sampling and Tuning for Personalized Preference Alignment with Limited Data</a></h3>
                    <p><strong>Authors:</strong> Thibaut Thonet, GermÃ¡n Kruszewski, Jos Rozen, Pierre Erbacher, Marc Dymetman</p>
                    <p><strong>Published:</strong> 8/6/2025</p>
                    <p><strong>Categories:</strong> cs.CL</p>
                    <p><strong>Summary:</strong> LLM-powered conversational assistants are often deployed in a one-size-fits-all manner, which fails to accommodate individual user preferences. Recently, LLM personalization -- tailoring models to align with specific user preferences -- has gained increasing attention as a way to bridge this gap. In this work, we specifically focus on a practical yet challenging setting where only a small set of preference annotations can be collected per user -- a problem we define as Personalized Preference Alignment with Limited Data (PPALLI). To support research in this area, we introduce two datasets -- DnD and ELIP -- and benchmark a variety of alignment techniques on them. We further propose FaST, a highly parameter-efficient approach that leverages high-level features automatically discovered from the data, achieving the best overall performance.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.04691v1" target="_blank">From MAS to MARS: Coordination Failures and Reasoning Trade-offs in Hierarchical Multi-Agent Robotic Systems within a Healthcare Scenario</a></h3>
                    <p><strong>Authors:</strong> Yuanchen Bai, Zijian Ding, Shaoyue Wen, Xiang Chang, Angelique Taylor</p>
                    <p><strong>Published:</strong> 8/6/2025</p>
                    <p><strong>Categories:</strong> cs.RO, cs.AI, cs.MA</p>
                    <p><strong>Summary:</strong> Multi-agent robotic systems (MARS) build upon multi-agent systems by integrating physical and task-related constraints, increasing the complexity of action execution and agent coordination. However, despite the availability of advanced multi-agent frameworks, their real-world deployment on robots remains limited, hindering the advancement of MARS research in practice. To bridge this gap, we conducted two studies to investigate performance trade-offs of hierarchical multi-agent frameworks in a simulated real-world multi-robot healthcare scenario. In Study 1, using CrewAI, we iteratively refine the systems knowledge base, to systematically identify and categorize coordination failures (e.g., tool access violations, lack of timely handling of failure reports) not resolvable by providing contextual knowledge alone. In Study 2, using AutoGen, we evaluate a redesigned bidirectional communication structure and further measure the trade-offs between reasoning and non-reasoning models operating within the same robotic team setting. Drawing from our empirical findings, we emphasize the tension between autonomy and stability and the importance of edge-case testing to improve system reliability and safety for future real-world deployment. Supplementary materials, including codes, task agent setup, trace outputs, and annotated examples of coordination failures and reasoning behaviors, are available at: https://byc-sophie.github.io/mas-to-mars/.</p>
                
            
                
                    <h3><a href="http://dx.doi.org/10.1109/VRW55335.2022.00178" target="_blank">MienCap: Realtime Performance-Based Facial Animation with Live Mood Dynamics</a></h3>
                    <p><strong>Authors:</strong> Ye Pan, Ruisi Zhang, Jingying Wang, Nengfu Chen, Yilin Qiu, Yu Ding, Kenny Mitchell</p>
                    <p><strong>Published:</strong> 8/6/2025</p>
                    <p><strong>Categories:</strong> cs.GR, cs.CV, I.3.2; I.4.10</p>
                    <p><strong>Summary:</strong> Our purpose is to improve performance-based animation which can drive believable 3D stylized characters that are truly perceptual. By combining traditional blendshape animation techniques with multiple machine learning models, we present both non-real time and real time solutions which drive character expressions in a geometrically consistent and perceptually valid way. For the non-real time system, we propose a 3D emotion transfer network makes use of a 2D human image to generate a stylized 3D rig parameters. For the real time system, we propose a blendshape adaption network which generates the character rig parameter motions with geometric consistency and temporally stability. We demonstrate the effectiveness of our system by comparing to a commercial product Faceware. Results reveal that ratings of the recognition, intensity, and attractiveness of expressions depicted for animated characters via our systems are statistically higher than Faceware. Our results may be implemented into the animation pipeline, and provide animators with a system for creating the expressions they wish to use more quickly and accurately.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.04681v1" target="_blank">Perceiving and Acting in First-Person: A Dataset and Benchmark for Egocentric Human-Object-Human Interactions</a></h3>
                    <p><strong>Authors:</strong> Liang Xu, Chengqun Yang, Zili Lin, Fei Xu, Yifan Liu, Congsheng Xu, Yiyi Zhang, Jie Qin, Xingdong Sheng, Yunhui Liu, Xin Jin, Yichao Yan, Wenjun Zeng, Xiaokang Yang</p>
                    <p><strong>Published:</strong> 8/6/2025</p>
                    <p><strong>Categories:</strong> cs.CV</p>
                    <p><strong>Summary:</strong> Learning action models from real-world human-centric interaction datasets is important towards building general-purpose intelligent assistants with efficiency. However, most existing datasets only offer specialist interaction category and ignore that AI assistants perceive and act based on first-person acquisition. We urge that both the generalist interaction knowledge and egocentric modality are indispensable. In this paper, we embed the manual-assisted task into a vision-language-action framework, where the assistant provides services to the instructor following egocentric vision and commands. With our hybrid RGB-MoCap system, pairs of assistants and instructors engage with multiple objects and the scene following GPT-generated scripts. Under this setting, we accomplish InterVLA, the first large-scale human-object-human interaction dataset with 11.4 hours and 1.2M frames of multimodal data, spanning 2 egocentric and 5 exocentric videos, accurate human/object motions and verbal commands. Furthermore, we establish novel benchmarks on egocentric human motion estimation, interaction synthesis, and interaction prediction with comprehensive analysis. We believe that our InterVLA testbed and the benchmarks will foster future works on building AI agents in the physical world.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.04679v1" target="_blank">MisVisFix: An Interactive Dashboard for Detecting, Explaining, and Correcting Misleading Visualizations using Large Language Models</a></h3>
                    <p><strong>Authors:</strong> Amit Kumar Das, Klaus Mueller</p>
                    <p><strong>Published:</strong> 8/6/2025</p>
                    <p><strong>Categories:</strong> cs.HC</p>
                    <p><strong>Summary:</strong> Misleading visualizations pose a significant challenge to accurate data interpretation. While recent research has explored the use of Large Language Models (LLMs) for detecting such misinformation, practical tools that also support explanation and correction remain limited. We present MisVisFix, an interactive dashboard that leverages both Claude and GPT models to support the full workflow of detecting, explaining, and correcting misleading visualizations. MisVisFix correctly identifies 96% of visualization issues and addresses all 74 known visualization misinformation types, classifying them as major, minor, or potential concerns. It provides detailed explanations, actionable suggestions, and automatically generates corrected charts. An interactive chat interface allows users to ask about specific chart elements or request modifications. The dashboard adapts to newly emerging misinformation strategies through targeted user interactions. User studies with visualization experts and developers of fact-checking tools show that MisVisFix accurately identifies issues and offers useful suggestions for improvement. By transforming LLM-based detection into an accessible, interactive platform, MisVisFix advances visualization literacy and supports more trustworthy data communication.</p>
                
            
                
                    <h3><a href="http://dx.doi.org/10.1177/02783649251369549" target="_blank">Open Scene Graphs for Open-World Object-Goal Navigation</a></h3>
                    <p><strong>Authors:</strong> Joel Loo, Zhanxin Wu, David Hsu</p>
                    <p><strong>Published:</strong> 8/6/2025</p>
                    <p><strong>Categories:</strong> cs.RO</p>
                    <p><strong>Summary:</strong> How can we build general-purpose robot systems for open-world semantic navigation, e.g., searching a novel environment for a target object specified in natural language? To tackle this challenge, we introduce OSG Navigator, a modular system composed of foundation models, for open-world Object-Goal Navigation (ObjectNav). Foundation models provide enormous semantic knowledge about the world, but struggle to organise and maintain spatial information effectively at scale. Key to OSG Navigator is the Open Scene Graph representation, which acts as spatial memory for OSG Navigator. It organises spatial information hierarchically using OSG schemas, which are templates, each describing the common structure of a class of environments. OSG schemas can be automatically generated from simple semantic labels of a given environment, e.g., home or supermarket. They enable OSG Navigator to adapt zero-shot to new environment types. We conducted experiments using both Fetch and Spot robots in simulation and in the real world, showing that OSG Navigator achieves state-of-the-art performance on ObjectNav benchmarks and generalises zero-shot over diverse goals, environments, and robot embodiments.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.04669v1" target="_blank">Cybersecurity of Quantum Key Distribution Implementations</a></h3>
                    <p><strong>Authors:</strong> Ittay Alfassi, Ran Gelles, Rotem Liss, Tal Mor</p>
                    <p><strong>Published:</strong> 8/6/2025</p>
                    <p><strong>Categories:</strong> quant-ph, cs.CR</p>
                    <p><strong>Summary:</strong> Practical implementations of Quantum Key Distribution (QKD) often deviate from the theoretical protocols, exposing the implementations to various attacks even when the underlying (ideal) protocol is proven secure. We present new analysis tools and methodologies for quantum cybersecurity, adapting the concepts of vulnerabilities, attack surfaces, and exploits from classical cybersecurity to QKD implementation attacks. We present three additional concepts, derived from the connection between classical and quantum cybersecurity: Quantum Fuzzing, which is the first tool for black-box vulnerability research on QKD implementations; Reversed-Space Attacks, which are a generic exploit method using the attack surface of imperfect receivers; and a concrete quantum-mechanical definition of Quantum Side-Channel Attacks, meaningfully distinguishing them from other types of attacks. Using our tools, we analyze multiple existing QKD attacks and show that the Bright Illumination attack could have been fully constructed even with minimal knowledge of the device implementation. This work begins to bridge the gap between current analysis methods for experimental attacks on QKD implementations and the decades-long research in the field of classical cybersecurity, improving the practical security of QKD products and enhancing their usefulness in real-world systems.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.04667v1" target="_blank">How are CS students using resources and AI tools for coding tasks?</a></h3>
                    <p><strong>Authors:</strong> Natalia Echeverry, Arun Lekshmi Narayanan</p>
                    <p><strong>Published:</strong> 8/6/2025</p>
                    <p><strong>Categories:</strong> cs.HC, cs.AI</p>
                    <p><strong>Summary:</strong> A survey of 26 CS students reveals that AI coding assistants are mainly used for writing code (second to online searches) while AI chatbots are the top resource for debugging. Participants with different coding experience prefer online help over direct human help from peers and instructors.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.04664v1" target="_blank">Sculptor: Empowering LLMs with Cognitive Agency via Active Context Management</a></h3>
                    <p><strong>Authors:</strong> Mo Li, L. H. Xu, Qitai Tan, Ting Cao, Yunxin Liu</p>
                    <p><strong>Published:</strong> 8/6/2025</p>
                    <p><strong>Categories:</strong> cs.CL, cs.AI, cs.LG</p>
                    <p><strong>Summary:</strong> Large Language Models (LLMs) suffer from significant performance degradation when processing long contexts due to proactive interference, where irrelevant information in earlier parts of the context disrupts reasoning and memory recall. While most research focuses on external memory systems to augment LLMs capabilities, we propose a complementary approach: empowering LLMs with Active Context Management (ACM) tools to actively sculpt their internal working memory. We introduce Sculptor, a framework that equips LLMs with three categories of tools: (1) context fragmentation, (2) summary, hide, and restore, and (3) intelligent search. Our approach enables LLMs to proactively manage their attention and working memory, analogous to how humans selectively focus on relevant information while filtering out distractions. Experimental evaluation on information-sparse benchmarks-PI-LLM (proactive interference) and NeedleBench Multi-Needle Reasoning-demonstrates that Sculptor significantly improves performance even without specific training, leveraging LLMs inherent tool calling generalization capabilities. By enabling Active Context Management, Sculptor not only mitigates proactive interference but also provides a cognitive foundation for more reliable reasoning across diverse long-context tasks-highlighting that explicit context-control strategies, rather than merely larger token windows, are key to robustness at scale.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.04660v1" target="_blank">Multi-module GRPO: Composing Policy Gradients and Prompt Optimization for Language Model Programs</a></h3>
                    <p><strong>Authors:</strong> Noah Ziems, Dilara Soylu, Lakshya A Agrawal, Isaac Miller, Liheng Lai, Chen Qian, Kaiqiang Song, Meng Jiang, Dan Klein, Matei Zaharia, Karel DOosterlinck, Christopher Potts, Omar Khattab</p>
                    <p><strong>Published:</strong> 8/6/2025</p>
                    <p><strong>Categories:</strong> cs.CL</p>
                    <p><strong>Summary:</strong> Group Relative Policy Optimization (GRPO) has proven to be an effective tool for post-training language models (LMs). However, AI systems are increasingly expressed as modular programs that mix together multiple LM calls with distinct prompt templates and other tools, and it is not clear how best to leverage GRPO to improve these systems. We begin to address this challenge by defining mmGRPO, a simple multi-module generalization of GRPO that groups LM calls by module across rollouts and handles variable-length and interrupted trajectories. We find that mmGRPO, composed with automatic prompt optimization, improves accuracy by 11% on average across classification, many-hop search, and privacy-preserving delegation tasks against the post-trained LM, and by 5% against prompt optimization on its own. We open-source mmGRPO in DSPy as the dspy.GRPO optimizer.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.04658v1" target="_blank">YOLOv8-Based Deep Learning Model for Automated Poultry Disease Detection and Health Monitoring paper</a></h3>
                    <p><strong>Authors:</strong> Akhil Saketh Reddy Sabbella, Ch. Lakshmi Prachothan, Eswar Kumar Panta</p>
                    <p><strong>Published:</strong> 8/6/2025</p>
                    <p><strong>Categories:</strong> cs.CV, cs.AI</p>
                    <p><strong>Summary:</strong> In the poultry industry, detecting chicken illnesses is essential to avoid financial losses. Conventional techniques depend on manual observation, which is laborious and prone to mistakes. Using YOLO v8 a deep learning model for real-time object recognition. This study suggests an AI based approach, by developing a system that analyzes high resolution chicken photos, YOLO v8 detects signs of illness, such as abnormalities in behavior and appearance. A sizable, annotated dataset has been used to train the algorithm, which provides accurate real-time identification of infected chicken and prompt warnings to farm operators for prompt action. By facilitating early infection identification, eliminating the need for human inspection, and enhancing biosecurity in large-scale farms, this AI technology improves chicken health management. The real-time features of YOLO v8 provide a scalable and effective method for improving farm management techniques.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.04651v1" target="_blank">Live Music Models</a></h3>
                    <p><strong>Authors:</strong> Lyria Team, Antoine Caillon, Brian McWilliams, Cassie Tarakajian, Ian Simon, Ilaria Manco, Jesse Engel, Noah Constant, Pen Li, Timo I. Denk, Alberto Lalama, Andrea Agostinelli, Anna Huang, Ethan Manilow, George Brower, Hakan Erdogan, Heidi Lei, Itai Rolnick, Ivan Grishchenko, Manu Orsini, Matej Kastelic, Mauricio Zuluaga, Mauro Verzetti, Michael Dooley, Ondrej Skopek, Rafael Ferrer, ZalÃ¡n Borsos, Ã„aron van den Oord, Douglas Eck, Eli Collins, Jason Baldridge, Tom Hume, Chris Donahue, Kehang Han, Adam Roberts</p>
                    <p><strong>Published:</strong> 8/6/2025</p>
                    <p><strong>Categories:</strong> cs.SD, cs.HC, cs.LG</p>
                    <p><strong>Summary:</strong> We introduce a new class of generative models for music called live music models that produce a continuous stream of music in real-time with synchronized user control. We release Magenta RealTime, an open-weights live music model that can be steered using text or audio prompts to control acoustic style. On automatic metrics of music quality, Magenta RealTime outperforms other open-weights music generation models, despite using fewer parameters and offering first-of-its-kind live generation capabilities. We also release Lyria RealTime, an API-based model with extended controls, offering access to our most powerful model with wide prompt coverage. These models demonstrate a new paradigm for AI-assisted music creation that emphasizes human-in-the-loop interaction for live music performance.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.04649v1" target="_blank">Estimating breast cancer recurrence in a population-based registry in Georgia, US</a></h3>
                    <p><strong>Authors:</strong> Chrystelle Kiang, Micah Streiff, Rebecca Nash, Robert H. Lyles, Deirdre Cronin-Fenton, Anke Huels, Timothy L. Lash, Kevin C. Ward</p>
                    <p><strong>Published:</strong> 8/6/2025</p>
                    <p><strong>Categories:</strong> q-bio.PE</p>
                    <p><strong>Summary:</strong> Although the descriptive epidemiology of primary breast cancer is well characterized in the US, breast cancer recurrence rates have not been measured in an unselected population. The number of breast cancer survivors at risk for recurrence is growing each year, so recurrence surveillance is a pressing need. We used missing data methods to impute breast cancer recurrence and estimate the risk of recurrence in the Cancer Recurrence Information and Surveillance Program (CRISP) cohort in the Georgia Cancer Registry. The imputation model was based on an internal validation substudy and indicators recorded in the registry (e.g., pathology reports, imaging claims), prognostic variables (e.g., stage at diagnosis), and characteristics associated with missing data (e.g., insurance coverage). We pooled hazard ratios (HR) and 95% Confidence Intervals (CI) across 1000 imputed datasets, adjusted for age, stage, grade, subtype, race and ethnicity, marital status, and urban/rural county at diagnosis. There were 1,606 patients with a validated outcome (75% with breast cancer recurrence) and we imputed the outcome for the remaining 23,439 patients. We estimated an overall 7.2% incidence of recurrence between at least 1 year after diagnosis and up to 5 years of follow up. When comparing the hazards pooled across imputations, we found that some patterns differed from established patterns in mortality or survival, notably by race and ethnicity, underscoring the need for continued research on the descriptive epidemiology of breast cancer recurrence. These results provide new insights into surveillance for breast cancer survivors in Georgia, especially those with higher stage and grade tumors, of Hispanic ethnicity, and who may be lacking social support.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.04645v1" target="_blank">A Scalable Pretraining Framework for Link Prediction with Efficient Adaptation</a></h3>
                    <p><strong>Authors:</strong> Yu Song, Zhigang Hua, Harry Shomer, Yan Xie, Jingzhe Liu, Bo Long, Hui Liu</p>
                    <p><strong>Published:</strong> 8/6/2025</p>
                    <p><strong>Categories:</strong> cs.LG, cs.AI</p>
                    <p><strong>Summary:</strong> Link Prediction (LP) is a critical task in graph machine learning. While Graph Neural Networks (GNNs) have significantly advanced LP performance recently, existing methods face key challenges including limited supervision from sparse connectivity, sensitivity to initialization, and poor generalization under distribution shifts. We explore pretraining as a solution to address these challenges. Unlike node classification, LP is inherently a pairwise task, which requires the integration of both node- and edge-level information. In this work, we present the first systematic study on the transferability of these distinct modules and propose a late fusion strategy to effectively combine their outputs for improved performance. To handle the diversity of pretraining data and avoid negative transfer, we introduce a Mixture-of-Experts (MoE) framework that captures distinct patterns in separate experts, facilitating seamless application of the pretrained model on diverse downstream datasets. For fast adaptation, we develop a parameter-efficient tuning strategy that allows the pretrained model to adapt to unseen datasets with minimal computational overhead. Experiments on 16 datasets across two domains demonstrate the effectiveness of our approach, achieving state-of-the-art performance on low-resource link prediction while obtaining competitive results compared to end-to-end trained methods, with over 10,000x lower computational overhead.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.04641v1" target="_blank">4-Swap: Achieving Grief-Free and Bribery-Safe Atomic Swaps Using Four Transactions</a></h3>
                    <p><strong>Authors:</strong> Kirti Singh, Vinay J. Ribeiro, Susmita Mandal</p>
                    <p><strong>Published:</strong> 8/6/2025</p>
                    <p><strong>Categories:</strong> cs.CR, C.2.4</p>
                    <p><strong>Summary:</strong> Cross-chain asset exchange is crucial for blockchain interoperability. Existing solutions rely on trusted third parties and risk asset loss, or use decentralized alternatives like atomic swaps, which suffer from grief attacks. Griefing occurs when a party prematurely exits, locking the counterpartys assets until a timelock expires. Hedged Atomic Swaps mitigate griefing by introducing a penalty premium; however, they increase the number of transactions from four (as in Tier Nolans swap) to six, which in turn introduces new griefing risks. Grief-Free (GF) Swap reduces this to five transactions by consolidating assets and premiums on a single chain. However, no existing protocol achieves grief-free asset exchange in just four transactions. This paper presents 4-Swap, the first cross-chain atomic swap protocol that is both grief-free and bribery-safe, while completing asset exchange in just four transactions. By combining the griefing premium and principal into a single transaction per chain, 4-Swap reduces on-chain transactions, leading to faster execution compared to previous grief-free solutions. It is fully compatible with Bitcoin and operates without the need for any new opcodes. A game-theoretic analysis shows that rational participants have no incentive to deviate from the protocol, ensuring robust compliance and security.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.04638v1" target="_blank">Can NLP Tackle Hate Speech in the Real World? Stakeholder-Informed Feedback and Survey on Counterspeech</a></h3>
                    <p><strong>Authors:</strong> Tanvi Dinkar, Aiqi Jiang, Simona Frenda, Poppy Gerrard-Abbott, Nancie Gunson, Gavin Abercrombie, Ioannis Konstas</p>
                    <p><strong>Published:</strong> 8/6/2025</p>
                    <p><strong>Categories:</strong> cs.CL</p>
                    <p><strong>Summary:</strong> Counterspeech, i.e. the practice of responding to online hate speech, has gained traction in NLP as a promising intervention. While early work emphasised collaboration with non-governmental organisation stakeholders, recent research trends have shifted toward automated pipelines that reuse a small set of legacy datasets, often without input from affected communities. This paper presents a systematic review of 74 NLP studies on counterspeech, analysing the extent to which stakeholder participation influences dataset creation, model development, and evaluation. To complement this analysis, we conducted a participatory case study with five NGOs specialising in online Gender-Based Violence (oGBV), identifying stakeholder-informed practices for counterspeech generation. Our findings reveal a growing disconnect between current NLP research and the needs of communities most impacted by toxic online content. We conclude with concrete recommendations for re-centring stakeholder expertise in counterspeech research.</p>
                
            
                
                    <h3><a href="http://dx.doi.org/10.1145/3746058.3758994" target="_blank">VirtLab: An AI-Powered System for Flexible, Customizable, and Large-scale Team Simulations</a></h3>
                    <p><strong>Authors:</strong> Mohammed Almutairi, Charles Chiang, Haoze Guo, Matthew Belcher, Nandini Banerjee, Maria Milkowski, Svitlana Volkova, Daniel Nguyen, Tim Weninger, Michael Yankoski, Trenton W. Ford, Diego Gomez-Zara</p>
                    <p><strong>Published:</strong> 8/6/2025</p>
                    <p><strong>Categories:</strong> cs.HC</p>
                    <p><strong>Summary:</strong> Simulating how team members collaborate within complex environments using Agentic AI is a promising approach to explore hypotheses grounded in social science theories and study team behaviors. We introduce VirtLab, a user-friendly, customizable, multi-agent, and scalable team simulation system that enables testing teams with LLM-based agents in spatial and temporal settings. This system addresses the current frameworks design and technical limitations that do not consider flexible simulation scenarios and spatial settings. VirtLab contains a simulation engine and a web interface that enables both technical and non-technical users to formulate, run, and analyze team simulations without programming. We demonstrate the systems utility by comparing ground truth data with simulated scenarios.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.04632v1" target="_blank">IFDECORATOR: Wrapping Instruction Following Reinforcement Learning with Verifiable Rewards</a></h3>
                    <p><strong>Authors:</strong> Xu Guo, Tianyi Liang, Tong Jian, Xiaogui Yang, Ling-I Wu, Chenhui Li, Zhihui Lu, Qipeng Guo, Kai Chen</p>
                    <p><strong>Published:</strong> 8/6/2025</p>
                    <p><strong>Categories:</strong> cs.CL</p>
                    <p><strong>Summary:</strong> Reinforcement Learning with Verifiable Rewards (RLVR) improves instruction following capabilities of large language models (LLMs), but suffers from training inefficiency due to inadequate difficulty assessment. Moreover, RLVR is prone to over-optimization, where LLMs exploit verification shortcuts without aligning to the actual intent of user instructions. We introduce Instruction Following Decorator (IFDecorator}, a framework that wraps RLVR training into a robust and sample-efficient pipeline. It consists of three components: (1) a cooperative-adversarial data flywheel that co-evolves instructions and hybrid verifications, generating progressively more challenging instruction-verification pairs; (2) IntentCheck, a bypass module enforcing intent alignment; and (3) trip wires, a diagnostic mechanism that detects reward hacking via trap instructions, which trigger and capture shortcut exploitation behaviors. Our Qwen2.5-32B-Instruct-IFDecorator achieves 87.43% accuracy on IFEval, outperforming larger proprietary models such as GPT-4o. Additionally, we demonstrate substantial improvements on FollowBench while preserving general capabilities. Our trip wires show significant reductions in reward hacking rates. We will release models, code, and data for future research.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.04625v1" target="_blank">FinMMR: Make Financial Numerical Reasoning More Multimodal, Comprehensive, and Challenging</a></h3>
                    <p><strong>Authors:</strong> Zichen Tang, Haihong E, Jiacheng Liu, Zhongjun Yang, Rongjin Li, Zihua Rong, Haoyang He, Zhuodi Hao, Xinyang Hu, Kun Ji, Ziyan Ma, Mengyuan Ji, Jun Zhang, Chenghao Ma, Qianhe Zheng, Yang Liu, Yiling Huang, Xinyi Hu, Qing Huang, Zijian Xie, Shiyao Peng</p>
                    <p><strong>Published:</strong> 8/6/2025</p>
                    <p><strong>Categories:</strong> cs.CV, cs.CE</p>
                    <p><strong>Summary:</strong> We present FinMMR, a novel bilingual multimodal benchmark tailored to evaluate the reasoning capabilities of multimodal large language models (MLLMs) in financial numerical reasoning tasks. Compared to existing benchmarks, our work introduces three significant advancements. (1) Multimodality: We meticulously transform existing financial reasoning benchmarks, and construct novel questions from the latest Chinese financial research reports. FinMMR comprises 4.3K questions and 8.7K images spanning 14 categories, including tables, bar charts, and ownership structure charts. (2) Comprehensiveness: FinMMR encompasses 14 financial subdomains, including corporate finance, banking, and industry analysis, significantly exceeding existing benchmarks in financial domain knowledge breadth. (3) Challenge: Models are required to perform multi-step precise numerical reasoning by integrating financial knowledge with the understanding of complex financial images and text. The best-performing MLLM achieves only 53.0% accuracy on Hard problems. We believe that FinMMR will drive advancements in enhancing the reasoning capabilities of MLLMs in real-world scenarios.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.04612v1" target="_blank">A Reproducible, Scalable Pipeline for Synthesizing Autoregressive Model Literature</a></h3>
                    <p><strong>Authors:</strong> Faruk Alpay, Bugra Kilictas, Hamdi Alakkad</p>
                    <p><strong>Published:</strong> 8/6/2025</p>
                    <p><strong>Categories:</strong> cs.IR, cs.DL, cs.LG, 68P20, 68T05, 68T50, H.3.3; H.3.7; I.2.6; I.2.7</p>
                    <p><strong>Summary:</strong> The accelerating pace of research on autoregressive generative models has produced thousands of papers, making manual literature surveys and reproduction studies increasingly impractical. We present a fully open-source, reproducible pipeline that automatically retrieves candidate documents from public repositories, filters them for relevance, extracts metadata, hyper-parameters and reported results, clusters topics, produces retrieval-augmented summaries and generates containerised scripts for re-running selected experiments. Quantitative evaluation on 50 manually-annotated papers shows F1 scores above 0.85 for relevance classification, hyper-parameter extraction and citation identification. Experiments on corpora of up to 1000 papers demonstrate near-linear scalability with eight CPU workers. Three case studies -- AWD-LSTM on WikiText-2, Transformer-XL on WikiText-103 and an autoregressive music model on the Lakh MIDI dataset -- confirm that the extracted settings support faithful reproduction, achieving test perplexities within 1--3% of the original reports.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.04608v1" target="_blank">Assortativity in geometric and scale-free networks</a></h3>
                    <p><strong>Authors:</strong> Marc Kaufmann, Ulysse Schaller, Thomas BlÃ¤sius, Johannes Lengler</p>
                    <p><strong>Published:</strong> 8/6/2025</p>
                    <p><strong>Categories:</strong> cs.SI, math.PR</p>
                    <p><strong>Summary:</strong> The assortative behavior of a network is the tendency of similar (or dissimilar) nodes to connect to each other. This tendency can have an influence on various properties of the network, such as its robustness or the dynamics of spreading processes. In this paper, we study degree assortativity both in real-world networks and in several generative models for networks with heavy-tailed degree distribution based on latent spaces. In particular, we study Chung-Lu Graphs and Geometric Inhomogeneous Random Graphs (GIRGs). Previous research on assortativity has primarily focused on measuring the degree assortativity in real-world networks using the Pearson assortativity coefficient, despite reservations against this coefficient. We rigorously confirm these reservations by mathematically proving that the Pearson assortativity coefficient does not measure assortativity in any network with sufficiently heavy-tailed degree distributions, which is typical for real-world networks. Moreover, we find that other single-valued assortativity coefficients also do not sufficiently capture the wiring preferences of nodes, which often vary greatly by node degree. We therefore take a more fine-grained approach, analyzing a wide range of conditional and joint weight and degree distributions of connected nodes, both numerically in real-world networks and mathematically in the generative graph models. We provide several methods of visualizing the results. We show that the generative models are assortativity-neutral, while many real-world networks are not. Therefore, we also propose an extension of the GIRG model which retains the manifold desirable properties induced by the degree distribution and the latent space, but also exhibits tunable assortativity. We analyze the resulting model mathematically, and give a fine-grained quantification of its assortativity.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.04604v1" target="_blank">TURA: Tool-Augmented Unified Retrieval Agent for AI Search</a></h3>
                    <p><strong>Authors:</strong> Zhejun Zhao, Yuehu Dong, Alley Liu, Lixue Zheng, Pingsheng Liu, Dongdong Shen, Long Xia, Jiashu Zhao, Dawei Yin</p>
                    <p><strong>Published:</strong> 8/6/2025</p>
                    <p><strong>Categories:</strong> cs.CL, cs.AI, cs.IR</p>
                    <p><strong>Summary:</strong> The advent of Large Language Models (LLMs) is transforming search engines into conversational AI search products, primarily using Retrieval-Augmented Generation (RAG) on web corpora. However, this paradigm has significant industrial limitations. Traditional RAG approaches struggle with real-time needs and structured queries that require accessing dynamically generated content like ticket availability or inventory. Limited to indexing static pages, search engines cannot perform the interactive queries needed for such time-sensitive data. Academic research has focused on optimizing RAG for static content, overlooking complex intents and the need for dynamic sources like databases and real-time APIs. To bridge this gap, we introduce TURA (Tool-Augmented Unified Retrieval Agent for AI Search), a novel three-stage framework that combines RAG with agentic tool-use to access both static content and dynamic, real-time information. TURA has three key components: an Intent-Aware Retrieval module to decompose queries and retrieve information sources encapsulated as Model Context Protocol (MCP) Servers, a DAG-based Task Planner that models task dependencies as a Directed Acyclic Graph (DAG) for optimal parallel execution, and a lightweight Distilled Agent Executor for efficient tool calling. TURA is the first architecture to systematically bridge the gap between static RAG and dynamic information sources for a world-class AI search product. Serving tens of millions of users, it leverages an agentic framework to deliver robust, real-time answers while meeting the low-latency demands of a large-scale industrial system.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.04586v1" target="_blank">Position: The Current AI Conference Model is Unsustainable! Diagnosing the Crisis of Centralized AI Conference</a></h3>
                    <p><strong>Authors:</strong> Nuo Chen, Moming Duan, Andre Huikai Lin, Qian Wang, Jiaying Wu, Bingsheng He</p>
                    <p><strong>Published:</strong> 8/6/2025</p>
                    <p><strong>Categories:</strong> cs.CY, cs.AI, cs.CL</p>
                    <p><strong>Summary:</strong> Artificial Intelligence (AI) conferences are essential for advancing research, sharing knowledge, and fostering academic community. However, their rapid expansion has rendered the centralized conference model increasingly unsustainable. This paper offers a data-driven diagnosis of a structural crisis that threatens the foundational goals of scientific dissemination, equity, and community well-being. We identify four key areas of strain: (1) scientifically, with per-author publication rates more than doubling over the past decade to over 4.5 papers annually; (2) environmentally, with the carbon footprint of a single conference exceeding the daily emissions of its host city; (3) psychologically, with 71% of online community discourse reflecting negative sentiment and 35% referencing mental health concerns; and (4) logistically, with attendance at top conferences such as NeurIPS 2024 beginning to outpace venue capacity. These pressures point to a system that is misaligned with its core mission. In response, we propose the Community-Federated Conference (CFC) model, which separates peer review, presentation, and networking into globally coordinated but locally organized components, offering a more sustainable, inclusive, and resilient path forward for AI research.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.04585v1" target="_blank">UniTalker: Conversational Speech-Visual Synthesis</a></h3>
                    <p><strong>Authors:</strong> Yifan Hu, Rui Liu, Yi Ren, Xiang Yin, Haizhou Li</p>
                    <p><strong>Published:</strong> 8/6/2025</p>
                    <p><strong>Categories:</strong> eess.AS</p>
                    <p><strong>Summary:</strong> Conversational Speech Synthesis (CSS) is a key task in the user-agent interaction area, aiming to generate more expressive and empathetic speech for users. However, it is well-known that listening and eye contact play crucial roles in conveying emotions during real-world interpersonal communication. Existing CSS research is limited to perceiving only text and speech within the dialogue context, which restricts its effectiveness. Moreover, speech-only responses further constrain the interactive experience. To address these limitations, we introduce a Conversational Speech-Visual Synthesis (CSVS) task as an extension of traditional CSS. By leveraging multimodal dialogue context, it provides users with coherent audiovisual responses. To this end, we develop a CSVS system named UniTalker, which is a unified model that seamlessly integrates multimodal perception and multimodal rendering capabilities. Specifically, it leverages a large-scale language model to comprehensively understand multimodal cues in the dialogue context, including speaker, text, speech, and the talking-face animations. After that, it employs multi-task sequence prediction to first infer the target utterances emotion and then generate empathetic speech and natural talking-face animations. To ensure that the generated speech-visual content remains consistent in terms of emotion, content, and duration, we introduce three key optimizations: 1) Designing a specialized neural landmark codec to tokenize and reconstruct facial expression sequences. 2) Proposing a bimodal speech-visual hard alignment decoding strategy. 3) Applying emotion-guided rendering during the generation stage. Comprehensive objective and subjective experiments demonstrate that our model synthesizes more empathetic speech and provides users with more natural and emotionally consistent talking-face animations.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.04583v1" target="_blank">Measuring the Carbon Footprint of Cryptographic Privacy-Enhancing Technologies</a></h3>
                    <p><strong>Authors:</strong> Marc Damie, Mihai Pop, Merijn Posthuma</p>
                    <p><strong>Published:</strong> 8/6/2025</p>
                    <p><strong>Categories:</strong> cs.CR</p>
                    <p><strong>Summary:</strong> Privacy-enhancing technologies (PETs) have attracted significant attention in response to privacy regulations, driving the development of applications that prioritize user data protection. At the same time, the information and communication technology (ICT) sector faces growing pressure to reduce its environmental footprint, particularly its carbon emissions. While numerous studies have assessed the energy footprint of various ICT applications, the environmental footprint of cryptographic PETs remains largely unexplored. Our work addresses this gap by proposing a standardized methodology for evaluating the carbon footprint of PETs. To demonstrate this methodology, we focus on PETs supporting client-server applications as they are the simplest to deploy. In particular, we measure the energy consumption and carbon footprint increase induced by five cryptographic PETs (compared to their non-private equivalent): HTTPS web browsing, encrypted machine learning (ML) inference, encrypted ML training, encrypted databases, and encrypted emails. Our findings reveal significant variability in carbon footprint increases, ranging from a twofold increase in HTTPS web browsing to a 100,000-fold increase in encrypted ML. Our study provides essential data to help decision-makers assess privacy-carbon trade-offs in such applications. Finally, we outline key research directions for developing PETs that balance strong privacy protection with environmental sustainability.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.04581v1" target="_blank">Share Your Attention: Transformer Weight Sharing via Matrix-based Dictionary Learning</a></h3>
                    <p><strong>Authors:</strong> Magauiya Zhussip, Dmitriy Shopkhoev, Ammar Ali, Stamatios Lefkimmiatis</p>
                    <p><strong>Published:</strong> 8/6/2025</p>
                    <p><strong>Categories:</strong> cs.CL, cs.AI</p>
                    <p><strong>Summary:</strong> Large language models (LLMs) have revolutionized AI applications, yet their high computational and memory demands hinder their widespread deployment. Existing compression techniques focus on intra-block optimizations (e.g. low-rank approximation, attention head pruning), while the repetitive layered structure of transformers implies significant inter-block redundancy - a dimension largely unexplored beyond key-value (KV) caching. Inspired by dictionary learning in CNNs, we propose a framework for structured weight sharing across transformer layers. Our approach decomposes attention projection matrices into shared dictionary atoms, reducing the attention modules parameters by 66.7% while achieving on-par performance. Unlike complex methods requiring distillation or architectural changes, MASA (Matrix Atom Sharing in Attention) operates as a drop-in replacement - trained with standard optimizers - and represents each layers weights as linear combinations of shared matrix atoms. Experiments across scales (100M-700M parameters) show that MASA achieves better benchmark accuracy and perplexity than grouped-query attention (GQA), low-rank baselines and recently proposed Repeat-all-over/Sequential sharing at comparable parameter budgets. Ablation studies confirm robustness to the dictionary size and the efficacy of shared representations in capturing cross-layer statistical regularities. Extending to Vision Transformers (ViT), MASA matches performance metrics on image classification and detection tasks with 66.7% fewer attention parameters. By combining dictionary learning strategies with transformer efficiency, MASA offers a scalable blueprint for parameter-efficient models without sacrificing performance. Finally, we investigate the possibility of employing MASA on pretrained LLMs to reduce their number of parameters without experiencing any significant drop in their performance.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.04576v1" target="_blank">ConfProBench: A Confidence Evaluation Benchmark for MLLM-Based Process Judges</a></h3>
                    <p><strong>Authors:</strong> Yue Zhou, Yi Chang, Yuan Wu</p>
                    <p><strong>Published:</strong> 8/6/2025</p>
                    <p><strong>Categories:</strong> cs.AI, I.2.6; I.2.7; D.2.8</p>
                    <p><strong>Summary:</strong> Reasoning is a critical capability of multimodal large language models (MLLMs) for solving complex multimodal tasks, and judging the correctness of reasoning steps is crucial for improving this capability. Recently, MLLM-based process judges (MPJs) have been widely used to assess the correctness of reasoning steps in multimodal tasks. Therefore, evaluating MPJs is important for identifying their limitations and guiding future improvements. However, existing benchmarks for MPJs mainly focus on tasks such as step correctness classification and reasoning process search, while overlooking a key aspect: whether the confidence scores produced by MPJs at the step level are reliable. To address this gap, we propose ConfProBench, the first comprehensive benchmark designed to systematically evaluate the reliability of step-level confidence scores generated by MPJs. Our benchmark constructs three types of adversarially perturbed reasoning steps: Synonym Substitution, Syntactic Transformation, and Image Perturbation, to test the robustness of MPJ confidence under perturbations. In addition, we introduce three novel evaluation metrics: Confidence Robustness Score (CRS), Confidence Sensitivity Score (CSS), and Confidence Calibration Score (CCS), which evaluate robustness, sensitivity, and calibration, respectively. We evaluate 14 state-of-the-art MLLMs, including both proprietary and open-source models. Experiments reveal limitations in current MPJs confidence performance and offer competitive baselines to support future research.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.04575v1" target="_blank">Beyond Brainstorming: What Drives High-Quality Scientific Ideas? Lessons from Multi-Agent Collaboration</a></h3>
                    <p><strong>Authors:</strong> Nuo Chen, Yicheng Tong, Jiaying Wu, Minh Duc Duong, Qian Wang, Qingyun Zou, Bryan Hooi, Bingsheng He</p>
                    <p><strong>Published:</strong> 8/6/2025</p>
                    <p><strong>Categories:</strong> cs.CL, cs.AI, cs.CY</p>
                    <p><strong>Summary:</strong> While AI agents show potential in scientific ideation, most existing frameworks rely on single-agent refinement, limiting creativity due to bounded knowledge and perspective. Inspired by real-world research dynamics, this paper investigates whether structured multi-agent discussions can surpass solitary ideation. We propose a cooperative multi-agent framework for generating research proposals and systematically compare configurations including group size, leaderled versus leaderless structures, and team compositions varying in interdisciplinarity and seniority. To assess idea quality, we employ a comprehensive protocol with agent-based scoring and human review across dimensions such as novelty, strategic vision, and integration depth. Our results show that multi-agent discussions substantially outperform solitary baselines. A designated leader acts as a catalyst, transforming discussion into more integrated and visionary proposals. Notably, we find that cognitive diversity is a primary driver of quality, yet expertise is a non-negotiable prerequisite, as teams lacking a foundation of senior knowledge fail to surpass even a single competent agent. These findings offer actionable insights for designing collaborative AI ideation systems and shed light on how team structure influences creative outcomes.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.04571v1" target="_blank">Do Recommender Systems Really Leverage Multimodal Content? A Comprehensive Analysis on Multimodal Representations for Recommendation</a></h3>
                    <p><strong>Authors:</strong> Claudio Pomo, Matteo Attimonelli, Danilo Danese, Fedelucio Narducci, Tommaso Di Noia</p>
                    <p><strong>Published:</strong> 8/6/2025</p>
                    <p><strong>Categories:</strong> cs.IR, cs.CL, cs.LG</p>
                    <p><strong>Summary:</strong> Multimodal Recommender Systems aim to improve recommendation accuracy by integrating heterogeneous content, such as images and textual metadata. While effective, it remains unclear whether their gains stem from true multimodal understanding or increased model complexity. This work investigates the role of multimodal item embeddings, emphasizing the semantic informativeness of the representations. Initial experiments reveal that embeddings from standard extractors (e.g., ResNet50, Sentence-Bert) enhance performance, but rely on modality-specific encoders and ad hoc fusion strategies that lack control over cross-modal alignment. To overcome these limitations, we leverage Large Vision-Language Models (LVLMs) to generate multimodal-by-design embeddings via structured prompts. This approach yields semantically aligned representations without requiring any fusion. Experiments across multiple settings show notable performance improvements. Furthermore, LVLMs embeddings offer a distinctive advantage: they can be decoded into structured textual descriptions, enabling direct assessment of their multimodal comprehension. When such descriptions are incorporated as side content into recommender systems, they improve recommendation performance, empirically validating the semantic depth and alignment encoded within LVLMs outputs. Our study highlights the importance of semantically rich representations and positions LVLMs as a compelling foundation for building robust and meaningful multimodal representations in recommendation tasks.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.04564v1" target="_blank">Drone Detection with Event Cameras</a></h3>
                    <p><strong>Authors:</strong> Gabriele Magrini, Lorenzo Berlincioni, Luca Cultrera, Federico Becattini, Pietro Pala</p>
                    <p><strong>Published:</strong> 8/6/2025</p>
                    <p><strong>Categories:</strong> cs.CV</p>
                    <p><strong>Summary:</strong> The diffusion of drones presents significant security and safety challenges. Traditional surveillance systems, particularly conventional frame-based cameras, struggle to reliably detect these targets due to their small size, high agility, and the resulting motion blur and poor performance in challenging lighting conditions. This paper surveys the emerging field of event-based vision as a robust solution to these problems. Event cameras virtually eliminate motion blur and enable consistent detection in extreme lighting. Their sparse, asynchronous output suppresses static backgrounds, enabling low-latency focus on motion cues. We review the state-of-the-art in event-based drone detection, from data representation methods to advanced processing pipelines using spiking neural networks. The discussion extends beyond simple detection to cover more sophisticated tasks such as real-time tracking, trajectory forecasting, and unique identification through propeller signature analysis. By examining current methodologies, available datasets, and the distinct advantages of the technology, this work demonstrates that event-based vision provides a powerful foundation for the next generation of reliable, low-latency, and efficient counter-UAV systems.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.04542v1" target="_blank">Privacy Risk Predictions Based on Fundamental Understanding of Personal Data and an Evolving Threat Landscape</a></h3>
                    <p><strong>Authors:</strong> Haoran Niu, K. Suzanne Barber</p>
                    <p><strong>Published:</strong> 8/6/2025</p>
                    <p><strong>Categories:</strong> cs.LG, cs.CR, cs.SI</p>
                    <p><strong>Summary:</strong> It is difficult for individuals and organizations to protect personal information without a fundamental understanding of relative privacy risks. By analyzing over 5,000 empirical identity theft and fraud cases, this research identifies which types of personal data are exposed, how frequently exposures occur, and what the consequences of those exposures are. We construct an Identity Ecosystem graph--a foundational, graph-based model in which nodes represent personally identifiable information (PII) attributes and edges represent empirical disclosure relationships between them (e.g., the probability that one PII attribute is exposed due to the exposure of another). Leveraging this graph structure, we develop a privacy risk prediction framework that uses graph theory and graph neural networks to estimate the likelihood of further disclosures when certain PII attributes are compromised. The results show that our approach effectively answers the core question: Can the disclosure of a given identity attribute possibly lead to the disclosure of another attribute?</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.04540v1" target="_blank">InceptoFormer: A Multi-Signal Neural Framework for Parkinsons Disease Severity Evaluation from Gait</a></h3>
                    <p><strong>Authors:</strong> Safwen Naimi, Arij Said, Wassim Bouachir, Guillaume-Alexandre Bilodeau</p>
                    <p><strong>Published:</strong> 8/6/2025</p>
                    <p><strong>Categories:</strong> cs.CV</p>
                    <p><strong>Summary:</strong> We present InceptoFormer, a multi-signal neural framework designed for Parkinsons Disease (PD) severity evaluation via gait dynamics analysis. Our architecture introduces a 1D adaptation of the Inception model, which we refer to as Inception1D, along with a Transformer-based framework to stage PD severity according to the Hoehn and Yahr (HY) scale. The Inception1D component captures multi-scale temporal features by employing parallel 1D convolutional filters with varying kernel sizes, thereby extracting features across multiple temporal scales. The transformer component efficiently models long-range dependencies within gait sequences, providing a comprehensive understanding of both local and global patterns. To address the issue of class imbalance in PD severity staging, we propose a data structuring and preprocessing strategy based on oversampling to enhance the representation of underrepresented severity levels. The overall design enables to capture fine-grained temporal variations and global dynamics in gait signal, significantly improving classification performance for PD severity evaluation. Through extensive experimentation, InceptoFormer achieves an accuracy of 96.6%, outperforming existing state-of-the-art methods in PD severity assessment. The source code for our implementation is publicly available at https://github.com/SafwenNaimi/InceptoFormer</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.04538v1" target="_blank">Bridging Simulation and Experiment: A Self-Supervised Domain Adaptation Framework for Concrete Damage Classification</a></h3>
                    <p><strong>Authors:</strong> Chen Xu, Giao Vu, Ba Trung Cao, Zhen Liu, Fabian Diewald, Yong Yuan, GÃ¼nther Meschke</p>
                    <p><strong>Published:</strong> 8/6/2025</p>
                    <p><strong>Categories:</strong> cs.CE</p>
                    <p><strong>Summary:</strong> Reliable assessment of concrete degradation is critical for ensuring structural safety and longevity of engineering structures. This study proposes a self-supervised domain adaptation framework for robust concrete damage classification using coda wave signals. To support this framework, an advanced virtual testing platform is developed, combining multiscale modeling of concrete degradation with ultrasonic wave propagation simulations. This setup enables the generation of large-scale labeled synthetic data under controlled conditions, reducing the dependency on costly and time-consuming experimental labeling. However, neural networks trained solely on synthetic data often suffer from degraded performance when applied to experimental data due to domain shifts. To bridge this domain gap, the proposed framework integrates domain adversarial training, minimum class confusion loss, and the Bootstrap Your Own Latent (BYOL) strategy. These components work jointly to facilitate effective knowledge transfer from the labeled simulation domain to the unlabeled experimental domain, achieving accurate and reliable damage classification in concrete. Extensive experiments demonstrate that the proposed method achieves notable performance improvements, reaching an accuracy of 0.7762 and a macro F1 score of 0.7713, outperforming both the plain 1D CNN baseline and six representative domain adaptation techniques. Moreover, the method exhibits high robustness across training runs and introduces only minimal additional computational cost. These findings highlight the practical potential of the proposed simulation-driven and label-efficient framework for real-world applications in structural health monitoring.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.04534v1" target="_blank">No Masks Needed: Explainable AI for Deriving Segmentation from Classification</a></h3>
                    <p><strong>Authors:</strong> Mosong Ma, Tania Stathaki, Michalis Lazarou</p>
                    <p><strong>Published:</strong> 8/6/2025</p>
                    <p><strong>Categories:</strong> cs.CV</p>
                    <p><strong>Summary:</strong> Medical image segmentation is vital for modern healthcare and is a key element of computer-aided diagnosis. While recent advancements in computer vision have explored unsupervised segmentation using pre-trained models, these methods have not been translated well to the medical imaging domain. In this work, we introduce a novel approach that fine-tunes pre-trained models specifically for medical images, achieving accurate segmentation with extensive processing. Our method integrates Explainable AI to generate relevance scores, enhancing the segmentation process. Unlike traditional methods that excel in standard benchmarks but falter in medical applications, our approach achieves improved results on datasets like CBIS-DDSM, NuInsSeg and Kvasir-SEG.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.04531v1" target="_blank">Unveiling the Landscape of Clinical Depression Assessment: From Behavioral Signatures to Psychiatric Reasoning</a></h3>
                    <p><strong>Authors:</strong> Zhuang Chen, Guanqun Bi, Wen Zhang, Jiawei Hu, Aoyun Wang, Xiyao Xiao, Kun Feng, Minlie Huang</p>
                    <p><strong>Published:</strong> 8/6/2025</p>
                    <p><strong>Categories:</strong> cs.CL, cs.AI</p>
                    <p><strong>Summary:</strong> Depression is a widespread mental disorder that affects millions worldwide. While automated depression assessment shows promise, most studies rely on limited or non-clinically validated data, and often prioritize complex model design over real-world effectiveness. In this paper, we aim to unveil the landscape of clinical depression assessment. We introduce C-MIND, a clinical neuropsychiatric multimodal diagnosis dataset collected over two years from real hospital visits. Each participant completes three structured psychiatric tasks and receives a final diagnosis from expert clinicians, with informative audio, video, transcript, and functional near-infrared spectroscopy (fNIRS) signals recorded. Using C-MIND, we first analyze behavioral signatures relevant to diagnosis. We train a range of classical models to quantify how different tasks and modalities contribute to diagnostic performance, and dissect the effectiveness of their combinations. We then explore whether LLMs can perform psychiatric reasoning like clinicians and identify their clear limitations in realistic clinical settings. In response, we propose to guide the reasoning process with clinical expertise and consistently improves LLM diagnostic performance by up to 10% in Macro-F1 score. We aim to build an infrastructure for clinical depression assessment from both data and algorithmic perspectives, enabling C-MIND to facilitate grounded and reliable research for mental healthcare.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.04526v1" target="_blank">Policy Design in Zero-Trust Distributed Networks: Challenges and Solutions</a></h3>
                    <p><strong>Authors:</strong> Fannya R. Sandjaja, Ayesha A. Majeed, Abdullah Abdullah, Gyan Wickremasinghe, Karen Rafferty, Vishal Sharma</p>
                    <p><strong>Published:</strong> 8/6/2025</p>
                    <p><strong>Categories:</strong> cs.NI, cs.DC</p>
                    <p><strong>Summary:</strong> Traditional security architectures are becoming more vulnerable to distributed attacks due to significant dependence on trust. This will further escalate when implementing agentic AI within the systems, as more components must be secured over a similar distributed space. These scenarios can be observed in consumer technologies, such as the dense Internet of things (IoT). Here, zero-trust architecture (ZTA) can be seen as a potential solution, which relies on a key principle of not giving users explicit trust, instead always verifying their privileges whenever a request is made. However, the overall security in ZTA is managed through its policies, and unverified policies can lead to unauthorized access. Thus, this paper explores challenges and solutions for ZTA policy design in the context of distributed networks, which is referred to as zero-trust distributed networks (ZTDN). This is followed by a case-study on formal verification of policies using UPPAAL. Subsequently, the importance of accountability and responsibility in the systems security is discussed.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.04524v1" target="_blank">RAIDX: A Retrieval-Augmented Generation and GRPO Reinforcement Learning Framework for Explainable Deepfake Detection</a></h3>
                    <p><strong>Authors:</strong> Tianxiao Li, Zhenglin Huang, Haiquan Wen, Yiwei He, Shuchang Lyu, Baoyuan Wu, Guangliang Cheng</p>
                    <p><strong>Published:</strong> 8/6/2025</p>
                    <p><strong>Categories:</strong> cs.CV, cs.AI</p>
                    <p><strong>Summary:</strong> The rapid advancement of AI-generation models has enabled the creation of hyperrealistic imagery, posing ethical risks through widespread misinformation. Current deepfake detection methods, categorized as face specific detectors or general AI-generated detectors, lack transparency by framing detection as a classification task without explaining decisions. While several LLM-based approaches offer explainability, they suffer from coarse-grained analyses and dependency on labor-intensive annotations. This paper introduces RAIDX (Retrieval-Augmented Image Deepfake Detection and Explainability), a novel deepfake detection framework integrating Retrieval-Augmented Generation (RAG) and Group Relative Policy Optimization (GRPO) to enhance detection accuracy and decision explainability. Specifically, RAIDX leverages RAG to incorporate external knowledge for improved detection accuracy and employs GRPO to autonomously generate fine-grained textual explanations and saliency maps, eliminating the need for extensive manual annotations. Experiments on multiple benchmarks demonstrate RAIDXs effectiveness in identifying real or fake, and providing interpretable rationales in both textual descriptions and saliency maps, achieving state-of-the-art detection performance while advancing transparency in deepfake identification. RAIDX represents the first unified framework to synergize RAG and GRPO, addressing critical gaps in accuracy and explainability. Our code and models will be publicly available.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.04522v1" target="_blank">Conditional Fetal Brain Atlas Learning for Automatic Tissue Segmentation</a></h3>
                    <p><strong>Authors:</strong> Johannes Tischer, Patric Kienast, Marlene StÃ¼mpflen, Gregor Kasprian, Georg Langs, Roxane Licandro</p>
                    <p><strong>Published:</strong> 8/6/2025</p>
                    <p><strong>Categories:</strong> eess.IV, cs.CV, cs.LG, 68T07 (Primary) 92C50 (Secondary), I.4.9; I.4.6; I.2.0</p>
                    <p><strong>Summary:</strong> Magnetic Resonance Imaging (MRI) of the fetal brain has become a key tool for studying brain development in vivo. Yet, its assessment remains challenging due to variability in brain maturation, imaging protocols, and uncertain estimates of Gestational Age (GA). To overcome these, brain atlases provide a standardized reference framework that facilitates objective evaluation and comparison across subjects by aligning the atlas and subjects in a common coordinate system. In this work, we introduce a novel deep-learning framework for generating continuous, age-specific fetal brain atlases for real-time fetal brain tissue segmentation. The framework combines a direct registration model with a conditional discriminator. Trained on a curated dataset of 219 neurotypical fetal MRIs spanning from 21 to 37 weeks of gestation. The method achieves high registration accuracy, captures dynamic anatomical changes with sharp structural detail, and robust segmentation performance with an average Dice Similarity Coefficient (DSC) of 86.3% across six brain tissues. Furthermore, volumetric analysis of the generated atlases reveals detailed neurotypical growth trajectories, providing valuable insights into the maturation of the fetal brain. This approach enables individualized developmental assessment with minimal pre-processing and real-time performance, supporting both research and clinical applications. The model code is available at https://github.com/cirmuw/fetal-brain-atlas</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.04512v1" target="_blank">Pitfalls and Limits in Automatic Dementia Assessment</a></h3>
                    <p><strong>Authors:</strong> Franziska Braun, Christopher Witzl, Andreas Erzigkeit, Hartmut Lehfeld, Thomas Hillemacher, Tobias Bocklet, Korbinian Riedhammer</p>
                    <p><strong>Published:</strong> 8/6/2025</p>
                    <p><strong>Categories:</strong> eess.AS</p>
                    <p><strong>Summary:</strong> Current work on speech-based dementia assessment focuses on either feature extraction to predict assessment scales, or on the automation of existing test procedures. Most research uses public data unquestioningly and rarely performs a detailed error analysis, focusing primarily on numerical performance. We perform an in-depth analysis of an automated standardized dementia assessment, the Syndrom-Kurz-Test. We find that while there is a high overall correlation with human annotators, due to certain artifacts, we observe high correlations for the severely impaired individuals, which is less true for the healthy or mildly impaired ones. Speech production decreases with cognitive decline, leading to overoptimistic correlations when test scoring relies on word naming. Depending on the test design, fallback handling introduces further biases that favor certain groups. These pitfalls remain independent of group distributions in datasets and require differentiated analysis of target groups.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.04511v1" target="_blank">Argumentative Debates for Transparent Bias Detection [Technical Report]</a></h3>
                    <p><strong>Authors:</strong> Hamed Ayoobi, Nico Potyka, Anna Rapberger, Francesca Toni</p>
                    <p><strong>Published:</strong> 8/6/2025</p>
                    <p><strong>Categories:</strong> cs.AI, cs.LG</p>
                    <p><strong>Summary:</strong> As the use of AI systems in society grows, addressing potential biases that emerge from data or are learned by models is essential to prevent systematic disadvantages against specific groups. Several notions of (un)fairness have been proposed in the literature, alongside corresponding algorithmic methods for detecting and mitigating unfairness, but, with very few exceptions, these tend to ignore transparency. Instead, interpretability and explainability are core requirements for algorithmic fairness, even more so than for other algorithmic solutions, given the human-oriented nature of fairness. In this paper, we contribute a novel interpretable, explainable method for bias detection relying on debates about the presence of bias against individuals, based on the values of protected features for the individuals and others in their neighbourhoods. Our method builds upon techniques from formal and computational argumentation, whereby debates result from arguing about biases within and across neighbourhoods. We provide formal, quantitative, and qualitative evaluations of our method, highlighting its strengths in performance against baselines, as well as its interpretability and explainability.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.04504v1" target="_blank">Moving beyond harm. A critical review of how NLP research approaches discrimination</a></h3>
                    <p><strong>Authors:</strong> Katrin Schulz, Marjolein Lanzing, Giulia Martinez Brenner</p>
                    <p><strong>Published:</strong> 8/6/2025</p>
                    <p><strong>Categories:</strong> cs.CY</p>
                    <p><strong>Summary:</strong> How to avoid discrimination in the context of NLP technology is one of the major challenges in the field. We propose that a different and more substantiated framing of the problem could help to find more productive approaches. In the first part of the paper we report on a case study: a qualitative review on papers published in the ACL anthologies 2022 on discriminatory behavior of NLP systems. We find that the field (i) still has a strong focus on a technological fix of algorithmic discrimination, and (ii) is struggling with a firm grounding of their ethical or normative vocabulary. Furthermore, this vocabulary is very limited, focusing mostly on the terms harm and bias. In the second part of the paper we argue that addressing the latter problems might help with the former. The understanding of algorithmic discrimination as a technological problem is reflected in and reproduced by the vocabulary in use. The notions of harm and bias implicate a narrow framing of the issue of discrimination as one of the system-user interface. We argue that instead of harm the debate should make injustice the key notion. This would force us to understand the problem of algorithmic discrimination as a systemic problem. Thereby, it would broaden our perspective on the complex interactions that make NLP technology participate in discrimination. With that gain in perspective we can consider new angles for solutions.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.04492v1" target="_blank">Learning Robust Intervention Representations with Delta Embeddings</a></h3>
                    <p><strong>Authors:</strong> Panagiotis Alimisis, Christos Diou</p>
                    <p><strong>Published:</strong> 8/6/2025</p>
                    <p><strong>Categories:</strong> cs.CV, cs.AI</p>
                    <p><strong>Summary:</strong> Causal representation learning has attracted significant research interest during the past few years, as a means for improving model generalization and robustness. Causal representations of interventional image pairs, have the property that only variables corresponding to scene elements affected by the intervention / action are changed between the start state and the end state. While most work in this area has focused on identifying and representing the variables of the scene under a causal model, fewer efforts have focused on representations of the interventions themselves. In this work, we show that an effective strategy for improving out of distribution (OOD) robustness is to focus on the representation of interventions in the latent space. Specifically, we propose that an intervention can be represented by a Causal Delta Embedding that is invariant to the visual scene and sparse in terms of the causal variables it affects. Leveraging this insight, we propose a framework that is capable of learning causal representations from image pairs, without any additional supervision. Experiments in the Causal Triplet challenge demonstrate that Causal Delta Embeddings are highly effective in OOD settings, significantly exceeding baseline performance in both synthetic and real-world benchmarks.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.04491v1" target="_blank">OpenDCVCs: A PyTorch Open Source Implementation and Performance Evaluation of the DCVC series Video Codecs</a></h3>
                    <p><strong>Authors:</strong> Yichi Zhang, Fengqing Zhu</p>
                    <p><strong>Published:</strong> 8/6/2025</p>
                    <p><strong>Categories:</strong> eess.IV, cs.CV</p>
                    <p><strong>Summary:</strong> We present OpenDCVCs, an open-source PyTorch implementation designed to advance reproducible research in learned video compression. OpenDCVCs provides unified and training-ready implementations of four representative Deep Contextual Video Compression (DCVC) models--DCVC, DCVC with Temporal Context Modeling (DCVC-TCM), DCVC with Hybrid Entropy Modeling (DCVC-HEM), and DCVC with Diverse Contexts (DCVC-DC). While the DCVC series achieves substantial bitrate reductions over both classical codecs and advanced learned models, previous public code releases have been limited to evaluation codes, presenting significant barriers to reproducibility, benchmarking, and further development. OpenDCVCs bridges this gap by offering a comprehensive, self-contained framework that supports both end-to-end training and evaluation for all included algorithms. The implementation includes detailed documentation, evaluation protocols, and extensive benchmarking results across diverse datasets, providing a transparent and consistent foundation for comparison and extension. All code and experimental tools are publicly available at https://gitlab.com/viper-purdue/opendcvcs, empowering the community to accelerate research and foster collaboration.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.04482v1" target="_blank">OS Agents: A Survey on MLLM-based Agents for General Computing Devices Use</a></h3>
                    <p><strong>Authors:</strong> Xueyu Hu, Tao Xiong, Biao Yi, Zishu Wei, Ruixuan Xiao, Yurun Chen, Jiasheng Ye, Meiling Tao, Xiangxin Zhou, Ziyu Zhao, Yuhuai Li, Shengze Xu, Shenzhi Wang, Xinchen Xu, Shuofei Qiao, Zhaokai Wang, Kun Kuang, Tieyong Zeng, Liang Wang, Jiwei Li, Yuchen Eleanor Jiang, Wangchunshu Zhou, Guoyin Wang, Keting Yin, Zhou Zhao, Hongxia Yang, Fan Wu, Shengyu Zhang, Fei Wu</p>
                    <p><strong>Published:</strong> 8/6/2025</p>
                    <p><strong>Categories:</strong> cs.AI, cs.CL, cs.CV, cs.LG</p>
                    <p><strong>Summary:</strong> The dream to create AI assistants as capable and versatile as the fictional J.A.R.V.I.S from Iron Man has long captivated imaginations. With the evolution of (multi-modal) large language models ((M)LLMs), this dream is closer to reality, as (M)LLM-based Agents using computing devices (e.g., computers and mobile phones) by operating within the environments and interfaces (e.g., Graphical User Interface (GUI)) provided by operating systems (OS) to automate tasks have significantly advanced. This paper presents a comprehensive survey of these advanced agents, designated as OS Agents. We begin by elucidating the fundamentals of OS Agents, exploring their key components including the environment, observation space, and action space, and outlining essential capabilities such as understanding, planning, and grounding. We then examine methodologies for constructing OS Agents, focusing on domain-specific foundation models and agent frameworks. A detailed review of evaluation protocols and benchmarks highlights how OS Agents are assessed across diverse tasks. Finally, we discuss current challenges and identify promising directions for future research, including safety and privacy, personalization and self-evolution. This survey aims to consolidate the state of OS Agents research, providing insights to guide both academic inquiry and industrial development. An open-source GitHub repository is maintained as a dynamic resource to foster further innovation in this field. We present a 9-page version of our work, accepted by ACL 2025, to provide a concise overview to the domain.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.04479v1" target="_blank">Manifestations of Empathy in Software Engineering: How, Why, and When It Matters</a></h3>
                    <p><strong>Authors:</strong> Hashini Gunatilake, John Grundy, Rashina Hoda, Ingo Mueller</p>
                    <p><strong>Published:</strong> 8/6/2025</p>
                    <p><strong>Categories:</strong> cs.SE</p>
                    <p><strong>Summary:</strong> Empathy plays a crucial role in software engineering (SE), influencing collaboration, communication, and decision-making. While prior research has highlighted the importance of empathy in SE, there is limited understanding of how empathy manifests in SE practice, what motivates SE practitioners to demonstrate empathy, and the factors that influence empathy in SE work. Our study explores these aspects through 22 interviews and a large scale survey with 116 software practitioners. Our findings provide insights into the expression of empathy in SE, the drivers behind empathetic practices, SE activities where empathy is perceived as useful or not, and the other factors that influence empathy. In addition, we offer practical implications for SE practitioners and researchers, offering a deeper understanding of how to effectively integrate empathy into SE processes.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.04470v1" target="_blank">FedHiP: Heterogeneity-Invariant Personalized Federated Learning Through Closed-Form Solutions</a></h3>
                    <p><strong>Authors:</strong> Jianheng Tang, Zhirui Yang, Jingchao Wang, Kejia Fan, Jinfeng Xu, Huiping Zhuang, Anfeng Liu, Houbing Herbert Song, Leye Wang, Yunhuai Liu</p>
                    <p><strong>Published:</strong> 8/6/2025</p>
                    <p><strong>Categories:</strong> cs.LG</p>
                    <p><strong>Summary:</strong> Lately, Personalized Federated Learning (PFL) has emerged as a prevalent paradigm to deliver personalized models by collaboratively training while simultaneously adapting to each clients local applications. Existing PFL methods typically face a significant challenge due to the ubiquitous data heterogeneity (i.e., non-IID data) across clients, which severely hinders convergence and degrades performance. We identify that the root issue lies in the long-standing reliance on gradient-based updates, which are inherently sensitive to non-IID data. To fundamentally address this issue and bridge the research gap, in this paper, we propose a Heterogeneity-invariant Personalized Federated learning scheme, named FedHiP, through analytical (i.e., closed-form) solutions to avoid gradient-based updates. Specifically, we exploit the trend of self-supervised pre-training, leveraging a foundation model as a frozen backbone for gradient-free feature extraction. Following the feature extractor, we further develop an analytic classifier for gradient-free training. To support both collective generalization and individual personalization, our FedHiP scheme incorporates three phases: analytic local training, analytic global aggregation, and analytic local personalization. The closed-form solutions of our FedHiP scheme enable its ideal property of heterogeneity invariance, meaning that each personalized model remains identical regardless of how non-IID the data are distributed across all other clients. Extensive experiments on benchmark datasets validate the superiority of our FedHiP scheme, outperforming the state-of-the-art baselines by at least 5.79%-20.97% in accuracy.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.04469v1" target="_blank">FrEVL: Leveraging Frozen Pretrained Embeddings for Efficient Vision-Language Understanding</a></h3>
                    <p><strong>Authors:</strong> Emmanuelle Bourigault, Pauline Bourigault</p>
                    <p><strong>Published:</strong> 8/6/2025</p>
                    <p><strong>Categories:</strong> cs.CV, cs.CL</p>
                    <p><strong>Summary:</strong> The deployment of vision-language models remains constrained by substantial computational requirements. We present \textbf{FrEVL}, a framework exploring whether frozen pretrained embeddings can support effective vision-language understanding. Our analysis reveals that frozen embeddings contain rich information for discriminative tasks, achieving 85\% to 95\% of state-of-the-art performance on standard benchmarks with only 68.4M trainable parameters. This performance dichotomy reveals a critical insight: frozen embedding effectiveness depends on alignment between pretraining objectives and downstream task requirements. When accounting for end-to-end computation including embedding extraction, FrEVL provides $2.3\times$ speedup with 52\% lower energy consumption, making it suitable for scenarios with pre-computable inputs or when deployment constraints outweigh marginal performance gains. Our evaluation provides practitioners with guidance on when frozen embedding approaches represent viable alternatives to full model deployment. We will release our complete implementation and evaluation framework to facilitate further research into efficient multi-modal understanding.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.04461v1" target="_blank">Small transformer architectures for task switching</a></h3>
                    <p><strong>Authors:</strong> Claudius Gros</p>
                    <p><strong>Published:</strong> 8/6/2025</p>
                    <p><strong>Categories:</strong> cs.LG, cs.AI</p>
                    <p><strong>Summary:</strong> The rapid progress seen in terms of large-scale generative AI is largely based on the attention mechanism. It is conversely non-trivial to conceive small-scale applications for which attention-based architectures outperform traditional approaches, such as multi-layer perceptrons or recurrent networks. We examine this problem in the context of task switching. In this framework models work on ongoing token sequences with the current task being determined by stochastically interspersed control tokens. We show that standard transformers cannot solve a basic task switching reference model based on finite domain arithmetics which contains subtasks dedicated to increment / addition / reverse copy / context (IARC). We show that transformers, long short-term memory recurrent networks (LSTM), and plain multi-layer perceptrons (MLPs) achieve similar, but only modest prediction accuracies. We enlarge our comparative study by including an extension of the standard transformer architecture to its non-translational invariant counterpart, the cisformer, and an alternative attention mechanism, extensive attention. A combination of the latter is found to be the only model able to achieve considerable performance levels, of around 95%. Our results indicate that the workings of attention can be understood better, and even improved, when comparing qualitatively different formulations in task-switching settings.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.04459v1" target="_blank">Case Studies of Generative Machine Learning Models for Dynamical Systems</a></h3>
                    <p><strong>Authors:</strong> Nachiket U. Bapat, Randy C. Paffenroth, Raghvendra V. Cowlagi</p>
                    <p><strong>Published:</strong> 8/6/2025</p>
                    <p><strong>Categories:</strong> eess.SY, cs.NE, cs.SY</p>
                    <p><strong>Summary:</strong> Systems like aircraft and spacecraft are expensive to operate in the real world. The design, validation, and testing for such systems therefore relies on a combination of mathematical modeling, abundant numerical simulations, and a relatively small set of real-world experiments. Due to modeling errors, simplifications, and uncertainties, the data synthesized by simulation models often does not match data from the systems real-world operation. We consider the broad research question of whether this model mismatch can be significantly reduced by generative artificial intelligence models (GAIMs). Unlike text- or image-processing, where generative models have attained recent successes, GAIM development for aerospace engineering applications must not only train with scarce operational data, but their outputs must also satisfy governing equations based on natural laws, e.g., conservation laws. The scope of this paper primarily focuses on two case studies of optimally controlled systems that are commonly understood and employed in aircraft guidance, namely: minimum-time navigation in a wind field and minimum-exposure navigation in a threat field. We report GAIMs that are trained with a relatively small set, of the order of a few hundred, of examples and with underlying governing equations. By focusing on optimally controlled systems, we formulate training loss functions based on invariance of the Hamiltonian function along system trajectories. We investigate three GAIM architectures, namely: the generative adversarial network (GAN) and two variants of the variational autoencoder (VAE). We provide architectural details and thorough performance analyses of these models. The main finding is that our new models, especially the VAE-based models, are able to synthesize data that satisfy the governing equations and are statistically similar to the training data despite small volumes of training data.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.04699v1" target="_blank">Hop, Skip, and Overthink: Diagnosing Why Reasoning Models Fumble during Multi-Hop Analysis</a></h3>
                    <p><strong>Authors:</strong> Anushka Yadav, Isha Nalawade, Srujana Pillarichety, Yashwanth Babu, Reshmi Ghosh, Samyadeep Basu, Wenlong Zhao, Ali Nasaeh, Sriram Balasubramanian, Soundararajan Srinivasan</p>
                    <p><strong>Published:</strong> 8/6/2025</p>
                    <p><strong>Categories:</strong> cs.CL, cs.AI</p>
                    <p><strong>Summary:</strong> The emergence of reasoning models and their integration into practical AI chat bots has led to breakthroughs in solving advanced math, deep search, and extractive question answering problems that requires a complex and multi-step thought process. Yet, a complete understanding of why these models hallucinate more than general purpose language models is missing. In this investigative study, we systematicallyexplore reasoning failures of contemporary language models on multi-hop question answering tasks. We introduce a novel, nuanced error categorization framework that examines failures across three critical dimensions: the diversity and uniqueness of source documents involved (hops), completeness in capturing relevant information (coverage), and cognitive inefficiency (overthinking). Through rigorous hu-man annotation, supported by complementary automated metrics, our exploration uncovers intricate error patterns often hidden by accuracy-centric evaluations. This investigative approach provides deeper insights into the cognitive limitations of current models and offers actionable guidance toward enhancing reasoning fidelity, transparency, and robustness in future language modeling efforts.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.04698v1" target="_blank">FaST: Feature-aware Sampling and Tuning for Personalized Preference Alignment with Limited Data</a></h3>
                    <p><strong>Authors:</strong> Thibaut Thonet, GermÃ¡n Kruszewski, Jos Rozen, Pierre Erbacher, Marc Dymetman</p>
                    <p><strong>Published:</strong> 8/6/2025</p>
                    <p><strong>Categories:</strong> cs.CL</p>
                    <p><strong>Summary:</strong> LLM-powered conversational assistants are often deployed in a one-size-fits-all manner, which fails to accommodate individual user preferences. Recently, LLM personalization -- tailoring models to align with specific user preferences -- has gained increasing attention as a way to bridge this gap. In this work, we specifically focus on a practical yet challenging setting where only a small set of preference annotations can be collected per user -- a problem we define as Personalized Preference Alignment with Limited Data (PPALLI). To support research in this area, we introduce two datasets -- DnD and ELIP -- and benchmark a variety of alignment techniques on them. We further propose FaST, a highly parameter-efficient approach that leverages high-level features automatically discovered from the data, achieving the best overall performance.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.04696v1" target="_blank">Achieving Precise and Reliable Locomotion with Differentiable Simulation-Based System Identification</a></h3>
                    <p><strong>Authors:</strong> Vyacheslav Kovalev, Ekaterina Chaikovskaia, Egor Davydenko, Roman Gorbachev</p>
                    <p><strong>Published:</strong> 8/6/2025</p>
                    <p><strong>Categories:</strong> cs.RO</p>
                    <p><strong>Summary:</strong> Accurate system identification is crucial for reducing trajectory drift in bipedal locomotion, particularly in reinforcement learning and model-based control. In this paper, we present a novel control framework that integrates system identification into the reinforcement learning training loop using differentiable simulation. Unlike traditional approaches that rely on direct torque measurements, our method estimates system parameters using only trajectory data (positions, velocities) and control inputs. We leverage the differentiable simulator MuJoCo-XLA to optimize system parameters, ensuring that simulated robot behavior closely aligns with real-world motion. This framework enables scalable and flexible parameter optimization. Accurate system identification is crucial for reducing trajectory drift in bipedal locomotion, particularly in reinforcement learning and model-based control. In this paper, we present a novel control framework that integrates system identification into the reinforcement learning training loop using differentiable simulation. Unlike traditional approaches that rely on direct torque measurements, our method estimates system parameters using only trajectory data (positions, velocities) and control inputs. We leverage the differentiable simulator MuJoCo-XLA to optimize system parameters, ensuring that simulated robot behavior closely aligns with real-world motion. This framework enables scalable and flexible parameter optimization. It supports fundamental physical properties such as mass and inertia. Additionally, it handles complex system nonlinear behaviors, including advanced friction models, through neural network approximations. Experimental results show that our framework significantly improves trajectory following.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.04691v1" target="_blank">From MAS to MARS: Coordination Failures and Reasoning Trade-offs in Hierarchical Multi-Agent Robotic Systems within a Healthcare Scenario</a></h3>
                    <p><strong>Authors:</strong> Yuanchen Bai, Zijian Ding, Shaoyue Wen, Xiang Chang, Angelique Taylor</p>
                    <p><strong>Published:</strong> 8/6/2025</p>
                    <p><strong>Categories:</strong> cs.RO, cs.AI, cs.MA</p>
                    <p><strong>Summary:</strong> Multi-agent robotic systems (MARS) build upon multi-agent systems by integrating physical and task-related constraints, increasing the complexity of action execution and agent coordination. However, despite the availability of advanced multi-agent frameworks, their real-world deployment on robots remains limited, hindering the advancement of MARS research in practice. To bridge this gap, we conducted two studies to investigate performance trade-offs of hierarchical multi-agent frameworks in a simulated real-world multi-robot healthcare scenario. In Study 1, using CrewAI, we iteratively refine the systems knowledge base, to systematically identify and categorize coordination failures (e.g., tool access violations, lack of timely handling of failure reports) not resolvable by providing contextual knowledge alone. In Study 2, using AutoGen, we evaluate a redesigned bidirectional communication structure and further measure the trade-offs between reasoning and non-reasoning models operating within the same robotic team setting. Drawing from our empirical findings, we emphasize the tension between autonomy and stability and the importance of edge-case testing to improve system reliability and safety for future real-world deployment. Supplementary materials, including codes, task agent setup, trace outputs, and annotated examples of coordination failures and reasoning behaviors, are available at: https://byc-sophie.github.io/mas-to-mars/.</p>
                
            
                
                    <h3><a href="http://dx.doi.org/10.1109/VRW55335.2022.00178" target="_blank">MienCap: Realtime Performance-Based Facial Animation with Live Mood Dynamics</a></h3>
                    <p><strong>Authors:</strong> Ye Pan, Ruisi Zhang, Jingying Wang, Nengfu Chen, Yilin Qiu, Yu Ding, Kenny Mitchell</p>
                    <p><strong>Published:</strong> 8/6/2025</p>
                    <p><strong>Categories:</strong> cs.GR, cs.CV, I.3.2; I.4.10</p>
                    <p><strong>Summary:</strong> Our purpose is to improve performance-based animation which can drive believable 3D stylized characters that are truly perceptual. By combining traditional blendshape animation techniques with multiple machine learning models, we present both non-real time and real time solutions which drive character expressions in a geometrically consistent and perceptually valid way. For the non-real time system, we propose a 3D emotion transfer network makes use of a 2D human image to generate a stylized 3D rig parameters. For the real time system, we propose a blendshape adaption network which generates the character rig parameter motions with geometric consistency and temporally stability. We demonstrate the effectiveness of our system by comparing to a commercial product Faceware. Results reveal that ratings of the recognition, intensity, and attractiveness of expressions depicted for animated characters via our systems are statistically higher than Faceware. Our results may be implemented into the animation pipeline, and provide animators with a system for creating the expressions they wish to use more quickly and accurately.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.04681v1" target="_blank">Perceiving and Acting in First-Person: A Dataset and Benchmark for Egocentric Human-Object-Human Interactions</a></h3>
                    <p><strong>Authors:</strong> Liang Xu, Chengqun Yang, Zili Lin, Fei Xu, Yifan Liu, Congsheng Xu, Yiyi Zhang, Jie Qin, Xingdong Sheng, Yunhui Liu, Xin Jin, Yichao Yan, Wenjun Zeng, Xiaokang Yang</p>
                    <p><strong>Published:</strong> 8/6/2025</p>
                    <p><strong>Categories:</strong> cs.CV</p>
                    <p><strong>Summary:</strong> Learning action models from real-world human-centric interaction datasets is important towards building general-purpose intelligent assistants with efficiency. However, most existing datasets only offer specialist interaction category and ignore that AI assistants perceive and act based on first-person acquisition. We urge that both the generalist interaction knowledge and egocentric modality are indispensable. In this paper, we embed the manual-assisted task into a vision-language-action framework, where the assistant provides services to the instructor following egocentric vision and commands. With our hybrid RGB-MoCap system, pairs of assistants and instructors engage with multiple objects and the scene following GPT-generated scripts. Under this setting, we accomplish InterVLA, the first large-scale human-object-human interaction dataset with 11.4 hours and 1.2M frames of multimodal data, spanning 2 egocentric and 5 exocentric videos, accurate human/object motions and verbal commands. Furthermore, we establish novel benchmarks on egocentric human motion estimation, interaction synthesis, and interaction prediction with comprehensive analysis. We believe that our InterVLA testbed and the benchmarks will foster future works on building AI agents in the physical world.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.04679v1" target="_blank">MisVisFix: An Interactive Dashboard for Detecting, Explaining, and Correcting Misleading Visualizations using Large Language Models</a></h3>
                    <p><strong>Authors:</strong> Amit Kumar Das, Klaus Mueller</p>
                    <p><strong>Published:</strong> 8/6/2025</p>
                    <p><strong>Categories:</strong> cs.HC</p>
                    <p><strong>Summary:</strong> Misleading visualizations pose a significant challenge to accurate data interpretation. While recent research has explored the use of Large Language Models (LLMs) for detecting such misinformation, practical tools that also support explanation and correction remain limited. We present MisVisFix, an interactive dashboard that leverages both Claude and GPT models to support the full workflow of detecting, explaining, and correcting misleading visualizations. MisVisFix correctly identifies 96% of visualization issues and addresses all 74 known visualization misinformation types, classifying them as major, minor, or potential concerns. It provides detailed explanations, actionable suggestions, and automatically generates corrected charts. An interactive chat interface allows users to ask about specific chart elements or request modifications. The dashboard adapts to newly emerging misinformation strategies through targeted user interactions. User studies with visualization experts and developers of fact-checking tools show that MisVisFix accurately identifies issues and offers useful suggestions for improvement. By transforming LLM-based detection into an accessible, interactive platform, MisVisFix advances visualization literacy and supports more trustworthy data communication.</p>
                
            
                
                    <h3><a href="http://dx.doi.org/10.1177/02783649251369549" target="_blank">Open Scene Graphs for Open-World Object-Goal Navigation</a></h3>
                    <p><strong>Authors:</strong> Joel Loo, Zhanxin Wu, David Hsu</p>
                    <p><strong>Published:</strong> 8/6/2025</p>
                    <p><strong>Categories:</strong> cs.RO</p>
                    <p><strong>Summary:</strong> How can we build general-purpose robot systems for open-world semantic navigation, e.g., searching a novel environment for a target object specified in natural language? To tackle this challenge, we introduce OSG Navigator, a modular system composed of foundation models, for open-world Object-Goal Navigation (ObjectNav). Foundation models provide enormous semantic knowledge about the world, but struggle to organise and maintain spatial information effectively at scale. Key to OSG Navigator is the Open Scene Graph representation, which acts as spatial memory for OSG Navigator. It organises spatial information hierarchically using OSG schemas, which are templates, each describing the common structure of a class of environments. OSG schemas can be automatically generated from simple semantic labels of a given environment, e.g., home or supermarket. They enable OSG Navigator to adapt zero-shot to new environment types. We conducted experiments using both Fetch and Spot robots in simulation and in the real world, showing that OSG Navigator achieves state-of-the-art performance on ObjectNav benchmarks and generalises zero-shot over diverse goals, environments, and robot embodiments.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.04677v1" target="_blank">ANPrompt: Anti-noise Prompt Tuning for Vision-Language Models</a></h3>
                    <p><strong>Authors:</strong> Yansheng Gao, Yufei Zheng, Jinghan Qu, Zixi Zhu, Yukuan Zhang, Shengsheng Wang</p>
                    <p><strong>Published:</strong> 8/6/2025</p>
                    <p><strong>Categories:</strong> cs.CV</p>
                    <p><strong>Summary:</strong> Prompt tuning has emerged as an efficient and effective technique for adapting vision-language models (VLMs) with low computational overhead. However, existing methods often overlook the vulnerability of prompt-tuned VLMs to weak semantic perturbations-such as subtle image or text noise-that degrade their generalization to unseen classes. To address this limitation, we propose ANPrompt, a novel prompt tuning framework designed to enhance robustness under such perturbations. ANPrompt first constructs weak noise text features by fusing original and noise-perturbed text embeddings, which are then clustered to form noise prompts. These noise prompts are integrated with learnable prompt tokens to generate anti-noise prompts, which are injected into the deeper layers of both image and text encoders. To further capture the noise-aware visual semantics, ANPrompt computes the Noise-Resistant Visual Prompt Prototype (NRVPP) by averaging the output prompt tokens from the vision encoder. Finally, ANPrompt introduces alignment, robustness, and anti-noise objectives by computing a Weak semantic noise Alignment Loss (WALoss) alongside the standard cross-entropy and sim loss. Experiments across 11 benchmarks demonstrate that ANPrompt consistently outperforms existing prompt tuning approaches, achieving superior robustness to semantic noise and improved generalization to novel categories.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.04669v1" target="_blank">Cybersecurity of Quantum Key Distribution Implementations</a></h3>
                    <p><strong>Authors:</strong> Ittay Alfassi, Ran Gelles, Rotem Liss, Tal Mor</p>
                    <p><strong>Published:</strong> 8/6/2025</p>
                    <p><strong>Categories:</strong> quant-ph, cs.CR</p>
                    <p><strong>Summary:</strong> Practical implementations of Quantum Key Distribution (QKD) often deviate from the theoretical protocols, exposing the implementations to various attacks even when the underlying (ideal) protocol is proven secure. We present new analysis tools and methodologies for quantum cybersecurity, adapting the concepts of vulnerabilities, attack surfaces, and exploits from classical cybersecurity to QKD implementation attacks. We present three additional concepts, derived from the connection between classical and quantum cybersecurity: Quantum Fuzzing, which is the first tool for black-box vulnerability research on QKD implementations; Reversed-Space Attacks, which are a generic exploit method using the attack surface of imperfect receivers; and a concrete quantum-mechanical definition of Quantum Side-Channel Attacks, meaningfully distinguishing them from other types of attacks. Using our tools, we analyze multiple existing QKD attacks and show that the Bright Illumination attack could have been fully constructed even with minimal knowledge of the device implementation. This work begins to bridge the gap between current analysis methods for experimental attacks on QKD implementations and the decades-long research in the field of classical cybersecurity, improving the practical security of QKD products and enhancing their usefulness in real-world systems.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.04667v1" target="_blank">How are CS students using resources and AI tools for coding tasks?</a></h3>
                    <p><strong>Authors:</strong> Natalia Echeverry, Arun Lekshmi Narayanan</p>
                    <p><strong>Published:</strong> 8/6/2025</p>
                    <p><strong>Categories:</strong> cs.HC, cs.AI</p>
                    <p><strong>Summary:</strong> A survey of 26 CS students reveals that AI coding assistants are mainly used for writing code (second to online searches) while AI chatbots are the top resource for debugging. Participants with different coding experience prefer online help over direct human help from peers and instructors.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.04664v1" target="_blank">Sculptor: Empowering LLMs with Cognitive Agency via Active Context Management</a></h3>
                    <p><strong>Authors:</strong> Mo Li, L. H. Xu, Qitai Tan, Ting Cao, Yunxin Liu</p>
                    <p><strong>Published:</strong> 8/6/2025</p>
                    <p><strong>Categories:</strong> cs.CL, cs.AI, cs.LG</p>
                    <p><strong>Summary:</strong> Large Language Models (LLMs) suffer from significant performance degradation when processing long contexts due to proactive interference, where irrelevant information in earlier parts of the context disrupts reasoning and memory recall. While most research focuses on external memory systems to augment LLMs capabilities, we propose a complementary approach: empowering LLMs with Active Context Management (ACM) tools to actively sculpt their internal working memory. We introduce Sculptor, a framework that equips LLMs with three categories of tools: (1) context fragmentation, (2) summary, hide, and restore, and (3) intelligent search. Our approach enables LLMs to proactively manage their attention and working memory, analogous to how humans selectively focus on relevant information while filtering out distractions. Experimental evaluation on information-sparse benchmarks-PI-LLM (proactive interference) and NeedleBench Multi-Needle Reasoning-demonstrates that Sculptor significantly improves performance even without specific training, leveraging LLMs inherent tool calling generalization capabilities. By enabling Active Context Management, Sculptor not only mitigates proactive interference but also provides a cognitive foundation for more reliable reasoning across diverse long-context tasks-highlighting that explicit context-control strategies, rather than merely larger token windows, are key to robustness at scale.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.04662v1" target="_blank">Tailored Thermal and Mechanical Performance of Biodegradable PLA-P(VDF-TrFE) Polymer Blends</a></h3>
                    <p><strong>Authors:</strong> G Suresh, B. Satyanarayana, C. Thirmal, Kaushal Jagarlamudi, T Komala, Jimlee Patowary, Ashutosh Kumar</p>
                    <p><strong>Published:</strong> 8/6/2025</p>
                    <p><strong>Categories:</strong> cond-mat.mtrl-sci, cond-mat.soft</p>
                    <p><strong>Summary:</strong> The development of polymer blends has emerged as a strategic approach for designing multifunctional materials with enhanced tailored characteristics. Current work investigates and reports for the first time, the structure-property relationships in free-standing blend films of poly(vinylidene fluoride-trifluoroethylene) (P(VDF-TrFE)) and polylactic acid (PLA), prepared to evaluate their suitability for functional applications. For this investigation, films of approximately 40 $\mu$m thick were fabricated by systematically varying the P(VDF-TrFE):PLA ratio. Thermal analysis revealed a higher PLA crystallinity at 25\% P(VDF-TrFE) content, while Fourier-transform infrared spectroscopy showed the electroactive $\beta$-phase fraction to be highest in the 50:50 composition. These findings correlated with tensile strength measurements and morphology, demonstrating that molecular ordering and phase distribution significantly influence the mechanical performance. The 25:75 blend exhibited superior mechanical strength due to enhanced PLA crystallization and polymer chain alignment. In contrast, the 50:50 blend achieved a balance between tensile modulus and electroactive phase development, marking it a promising candidate for sensors and 3D printing applications. At higher P(VDF-TrFE) content, reduced crystallinity in PLA resulted in softer, more compliant films which would be suitable for flexible electronic applications. These results establish a pathway to tune mechanical and functional properties in semicrystalline polymer blends through facile compositional control.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.04660v1" target="_blank">Multi-module GRPO: Composing Policy Gradients and Prompt Optimization for Language Model Programs</a></h3>
                    <p><strong>Authors:</strong> Noah Ziems, Dilara Soylu, Lakshya A Agrawal, Isaac Miller, Liheng Lai, Chen Qian, Kaiqiang Song, Meng Jiang, Dan Klein, Matei Zaharia, Karel DOosterlinck, Christopher Potts, Omar Khattab</p>
                    <p><strong>Published:</strong> 8/6/2025</p>
                    <p><strong>Categories:</strong> cs.CL</p>
                    <p><strong>Summary:</strong> Group Relative Policy Optimization (GRPO) has proven to be an effective tool for post-training language models (LMs). However, AI systems are increasingly expressed as modular programs that mix together multiple LM calls with distinct prompt templates and other tools, and it is not clear how best to leverage GRPO to improve these systems. We begin to address this challenge by defining mmGRPO, a simple multi-module generalization of GRPO that groups LM calls by module across rollouts and handles variable-length and interrupted trajectories. We find that mmGRPO, composed with automatic prompt optimization, improves accuracy by 11% on average across classification, many-hop search, and privacy-preserving delegation tasks against the post-trained LM, and by 5% against prompt optimization on its own. We open-source mmGRPO in DSPy as the dspy.GRPO optimizer.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.04659v1" target="_blank">PixCuboid: Room Layout Estimation from Multi-view Featuremetric Alignment</a></h3>
                    <p><strong>Authors:</strong> Gustav Hanning, Kalle Ã…strÃ¶m, Viktor Larsson</p>
                    <p><strong>Published:</strong> 8/6/2025</p>
                    <p><strong>Categories:</strong> cs.CV, I.4</p>
                    <p><strong>Summary:</strong> Coarse room layout estimation provides important geometric cues for many downstream tasks. Current state-of-the-art methods are predominantly based on single views and often assume panoramic images. We introduce PixCuboid, an optimization-based approach for cuboid-shaped room layout estimation, which is based on multi-view alignment of dense deep features. By training with the optimization end-to-end, we learn feature maps that yield large convergence basins and smooth loss landscapes in the alignment. This allows us to initialize the room layout using simple heuristics. For the evaluation we propose two new benchmarks based on ScanNet++ and 2D-3D-Semantics, with manually verified ground truth 3D cuboids. In thorough experiments we validate our approach and significantly outperform the competition. Finally, while our network is trained with single cuboids, the flexibility of the optimization-based approach allow us to easily extend to multi-room estimation, e.g. larger apartments or offices. Code and model weights are available at https://github.com/ghanning/PixCuboid.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.04658v1" target="_blank">YOLOv8-Based Deep Learning Model for Automated Poultry Disease Detection and Health Monitoring paper</a></h3>
                    <p><strong>Authors:</strong> Akhil Saketh Reddy Sabbella, Ch. Lakshmi Prachothan, Eswar Kumar Panta</p>
                    <p><strong>Published:</strong> 8/6/2025</p>
                    <p><strong>Categories:</strong> cs.CV, cs.AI</p>
                    <p><strong>Summary:</strong> In the poultry industry, detecting chicken illnesses is essential to avoid financial losses. Conventional techniques depend on manual observation, which is laborious and prone to mistakes. Using YOLO v8 a deep learning model for real-time object recognition. This study suggests an AI based approach, by developing a system that analyzes high resolution chicken photos, YOLO v8 detects signs of illness, such as abnormalities in behavior and appearance. A sizable, annotated dataset has been used to train the algorithm, which provides accurate real-time identification of infected chicken and prompt warnings to farm operators for prompt action. By facilitating early infection identification, eliminating the need for human inspection, and enhancing biosecurity in large-scale farms, this AI technology improves chicken health management. The real-time features of YOLO v8 provide a scalable and effective method for improving farm management techniques.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.04651v1" target="_blank">Live Music Models</a></h3>
                    <p><strong>Authors:</strong> Lyria Team, Antoine Caillon, Brian McWilliams, Cassie Tarakajian, Ian Simon, Ilaria Manco, Jesse Engel, Noah Constant, Pen Li, Timo I. Denk, Alberto Lalama, Andrea Agostinelli, Anna Huang, Ethan Manilow, George Brower, Hakan Erdogan, Heidi Lei, Itai Rolnick, Ivan Grishchenko, Manu Orsini, Matej Kastelic, Mauricio Zuluaga, Mauro Verzetti, Michael Dooley, Ondrej Skopek, Rafael Ferrer, ZalÃ¡n Borsos, Ã„aron van den Oord, Douglas Eck, Eli Collins, Jason Baldridge, Tom Hume, Chris Donahue, Kehang Han, Adam Roberts</p>
                    <p><strong>Published:</strong> 8/6/2025</p>
                    <p><strong>Categories:</strong> cs.SD, cs.HC, cs.LG</p>
                    <p><strong>Summary:</strong> We introduce a new class of generative models for music called live music models that produce a continuous stream of music in real-time with synchronized user control. We release Magenta RealTime, an open-weights live music model that can be steered using text or audio prompts to control acoustic style. On automatic metrics of music quality, Magenta RealTime outperforms other open-weights music generation models, despite using fewer parameters and offering first-of-its-kind live generation capabilities. We also release Lyria RealTime, an API-based model with extended controls, offering access to our most powerful model with wide prompt coverage. These models demonstrate a new paradigm for AI-assisted music creation that emphasizes human-in-the-loop interaction for live music performance.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.04649v1" target="_blank">Estimating breast cancer recurrence in a population-based registry in Georgia, US</a></h3>
                    <p><strong>Authors:</strong> Chrystelle Kiang, Micah Streiff, Rebecca Nash, Robert H. Lyles, Deirdre Cronin-Fenton, Anke Huels, Timothy L. Lash, Kevin C. Ward</p>
                    <p><strong>Published:</strong> 8/6/2025</p>
                    <p><strong>Categories:</strong> q-bio.PE</p>
                    <p><strong>Summary:</strong> Although the descriptive epidemiology of primary breast cancer is well characterized in the US, breast cancer recurrence rates have not been measured in an unselected population. The number of breast cancer survivors at risk for recurrence is growing each year, so recurrence surveillance is a pressing need. We used missing data methods to impute breast cancer recurrence and estimate the risk of recurrence in the Cancer Recurrence Information and Surveillance Program (CRISP) cohort in the Georgia Cancer Registry. The imputation model was based on an internal validation substudy and indicators recorded in the registry (e.g., pathology reports, imaging claims), prognostic variables (e.g., stage at diagnosis), and characteristics associated with missing data (e.g., insurance coverage). We pooled hazard ratios (HR) and 95% Confidence Intervals (CI) across 1000 imputed datasets, adjusted for age, stage, grade, subtype, race and ethnicity, marital status, and urban/rural county at diagnosis. There were 1,606 patients with a validated outcome (75% with breast cancer recurrence) and we imputed the outcome for the remaining 23,439 patients. We estimated an overall 7.2% incidence of recurrence between at least 1 year after diagnosis and up to 5 years of follow up. When comparing the hazards pooled across imputations, we found that some patterns differed from established patterns in mortality or survival, notably by race and ethnicity, underscoring the need for continued research on the descriptive epidemiology of breast cancer recurrence. These results provide new insights into surveillance for breast cancer survivors in Georgia, especially those with higher stage and grade tumors, of Hispanic ethnicity, and who may be lacking social support.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.04645v1" target="_blank">A Scalable Pretraining Framework for Link Prediction with Efficient Adaptation</a></h3>
                    <p><strong>Authors:</strong> Yu Song, Zhigang Hua, Harry Shomer, Yan Xie, Jingzhe Liu, Bo Long, Hui Liu</p>
                    <p><strong>Published:</strong> 8/6/2025</p>
                    <p><strong>Categories:</strong> cs.LG, cs.AI</p>
                    <p><strong>Summary:</strong> Link Prediction (LP) is a critical task in graph machine learning. While Graph Neural Networks (GNNs) have significantly advanced LP performance recently, existing methods face key challenges including limited supervision from sparse connectivity, sensitivity to initialization, and poor generalization under distribution shifts. We explore pretraining as a solution to address these challenges. Unlike node classification, LP is inherently a pairwise task, which requires the integration of both node- and edge-level information. In this work, we present the first systematic study on the transferability of these distinct modules and propose a late fusion strategy to effectively combine their outputs for improved performance. To handle the diversity of pretraining data and avoid negative transfer, we introduce a Mixture-of-Experts (MoE) framework that captures distinct patterns in separate experts, facilitating seamless application of the pretrained model on diverse downstream datasets. For fast adaptation, we develop a parameter-efficient tuning strategy that allows the pretrained model to adapt to unseen datasets with minimal computational overhead. Experiments on 16 datasets across two domains demonstrate the effectiveness of our approach, achieving state-of-the-art performance on low-resource link prediction while obtaining competitive results compared to end-to-end trained methods, with over 10,000x lower computational overhead.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.04641v1" target="_blank">4-Swap: Achieving Grief-Free and Bribery-Safe Atomic Swaps Using Four Transactions</a></h3>
                    <p><strong>Authors:</strong> Kirti Singh, Vinay J. Ribeiro, Susmita Mandal</p>
                    <p><strong>Published:</strong> 8/6/2025</p>
                    <p><strong>Categories:</strong> cs.CR, C.2.4</p>
                    <p><strong>Summary:</strong> Cross-chain asset exchange is crucial for blockchain interoperability. Existing solutions rely on trusted third parties and risk asset loss, or use decentralized alternatives like atomic swaps, which suffer from grief attacks. Griefing occurs when a party prematurely exits, locking the counterpartys assets until a timelock expires. Hedged Atomic Swaps mitigate griefing by introducing a penalty premium; however, they increase the number of transactions from four (as in Tier Nolans swap) to six, which in turn introduces new griefing risks. Grief-Free (GF) Swap reduces this to five transactions by consolidating assets and premiums on a single chain. However, no existing protocol achieves grief-free asset exchange in just four transactions. This paper presents 4-Swap, the first cross-chain atomic swap protocol that is both grief-free and bribery-safe, while completing asset exchange in just four transactions. By combining the griefing premium and principal into a single transaction per chain, 4-Swap reduces on-chain transactions, leading to faster execution compared to previous grief-free solutions. It is fully compatible with Bitcoin and operates without the need for any new opcodes. A game-theoretic analysis shows that rational participants have no incentive to deviate from the protocol, ensuring robust compliance and security.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.04638v1" target="_blank">Can NLP Tackle Hate Speech in the Real World? Stakeholder-Informed Feedback and Survey on Counterspeech</a></h3>
                    <p><strong>Authors:</strong> Tanvi Dinkar, Aiqi Jiang, Simona Frenda, Poppy Gerrard-Abbott, Nancie Gunson, Gavin Abercrombie, Ioannis Konstas</p>
                    <p><strong>Published:</strong> 8/6/2025</p>
                    <p><strong>Categories:</strong> cs.CL</p>
                    <p><strong>Summary:</strong> Counterspeech, i.e. the practice of responding to online hate speech, has gained traction in NLP as a promising intervention. While early work emphasised collaboration with non-governmental organisation stakeholders, recent research trends have shifted toward automated pipelines that reuse a small set of legacy datasets, often without input from affected communities. This paper presents a systematic review of 74 NLP studies on counterspeech, analysing the extent to which stakeholder participation influences dataset creation, model development, and evaluation. To complement this analysis, we conducted a participatory case study with five NGOs specialising in online Gender-Based Violence (oGBV), identifying stakeholder-informed practices for counterspeech generation. Our findings reveal a growing disconnect between current NLP research and the needs of communities most impacted by toxic online content. We conclude with concrete recommendations for re-centring stakeholder expertise in counterspeech research.</p>
                
            
                
                    <h3><a href="http://dx.doi.org/10.1145/3746058.3758994" target="_blank">VirtLab: An AI-Powered System for Flexible, Customizable, and Large-scale Team Simulations</a></h3>
                    <p><strong>Authors:</strong> Mohammed Almutairi, Charles Chiang, Haoze Guo, Matthew Belcher, Nandini Banerjee, Maria Milkowski, Svitlana Volkova, Daniel Nguyen, Tim Weninger, Michael Yankoski, Trenton W. Ford, Diego Gomez-Zara</p>
                    <p><strong>Published:</strong> 8/6/2025</p>
                    <p><strong>Categories:</strong> cs.HC</p>
                    <p><strong>Summary:</strong> Simulating how team members collaborate within complex environments using Agentic AI is a promising approach to explore hypotheses grounded in social science theories and study team behaviors. We introduce VirtLab, a user-friendly, customizable, multi-agent, and scalable team simulation system that enables testing teams with LLM-based agents in spatial and temporal settings. This system addresses the current frameworks design and technical limitations that do not consider flexible simulation scenarios and spatial settings. VirtLab contains a simulation engine and a web interface that enables both technical and non-technical users to formulate, run, and analyze team simulations without programming. We demonstrate the systems utility by comparing ground truth data with simulated scenarios.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.04632v1" target="_blank">IFDECORATOR: Wrapping Instruction Following Reinforcement Learning with Verifiable Rewards</a></h3>
                    <p><strong>Authors:</strong> Xu Guo, Tianyi Liang, Tong Jian, Xiaogui Yang, Ling-I Wu, Chenhui Li, Zhihui Lu, Qipeng Guo, Kai Chen</p>
                    <p><strong>Published:</strong> 8/6/2025</p>
                    <p><strong>Categories:</strong> cs.CL</p>
                    <p><strong>Summary:</strong> Reinforcement Learning with Verifiable Rewards (RLVR) improves instruction following capabilities of large language models (LLMs), but suffers from training inefficiency due to inadequate difficulty assessment. Moreover, RLVR is prone to over-optimization, where LLMs exploit verification shortcuts without aligning to the actual intent of user instructions. We introduce Instruction Following Decorator (IFDecorator}, a framework that wraps RLVR training into a robust and sample-efficient pipeline. It consists of three components: (1) a cooperative-adversarial data flywheel that co-evolves instructions and hybrid verifications, generating progressively more challenging instruction-verification pairs; (2) IntentCheck, a bypass module enforcing intent alignment; and (3) trip wires, a diagnostic mechanism that detects reward hacking via trap instructions, which trigger and capture shortcut exploitation behaviors. Our Qwen2.5-32B-Instruct-IFDecorator achieves 87.43% accuracy on IFEval, outperforming larger proprietary models such as GPT-4o. Additionally, we demonstrate substantial improvements on FollowBench while preserving general capabilities. Our trip wires show significant reductions in reward hacking rates. We will release models, code, and data for future research.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.04626v1" target="_blank">P-Aligner: Enabling Pre-Alignment of Language Models via Principled Instruction Synthesis</a></h3>
                    <p><strong>Authors:</strong> Feifan Song, Bofei Gao, Yifan Song, Yi Liu, Weimin Xiong, Yuyang Song, Tianyu Liu, Guoyin Wang, Houfeng Wang</p>
                    <p><strong>Published:</strong> 8/6/2025</p>
                    <p><strong>Categories:</strong> cs.CL, cs.AI</p>
                    <p><strong>Summary:</strong> Large Language Models (LLMs) are expected to produce safe, helpful, and honest content during interaction with human users, but they frequently fail to align with such values when given flawed instructions, e.g., missing context, ambiguous directives, or inappropriate tone, leaving substantial room for improvement along multiple dimensions. A cost-effective yet high-impact way is to pre-align instructions before the model begins decoding. Existing approaches either rely on prohibitive test-time search costs or end-to-end model rewrite, which is powered by a customized training corpus with unclear objectives. In this work, we demonstrate that the goal of efficient and effective preference alignment can be achieved by P-Aligner, a lightweight module generating instructions that preserve the original intents while being expressed in a more human-preferred form. P-Aligner is trained on UltraPrompt, a new dataset synthesized via a proposed principle-guided pipeline using Monte-Carlo Tree Search, which systematically explores the space of candidate instructions that are closely tied to human preference. Experiments across different methods show that P-Aligner generally outperforms strong baselines across various models and benchmarks, including average win-rate gains of 28.35% and 8.69% on GPT-4-turbo and Gemma-2-SimPO, respectively. Further analyses validate its effectiveness and efficiency through multiple perspectives, including data quality, search strategies, iterative deployment, and time overhead.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.04625v1" target="_blank">FinMMR: Make Financial Numerical Reasoning More Multimodal, Comprehensive, and Challenging</a></h3>
                    <p><strong>Authors:</strong> Zichen Tang, Haihong E, Jiacheng Liu, Zhongjun Yang, Rongjin Li, Zihua Rong, Haoyang He, Zhuodi Hao, Xinyang Hu, Kun Ji, Ziyan Ma, Mengyuan Ji, Jun Zhang, Chenghao Ma, Qianhe Zheng, Yang Liu, Yiling Huang, Xinyi Hu, Qing Huang, Zijian Xie, Shiyao Peng</p>
                    <p><strong>Published:</strong> 8/6/2025</p>
                    <p><strong>Categories:</strong> cs.CV, cs.CE</p>
                    <p><strong>Summary:</strong> We present FinMMR, a novel bilingual multimodal benchmark tailored to evaluate the reasoning capabilities of multimodal large language models (MLLMs) in financial numerical reasoning tasks. Compared to existing benchmarks, our work introduces three significant advancements. (1) Multimodality: We meticulously transform existing financial reasoning benchmarks, and construct novel questions from the latest Chinese financial research reports. FinMMR comprises 4.3K questions and 8.7K images spanning 14 categories, including tables, bar charts, and ownership structure charts. (2) Comprehensiveness: FinMMR encompasses 14 financial subdomains, including corporate finance, banking, and industry analysis, significantly exceeding existing benchmarks in financial domain knowledge breadth. (3) Challenge: Models are required to perform multi-step precise numerical reasoning by integrating financial knowledge with the understanding of complex financial images and text. The best-performing MLLM achieves only 53.0% accuracy on Hard problems. We believe that FinMMR will drive advancements in enhancing the reasoning capabilities of MLLMs in real-world scenarios.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.04618v1" target="_blank">HiD-VAE: Interpretable Generative Recommendation via Hierarchical and Disentangled Semantic IDs</a></h3>
                    <p><strong>Authors:</strong> Dengzhao Fang, Jingtong Gao, Chengcheng Zhu, Yu Li, Xiangyu Zhao, Yi Chang</p>
                    <p><strong>Published:</strong> 8/6/2025</p>
                    <p><strong>Categories:</strong> cs.IR, cs.AI</p>
                    <p><strong>Summary:</strong> Recommender systems are indispensable for helping users navigate the immense item catalogs of modern online platforms. Recently, generative recommendation has emerged as a promising paradigm, unifying the conventional retrieve-and-rank pipeline into an end-to-end model capable of dynamic generation. However, existing generative methods are fundamentally constrained by their unsupervised tokenization, which generates semantic IDs suffering from two critical flaws: (1) they are semantically flat and uninterpretable, lacking a coherent hierarchy, and (2) they are prone to representation entanglement (i.e., ``ID collisions), which harms recommendation accuracy and diversity. To overcome these limitations, we propose HiD-VAE, a novel framework that learns hierarchically disentangled item representations through two core innovations. First, HiD-VAE pioneers a hierarchically-supervised quantization process that aligns discrete codes with multi-level item tags, yielding more uniform and disentangled IDs. Crucially, the trained codebooks can predict hierarchical tags, providing a traceable and interpretable semantic path for each recommendation. Second, to combat representation entanglement, HiD-VAE incorporates a novel uniqueness loss that directly penalizes latent space overlap. This mechanism not only resolves the critical ID collision problem but also promotes recommendation diversity by ensuring a more comprehensive utilization of the item representation space. These high-quality, disentangled IDs provide a powerful foundation for downstream generative models. Extensive experiments on three public benchmarks validate HiD-VAEs superior performance against state-of-the-art methods. The code is available at https://anonymous.4open.science/r/HiD-VAE-84B2.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.04614v1" target="_blank">How Does Bilateral Ear Symmetry Affect Deep Ear Features?</a></h3>
                    <p><strong>Authors:</strong> Kagan Ozturk, Deeksha Arun, Kevin W. Bowyer, Patrick Flynn</p>
                    <p><strong>Published:</strong> 8/6/2025</p>
                    <p><strong>Categories:</strong> cs.CV</p>
                    <p><strong>Summary:</strong> Ear recognition has gained attention as a reliable biometric technique due to the distinctive characteristics of human ears. With the increasing availability of large-scale datasets, convolutional neural networks (CNNs) have been widely adopted to learn features directly from raw ear images, outperforming traditional hand-crafted methods. However, the effect of bilateral ear symmetry on the features learned by CNNs has received little attention in recent studies. In this paper, we investigate how bilateral ear symmetry influences the effectiveness of CNN-based ear recognition. To this end, we first develop an ear side classifier to automatically categorize ear images as either left or right. We then explore the impact of incorporating this side information during both training and test. Cross-dataset evaluations are conducted on five datasets. Our results suggest that treating left and right ears separately during training and testing can lead to notable performance improvements. Furthermore, our ablation studies on alignment strategies, input sizes, and various hyperparameter settings provide practical insights into training CNN-based ear recognition systems on large-scale datasets to achieve higher verification rates.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.04612v1" target="_blank">A Reproducible, Scalable Pipeline for Synthesizing Autoregressive Model Literature</a></h3>
                    <p><strong>Authors:</strong> Faruk Alpay, Bugra Kilictas, Hamdi Alakkad</p>
                    <p><strong>Published:</strong> 8/6/2025</p>
                    <p><strong>Categories:</strong> cs.IR, cs.DL, cs.LG, 68P20, 68T05, 68T50, H.3.3; H.3.7; I.2.6; I.2.7</p>
                    <p><strong>Summary:</strong> The accelerating pace of research on autoregressive generative models has produced thousands of papers, making manual literature surveys and reproduction studies increasingly impractical. We present a fully open-source, reproducible pipeline that automatically retrieves candidate documents from public repositories, filters them for relevance, extracts metadata, hyper-parameters and reported results, clusters topics, produces retrieval-augmented summaries and generates containerised scripts for re-running selected experiments. Quantitative evaluation on 50 manually-annotated papers shows F1 scores above 0.85 for relevance classification, hyper-parameter extraction and citation identification. Experiments on corpora of up to 1000 papers demonstrate near-linear scalability with eight CPU workers. Three case studies -- AWD-LSTM on WikiText-2, Transformer-XL on WikiText-103 and an autoregressive music model on the Lakh MIDI dataset -- confirm that the extracted settings support faithful reproduction, achieving test perplexities within 1--3% of the original reports.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.04611v1" target="_blank">OmniDepth: Bridging Monocular and Stereo Reasoning with Latent Alignment</a></h3>
                    <p><strong>Authors:</strong> Tongfan Guan, Jiaxin Guo, Chen Wang, Yun-Hui Liu</p>
                    <p><strong>Published:</strong> 8/6/2025</p>
                    <p><strong>Categories:</strong> cs.CV, cs.RO</p>
                    <p><strong>Summary:</strong> Monocular and stereo depth estimation offer complementary strengths: monocular methods capture rich contextual priors but lack geometric precision, while stereo approaches leverage epipolar geometry yet struggle with ambiguities such as reflective or textureless surfaces. Despite post-hoc synergies, these paradigms remain largely disjoint in practice. We introduce OmniDepth, a unified framework that bridges both through iterative bidirectional alignment of their latent representations. At its core, a novel cross-attentive alignment mechanism dynamically synchronizes monocular contextual cues with stereo hypothesis representations during stereo reasoning. This mutual alignment resolves stereo ambiguities (e.g., specular surfaces) by injecting monocular structure priors while refining monocular depth with stereo geometry within a single network. Extensive experiments demonstrate state-of-the-art results: \textbf{OmniDepth reduces zero-shot generalization error by $\!\!40\%$ on Middlebury and ETH3D}, while addressing longstanding failures on transparent and reflective surfaces. By harmonizing multi-view geometry with monocular context, OmniDepth enables robust 3D perception that transcends modality-specific limitations. Codes available at https://github.com/aeolusguan/OmniDepth.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.04608v1" target="_blank">Assortativity in geometric and scale-free networks</a></h3>
                    <p><strong>Authors:</strong> Marc Kaufmann, Ulysse Schaller, Thomas BlÃ¤sius, Johannes Lengler</p>
                    <p><strong>Published:</strong> 8/6/2025</p>
                    <p><strong>Categories:</strong> cs.SI, math.PR</p>
                    <p><strong>Summary:</strong> The assortative behavior of a network is the tendency of similar (or dissimilar) nodes to connect to each other. This tendency can have an influence on various properties of the network, such as its robustness or the dynamics of spreading processes. In this paper, we study degree assortativity both in real-world networks and in several generative models for networks with heavy-tailed degree distribution based on latent spaces. In particular, we study Chung-Lu Graphs and Geometric Inhomogeneous Random Graphs (GIRGs). Previous research on assortativity has primarily focused on measuring the degree assortativity in real-world networks using the Pearson assortativity coefficient, despite reservations against this coefficient. We rigorously confirm these reservations by mathematically proving that the Pearson assortativity coefficient does not measure assortativity in any network with sufficiently heavy-tailed degree distributions, which is typical for real-world networks. Moreover, we find that other single-valued assortativity coefficients also do not sufficiently capture the wiring preferences of nodes, which often vary greatly by node degree. We therefore take a more fine-grained approach, analyzing a wide range of conditional and joint weight and degree distributions of connected nodes, both numerically in real-world networks and mathematically in the generative graph models. We provide several methods of visualizing the results. We show that the generative models are assortativity-neutral, while many real-world networks are not. Therefore, we also propose an extension of the GIRG model which retains the manifold desirable properties induced by the degree distribution and the latent space, but also exhibits tunable assortativity. We analyze the resulting model mathematically, and give a fine-grained quantification of its assortativity.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.04604v1" target="_blank">TURA: Tool-Augmented Unified Retrieval Agent for AI Search</a></h3>
                    <p><strong>Authors:</strong> Zhejun Zhao, Yuehu Dong, Alley Liu, Lixue Zheng, Pingsheng Liu, Dongdong Shen, Long Xia, Jiashu Zhao, Dawei Yin</p>
                    <p><strong>Published:</strong> 8/6/2025</p>
                    <p><strong>Categories:</strong> cs.CL, cs.AI, cs.IR</p>
                    <p><strong>Summary:</strong> The advent of Large Language Models (LLMs) is transforming search engines into conversational AI search products, primarily using Retrieval-Augmented Generation (RAG) on web corpora. However, this paradigm has significant industrial limitations. Traditional RAG approaches struggle with real-time needs and structured queries that require accessing dynamically generated content like ticket availability or inventory. Limited to indexing static pages, search engines cannot perform the interactive queries needed for such time-sensitive data. Academic research has focused on optimizing RAG for static content, overlooking complex intents and the need for dynamic sources like databases and real-time APIs. To bridge this gap, we introduce TURA (Tool-Augmented Unified Retrieval Agent for AI Search), a novel three-stage framework that combines RAG with agentic tool-use to access both static content and dynamic, real-time information. TURA has three key components: an Intent-Aware Retrieval module to decompose queries and retrieve information sources encapsulated as Model Context Protocol (MCP) Servers, a DAG-based Task Planner that models task dependencies as a Directed Acyclic Graph (DAG) for optimal parallel execution, and a lightweight Distilled Agent Executor for efficient tool calling. TURA is the first architecture to systematically bridge the gap between static RAG and dynamic information sources for a world-class AI search product. Serving tens of millions of users, it leverages an agentic framework to deliver robust, real-time answers while meeting the low-latency demands of a large-scale industrial system.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.04601v1" target="_blank">Enhancing the Propagation Length of Graphene Surface Plasmon Polaritons using a Metamaterial Substrate with a Near-Zero Refractive Index</a></h3>
                    <p><strong>Authors:</strong> Zoya Eremenko, Igor Volovichev</p>
                    <p><strong>Published:</strong> 8/6/2025</p>
                    <p><strong>Categories:</strong> physics.optics</p>
                    <p><strong>Summary:</strong> This paper aims to investigate the conditions necessary to control, enhance, and modify the propagation length of graphene surface plasmon polaritons (SPPs) at room temperature, using an all-dielectric metamaterial substrate in comparison to suspended graphene. The analysis is conducted within a photonic crystal framework using COMSOL Multiphysics 6.2 to study the resonant modes of the all-dielectric metamaterial. Our results confirm the existence of an near-zero effective refractive index (NZERI) regime at the $\Gamma\text{-point}$ point in the photonic crystal approach. At this NZERI regime a consequence of triply degenerate eigenmodes in a certain frequency range occurs when the effective refractive index of the metasurface approaches zero. Our central idea is that the NZERI regime in the all-dielectric metasurface of a graphene substrate can be used to control, enhance, and modify the propagation of SPPs. We applied several independent theoretical methods to obtain the effective refractive index of the metasurface at NZERI frequency range for two-dimensional and three-dimensional metasurface structures with a graphene layer. Simulation results demonstrate that the effective permittivity and permeability simultaneously attain near-zero values at closely spaced yet distinct frequencies, thereby establishing spectral regions with an effectively vanishing refractive index. Key contributions include the first demonstration of NZERI metasurfaces as graphene-supporting platforms for enhancing SPPs, a quantitative approach to balancing propagation distance and field confinement, and practical design guidelines that align with current nanofabrication capabilities. Our simulations further demonstrate that when graphene is placed on (or between two) all-dielectric metasurfaces operating in the NZERI regime, the SPP propagation length can be significantly increased.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.04599v1" target="_blank">Parallel Alignments between Magnetic Fields and Dense Structures in the Central Molecular Zone</a></h3>
                    <p><strong>Authors:</strong> Xing Pan, Qizhou Zhang, Keping Qiu, Dylan Pare, David Chuss, Natalie Butterfield, Robin Tress, Mattia Sormani, Yuping Tang, Steven Longmore, Thushara Pillai</p>
                    <p><strong>Published:</strong> 8/6/2025</p>
                    <p><strong>Categories:</strong> astro-ph.GA</p>
                    <p><strong>Summary:</strong> The recent Far-Infrared Polarimetric Large-Area Central Molecular Zone Exploration (FIREPLACE) survey with SOFIA has mapped plane-of-the-sky magnetic field orientations within the Central Molecular Zone (CMZ) of the Milky Way. Applying the Histogram of Relative Orientation (HRO) analysis to the FIREPLACE data, we find that the relative orientation between magnetic fields and column density structures is random in low-density regions (2x10^2210^23 cm^{-2}). This trend is in contrast with that of the nearby molecular clouds, where the relative orientation transitions from parallel to perpendicular with increasing column densities. However, the relative orientation varies between individual CMZ clouds. Comparisons with MHD simulations specific to the CMZ conditions suggest that the observed parallel alignment is intrinsic rather than artifacts caused by the projection effect. The origin of this parallel configuration may arise from the fact that most dense structures in the CMZ are not self-gravitating, as they are in super-virial states, except for the mini-starburst region Sgr B2. These findings are consistent with the low star formation efficiency observed in the CMZ compared to that in the Galactic disk.</p>
                
            
                
                    <h3><a href="http://dx.doi.org/10.1021/acsaom.4c00522" target="_blank">Growth of few-layer molecular crystals of PTCDI on hexagonal boron nitride by microspacing air-gap sublimation</a></h3>
                    <p><strong>Authors:</strong> Nils LeCoutre, Tolibjon Abdurakhmonov, Paul Weinbrenner, Kenji Watanabe, Takashi Taniguchi, Tobias Korn, Franziska Fennel, Oliver KÃ¼hn, Friedemann Reinhard</p>
                    <p><strong>Published:</strong> 8/6/2025</p>
                    <p><strong>Categories:</strong> cond-mat.mtrl-sci, cond-mat.mes-hall</p>
                    <p><strong>Summary:</strong> Extended two-dimensional (2D) crystals of dye molecules adsorbed on 2D material substrates like boron nitride have recently become a subject of intense study, with potential applications ranging from quantum technology to optoelectronics. The most established technique for the production of these films is physical vapor transport in vacuum. We demonstrate that few-layer crystalline films of the organic dye molecule PTCDI on boron nitride can be produced by microspacing in-air sublimation, a radically simplified technique, not requiring complicated vacuum systems. The resulting layers display clearly resolved atomic step terraces in atomic force microscopy, and a clear polarization anisotropy in their fluorescence, confirming molecular alignment and long-range order. Using density functional theory and classical molecular dynamics simulations, the canted motive is identified as the most likely building block for the morphology of a PTDCI monolayer on the hBN substrate.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.04586v1" target="_blank">Position: The Current AI Conference Model is Unsustainable! Diagnosing the Crisis of Centralized AI Conference</a></h3>
                    <p><strong>Authors:</strong> Nuo Chen, Moming Duan, Andre Huikai Lin, Qian Wang, Jiaying Wu, Bingsheng He</p>
                    <p><strong>Published:</strong> 8/6/2025</p>
                    <p><strong>Categories:</strong> cs.CY, cs.AI, cs.CL</p>
                    <p><strong>Summary:</strong> Artificial Intelligence (AI) conferences are essential for advancing research, sharing knowledge, and fostering academic community. However, their rapid expansion has rendered the centralized conference model increasingly unsustainable. This paper offers a data-driven diagnosis of a structural crisis that threatens the foundational goals of scientific dissemination, equity, and community well-being. We identify four key areas of strain: (1) scientifically, with per-author publication rates more than doubling over the past decade to over 4.5 papers annually; (2) environmentally, with the carbon footprint of a single conference exceeding the daily emissions of its host city; (3) psychologically, with 71% of online community discourse reflecting negative sentiment and 35% referencing mental health concerns; and (4) logistically, with attendance at top conferences such as NeurIPS 2024 beginning to outpace venue capacity. These pressures point to a system that is misaligned with its core mission. In response, we propose the Community-Federated Conference (CFC) model, which separates peer review, presentation, and networking into globally coordinated but locally organized components, offering a more sustainable, inclusive, and resilient path forward for AI research.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.04585v1" target="_blank">UniTalker: Conversational Speech-Visual Synthesis</a></h3>
                    <p><strong>Authors:</strong> Yifan Hu, Rui Liu, Yi Ren, Xiang Yin, Haizhou Li</p>
                    <p><strong>Published:</strong> 8/6/2025</p>
                    <p><strong>Categories:</strong> eess.AS</p>
                    <p><strong>Summary:</strong> Conversational Speech Synthesis (CSS) is a key task in the user-agent interaction area, aiming to generate more expressive and empathetic speech for users. However, it is well-known that listening and eye contact play crucial roles in conveying emotions during real-world interpersonal communication. Existing CSS research is limited to perceiving only text and speech within the dialogue context, which restricts its effectiveness. Moreover, speech-only responses further constrain the interactive experience. To address these limitations, we introduce a Conversational Speech-Visual Synthesis (CSVS) task as an extension of traditional CSS. By leveraging multimodal dialogue context, it provides users with coherent audiovisual responses. To this end, we develop a CSVS system named UniTalker, which is a unified model that seamlessly integrates multimodal perception and multimodal rendering capabilities. Specifically, it leverages a large-scale language model to comprehensively understand multimodal cues in the dialogue context, including speaker, text, speech, and the talking-face animations. After that, it employs multi-task sequence prediction to first infer the target utterances emotion and then generate empathetic speech and natural talking-face animations. To ensure that the generated speech-visual content remains consistent in terms of emotion, content, and duration, we introduce three key optimizations: 1) Designing a specialized neural landmark codec to tokenize and reconstruct facial expression sequences. 2) Proposing a bimodal speech-visual hard alignment decoding strategy. 3) Applying emotion-guided rendering during the generation stage. Comprehensive objective and subjective experiments demonstrate that our model synthesizes more empathetic speech and provides users with more natural and emotionally consistent talking-face animations.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.04583v1" target="_blank">Measuring the Carbon Footprint of Cryptographic Privacy-Enhancing Technologies</a></h3>
                    <p><strong>Authors:</strong> Marc Damie, Mihai Pop, Merijn Posthuma</p>
                    <p><strong>Published:</strong> 8/6/2025</p>
                    <p><strong>Categories:</strong> cs.CR</p>
                    <p><strong>Summary:</strong> Privacy-enhancing technologies (PETs) have attracted significant attention in response to privacy regulations, driving the development of applications that prioritize user data protection. At the same time, the information and communication technology (ICT) sector faces growing pressure to reduce its environmental footprint, particularly its carbon emissions. While numerous studies have assessed the energy footprint of various ICT applications, the environmental footprint of cryptographic PETs remains largely unexplored. Our work addresses this gap by proposing a standardized methodology for evaluating the carbon footprint of PETs. To demonstrate this methodology, we focus on PETs supporting client-server applications as they are the simplest to deploy. In particular, we measure the energy consumption and carbon footprint increase induced by five cryptographic PETs (compared to their non-private equivalent): HTTPS web browsing, encrypted machine learning (ML) inference, encrypted ML training, encrypted databases, and encrypted emails. Our findings reveal significant variability in carbon footprint increases, ranging from a twofold increase in HTTPS web browsing to a 100,000-fold increase in encrypted ML. Our study provides essential data to help decision-makers assess privacy-carbon trade-offs in such applications. Finally, we outline key research directions for developing PETs that balance strong privacy protection with environmental sustainability.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.04581v1" target="_blank">Share Your Attention: Transformer Weight Sharing via Matrix-based Dictionary Learning</a></h3>
                    <p><strong>Authors:</strong> Magauiya Zhussip, Dmitriy Shopkhoev, Ammar Ali, Stamatios Lefkimmiatis</p>
                    <p><strong>Published:</strong> 8/6/2025</p>
                    <p><strong>Categories:</strong> cs.CL, cs.AI</p>
                    <p><strong>Summary:</strong> Large language models (LLMs) have revolutionized AI applications, yet their high computational and memory demands hinder their widespread deployment. Existing compression techniques focus on intra-block optimizations (e.g. low-rank approximation, attention head pruning), while the repetitive layered structure of transformers implies significant inter-block redundancy - a dimension largely unexplored beyond key-value (KV) caching. Inspired by dictionary learning in CNNs, we propose a framework for structured weight sharing across transformer layers. Our approach decomposes attention projection matrices into shared dictionary atoms, reducing the attention modules parameters by 66.7% while achieving on-par performance. Unlike complex methods requiring distillation or architectural changes, MASA (Matrix Atom Sharing in Attention) operates as a drop-in replacement - trained with standard optimizers - and represents each layers weights as linear combinations of shared matrix atoms. Experiments across scales (100M-700M parameters) show that MASA achieves better benchmark accuracy and perplexity than grouped-query attention (GQA), low-rank baselines and recently proposed Repeat-all-over/Sequential sharing at comparable parameter budgets. Ablation studies confirm robustness to the dictionary size and the efficacy of shared representations in capturing cross-layer statistical regularities. Extending to Vision Transformers (ViT), MASA matches performance metrics on image classification and detection tasks with 66.7% fewer attention parameters. By combining dictionary learning strategies with transformer efficiency, MASA offers a scalable blueprint for parameter-efficient models without sacrificing performance. Finally, we investigate the possibility of employing MASA on pretrained LLMs to reduce their number of parameters without experiencing any significant drop in their performance.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.04576v1" target="_blank">ConfProBench: A Confidence Evaluation Benchmark for MLLM-Based Process Judges</a></h3>
                    <p><strong>Authors:</strong> Yue Zhou, Yi Chang, Yuan Wu</p>
                    <p><strong>Published:</strong> 8/6/2025</p>
                    <p><strong>Categories:</strong> cs.AI, I.2.6; I.2.7; D.2.8</p>
                    <p><strong>Summary:</strong> Reasoning is a critical capability of multimodal large language models (MLLMs) for solving complex multimodal tasks, and judging the correctness of reasoning steps is crucial for improving this capability. Recently, MLLM-based process judges (MPJs) have been widely used to assess the correctness of reasoning steps in multimodal tasks. Therefore, evaluating MPJs is important for identifying their limitations and guiding future improvements. However, existing benchmarks for MPJs mainly focus on tasks such as step correctness classification and reasoning process search, while overlooking a key aspect: whether the confidence scores produced by MPJs at the step level are reliable. To address this gap, we propose ConfProBench, the first comprehensive benchmark designed to systematically evaluate the reliability of step-level confidence scores generated by MPJs. Our benchmark constructs three types of adversarially perturbed reasoning steps: Synonym Substitution, Syntactic Transformation, and Image Perturbation, to test the robustness of MPJ confidence under perturbations. In addition, we introduce three novel evaluation metrics: Confidence Robustness Score (CRS), Confidence Sensitivity Score (CSS), and Confidence Calibration Score (CCS), which evaluate robustness, sensitivity, and calibration, respectively. We evaluate 14 state-of-the-art MLLMs, including both proprietary and open-source models. Experiments reveal limitations in current MPJs confidence performance and offer competitive baselines to support future research.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.04575v1" target="_blank">Beyond Brainstorming: What Drives High-Quality Scientific Ideas? Lessons from Multi-Agent Collaboration</a></h3>
                    <p><strong>Authors:</strong> Nuo Chen, Yicheng Tong, Jiaying Wu, Minh Duc Duong, Qian Wang, Qingyun Zou, Bryan Hooi, Bingsheng He</p>
                    <p><strong>Published:</strong> 8/6/2025</p>
                    <p><strong>Categories:</strong> cs.CL, cs.AI, cs.CY</p>
                    <p><strong>Summary:</strong> While AI agents show potential in scientific ideation, most existing frameworks rely on single-agent refinement, limiting creativity due to bounded knowledge and perspective. Inspired by real-world research dynamics, this paper investigates whether structured multi-agent discussions can surpass solitary ideation. We propose a cooperative multi-agent framework for generating research proposals and systematically compare configurations including group size, leaderled versus leaderless structures, and team compositions varying in interdisciplinarity and seniority. To assess idea quality, we employ a comprehensive protocol with agent-based scoring and human review across dimensions such as novelty, strategic vision, and integration depth. Our results show that multi-agent discussions substantially outperform solitary baselines. A designated leader acts as a catalyst, transforming discussion into more integrated and visionary proposals. Notably, we find that cognitive diversity is a primary driver of quality, yet expertise is a non-negotiable prerequisite, as teams lacking a foundation of senior knowledge fail to surpass even a single competent agent. These findings offer actionable insights for designing collaborative AI ideation systems and shed light on how team structure influences creative outcomes.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.04572v1" target="_blank">Knowledge to Sight: Reasoning over Visual Attributes via Knowledge Decomposition for Abnormality Grounding</a></h3>
                    <p><strong>Authors:</strong> Jun Li, Che Liu, Wenjia Bai, Mingxuan Liu, Rossella Arcucci, Cosmin I. Bercea, Julia A. Schnabel</p>
                    <p><strong>Published:</strong> 8/6/2025</p>
                    <p><strong>Categories:</strong> cs.CV</p>
                    <p><strong>Summary:</strong> In this work, we address the problem of grounding abnormalities in medical images, where the goal is to localize clinical findings based on textual descriptions. While generalist Vision-Language Models (VLMs) excel in natural grounding tasks, they often struggle in the medical domain due to rare, compositional, and domain-specific terms that are poorly aligned with visual patterns. Specialized medical VLMs address this challenge via large-scale domain pretraining, but at the cost of substantial annotation and computational resources. To overcome these limitations, we propose \textbf{Knowledge to Sight (K2Sight)}, a framework that introduces structured semantic supervision by decomposing clinical concepts into interpretable visual attributes, such as shape, density, and anatomical location. These attributes are distilled from domain ontologies and encoded into concise instruction-style prompts, which guide region-text alignment during training. Unlike conventional report-level supervision, our approach explicitly bridges domain knowledge and spatial structure, enabling data-efficient training of compact models. We train compact models with 0.23B and 2B parameters using only 1.5\% of the data required by state-of-the-art medical VLMs. Despite their small size and limited training data, these models achieve performance on par with or better than 7B+ medical VLMs, with up to 9.82\% improvement in $mAP_{50}$. Code and models: \href{https://lijunrio.github.io/K2Sight/}{\textcolor{SOTAPink}{https://lijunrio.github.io/K2Sight/}}.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.04571v1" target="_blank">Do Recommender Systems Really Leverage Multimodal Content? A Comprehensive Analysis on Multimodal Representations for Recommendation</a></h3>
                    <p><strong>Authors:</strong> Claudio Pomo, Matteo Attimonelli, Danilo Danese, Fedelucio Narducci, Tommaso Di Noia</p>
                    <p><strong>Published:</strong> 8/6/2025</p>
                    <p><strong>Categories:</strong> cs.IR, cs.CL, cs.LG</p>
                    <p><strong>Summary:</strong> Multimodal Recommender Systems aim to improve recommendation accuracy by integrating heterogeneous content, such as images and textual metadata. While effective, it remains unclear whether their gains stem from true multimodal understanding or increased model complexity. This work investigates the role of multimodal item embeddings, emphasizing the semantic informativeness of the representations. Initial experiments reveal that embeddings from standard extractors (e.g., ResNet50, Sentence-Bert) enhance performance, but rely on modality-specific encoders and ad hoc fusion strategies that lack control over cross-modal alignment. To overcome these limitations, we leverage Large Vision-Language Models (LVLMs) to generate multimodal-by-design embeddings via structured prompts. This approach yields semantically aligned representations without requiring any fusion. Experiments across multiple settings show notable performance improvements. Furthermore, LVLMs embeddings offer a distinctive advantage: they can be decoded into structured textual descriptions, enabling direct assessment of their multimodal comprehension. When such descriptions are incorporated as side content into recommender systems, they improve recommendation performance, empirically validating the semantic depth and alignment encoded within LVLMs outputs. Our study highlights the importance of semantically rich representations and positions LVLMs as a compelling foundation for building robust and meaningful multimodal representations in recommendation tasks.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.04565v1" target="_blank">TAlignDiff: Automatic Tooth Alignment assisted by Diffusion-based Transformation Learning</a></h3>
                    <p><strong>Authors:</strong> Yunbi Liu, Enqi Tang, Shiyu Li, Lei Ma, Juncheng Li, Shu Lou, Yongchu Pan, Qingshan Liu</p>
                    <p><strong>Published:</strong> 8/6/2025</p>
                    <p><strong>Categories:</strong> cs.CV</p>
                    <p><strong>Summary:</strong> Orthodontic treatment hinges on tooth alignment, which significantly affects occlusal function, facial aesthetics, and patients quality of life. Current deep learning approaches predominantly concentrate on predicting transformation matrices through imposing point-to-point geometric constraints for tooth alignment. Nevertheless, these matrices are likely associated with the anatomical structure of the human oral cavity and possess particular distribution characteristics that the deterministic point-to-point geometric constraints in prior work fail to capture. To address this, we introduce a new automatic tooth alignment method named TAlignDiff, which is supported by diffusion-based transformation learning. TAlignDiff comprises two main components: a primary point cloud-based regression network (PRN) and a diffusion-based transformation matrix denoising module (DTMD). Geometry-constrained losses supervise PRN learning for point cloud-level alignment. DTMD, as an auxiliary module, learns the latent distribution of transformation matrices from clinical data. We integrate point cloud-based transformation regression and diffusion-based transformation modeling into a unified framework, allowing bidirectional feedback between geometric constraints and diffusion refinement. Extensive ablation and comparative experiments demonstrate the effectiveness and superiority of our method, highlighting its potential in orthodontic treatment.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.04542v1" target="_blank">Privacy Risk Predictions Based on Fundamental Understanding of Personal Data and an Evolving Threat Landscape</a></h3>
                    <p><strong>Authors:</strong> Haoran Niu, K. Suzanne Barber</p>
                    <p><strong>Published:</strong> 8/6/2025</p>
                    <p><strong>Categories:</strong> cs.LG, cs.CR, cs.SI</p>
                    <p><strong>Summary:</strong> It is difficult for individuals and organizations to protect personal information without a fundamental understanding of relative privacy risks. By analyzing over 5,000 empirical identity theft and fraud cases, this research identifies which types of personal data are exposed, how frequently exposures occur, and what the consequences of those exposures are. We construct an Identity Ecosystem graph--a foundational, graph-based model in which nodes represent personally identifiable information (PII) attributes and edges represent empirical disclosure relationships between them (e.g., the probability that one PII attribute is exposed due to the exposure of another). Leveraging this graph structure, we develop a privacy risk prediction framework that uses graph theory and graph neural networks to estimate the likelihood of further disclosures when certain PII attributes are compromised. The results show that our approach effectively answers the core question: Can the disclosure of a given identity attribute possibly lead to the disclosure of another attribute?</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.04541v1" target="_blank">Measuring Information Richness in Product Images: Implications for Online Sales</a></h3>
                    <p><strong>Authors:</strong> Zhu Yuting, Cao Xinyu, Su Yuzhuo, Ma Yongbin</p>
                    <p><strong>Published:</strong> 8/6/2025</p>
                    <p><strong>Categories:</strong> cs.HC</p>
                    <p><strong>Summary:</strong> A common challenge for e-commerce sellers is to decide what product images to display on online shopping sites. In this paper, we propose and validate a novel metric, k-value, to quantify the information richness of an image set, and we further investigate its effect on consumers purchase decisions. We leverage patch-level embeddings from Vision Transformers (ViT) and apply k-means clustering to identify distinct visual features, defining k-value as the number of clusters. An online experiment demonstrates that k-value aligns with human-perceived information richness, validating the metric. A simulated online shopping experiment further reveals a significant yet counterintuitive result: while an image set with a higher k-value (richer information) shortens decision time, it paradoxically reduces purchase propensity. Our findings illuminate the complex relationship between visual information richness and consumer behavior, providing sellers a quantifiable tool for image selection.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.04540v1" target="_blank">InceptoFormer: A Multi-Signal Neural Framework for Parkinsons Disease Severity Evaluation from Gait</a></h3>
                    <p><strong>Authors:</strong> Safwen Naimi, Arij Said, Wassim Bouachir, Guillaume-Alexandre Bilodeau</p>
                    <p><strong>Published:</strong> 8/6/2025</p>
                    <p><strong>Categories:</strong> cs.CV</p>
                    <p><strong>Summary:</strong> We present InceptoFormer, a multi-signal neural framework designed for Parkinsons Disease (PD) severity evaluation via gait dynamics analysis. Our architecture introduces a 1D adaptation of the Inception model, which we refer to as Inception1D, along with a Transformer-based framework to stage PD severity according to the Hoehn and Yahr (HY) scale. The Inception1D component captures multi-scale temporal features by employing parallel 1D convolutional filters with varying kernel sizes, thereby extracting features across multiple temporal scales. The transformer component efficiently models long-range dependencies within gait sequences, providing a comprehensive understanding of both local and global patterns. To address the issue of class imbalance in PD severity staging, we propose a data structuring and preprocessing strategy based on oversampling to enhance the representation of underrepresented severity levels. The overall design enables to capture fine-grained temporal variations and global dynamics in gait signal, significantly improving classification performance for PD severity evaluation. Through extensive experimentation, InceptoFormer achieves an accuracy of 96.6%, outperforming existing state-of-the-art methods in PD severity assessment. The source code for our implementation is publicly available at https://github.com/SafwenNaimi/InceptoFormer</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.04534v1" target="_blank">No Masks Needed: Explainable AI for Deriving Segmentation from Classification</a></h3>
                    <p><strong>Authors:</strong> Mosong Ma, Tania Stathaki, Michalis Lazarou</p>
                    <p><strong>Published:</strong> 8/6/2025</p>
                    <p><strong>Categories:</strong> cs.CV</p>
                    <p><strong>Summary:</strong> Medical image segmentation is vital for modern healthcare and is a key element of computer-aided diagnosis. While recent advancements in computer vision have explored unsupervised segmentation using pre-trained models, these methods have not been translated well to the medical imaging domain. In this work, we introduce a novel approach that fine-tunes pre-trained models specifically for medical images, achieving accurate segmentation with extensive processing. Our method integrates Explainable AI to generate relevance scores, enhancing the segmentation process. Unlike traditional methods that excel in standard benchmarks but falter in medical applications, our approach achieves improved results on datasets like CBIS-DDSM, NuInsSeg and Kvasir-SEG.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.04531v1" target="_blank">Unveiling the Landscape of Clinical Depression Assessment: From Behavioral Signatures to Psychiatric Reasoning</a></h3>
                    <p><strong>Authors:</strong> Zhuang Chen, Guanqun Bi, Wen Zhang, Jiawei Hu, Aoyun Wang, Xiyao Xiao, Kun Feng, Minlie Huang</p>
                    <p><strong>Published:</strong> 8/6/2025</p>
                    <p><strong>Categories:</strong> cs.CL, cs.AI</p>
                    <p><strong>Summary:</strong> Depression is a widespread mental disorder that affects millions worldwide. While automated depression assessment shows promise, most studies rely on limited or non-clinically validated data, and often prioritize complex model design over real-world effectiveness. In this paper, we aim to unveil the landscape of clinical depression assessment. We introduce C-MIND, a clinical neuropsychiatric multimodal diagnosis dataset collected over two years from real hospital visits. Each participant completes three structured psychiatric tasks and receives a final diagnosis from expert clinicians, with informative audio, video, transcript, and functional near-infrared spectroscopy (fNIRS) signals recorded. Using C-MIND, we first analyze behavioral signatures relevant to diagnosis. We train a range of classical models to quantify how different tasks and modalities contribute to diagnostic performance, and dissect the effectiveness of their combinations. We then explore whether LLMs can perform psychiatric reasoning like clinicians and identify their clear limitations in realistic clinical settings. In response, we propose to guide the reasoning process with clinical expertise and consistently improves LLM diagnostic performance by up to 10% in Macro-F1 score. We aim to build an infrastructure for clinical depression assessment from both data and algorithmic perspectives, enabling C-MIND to facilitate grounded and reliable research for mental healthcare.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.04526v1" target="_blank">Policy Design in Zero-Trust Distributed Networks: Challenges and Solutions</a></h3>
                    <p><strong>Authors:</strong> Fannya R. Sandjaja, Ayesha A. Majeed, Abdullah Abdullah, Gyan Wickremasinghe, Karen Rafferty, Vishal Sharma</p>
                    <p><strong>Published:</strong> 8/6/2025</p>
                    <p><strong>Categories:</strong> cs.NI, cs.DC</p>
                    <p><strong>Summary:</strong> Traditional security architectures are becoming more vulnerable to distributed attacks due to significant dependence on trust. This will further escalate when implementing agentic AI within the systems, as more components must be secured over a similar distributed space. These scenarios can be observed in consumer technologies, such as the dense Internet of things (IoT). Here, zero-trust architecture (ZTA) can be seen as a potential solution, which relies on a key principle of not giving users explicit trust, instead always verifying their privileges whenever a request is made. However, the overall security in ZTA is managed through its policies, and unverified policies can lead to unauthorized access. Thus, this paper explores challenges and solutions for ZTA policy design in the context of distributed networks, which is referred to as zero-trust distributed networks (ZTDN). This is followed by a case-study on formal verification of policies using UPPAAL. Subsequently, the importance of accountability and responsibility in the systems security is discussed.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.04524v1" target="_blank">RAIDX: A Retrieval-Augmented Generation and GRPO Reinforcement Learning Framework for Explainable Deepfake Detection</a></h3>
                    <p><strong>Authors:</strong> Tianxiao Li, Zhenglin Huang, Haiquan Wen, Yiwei He, Shuchang Lyu, Baoyuan Wu, Guangliang Cheng</p>
                    <p><strong>Published:</strong> 8/6/2025</p>
                    <p><strong>Categories:</strong> cs.CV, cs.AI</p>
                    <p><strong>Summary:</strong> The rapid advancement of AI-generation models has enabled the creation of hyperrealistic imagery, posing ethical risks through widespread misinformation. Current deepfake detection methods, categorized as face specific detectors or general AI-generated detectors, lack transparency by framing detection as a classification task without explaining decisions. While several LLM-based approaches offer explainability, they suffer from coarse-grained analyses and dependency on labor-intensive annotations. This paper introduces RAIDX (Retrieval-Augmented Image Deepfake Detection and Explainability), a novel deepfake detection framework integrating Retrieval-Augmented Generation (RAG) and Group Relative Policy Optimization (GRPO) to enhance detection accuracy and decision explainability. Specifically, RAIDX leverages RAG to incorporate external knowledge for improved detection accuracy and employs GRPO to autonomously generate fine-grained textual explanations and saliency maps, eliminating the need for extensive manual annotations. Experiments on multiple benchmarks demonstrate RAIDXs effectiveness in identifying real or fake, and providing interpretable rationales in both textual descriptions and saliency maps, achieving state-of-the-art detection performance while advancing transparency in deepfake identification. RAIDX represents the first unified framework to synergize RAG and GRPO, addressing critical gaps in accuracy and explainability. Our code and models will be publicly available.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.04705v1" target="_blank">Occupancy Learning with Spatiotemporal Memory</a></h3>
                    <p><strong>Authors:</strong> Ziyang Leng, Jiawei Yang, Wenlong Yi, Bolei Zhou</p>
                    <p><strong>Published:</strong> 8/6/2025</p>
                    <p><strong>Categories:</strong> cs.CV</p>
                    <p><strong>Summary:</strong> 3D occupancy becomes a promising perception representation for autonomous driving to model the surrounding environment at a fine-grained scale. However, it remains challenging to efficiently aggregate 3D occupancy over time across multiple input frames due to the high processing cost and the uncertainty and dynamics of voxels. To address this issue, we propose ST-Occ, a scene-level occupancy representation learning framework that effectively learns the spatiotemporal feature with temporal consistency. ST-Occ consists of two core designs: a spatiotemporal memory that captures comprehensive historical information and stores it efficiently through a scene-level representation and a memory attention that conditions the current occupancy representation on the spatiotemporal memory with a model of uncertainty and dynamic awareness. Our method significantly enhances the spatiotemporal representation learned for 3D occupancy prediction tasks by exploiting the temporal dependency between multi-frame inputs. Experiments show that our approach outperforms the state-of-the-art methods by a margin of 3 mIoU and reduces the temporal inconsistency by 29%.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.04704v1" target="_blank">Bell states for fermions in loop quantum gravity</a></h3>
                    <p><strong>Authors:</strong> Hanno Sahlmann, Martin ZeiÃŸ</p>
                    <p><strong>Published:</strong> 8/6/2025</p>
                    <p><strong>Categories:</strong> gr-qc, hep-th</p>
                    <p><strong>Summary:</strong> Fermion fields are fundamental for the description of nature and also fit very naturally into the framework of loop quantum gravity. Motivated partially by proposals to use gravitationally mediated entanglement of matter as a witness for the quantum nature of gravity, we investigate how such entanglement can be defined and investigated in loop quantum gravity. In particular, we ask how a pair of fermions in a Bell state could be described in loop quantum, gravity. We demonstrate that the notion of fermionic entanglement in loop quantum gravity is subtle, by showing that some potential ways to define it fail. We then investigate a kinematical observable involving both, fermionic and gravitational degrees of freedom, the component of the fermion spin normal to a surface. We study its properties, and compare it to the standard operator for components of spin in a given direction in quantum mechanics. Using these normal components of spin, we define a kinematical observable that measures the correlation between space-like separated fermions which closely mirrors the CHSH observable. Finally, we exhibit states of the fermions coupled to quantum geometry that violate the Bell-CHSH inequality.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.04702v1" target="_blank">BEVCon: Advancing Birds Eye View Perception with Contrastive Learning</a></h3>
                    <p><strong>Authors:</strong> Ziyang Leng, Jiawei Yang, Zhicheng Ren, Bolei Zhou</p>
                    <p><strong>Published:</strong> 8/6/2025</p>
                    <p><strong>Categories:</strong> cs.CV</p>
                    <p><strong>Summary:</strong> We present BEVCon, a simple yet effective contrastive learning framework designed to improve Birds Eye View (BEV) perception in autonomous driving. BEV perception offers a top-down-view representation of the surrounding environment, making it crucial for 3D object detection, segmentation, and trajectory prediction tasks. While prior work has primarily focused on enhancing BEV encoders and task-specific heads, we address the underexplored potential of representation learning in BEV models. BEVCon introduces two contrastive learning modules: an instance feature contrast module for refining BEV features and a perspective view contrast module that enhances the image backbone. The dense contrastive learning designed on top of detection losses leads to improved feature representations across both the BEV encoder and the backbone. Extensive experiments on the nuScenes dataset demonstrate that BEVCon achieves consistent performance gains, achieving up to +2.4% mAP improvement over state-of-the-art baselines. Our results highlight the critical role of representation learning in BEV perception and offer a complementary avenue to conventional task-specific optimizations.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.04700v1" target="_blank">SEAgent: Self-Evolving Computer Use Agent with Autonomous Learning from Experience</a></h3>
                    <p><strong>Authors:</strong> Zeyi Sun, Ziyu Liu, Yuhang Zang, Yuhang Cao, Xiaoyi Dong, Tong Wu, Dahua Lin, Jiaqi Wang</p>
                    <p><strong>Published:</strong> 8/6/2025</p>
                    <p><strong>Categories:</strong> cs.AI, cs.CL, cs.CV, cs.LG, cs.MA, cs.MM</p>
                    <p><strong>Summary:</strong> Repurposing large vision-language models (LVLMs) as computer use agents (CUAs) has led to substantial breakthroughs, primarily driven by human-labeled data. However, these models often struggle with novel and specialized software, particularly in scenarios lacking human annotations. To address this challenge, we propose SEAgent, an agentic self-evolving framework enabling CUAs to autonomously evolve through interactions with unfamiliar software. Specifically, SEAgent empowers computer-use agents to autonomously master novel software environments via experiential learning, where agents explore new software, learn through iterative trial-and-error, and progressively tackle auto-generated tasks organized from simple to complex. To achieve this goal, we design a World State Model for step-wise trajectory assessment, along with a Curriculum Generator that generates increasingly diverse and challenging tasks. The agents policy is updated through experiential learning, comprised of adversarial imitation of failure actions and Group Relative Policy Optimization (GRPO) on successful ones. Furthermore, we introduce a specialist-to-generalist training strategy that integrates individual experiential insights from specialist agents, facilitating the development of a stronger generalist CUA capable of continuous autonomous evolution. This unified agent ultimately achieves performance surpassing ensembles of individual specialist agents on their specialized software. We validate the effectiveness of SEAgent across five novel software environments within OS-World. Our approach achieves a significant improvement of 23.2% in success rate, from 11.3% to 34.5%, over a competitive open-source CUA, i.e., UI-TARS.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.04699v1" target="_blank">Hop, Skip, and Overthink: Diagnosing Why Reasoning Models Fumble during Multi-Hop Analysis</a></h3>
                    <p><strong>Authors:</strong> Anushka Yadav, Isha Nalawade, Srujana Pillarichety, Yashwanth Babu, Reshmi Ghosh, Samyadeep Basu, Wenlong Zhao, Ali Nasaeh, Sriram Balasubramanian, Soundararajan Srinivasan</p>
                    <p><strong>Published:</strong> 8/6/2025</p>
                    <p><strong>Categories:</strong> cs.CL, cs.AI</p>
                    <p><strong>Summary:</strong> The emergence of reasoning models and their integration into practical AI chat bots has led to breakthroughs in solving advanced math, deep search, and extractive question answering problems that requires a complex and multi-step thought process. Yet, a complete understanding of why these models hallucinate more than general purpose language models is missing. In this investigative study, we systematicallyexplore reasoning failures of contemporary language models on multi-hop question answering tasks. We introduce a novel, nuanced error categorization framework that examines failures across three critical dimensions: the diversity and uniqueness of source documents involved (hops), completeness in capturing relevant information (coverage), and cognitive inefficiency (overthinking). Through rigorous hu-man annotation, supported by complementary automated metrics, our exploration uncovers intricate error patterns often hidden by accuracy-centric evaluations. This investigative approach provides deeper insights into the cognitive limitations of current models and offers actionable guidance toward enhancing reasoning fidelity, transparency, and robustness in future language modeling efforts.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.04698v1" target="_blank">FaST: Feature-aware Sampling and Tuning for Personalized Preference Alignment with Limited Data</a></h3>
                    <p><strong>Authors:</strong> Thibaut Thonet, GermÃ¡n Kruszewski, Jos Rozen, Pierre Erbacher, Marc Dymetman</p>
                    <p><strong>Published:</strong> 8/6/2025</p>
                    <p><strong>Categories:</strong> cs.CL</p>
                    <p><strong>Summary:</strong> LLM-powered conversational assistants are often deployed in a one-size-fits-all manner, which fails to accommodate individual user preferences. Recently, LLM personalization -- tailoring models to align with specific user preferences -- has gained increasing attention as a way to bridge this gap. In this work, we specifically focus on a practical yet challenging setting where only a small set of preference annotations can be collected per user -- a problem we define as Personalized Preference Alignment with Limited Data (PPALLI). To support research in this area, we introduce two datasets -- DnD and ELIP -- and benchmark a variety of alignment techniques on them. We further propose FaST, a highly parameter-efficient approach that leverages high-level features automatically discovered from the data, achieving the best overall performance.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.04693v1" target="_blank">Finite 2-group gauge theory and its 3+1D lattice realization</a></h3>
                    <p><strong>Authors:</strong> Mo Huang</p>
                    <p><strong>Published:</strong> 8/6/2025</p>
                    <p><strong>Categories:</strong> math-ph, cond-mat.str-el, hep-th, math.CT, math.MP, math.QA</p>
                    <p><strong>Summary:</strong> In this work, we employ the Tannaka-Krein reconstruction to compute the quantum double $\mathcal D(\mathcal G)$ of a finite 2-group $\mathcal G$ as a Hopf monoidal category. We also construct a 3+1D lattice model from the Dijkgraaf-Witten TQFT functor for the 2-group $\mathcal G$, generalizing Kitaevs 2+1D quantum double model. Notably, the string-like local operators in this lattice model are shown to form $\mathcal D(\mathcal G)$. Specializing to $\mathcal G = \mathbb{Z}_2$, we demonstrate that the topological defects in the 3+1D toric code model are modules over $\mathcal D(\mathbb{Z}_2)$.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.04690v1" target="_blank">Controlled regularity at future null infinity from past asymptotic initial data: the wave equation</a></h3>
                    <p><strong>Authors:</strong> Jordan Marajh, Grigalius Taujanskas, Juan A. Valiente Kroon</p>
                    <p><strong>Published:</strong> 8/6/2025</p>
                    <p><strong>Categories:</strong> gr-qc, math.AP</p>
                    <p><strong>Summary:</strong> We study the relationship between asymptotic characteristic initial data for the wave equation at past null infinity and the regularity of the solution at future null infinity on the Minkowski spacetime. By constructing estimates on a causal rectangle reaching the conformal boundary, we prove that the solution admits an asymptotic expansion near null and spatial infinity whose regularity is controlled quantitatively in terms of the regularity of the data at past null infinity. In particular, our method gives rise to solutions to the wave equation in a neighbourhood of spatial infinity satisfying the peeling behaviour, for data on past null infinity with non-compact support. Our approach makes use of Friedrichs conformal representation of spatial infinity in which we prove delicate non-degenerate Gr\onwall estimates. We describe the relationship between the solution and the data both in terms of Friedrichs conformal coordinates and the usual physical coordinates on Minkowski space.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.04689v1" target="_blank">A probabilistic quantum algorithm for Lyapunov equations and matrix inversion</a></h3>
                    <p><strong>Authors:</strong> Marcello Benedetti, Ansis Rosmanis, Matthias Rosenkranz</p>
                    <p><strong>Published:</strong> 8/6/2025</p>
                    <p><strong>Categories:</strong> quant-ph</p>
                    <p><strong>Summary:</strong> We present a probabilistic quantum algorithm for preparing mixed states which, in expectation, are proportional to the solutions of Lyapunov equations -- linear matrix equations ubiquitous in the analysis of classical and quantum dynamical systems. Building on previous results by Zhang et al., arXiv:2304.04526, at each step the algorithm either returns the current state, applies a trace non-increasing completely positive map, or restarts depending on the outcomes of a biased coin flip and an ancilla measurement. We introduce a deterministic stopping rule which leads to an efficient algorithm with a bounded expected number of calls to a block-encoding and a state preparation circuit representing the two input matrices of the Lyapunov equations. We also consider approximating the normalized inverse of a positive definite matrix $A$ with condition number $\kappa$ up to trace distance error $\epsilon$. For this special case the algorithm requires, in expectation, at most $\lceil \kappa\ln(1/\epsilon) \rceil+1$ calls to a block-encoding of $\sqrt{A/\|A\|}$. This matches the optimal query complexity in $\kappa$ and $\epsilon$ of the related, but distinct, quantum linear system solvers. In its most general form, the algorithm generates mixed states which approximate matrix-valued weighted sums and integrals.</p>
                
            
                
                    <h3><a href="http://dx.doi.org/10.1109/VRW55335.2022.00178" target="_blank">MienCap: Realtime Performance-Based Facial Animation with Live Mood Dynamics</a></h3>
                    <p><strong>Authors:</strong> Ye Pan, Ruisi Zhang, Jingying Wang, Nengfu Chen, Yilin Qiu, Yu Ding, Kenny Mitchell</p>
                    <p><strong>Published:</strong> 8/6/2025</p>
                    <p><strong>Categories:</strong> cs.GR, cs.CV, I.3.2; I.4.10</p>
                    <p><strong>Summary:</strong> Our purpose is to improve performance-based animation which can drive believable 3D stylized characters that are truly perceptual. By combining traditional blendshape animation techniques with multiple machine learning models, we present both non-real time and real time solutions which drive character expressions in a geometrically consistent and perceptually valid way. For the non-real time system, we propose a 3D emotion transfer network makes use of a 2D human image to generate a stylized 3D rig parameters. For the real time system, we propose a blendshape adaption network which generates the character rig parameter motions with geometric consistency and temporally stability. We demonstrate the effectiveness of our system by comparing to a commercial product Faceware. Results reveal that ratings of the recognition, intensity, and attractiveness of expressions depicted for animated characters via our systems are statistically higher than Faceware. Our results may be implemented into the animation pipeline, and provide animators with a system for creating the expressions they wish to use more quickly and accurately.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.04685v1" target="_blank">Phase-Pole-Free Images and Smooth Coil Sensitivity Maps by Regularized Nonlinear Inversion</a></h3>
                    <p><strong>Authors:</strong> Moritz Blumenthal, Martin Uecker</p>
                    <p><strong>Published:</strong> 8/6/2025</p>
                    <p><strong>Categories:</strong> physics.med-ph, eess.SP</p>
                    <p><strong>Summary:</strong> Purpose: Phase singularities are a common problem in image reconstruction with auto-calibrated sensitivities due to an inherent ambiguity of the estimation problem. The purpose of this work is to develop a method for detecting and correcting phase poles in non-linear inverse (NLINV) reconstruction of MR images and coil sensitivity maps. Methods: Phase poles are detected in individual coil sensitivity maps by computing the curl in each pixel. A weighted average of the curl in each coil is computed to detect phase poles. Phase pole detection and correction is then integrated into the iteratively regularized Gauss-Newton method of the NLINV algorithm, which then avoid phase singularities in the reconstructed images. The method is evaluated for reconstruction of accelerated Cartesian MPRAGE data of the brain and interactive radial real-time MRI of the human heart. Results: Phase poles are reliably removed in NLINV reconstructions for both applications. NLINV with phase pole correction can reliably and efficiently estimate coil sensitivity profiles free from singularities even from very small ($7\times7$) auto-calibration (AC) regions. Conclusion: NLINV emerges as an efficient and reliable tool for image reconstruction and coil sensitivity estimation in challenging MRI applications.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.04684v1" target="_blank">Innermost stable circular orbit of Kerr-Bertotti-Robinson black holes and inspirals from it: Exact solutions</a></h3>
                    <p><strong>Authors:</strong> Tower Wang</p>
                    <p><strong>Published:</strong> 8/6/2025</p>
                    <p><strong>Categories:</strong> gr-qc, astro-ph.GA</p>
                    <p><strong>Summary:</strong> For an uncharged test particle in the Kerr-Bertotti-Robinson spacetime, two classes of remarkable orbits are worked out, both in exact forms. First, for both prograde and retrograde motions, the radii of innermost stable circular orbits are expressed fully in terms of the outer and inner horizon radii just like Kerr black holes, despite the fact that Kerr-Bertotti-Robinson black holes have three parameters. Second, closed analytic solutions are given to the problem of a test particle inspiraling toward the Kerr-Bertotti-Robinson black hole from innermost stable circular orbits at the infinitely distant past. These exact solutions can serve as a springboard for more general solutions and astrophysical applications in the future.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.04683v1" target="_blank">Query Attribute Modeling: Improving search relevance with Semantic Search and Meta Data Filtering</a></h3>
                    <p><strong>Authors:</strong> Karthik Menon, Batool Arhamna Haider, Muhammad Arham, Kanwal Mehreen, Ram Mohan Rao Kadiyala, Hamza Farooq</p>
                    <p><strong>Published:</strong> 8/6/2025</p>
                    <p><strong>Categories:</strong> cs.IR, cs.AI, cs.CL, cs.LG</p>
                    <p><strong>Summary:</strong> This study introduces Query Attribute Modeling (QAM), a hybrid framework that enhances search precision and relevance by decomposing open text queries into structured metadata tags and semantic elements. QAM addresses traditional search limitations by automatically extracting metadata filters from free-form text queries, reducing noise and enabling focused retrieval of relevant items. Experimental evaluation using the Amazon Toys Reviews dataset (10,000 unique items with 40,000+ reviews and detailed product attributes) demonstrated QAMs superior performance, achieving a mean average precision at 5 (mAP@5) of 52.99\%. This represents significant improvement over conventional methods, including BM25 keyword search, encoder-based semantic similarity search, cross-encoder re-ranking, and hybrid search combining BM25 and semantic results via Reciprocal Rank Fusion (RRF). The results establish QAM as a robust solution for Enterprise Search applications, particularly in e-commerce systems.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.04682v1" target="_blank">TurboTrain: Towards Efficient and Balanced Multi-Task Learning for Multi-Agent Perception and Prediction</a></h3>
                    <p><strong>Authors:</strong> Zewei Zhou, Seth Z. Zhao, Tianhui Cai, Zhiyu Huang, Bolei Zhou, Jiaqi Ma</p>
                    <p><strong>Published:</strong> 8/6/2025</p>
                    <p><strong>Categories:</strong> cs.CV</p>
                    <p><strong>Summary:</strong> End-to-end training of multi-agent systems offers significant advantages in improving multi-task performance. However, training such models remains challenging and requires extensive manual design and monitoring. In this work, we introduce TurboTrain, a novel and efficient training framework for multi-agent perception and prediction. TurboTrain comprises two key components: a multi-agent spatiotemporal pretraining scheme based on masked reconstruction learning and a balanced multi-task learning strategy based on gradient conflict suppression. By streamlining the training process, our framework eliminates the need for manually designing and tuning complex multi-stage training pipelines, substantially reducing training time and improving performance. We evaluate TurboTrain on a real-world cooperative driving dataset, V2XPnP-Seq, and demonstrate that it further improves the performance of state-of-the-art multi-agent perception and prediction models. Our results highlight that pretraining effectively captures spatiotemporal multi-agent features and significantly benefits downstream tasks. Moreover, the proposed balanced multi-task learning strategy enhances detection and prediction.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.04681v1" target="_blank">Perceiving and Acting in First-Person: A Dataset and Benchmark for Egocentric Human-Object-Human Interactions</a></h3>
                    <p><strong>Authors:</strong> Liang Xu, Chengqun Yang, Zili Lin, Fei Xu, Yifan Liu, Congsheng Xu, Yiyi Zhang, Jie Qin, Xingdong Sheng, Yunhui Liu, Xin Jin, Yichao Yan, Wenjun Zeng, Xiaokang Yang</p>
                    <p><strong>Published:</strong> 8/6/2025</p>
                    <p><strong>Categories:</strong> cs.CV</p>
                    <p><strong>Summary:</strong> Learning action models from real-world human-centric interaction datasets is important towards building general-purpose intelligent assistants with efficiency. However, most existing datasets only offer specialist interaction category and ignore that AI assistants perceive and act based on first-person acquisition. We urge that both the generalist interaction knowledge and egocentric modality are indispensable. In this paper, we embed the manual-assisted task into a vision-language-action framework, where the assistant provides services to the instructor following egocentric vision and commands. With our hybrid RGB-MoCap system, pairs of assistants and instructors engage with multiple objects and the scene following GPT-generated scripts. Under this setting, we accomplish InterVLA, the first large-scale human-object-human interaction dataset with 11.4 hours and 1.2M frames of multimodal data, spanning 2 egocentric and 5 exocentric videos, accurate human/object motions and verbal commands. Furthermore, we establish novel benchmarks on egocentric human motion estimation, interaction synthesis, and interaction prediction with comprehensive analysis. We believe that our InterVLA testbed and the benchmarks will foster future works on building AI agents in the physical world.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.04677v1" target="_blank">ANPrompt: Anti-noise Prompt Tuning for Vision-Language Models</a></h3>
                    <p><strong>Authors:</strong> Yansheng Gao, Yufei Zheng, Jinghan Qu, Zixi Zhu, Yukuan Zhang, Shengsheng Wang</p>
                    <p><strong>Published:</strong> 8/6/2025</p>
                    <p><strong>Categories:</strong> cs.CV</p>
                    <p><strong>Summary:</strong> Prompt tuning has emerged as an efficient and effective technique for adapting vision-language models (VLMs) with low computational overhead. However, existing methods often overlook the vulnerability of prompt-tuned VLMs to weak semantic perturbations-such as subtle image or text noise-that degrade their generalization to unseen classes. To address this limitation, we propose ANPrompt, a novel prompt tuning framework designed to enhance robustness under such perturbations. ANPrompt first constructs weak noise text features by fusing original and noise-perturbed text embeddings, which are then clustered to form noise prompts. These noise prompts are integrated with learnable prompt tokens to generate anti-noise prompts, which are injected into the deeper layers of both image and text encoders. To further capture the noise-aware visual semantics, ANPrompt computes the Noise-Resistant Visual Prompt Prototype (NRVPP) by averaging the output prompt tokens from the vision encoder. Finally, ANPrompt introduces alignment, robustness, and anti-noise objectives by computing a Weak semantic noise Alignment Loss (WALoss) alongside the standard cross-entropy and sim loss. Experiments across 11 benchmarks demonstrate that ANPrompt consistently outperforms existing prompt tuning approaches, achieving superior robustness to semantic noise and improved generalization to novel categories.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.04676v1" target="_blank">GeRe: Towards Efficient Anti-Forgetting in Continual Learning of LLM via General Samples Replay</a></h3>
                    <p><strong>Authors:</strong> Yunan Zhang, Shuoran Jiang, Mengchen Zhao, Yuefeng Li, Yang Fan, Xiangping Wu, Qingcai Chen</p>
                    <p><strong>Published:</strong> 8/6/2025</p>
                    <p><strong>Categories:</strong> cs.CL, cs.AI, cs.LG</p>
                    <p><strong>Summary:</strong> The continual learning capability of large language models (LLMs) is crucial for advancing artificial general intelligence. However, continual fine-tuning LLMs across various domains often suffers from catastrophic forgetting, characterized by: 1) significant forgetting of their general capabilities, and 2) sharp performance declines in previously learned tasks. To simultaneously address both issues in a simple yet stable manner, we propose General Sample Replay (GeRe), a framework that use usual pretraining texts for efficient anti-forgetting. Beyond revisiting the most prevalent replay-based practices under GeRe, we further leverage neural states to introduce a enhanced activation states constrained optimization method using threshold-based margin (TM) loss, which maintains activation state consistency during replay learning. We are the first to validate that a small, fixed set of pre-collected general replay samples is sufficient to resolve both concerns--retaining general capabilities while promoting overall performance across sequential tasks. Indeed, the former can inherently facilitate the latter. Through controlled experiments, we systematically compare TM with different replay strategies under the GeRe framework, including vanilla label fitting, logit imitation via KL divergence and feature imitation via L1/L2 losses. Results demonstrate that TM consistently improves performance and exhibits better robustness. Our work paves the way for efficient replay of LLMs for the future. Our code and data are available at https://github.com/Qznan/GeRe.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.04672v1" target="_blank">MARTINI-based force fields for predicting gas separation performances of MOF/polymer composites</a></h3>
                    <p><strong>Authors:</strong> Cecilia M. S. Alvares, Rocio Semino</p>
                    <p><strong>Published:</strong> 8/6/2025</p>
                    <p><strong>Categories:</strong> physics.chem-ph</p>
                    <p><strong>Summary:</strong> MOF/polymer composites have been widely investigated in the past decade for gas separation applications. However, the impact of MOF nanoparticle morphology and size in gas separation have not yet been systematically studied by computer simulation techniques. In this work, coarse grained simulations are deployed to study gas adsorption in ZIF-8/PVDF at the nanoparticle level. Nanoparticles of different morphologies and sizes are explored, and adsorption of CO2, N2 and CH4 is investigated throughout the extension of the bulk and surface of the nanoparticle as well as of the polymer phase. Results reproduce the expected preference for CO2 over the other two gasses. Nanoparticles of smaller sizes provide better separation performance at ambient conditions, while rhombic dodecahedron nanoparticles perform better than cubic ones. This work presents a perspective on the merits and limitations of modelling gas adsorption in MOF/polymer composites at the nanoparticle level via particle-based coarse graining approaches, and provides a methodological set-up which can be integrated into high-throughput schemes, bringing us closer to reaching time- and length scales that can be directly compared with experimental data.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.04669v1" target="_blank">Cybersecurity of Quantum Key Distribution Implementations</a></h3>
                    <p><strong>Authors:</strong> Ittay Alfassi, Ran Gelles, Rotem Liss, Tal Mor</p>
                    <p><strong>Published:</strong> 8/6/2025</p>
                    <p><strong>Categories:</strong> quant-ph, cs.CR</p>
                    <p><strong>Summary:</strong> Practical implementations of Quantum Key Distribution (QKD) often deviate from the theoretical protocols, exposing the implementations to various attacks even when the underlying (ideal) protocol is proven secure. We present new analysis tools and methodologies for quantum cybersecurity, adapting the concepts of vulnerabilities, attack surfaces, and exploits from classical cybersecurity to QKD implementation attacks. We present three additional concepts, derived from the connection between classical and quantum cybersecurity: Quantum Fuzzing, which is the first tool for black-box vulnerability research on QKD implementations; Reversed-Space Attacks, which are a generic exploit method using the attack surface of imperfect receivers; and a concrete quantum-mechanical definition of Quantum Side-Channel Attacks, meaningfully distinguishing them from other types of attacks. Using our tools, we analyze multiple existing QKD attacks and show that the Bright Illumination attack could have been fully constructed even with minimal knowledge of the device implementation. This work begins to bridge the gap between current analysis methods for experimental attacks on QKD implementations and the decades-long research in the field of classical cybersecurity, improving the practical security of QKD products and enhancing their usefulness in real-world systems.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.04668v1" target="_blank">Inequality in the Age of Pseudonymity</a></h3>
                    <p><strong>Authors:</strong> Aviv Yaish, Nir Chemaya, Lin William Cong, Dahlia Malkhi</p>
                    <p><strong>Published:</strong> 8/6/2025</p>
                    <p><strong>Categories:</strong> cs.GT, cs.CY, econ.TH</p>
                    <p><strong>Summary:</strong> Inequality measures such as the Gini coefficient are used to inform and motivate policymaking, and are increasingly applied to digital platforms. We analyze how measures fare in pseudonymous settings, as common to internet-based or blockchain-based platforms. One key challenge that arises is the ability of actors to create multiple fake identities under fictitious false names, also known as ``Sybils. While some actors may do so to preserve their privacy, we show that this can inadvertently distort inequality metrics. As we show, when using inequality measures that satisfy literatures canonical set of desired properties, the presence of Sybils in an economy implies that it is impossible to properly measure the economys inequality. Then, we present several classes of Sybil-proof measures that satisfy relaxed versions of the aforementioned desired properties, and, by fully characterizing them, we prove that the structure imposed restricts their ability to assess inequality at a fine-grained level. In addition, we prove that popular inequality metrics, including the famous Gini coefficient, are vulnerable to Sybil manipulations, and examine the dynamics that result in the creation of Sybils, whether in pseudonymous settings or traditional ones.</p>
                
            
                
                    <h3><a href="http://dx.doi.org/10.1090/conm/815/16316" target="_blank">New formulas for Macdonald polynomials via the multispecies exclusion and zero range processes</a></h3>
                    <p><strong>Authors:</strong> Olya Mandelshtam</p>
                    <p><strong>Published:</strong> 8/6/2025</p>
                    <p><strong>Categories:</strong> math.CO, 05E05, 60G10</p>
                    <p><strong>Summary:</strong> We describe some recently discovered connections between one-dimensional interacting particle models and Macdonald polynomials. The first such model is the multispecies asymmetric simple exclusion process (ASEP) on a ring, linked to the symmetric Macdonald polynomial $P_{\lambda}(X;q,t)$ through its partition function. Through this connection, a new formula was found for $P_{\lambda}$ by generalizing multiline queues, which were introduced by Martin in 2018 to compute stationary probabilities of the ASEP. The second particle model is the multispecies totally asymmetric zero range process (TAZRP) on a ring, which was very recently found to have an analogous connection to the modified Macdonald polynomial $\widetilde{H}_{\lambda}(X;q,t)$ through its partition function. This discovery coincided with a new formula for $\widetilde{H}_{\lambda}$, this time in terms of tableaux with a queue inversion statistic, which also compute stationary probabilities of the TAZRP. We explain the plethystic relationship between multiline queues and queue inversion tableaux, and along the way, derive a new formula for $P_{\lambda}$ using the queue inversion statistic. This plethystic correspondence is closely related to fusion in the setting of integrable systems.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.04664v1" target="_blank">Sculptor: Empowering LLMs with Cognitive Agency via Active Context Management</a></h3>
                    <p><strong>Authors:</strong> Mo Li, L. H. Xu, Qitai Tan, Ting Cao, Yunxin Liu</p>
                    <p><strong>Published:</strong> 8/6/2025</p>
                    <p><strong>Categories:</strong> cs.CL, cs.AI, cs.LG</p>
                    <p><strong>Summary:</strong> Large Language Models (LLMs) suffer from significant performance degradation when processing long contexts due to proactive interference, where irrelevant information in earlier parts of the context disrupts reasoning and memory recall. While most research focuses on external memory systems to augment LLMs capabilities, we propose a complementary approach: empowering LLMs with Active Context Management (ACM) tools to actively sculpt their internal working memory. We introduce Sculptor, a framework that equips LLMs with three categories of tools: (1) context fragmentation, (2) summary, hide, and restore, and (3) intelligent search. Our approach enables LLMs to proactively manage their attention and working memory, analogous to how humans selectively focus on relevant information while filtering out distractions. Experimental evaluation on information-sparse benchmarks-PI-LLM (proactive interference) and NeedleBench Multi-Needle Reasoning-demonstrates that Sculptor significantly improves performance even without specific training, leveraging LLMs inherent tool calling generalization capabilities. By enabling Active Context Management, Sculptor not only mitigates proactive interference but also provides a cognitive foundation for more reliable reasoning across diverse long-context tasks-highlighting that explicit context-control strategies, rather than merely larger token windows, are key to robustness at scale.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.04663v1" target="_blank">HierarchicalPrune: Position-Aware Compression for Large-Scale Diffusion Models</a></h3>
                    <p><strong>Authors:</strong> Young D. Kwon, Rui Li, Sijia Li, Da Li, Sourav Bhattacharya, Stylianos I. Venieris</p>
                    <p><strong>Published:</strong> 8/6/2025</p>
                    <p><strong>Categories:</strong> cs.CV, cs.AI</p>
                    <p><strong>Summary:</strong> State-of-the-art text-to-image diffusion models (DMs) achieve remarkable quality, yet their massive parameter scale (8-11B) poses significant challenges for inferences on resource-constrained devices. In this paper, we present HierarchicalPrune, a novel compression framework grounded in a key observation: DM blocks exhibit distinct functional hierarchies, where early blocks establish semantic structures while later blocks handle texture refinements. HierarchicalPrune synergistically combines three techniques: (1) Hierarchical Position Pruning, which identifies and removes less essential later blocks based on position hierarchy; (2) Positional Weight Preservation, which systematically protects early model portions that are essential for semantic structural integrity; and (3) Sensitivity-Guided Distillation, which adjusts knowledge-transfer intensity based on our discovery of block-wise sensitivity variations. As a result, our framework brings billion-scale diffusion models into a range more suitable for on-device inference, while preserving the quality of the output images. Specifically, when combined with INT4 weight quantisation, HierarchicalPrune achieves 77.5-80.4% memory footprint reduction (e.g., from 15.8 GB to 3.2 GB) and 27.9-38.0% latency reduction, measured on server and consumer grade GPUs, with the minimum drop of 2.6% in GenEval score and 7% in HPSv2 score compared to the original model. Last but not least, our comprehensive user study with 85 participants demonstrates that HierarchicalPrune maintains perceptual quality comparable to the original model while significantly outperforming prior works.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.04660v1" target="_blank">Multi-module GRPO: Composing Policy Gradients and Prompt Optimization for Language Model Programs</a></h3>
                    <p><strong>Authors:</strong> Noah Ziems, Dilara Soylu, Lakshya A Agrawal, Isaac Miller, Liheng Lai, Chen Qian, Kaiqiang Song, Meng Jiang, Dan Klein, Matei Zaharia, Karel DOosterlinck, Christopher Potts, Omar Khattab</p>
                    <p><strong>Published:</strong> 8/6/2025</p>
                    <p><strong>Categories:</strong> cs.CL</p>
                    <p><strong>Summary:</strong> Group Relative Policy Optimization (GRPO) has proven to be an effective tool for post-training language models (LMs). However, AI systems are increasingly expressed as modular programs that mix together multiple LM calls with distinct prompt templates and other tools, and it is not clear how best to leverage GRPO to improve these systems. We begin to address this challenge by defining mmGRPO, a simple multi-module generalization of GRPO that groups LM calls by module across rollouts and handles variable-length and interrupted trajectories. We find that mmGRPO, composed with automatic prompt optimization, improves accuracy by 11% on average across classification, many-hop search, and privacy-preserving delegation tasks against the post-trained LM, and by 5% against prompt optimization on its own. We open-source mmGRPO in DSPy as the dspy.GRPO optimizer.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.04659v1" target="_blank">PixCuboid: Room Layout Estimation from Multi-view Featuremetric Alignment</a></h3>
                    <p><strong>Authors:</strong> Gustav Hanning, Kalle Ã…strÃ¶m, Viktor Larsson</p>
                    <p><strong>Published:</strong> 8/6/2025</p>
                    <p><strong>Categories:</strong> cs.CV, I.4</p>
                    <p><strong>Summary:</strong> Coarse room layout estimation provides important geometric cues for many downstream tasks. Current state-of-the-art methods are predominantly based on single views and often assume panoramic images. We introduce PixCuboid, an optimization-based approach for cuboid-shaped room layout estimation, which is based on multi-view alignment of dense deep features. By training with the optimization end-to-end, we learn feature maps that yield large convergence basins and smooth loss landscapes in the alignment. This allows us to initialize the room layout using simple heuristics. For the evaluation we propose two new benchmarks based on ScanNet++ and 2D-3D-Semantics, with manually verified ground truth 3D cuboids. In thorough experiments we validate our approach and significantly outperform the competition. Finally, while our network is trained with single cuboids, the flexibility of the optimization-based approach allow us to easily extend to multi-room estimation, e.g. larger apartments or offices. Code and model weights are available at https://github.com/ghanning/PixCuboid.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.04658v1" target="_blank">YOLOv8-Based Deep Learning Model for Automated Poultry Disease Detection and Health Monitoring paper</a></h3>
                    <p><strong>Authors:</strong> Akhil Saketh Reddy Sabbella, Ch. Lakshmi Prachothan, Eswar Kumar Panta</p>
                    <p><strong>Published:</strong> 8/6/2025</p>
                    <p><strong>Categories:</strong> cs.CV, cs.AI</p>
                    <p><strong>Summary:</strong> In the poultry industry, detecting chicken illnesses is essential to avoid financial losses. Conventional techniques depend on manual observation, which is laborious and prone to mistakes. Using YOLO v8 a deep learning model for real-time object recognition. This study suggests an AI based approach, by developing a system that analyzes high resolution chicken photos, YOLO v8 detects signs of illness, such as abnormalities in behavior and appearance. A sizable, annotated dataset has been used to train the algorithm, which provides accurate real-time identification of infected chicken and prompt warnings to farm operators for prompt action. By facilitating early infection identification, eliminating the need for human inspection, and enhancing biosecurity in large-scale farms, this AI technology improves chicken health management. The real-time features of YOLO v8 provide a scalable and effective method for improving farm management techniques.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.04655v1" target="_blank">X-SAM: From Segment Anything to Any Segmentation</a></h3>
                    <p><strong>Authors:</strong> Hao Wang, Limeng Qiao, Zequn Jie, Zhijian Huang, Chengjian Feng, Qingfang Zheng, Lin Ma, Xiangyuan Lan, Xiaodan Liang</p>
                    <p><strong>Published:</strong> 8/6/2025</p>
                    <p><strong>Categories:</strong> cs.CV, cs.AI</p>
                    <p><strong>Summary:</strong> Large Language Models (LLMs) demonstrate strong capabilities in broad knowledge representation, yet they are inherently deficient in pixel-level perceptual understanding. Although the Segment Anything Model (SAM) represents a significant advancement in visual-prompt-driven image segmentation, it exhibits notable limitations in multi-mask prediction and category-specific segmentation tasks, and it cannot integrate all segmentation tasks within a unified model architecture. To address these limitations, we present X-SAM, a streamlined Multimodal Large Language Model (MLLM) framework that extends the segmentation paradigm from \textit{segment anything} to \textit{any segmentation}. Specifically, we introduce a novel unified framework that enables more advanced pixel-level perceptual comprehension for MLLMs. Furthermore, we propose a new segmentation task, termed Visual GrounDed (VGD) segmentation, which segments all instance objects with interactive visual prompts and empowers MLLMs with visual grounded, pixel-wise interpretative capabilities. To enable effective training on diverse data sources, we present a unified training strategy that supports co-training across multiple datasets. Experimental results demonstrate that X-SAM achieves state-of-the-art performance on a wide range of image segmentation benchmarks, highlighting its efficiency for multimodal, pixel-level visual understanding. Code is available at https://github.com/wanghao9610/X-SAM.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.04650v1" target="_blank">EncQA: Benchmarking Vision-Language Models on Visual Encodings for Charts</a></h3>
                    <p><strong>Authors:</strong> Kushin Mukherjee, Donghao Ren, Dominik Moritz, Yannick Assogba</p>
                    <p><strong>Published:</strong> 8/6/2025</p>
                    <p><strong>Categories:</strong> cs.CV, I.2.0</p>
                    <p><strong>Summary:</strong> Multimodal vision-language models (VLMs) continue to achieve ever-improving scores on chart understanding benchmarks. Yet, we find that this progress does not fully capture the breadth of visual reasoning capabilities essential for interpreting charts. We introduce EncQA, a novel benchmark informed by the visualization literature, designed to provide systematic coverage of visual encodings and analytic tasks that are crucial for chart understanding. EncQA provides 2,076 synthetic question-answer pairs, enabling balanced coverage of six visual encoding channels (position, length, area, color quantitative, color nominal, and shape) and eight tasks (find extrema, retrieve value, find anomaly, filter values, compute derived value exact, compute derived value relative, correlate values, and correlate values relative). Our evaluation of 9 state-of-the-art VLMs reveals that performance varies significantly across encodings within the same task, as well as across tasks. Contrary to expectations, we observe that performance does not improve with model size for many task-encoding pairs. Our results suggest that advancing chart understanding requires targeted strategies addressing specific visual reasoning gaps, rather than solely scaling up model or dataset size.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.04648v1" target="_blank">Super Resolved Imaging with Adaptive Optics</a></h3>
                    <p><strong>Authors:</strong> Robin Swanson, Esther Y. H. Lin, Masen Lamb, Suresh Sivanandam, Kiriakos N. Kutulakos</p>
                    <p><strong>Published:</strong> 8/6/2025</p>
                    <p><strong>Categories:</strong> astro-ph.IM, cs.CV</p>
                    <p><strong>Summary:</strong> Astronomical telescopes suffer from a tradeoff between field of view (FoV) and image resolution: increasing the FoV leads to an optical field that is under-sampled by the science camera. This work presents a novel computational imaging approach to overcome this tradeoff by leveraging the existing adaptive optics (AO) systems in modern ground-based telescopes. Our key idea is to use the AO systems deformable mirror to apply a series of learned, precisely controlled distortions to the optical wavefront, producing a sequence of images that exhibit distinct, high-frequency, sub-pixel shifts. These images can then be jointly upsampled to yield the final super-resolved image. Crucially, we show this can be done while simultaneously maintaining the core AO operation--correcting for the unknown and rapidly changing wavefront distortions caused by Earths atmosphere. To achieve this, we incorporate end-to-end optimization of both the induced mirror distortions and the upsampling algorithm, such that telescope-specific optics and temporal statistics of atmospheric wavefront distortions are accounted for. Our experimental results with a hardware prototype, as well as simulations, demonstrate significant SNR improvements of up to 12 dB over non-AO super-resolution baselines, using only existing telescope optics and no hardware modifications. Moreover, by using a precise bench-top replica of a complete telescope and AO system, we show that our methodology can be readily transferred to an operational telescope. Project webpage: https://www.cs.toronto.edu/~robin/aosr/</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.04645v1" target="_blank">A Scalable Pretraining Framework for Link Prediction with Efficient Adaptation</a></h3>
                    <p><strong>Authors:</strong> Yu Song, Zhigang Hua, Harry Shomer, Yan Xie, Jingzhe Liu, Bo Long, Hui Liu</p>
                    <p><strong>Published:</strong> 8/6/2025</p>
                    <p><strong>Categories:</strong> cs.LG, cs.AI</p>
                    <p><strong>Summary:</strong> Link Prediction (LP) is a critical task in graph machine learning. While Graph Neural Networks (GNNs) have significantly advanced LP performance recently, existing methods face key challenges including limited supervision from sparse connectivity, sensitivity to initialization, and poor generalization under distribution shifts. We explore pretraining as a solution to address these challenges. Unlike node classification, LP is inherently a pairwise task, which requires the integration of both node- and edge-level information. In this work, we present the first systematic study on the transferability of these distinct modules and propose a late fusion strategy to effectively combine their outputs for improved performance. To handle the diversity of pretraining data and avoid negative transfer, we introduce a Mixture-of-Experts (MoE) framework that captures distinct patterns in separate experts, facilitating seamless application of the pretrained model on diverse downstream datasets. For fast adaptation, we develop a parameter-efficient tuning strategy that allows the pretrained model to adapt to unseen datasets with minimal computational overhead. Experiments on 16 datasets across two domains demonstrate the effectiveness of our approach, achieving state-of-the-art performance on low-resource link prediction while obtaining competitive results compared to end-to-end trained methods, with over 10,000x lower computational overhead.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.04643v1" target="_blank">Experimental device-independent certification of indefinite causal order</a></h3>
                    <p><strong>Authors:</strong> Dengke Qu, Quan Lin, Lei Xiao, Xiang Zhan, Peng Xue</p>
                    <p><strong>Published:</strong> 8/6/2025</p>
                    <p><strong>Categories:</strong> quant-ph</p>
                    <p><strong>Summary:</strong> Understanding the physical world fundamentally relies on the assumption that events are temporally ordered, with past events serving as causes for future ones. However, quantum mechanics permits events to occur in a superposition of causal orders, providing new types of quantum resources for quantum information tasks. Previous demonstrations of indefinite causal order have relied on a process known as quantum switch and depended on specific assumptions about the devices used in the laboratory. Recently, a theoretical scheme for the certification of indefinite causal order in the quantum switch has been obtained solely from the output statistics of the devices, analogous to the device-independent proofs of nonlocality through violations of the Bell inequality. Here, we report an experimental verification of the causal inequality using spacelike-separated entangled photons, where one photon functions as the control qubit in a quantum switch and the other serves as an additional observer. Through local measurement statistics, we observe a violation of the causal inequality by 24 standard deviations. This work provides evidence for a device-independent certification of indefinite causal order, relying solely on observed correlations without requiring device characterization. Our results pave the way toward a complete understanding of indefinite causal order and its potential applications in quantum information processing.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.04642v1" target="_blank">RoboTron-Sim: Improving Real-World Driving via Simulated Hard-Case</a></h3>
                    <p><strong>Authors:</strong> Baihui Xiao, Chengjian Feng, Zhijian Huang, Feng yan, Yujie Zhong, Lin Ma</p>
                    <p><strong>Published:</strong> 8/6/2025</p>
                    <p><strong>Categories:</strong> cs.RO, cs.CV</p>
                    <p><strong>Summary:</strong> Collecting real-world data for rare high-risk scenarios, long-tailed driving events, and complex interactions remains challenging, leading to poor performance of existing autonomous driving systems in these critical situations. In this paper, we propose RoboTron-Sim that improves real-world driving in critical situations by utilizing simulated hard cases. First, we develop a simulated dataset called Hard-case Augmented Synthetic Scenarios (HASS), which covers 13 high-risk edge-case categories, as well as balanced environmental conditions such as day/night and sunny/rainy. Second, we introduce Scenario-aware Prompt Engineering (SPE) and an Image-to-Ego Encoder (I2E Encoder) to enable multimodal large language models to effectively learn real-world challenging driving skills from HASS, via adapting to environmental deviations and hardware differences between real-world and simulated scenarios. Extensive experiments on nuScenes show that RoboTron-Sim improves driving performance in challenging scenarios by around 50%, achieving state-of-the-art results in real-world open-loop planning. Qualitative results further demonstrate the effectiveness of RoboTron-Sim in better managing rare high-risk driving scenarios. Project page: https://stars79689.github.io/RoboTron-Sim/</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.04638v1" target="_blank">Can NLP Tackle Hate Speech in the Real World? Stakeholder-Informed Feedback and Survey on Counterspeech</a></h3>
                    <p><strong>Authors:</strong> Tanvi Dinkar, Aiqi Jiang, Simona Frenda, Poppy Gerrard-Abbott, Nancie Gunson, Gavin Abercrombie, Ioannis Konstas</p>
                    <p><strong>Published:</strong> 8/6/2025</p>
                    <p><strong>Categories:</strong> cs.CL</p>
                    <p><strong>Summary:</strong> Counterspeech, i.e. the practice of responding to online hate speech, has gained traction in NLP as a promising intervention. While early work emphasised collaboration with non-governmental organisation stakeholders, recent research trends have shifted toward automated pipelines that reuse a small set of legacy datasets, often without input from affected communities. This paper presents a systematic review of 74 NLP studies on counterspeech, analysing the extent to which stakeholder participation influences dataset creation, model development, and evaluation. To complement this analysis, we conducted a participatory case study with five NGOs specialising in online Gender-Based Violence (oGBV), identifying stakeholder-informed practices for counterspeech generation. Our findings reveal a growing disconnect between current NLP research and the needs of communities most impacted by toxic online content. We conclude with concrete recommendations for re-centring stakeholder expertise in counterspeech research.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.04636v1" target="_blank">Extension of the Einstein-Dirac-axion-aether theory based on an effective metric with a spinor kernel</a></h3>
                    <p><strong>Authors:</strong> Alexander B. Balakin, Anna O. Efremova</p>
                    <p><strong>Published:</strong> 8/6/2025</p>
                    <p><strong>Categories:</strong> gr-qc</p>
                    <p><strong>Summary:</strong> We consider an axionic extension of the Einstein-Dirac-aether theory, which is based on the spinor modification of the kinetic term of the pseudoscalar field and describes the backreaction of the spinor field on the axionic dark matter. The main instrument of this extension is the effective metric constructed using the aether velocity four-vector and a kernel, which depends on the basic spinor scalar and on the square of the basic spinor pseudoscalar. The periodic potential of the axion field includes a guiding function, which regulates the dynamics of axions and predetermines the properties of their equilibrium states. This guiding function and the kernel of the effective metric, as well, are considered to be functions of the expansion scalar of the aether flow. The master equation for the guiding function is obtained as a consequence of the Lagrangian invariance with respect to the discrete transformations prescribed by the axion field shift symmetry. The self-consistent set of coupled master equations is derived, which includes new source-terms in the modified master equations for the spinor, axion, vector and gravitational fields. Cosmological applications of the extended theory are considered; new exact solutions of these equations are presented for the isotropic homogeneous model of the Universe evolution. Discussion is focused on the mechanism of backreaction of the spinor field on the axionic dark matter evolution.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.04632v1" target="_blank">IFDECORATOR: Wrapping Instruction Following Reinforcement Learning with Verifiable Rewards</a></h3>
                    <p><strong>Authors:</strong> Xu Guo, Tianyi Liang, Tong Jian, Xiaogui Yang, Ling-I Wu, Chenhui Li, Zhihui Lu, Qipeng Guo, Kai Chen</p>
                    <p><strong>Published:</strong> 8/6/2025</p>
                    <p><strong>Categories:</strong> cs.CL</p>
                    <p><strong>Summary:</strong> Reinforcement Learning with Verifiable Rewards (RLVR) improves instruction following capabilities of large language models (LLMs), but suffers from training inefficiency due to inadequate difficulty assessment. Moreover, RLVR is prone to over-optimization, where LLMs exploit verification shortcuts without aligning to the actual intent of user instructions. We introduce Instruction Following Decorator (IFDecorator}, a framework that wraps RLVR training into a robust and sample-efficient pipeline. It consists of three components: (1) a cooperative-adversarial data flywheel that co-evolves instructions and hybrid verifications, generating progressively more challenging instruction-verification pairs; (2) IntentCheck, a bypass module enforcing intent alignment; and (3) trip wires, a diagnostic mechanism that detects reward hacking via trap instructions, which trigger and capture shortcut exploitation behaviors. Our Qwen2.5-32B-Instruct-IFDecorator achieves 87.43% accuracy on IFEval, outperforming larger proprietary models such as GPT-4o. Additionally, we demonstrate substantial improvements on FollowBench while preserving general capabilities. Our trip wires show significant reductions in reward hacking rates. We will release models, code, and data for future research.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.04631v1" target="_blank">Abelian Hall categories</a></h3>
                    <p><strong>Authors:</strong> Sabin Cautis</p>
                    <p><strong>Published:</strong> 8/6/2025</p>
                    <p><strong>Categories:</strong> math.AG, math-ph, math.MP, math.RT</p>
                    <p><strong>Summary:</strong> To a quiver we associate a finite length monoidal abelian category which categorifies the corresponding preprojective K-theoretic Hall algebra of Varagnolo-Vasserot. The simples in this category provide a (dual) canonical basis of the Hall algebra. In particular, if the quiver is affine, this provides a basis for the positive half of the corresponding quantum toroidal algebra. We also show that this abelian category is naturally endowed with renormalized r-matrices.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.04628v1" target="_blank">Simulations of dielectric permittivity of water by Machine Learned Potentials with long-range Coulombic interactions</a></h3>
                    <p><strong>Authors:</strong> Kehan Cai, Chunyi Zhang, Xifan Wu</p>
                    <p><strong>Published:</strong> 8/6/2025</p>
                    <p><strong>Categories:</strong> cond-mat.soft, cond-mat.mtrl-sci, cond-mat.stat-mech, physics.comp-ph</p>
                    <p><strong>Summary:</strong> The dielectric permittivity of liquid water is a fundamental property that underlies its distinctive behaviors in numerious physical, biological, and chemical processes. Within a machine learning framework, we present a unified approach to compute the dielectric permittivity of water, systematically incorporating various electric boundary conditions. Our method employs a long-range-inclusive deep potential trained on data from hybrid density functional theory calculations. Dielectric response is evaluated using an auxiliary deep neural network that predicts the centers of maximally localized Wannier functions. We investigate three types of electric boundary conditions--metallic, insulating, and Kirkwood-Frohlich--to assess their influence on correlated dipole fluctuations and dielectric relaxation dynamics. In particular, we demonstrate a consistent methodology for computing the Kirkwood correlation factor, correlation length, and dielectric permittivity under each boundary condition, where long-range electrostatics play a critical role. This work establishes a robust and generalizable machine-learning framework for modeling the dielectric properties of polar liquids under diverse electrostatic environments.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.04626v1" target="_blank">P-Aligner: Enabling Pre-Alignment of Language Models via Principled Instruction Synthesis</a></h3>
                    <p><strong>Authors:</strong> Feifan Song, Bofei Gao, Yifan Song, Yi Liu, Weimin Xiong, Yuyang Song, Tianyu Liu, Guoyin Wang, Houfeng Wang</p>
                    <p><strong>Published:</strong> 8/6/2025</p>
                    <p><strong>Categories:</strong> cs.CL, cs.AI</p>
                    <p><strong>Summary:</strong> Large Language Models (LLMs) are expected to produce safe, helpful, and honest content during interaction with human users, but they frequently fail to align with such values when given flawed instructions, e.g., missing context, ambiguous directives, or inappropriate tone, leaving substantial room for improvement along multiple dimensions. A cost-effective yet high-impact way is to pre-align instructions before the model begins decoding. Existing approaches either rely on prohibitive test-time search costs or end-to-end model rewrite, which is powered by a customized training corpus with unclear objectives. In this work, we demonstrate that the goal of efficient and effective preference alignment can be achieved by P-Aligner, a lightweight module generating instructions that preserve the original intents while being expressed in a more human-preferred form. P-Aligner is trained on UltraPrompt, a new dataset synthesized via a proposed principle-guided pipeline using Monte-Carlo Tree Search, which systematically explores the space of candidate instructions that are closely tied to human preference. Experiments across different methods show that P-Aligner generally outperforms strong baselines across various models and benchmarks, including average win-rate gains of 28.35% and 8.69% on GPT-4-turbo and Gemma-2-SimPO, respectively. Further analyses validate its effectiveness and efficiency through multiple perspectives, including data quality, search strategies, iterative deployment, and time overhead.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.04625v1" target="_blank">FinMMR: Make Financial Numerical Reasoning More Multimodal, Comprehensive, and Challenging</a></h3>
                    <p><strong>Authors:</strong> Zichen Tang, Haihong E, Jiacheng Liu, Zhongjun Yang, Rongjin Li, Zihua Rong, Haoyang He, Zhuodi Hao, Xinyang Hu, Kun Ji, Ziyan Ma, Mengyuan Ji, Jun Zhang, Chenghao Ma, Qianhe Zheng, Yang Liu, Yiling Huang, Xinyi Hu, Qing Huang, Zijian Xie, Shiyao Peng</p>
                    <p><strong>Published:</strong> 8/6/2025</p>
                    <p><strong>Categories:</strong> cs.CV, cs.CE</p>
                    <p><strong>Summary:</strong> We present FinMMR, a novel bilingual multimodal benchmark tailored to evaluate the reasoning capabilities of multimodal large language models (MLLMs) in financial numerical reasoning tasks. Compared to existing benchmarks, our work introduces three significant advancements. (1) Multimodality: We meticulously transform existing financial reasoning benchmarks, and construct novel questions from the latest Chinese financial research reports. FinMMR comprises 4.3K questions and 8.7K images spanning 14 categories, including tables, bar charts, and ownership structure charts. (2) Comprehensiveness: FinMMR encompasses 14 financial subdomains, including corporate finance, banking, and industry analysis, significantly exceeding existing benchmarks in financial domain knowledge breadth. (3) Challenge: Models are required to perform multi-step precise numerical reasoning by integrating financial knowledge with the understanding of complex financial images and text. The best-performing MLLM achieves only 53.0% accuracy on Hard problems. We believe that FinMMR will drive advancements in enhancing the reasoning capabilities of MLLMs in real-world scenarios.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.04623v1" target="_blank">Lightweight Transformers for Zero-Shot and Fine-Tuned Text-to-SQL Generation Using Spider</a></h3>
                    <p><strong>Authors:</strong> Chirag Seth, Utkarsh Singh</p>
                    <p><strong>Published:</strong> 8/6/2025</p>
                    <p><strong>Categories:</strong> cs.CL, cs.IR, 68T50 % Natural language processing (in Computer Science), I.2.7; H.2.3</p>
                    <p><strong>Summary:</strong> Text-to-SQL translation enables non-expert users to query relational databases using natural language, with applications in education and business intelligence. This study evaluates three lightweight transformer models - T5-Small, BART-Small, and GPT-2 - on the Spider dataset, focusing on low-resource settings. We developed a reusable, model-agnostic pipeline that tailors schema formatting to each models architecture, training them across 1000 to 5000 iterations and evaluating on 1000 test samples using Logical Form Accuracy (LFAcc), BLEU, and Exact Match (EM) metrics. Fine-tuned T5-Small achieves the highest LFAcc (27.8%), outperforming BART-Small (23.98%) and GPT-2 (20.1%), highlighting encoder-decoder models superiority in schema-aware SQL generation. Despite resource constraints limiting performance, our pipelines modularity supports future enhancements, such as advanced schema linking or alternative base models. This work underscores the potential of compact transformers for accessible text-to-SQL solutions in resource-scarce environments.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.04622v1" target="_blank">Optimizing quantum transport via the quantum Doob transform</a></h3>
                    <p><strong>Authors:</strong> Dolores Esteve, Carlos PÃ©rez-Espigares, Ricardo GutiÃ©rrez, Daniel Manzano</p>
                    <p><strong>Published:</strong> 8/6/2025</p>
                    <p><strong>Categories:</strong> quant-ph, cond-mat.dis-nn, cond-mat.stat-mech</p>
                    <p><strong>Summary:</strong> Quantum transport plays a central role in both fundamental physics and the development of quantum technologies. While significant progress has been made in understanding transport phenomena in quantum systems, methods for optimizing transport properties remain limited, particularly in complex quantum networks. Building on recent advances in classical network optimization via the generalized Doob transform, we introduce a novel method that extends this approach to quantum networks. Our framework leverages a single diagonalization of the system generator to efficiently tailor both the Hamiltonian and dissipative contributions, optimizing transport observables such as currents and activities. We demonstrate the methods effectiveness through extensive numerical explorations, showing that optimal performance arises from non-trivial modifications to both coherent and incoherent dynamics. We also assess the robustness of the optimization under constraints that preserve specific physical features, such as fixed dissipative structures and input-output interactions. Finally, we discuss the connection between optimized transport and centrosymmetry, highlighting the relevance of this property for enhanced transport efficiency in quantum systems.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.04614v1" target="_blank">How Does Bilateral Ear Symmetry Affect Deep Ear Features?</a></h3>
                    <p><strong>Authors:</strong> Kagan Ozturk, Deeksha Arun, Kevin W. Bowyer, Patrick Flynn</p>
                    <p><strong>Published:</strong> 8/6/2025</p>
                    <p><strong>Categories:</strong> cs.CV</p>
                    <p><strong>Summary:</strong> Ear recognition has gained attention as a reliable biometric technique due to the distinctive characteristics of human ears. With the increasing availability of large-scale datasets, convolutional neural networks (CNNs) have been widely adopted to learn features directly from raw ear images, outperforming traditional hand-crafted methods. However, the effect of bilateral ear symmetry on the features learned by CNNs has received little attention in recent studies. In this paper, we investigate how bilateral ear symmetry influences the effectiveness of CNN-based ear recognition. To this end, we first develop an ear side classifier to automatically categorize ear images as either left or right. We then explore the impact of incorporating this side information during both training and test. Cross-dataset evaluations are conducted on five datasets. Our results suggest that treating left and right ears separately during training and testing can lead to notable performance improvements. Furthermore, our ablation studies on alignment strategies, input sizes, and various hyperparameter settings provide practical insights into training CNN-based ear recognition systems on large-scale datasets to achieve higher verification rates.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.04611v1" target="_blank">OmniDepth: Bridging Monocular and Stereo Reasoning with Latent Alignment</a></h3>
                    <p><strong>Authors:</strong> Tongfan Guan, Jiaxin Guo, Chen Wang, Yun-Hui Liu</p>
                    <p><strong>Published:</strong> 8/6/2025</p>
                    <p><strong>Categories:</strong> cs.CV, cs.RO</p>
                    <p><strong>Summary:</strong> Monocular and stereo depth estimation offer complementary strengths: monocular methods capture rich contextual priors but lack geometric precision, while stereo approaches leverage epipolar geometry yet struggle with ambiguities such as reflective or textureless surfaces. Despite post-hoc synergies, these paradigms remain largely disjoint in practice. We introduce OmniDepth, a unified framework that bridges both through iterative bidirectional alignment of their latent representations. At its core, a novel cross-attentive alignment mechanism dynamically synchronizes monocular contextual cues with stereo hypothesis representations during stereo reasoning. This mutual alignment resolves stereo ambiguities (e.g., specular surfaces) by injecting monocular structure priors while refining monocular depth with stereo geometry within a single network. Extensive experiments demonstrate state-of-the-art results: \textbf{OmniDepth reduces zero-shot generalization error by $\!\!40\%$ on Middlebury and ETH3D}, while addressing longstanding failures on transparent and reflective surfaces. By harmonizing multi-view geometry with monocular context, OmniDepth enables robust 3D perception that transcends modality-specific limitations. Codes available at https://github.com/aeolusguan/OmniDepth.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.04610v1" target="_blank">Neuromorphic Cybersecurity with Semi-supervised Lifelong Learning</a></h3>
                    <p><strong>Authors:</strong> Md Zesun Ahmed Mia, Malyaban Bal, Sen Lu, George M. Nishibuchi, Suhas Chelian, Srini Vasan, Abhronil Sengupta</p>
                    <p><strong>Published:</strong> 8/6/2025</p>
                    <p><strong>Categories:</strong> cs.LG, cs.AI, cs.ET, cs.NE</p>
                    <p><strong>Summary:</strong> Inspired by the brains hierarchical processing and energy efficiency, this paper presents a Spiking Neural Network (SNN) architecture for lifelong Network Intrusion Detection System (NIDS). The proposed system first employs an efficient static SNN to identify potential intrusions, which then activates an adaptive dynamic SNN responsible for classifying the specific attack type. Mimicking biological adaptation, the dynamic classifier utilizes Grow When Required (GWR)-inspired structural plasticity and a novel Adaptive Spike-Timing-Dependent Plasticity (Ad-STDP) learning rule. These bio-plausible mechanisms enable the network to learn new threats incrementally while preserving existing knowledge. Tested on the UNSW-NB15 benchmark in a continual learning setting, the architecture demonstrates robust adaptation, reduced catastrophic forgetting, and achieves $85.3$\% overall accuracy. Furthermore, simulations using the Intel Lava framework confirm high operational sparsity, highlighting the potential for low-power deployment on neuromorphic hardware.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.04606v1" target="_blank">Benchmarking electronic-structure methods for the description of dark transitions in carbonyls at and beyond the Franck-Condon point</a></h3>
                    <p><strong>Authors:</strong> Jasmine Bone, Javier Carmona-GarcÃ­a, Daniel Hollas, Basile F. E. Curchod</p>
                    <p><strong>Published:</strong> 8/6/2025</p>
                    <p><strong>Categories:</strong> physics.chem-ph</p>
                    <p><strong>Summary:</strong> Herein, we propose a comprehensive benchmark of electronic-structure methods to describe dark transitions, that is, transitions to excited electronic states characterized by a near-zero oscillator strength. This type of electronic state is particularly important for the photochemistry of molecules containing carbonyl groups, such as atmospheric volatile organic compounds (VOCs). The oscillator strength characterizing a dark transition can change dramatically by a slight alteration of the molecular geometry around its ground-state equilibrium, the so-called non-Condon effects. Hence, testing the performance of electronic-structure methods for dark transitions requires considering molecules at their Franck-Condon point (i.e., equilibrium geometry), but also beyond the Franck-Condon point. Our benchmark focuses on various electronic-structure methods - LR-TDDFT(/TDA), ADC(2), CC2, EOM-CCSD, CC2/3, XMS-CASPT2 - with CC3/aug-cc-pVTZ serving as a theoretical best estimate. These techniques are tested against a set of 16 carbonyl-containing VOCs at their equilibrium geometry. We then assess the performance of these methods to describe the dark transition of acetaldehyde beyond its Franck-Condon point by (i) distorting the molecule towards its S$_1$ minimum energy structure and (ii) sampling an approximate ground-state quantum distribution for the molecule and calculating photoabsorption cross-sections within the nuclear ensemble approach. Based on the calculated cross-sections, we calculate the photolysis half-life as depicted by the different electronic-structure methods - highlighting the impact of the different electronic-structure methods on predicted experimental photolysis observables.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.04604v1" target="_blank">TURA: Tool-Augmented Unified Retrieval Agent for AI Search</a></h3>
                    <p><strong>Authors:</strong> Zhejun Zhao, Yuehu Dong, Alley Liu, Lixue Zheng, Pingsheng Liu, Dongdong Shen, Long Xia, Jiashu Zhao, Dawei Yin</p>
                    <p><strong>Published:</strong> 8/6/2025</p>
                    <p><strong>Categories:</strong> cs.CL, cs.AI, cs.IR</p>
                    <p><strong>Summary:</strong> The advent of Large Language Models (LLMs) is transforming search engines into conversational AI search products, primarily using Retrieval-Augmented Generation (RAG) on web corpora. However, this paradigm has significant industrial limitations. Traditional RAG approaches struggle with real-time needs and structured queries that require accessing dynamically generated content like ticket availability or inventory. Limited to indexing static pages, search engines cannot perform the interactive queries needed for such time-sensitive data. Academic research has focused on optimizing RAG for static content, overlooking complex intents and the need for dynamic sources like databases and real-time APIs. To bridge this gap, we introduce TURA (Tool-Augmented Unified Retrieval Agent for AI Search), a novel three-stage framework that combines RAG with agentic tool-use to access both static content and dynamic, real-time information. TURA has three key components: an Intent-Aware Retrieval module to decompose queries and retrieve information sources encapsulated as Model Context Protocol (MCP) Servers, a DAG-based Task Planner that models task dependencies as a Directed Acyclic Graph (DAG) for optimal parallel execution, and a lightweight Distilled Agent Executor for efficient tool calling. TURA is the first architecture to systematically bridge the gap between static RAG and dynamic information sources for a world-class AI search product. Serving tens of millions of users, it leverages an agentic framework to deliver robust, real-time answers while meeting the low-latency demands of a large-scale industrial system.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.04603v1" target="_blank">Square packing with $O(x^{0.6})$ wasted area</a></h3>
                    <p><strong>Authors:</strong> Hong Duc Bui</p>
                    <p><strong>Published:</strong> 8/6/2025</p>
                    <p><strong>Categories:</strong> cs.CG</p>
                    <p><strong>Summary:</strong> We show a new construction for square packing, and prove that it is more efficient than previous results.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.04602v1" target="_blank">On existence of a compatible triangulation with the double circle order type</a></h3>
                    <p><strong>Authors:</strong> Hong Duc Bui</p>
                    <p><strong>Published:</strong> 8/6/2025</p>
                    <p><strong>Categories:</strong> math.CO, cs.CG</p>
                    <p><strong>Summary:</strong> We show that the double circle order type and some of its generalizations have a compatible triangulation with any other order types with the same number of points and number of edges on convex hull, thus proving another special case of the conjecture in Aichholzer (2003).</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.04600v1" target="_blank">Physics-Informed Neural Network for Elastic Wave-Mode Separation</a></h3>
                    <p><strong>Authors:</strong> E. A. B. Alves, P. D. S. de Lima, D. H. G. Duarte, M. S. Ferreira, J. M. de AraÃºjo, C. G. Bezerra</p>
                    <p><strong>Published:</strong> 8/6/2025</p>
                    <p><strong>Categories:</strong> physics.comp-ph, physics.geo-ph</p>
                    <p><strong>Summary:</strong> Mode conversion in non-homogeneous elastic media makes it challenging to interpret physical properties accurately. Decomposing these modes correctly is crucial across various scientific areas. Recent machine learning approaches have been proposed to address this problem, utilizing the Helmholtz decomposition technique. In this paper, we investigate the capabilities of a physics-informed neural network (PINN) in separating P and S modes by solving a scalar Poisson equation. This scalar formulation offers a dimensionally scalable reduction in computational cost compared to the traditional vector formulation. We verify the proposed method in both homogeneous and realistic non-homogeneous elastic models as showcases. The obtained separated modes closely match those from conventional numerical techniques, while exhibiting reduced transverse wave leakage.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.04597v1" target="_blank">Pseudo Depth Meets Gaussian: A Feed-forward RGB SLAM Baseline</a></h3>
                    <p><strong>Authors:</strong> Linqing Zhao, Xiuwei Xu, Yirui Wang, Hao Wang, Wenzhao Zheng, Yansong Tang, Haibin Yan, Jiwen Lu</p>
                    <p><strong>Published:</strong> 8/6/2025</p>
                    <p><strong>Categories:</strong> cs.CV</p>
                    <p><strong>Summary:</strong> Incrementally recovering real-sized 3D geometry from a pose-free RGB stream is a challenging task in 3D reconstruction, requiring minimal assumptions on input data. Existing methods can be broadly categorized into end-to-end and visual SLAM-based approaches, both of which either struggle with long sequences or depend on slow test-time optimization and depth sensors. To address this, we first integrate a depth estimator into an RGB-D SLAM system, but this approach is hindered by inaccurate geometric details in predicted depth. Through further investigation, we find that 3D Gaussian mapping can effectively solve this problem. Building on this, we propose an online 3D reconstruction method using 3D Gaussian-based SLAM, combined with a feed-forward recurrent prediction module to directly infer camera pose from optical flow. This approach replaces slow test-time optimization with fast network inference, significantly improving tracking speed. Additionally, we introduce a local graph rendering technique to enhance robustness in feed-forward pose prediction. Experimental results on the Replica and TUM-RGBD datasets, along with a real-world deployment demonstration, show that our method achieves performance on par with the state-of-the-art SplaTAM, while reducing tracking time by more than 90\%.</p>
                
            
        
        
            <p><em>Generated by AI News Agent</em></p>
        
    

