
    
        <h1>ðŸ¤– AI Research Report</h1>
        
            <strong>Date:</strong> 2025-08-15<br>
            <strong>Topics:</strong> ai safety research, ai alignment research, quantum computing<br>
            <strong>Papers Found:</strong> 150
        
        
        
            
                <h2>ðŸ¤– AI Summary</h2>
                <p>## ai safety research

The landscape of AI safety research is evolving rapidly, with recent papers highlighting significant trends and advancements. One major trend is the integration of large language models (LLMs) and explainable AI (XAI) techniques across various domains to improve safety and reliability. For instance, papers such as From Black Box to Transparency emphasize enhancing the interpretability of automated systems, making them more transparent and trustworthy, particularly in educational contexts. Similarly, the Medico 2025 challenge underscores the importance of explainable models in medical imaging, aiming to provide interpretable justifications that align with clinical reasoning, thereby enhancing trust in AI-assisted medical decision-making.

Another critical trend is the development of robust and generalizable AI systems to mitigate risks associated with adversarial attacks and data privacy. The Forgery Guided Learning Strategy with Dual Perception Network addresses the need for deepfake detection methods that adapt to unknown forgery techniques, highlighting the urgency to improve AI resilience against emerging threats. Additionally, the SoK: Data Minimization in Machine Learning paper introduces a framework for data minimization, which is crucial for aligning with data protection regulations and ensuring that AI systems are both safe and compliant. These efforts collectively demonstrate a growing focus on creating AI systems that are not only powerful but also safe, interpretable, and ethical, aligning with the broader goals of AI safety research to foster trust and accountability in AI technologies.

*Based on 50 research papers*

---

## quantum computing

The landscape of quantum computing is witnessing significant breakthroughs and emerging trends, as highlighted by recent research papers. One of the notable advances is in the realm of non-Hermitian quantum crystals, where researchers like Juan Pablo Esparza and Vladimir Juricic have demonstrated the persistence of flat bands in non-Hermitian systems. These exceptional flat bands, arising in bipartite lattices, offer new avenues for understanding correlated and topological phases in open quantum systems, with implications for synthetic platforms like photonic crystals and ultracold atom arrays.

Another critical development is the discovery of niobium hydride precipitates in superconducting qubits, reported by Zuhawn Sung and colleagues. This finding identifies a previously unknown source of decoherence that affects qubit performance, offering insights into potential improvements in superconducting qubit technology. Moreover, the proposal of a scalable qudit-based quantum computing platform using polar molecules, as outlined by Soleh Kh. Muminov and co-authors, presents an innovative approach to quantum information processing. This model leverages the rotational degrees of freedom of polar molecules, facilitating the implementation of entangling gates via dipole-dipole interactions. These advancements underscore the diverse and rapidly evolving nature of quantum computing, with each breakthrough contributing to the foundational knowledge and technological capabilities necessary for future quantum applications.

*Based on 50 research papers*</p>
            
        
        
        <h2>ðŸ“š Research Papers</h2>
        
                
                    <h3><a href="http://arxiv.org/abs/2508.10899v1" target="_blank">A Dataset for Distilling Knowledge Priors from Literature for Therapeutic Design</a></h3>
                    <p><strong>Authors:</strong> Haydn Thomas Jones, Natalie Maus, Josh Magnus Ludan, Maggie Ziyu Huan, Jiaming Liang, Marcelo Der Torossian Torres, Jiatao Liang, Zachary Ives, Yoseph Barash, Cesar de la Fuente-Nunez, Jacob R. Gardner, Mark Yatskar</p>
                    <p><strong>Published:</strong> 8/14/2025</p>
                    <p><strong>Categories:</strong> cs.LG</p>
                    <p><strong>Summary:</strong> AI-driven discovery can greatly reduce design time and enhance new therapeutics effectiveness. Models using simulators explore broad design spaces but risk violating implicit constraints due to a lack of experimental priors. For example, in a new analysis we performed on a diverse set of models on the GuacaMol benchmark using supervised classifiers, over 60\% of molecules proposed had high probability of being mutagenic. In this work, we introduce \ourdataset, a dataset of priors for design problems extracted from literature describing compounds used in lab settings. It is constructed with LLM pipelines for discovering therapeutic entities in relevant paragraphs and summarizing information in concise fair-use facts. \ourdataset~ consists of 32.3 million pairs of natural language facts, and appropriate entity representations (i.e. SMILES or refseq IDs). To demonstrate the potential of the data, we train LLM, CLIP, and LLava architectures to reason jointly about text and design targets and evaluate on tasks from the Therapeutic Data Commons (TDC). \ourdataset~is highly effective for creating models with strong priors: in supervised prediction problems that use our data as pretraining, our best models with 15M learnable parameters outperform larger 2B TxGemma on both regression and classification TDC tasks, and perform comparably to 9B models on average. Models built with \ourdataset~can be used as constraints while optimizing for novel molecules in GuacaMol, resulting in proposals that are safer and nearly as effective. We release our dataset at \href{https://huggingface.co/datasets/medexanon/Medex}{huggingface.co/datasets/medexanon/Medex}, and will provide expanded versions as available literature grows.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.10898v1" target="_blank">Puppeteer: Rig and Animate Your 3D Models</a></h3>
                    <p><strong>Authors:</strong> Chaoyue Song, Xiu Li, Fan Yang, Zhongcong Xu, Jiacheng Wei, Fayao Liu, Jiashi Feng, Guosheng Lin, Jianfeng Zhang</p>
                    <p><strong>Published:</strong> 8/14/2025</p>
                    <p><strong>Categories:</strong> cs.CV, cs.GR</p>
                    <p><strong>Summary:</strong> Modern interactive applications increasingly demand dynamic 3D content, yet the transformation of static 3D models into animated assets constitutes a significant bottleneck in content creation pipelines. While recent advances in generative AI have revolutionized static 3D model creation, rigging and animation continue to depend heavily on expert intervention. We present Puppeteer, a comprehensive framework that addresses both automatic rigging and animation for diverse 3D objects. Our system first predicts plausible skeletal structures via an auto-regressive transformer that introduces a joint-based tokenization strategy for compact representation and a hierarchical ordering methodology with stochastic perturbation that enhances bidirectional learning capabilities. It then infers skinning weights via an attention-based architecture incorporating topology-aware joint attention that explicitly encodes inter-joint relationships based on skeletal graph distances. Finally, we complement these rigging advances with a differentiable optimization-based animation pipeline that generates stable, high-fidelity animations while being computationally more efficient than existing approaches. Extensive evaluations across multiple benchmarks demonstrate that our method significantly outperforms state-of-the-art techniques in both skeletal prediction accuracy and skinning quality. The system robustly processes diverse 3D content, ranging from professionally designed game assets to AI-generated shapes, producing temporally coherent animations that eliminate the jittering issues common in existing methods.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.10891v1" target="_blank">Fuel Consumption in Platoons: A Literature Review</a></h3>
                    <p><strong>Authors:</strong> Oumaima Barhoumi, Ghazal Farhani, Taufiq Rahman, Mohamed H. Zaki, SofiÃ¨ne Tahar, Fadi Araji</p>
                    <p><strong>Published:</strong> 8/14/2025</p>
                    <p><strong>Categories:</strong> eess.SY, cs.SY</p>
                    <p><strong>Summary:</strong> Platooning has emerged as a promising strategy for improving fuel efficiency in automated vehicle systems, with significant implications for reducing emissions and operational costs. While existing literature on vehicle platooning primarily focuses on individual aspects such as aerodynamic drag reduction or specific control strategies, this work takes a more comprehensive approach by bringing together a wide range of factors and components that contribute to fuel savings in platoons. In this literature review, we examine the impact of platooning on fuel consumption, highlighting the key components of platoon systems, the factors and actors influencing fuel savings, methods for estimating fuel use, and the effect of platoon instability on efficiency. Furthermore, we study the role of reduced aerodynamic drag, vehicle coordination, and the challenges posed by instability in real-world conditions. By compiling insights from recent studies, this work provides a comprehensive overview of the latest advancements in platooning technologies and highlights both the challenges and opportunities for future research to maximize fuel savings in real-world scenarios.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.10881v1" target="_blank">ToonComposer: Streamlining Cartoon Production with Generative Post-Keyframing</a></h3>
                    <p><strong>Authors:</strong> Lingen Li, Guangzhi Wang, Zhaoyang Zhang, Yaowei Li, Xiaoyu Li, Qi Dou, Jinwei Gu, Tianfan Xue, Ying Shan</p>
                    <p><strong>Published:</strong> 8/14/2025</p>
                    <p><strong>Categories:</strong> cs.CV, cs.AI</p>
                    <p><strong>Summary:</strong> Traditional cartoon and anime production involves keyframing, inbetweening, and colorization stages, which require intensive manual effort. Despite recent advances in AI, existing methods often handle these stages separately, leading to error accumulation and artifacts. For instance, inbetweening approaches struggle with large motions, while colorization methods require dense per-frame sketches. To address this, we introduce ToonComposer, a generative model that unifies inbetweening and colorization into a single post-keyframing stage. ToonComposer employs a sparse sketch injection mechanism to provide precise control using keyframe sketches. Additionally, it uses a cartoon adaptation method with the spatial low-rank adapter to tailor a modern video foundation model to the cartoon domain while keeping its temporal prior intact. Requiring as few as a single sketch and a colored reference frame, ToonComposer excels with sparse inputs, while also supporting multiple sketches at any temporal location for more precise motion control. This dual capability reduces manual workload and improves flexibility, empowering artists in real-world scenarios. To evaluate our model, we further created PKBench, a benchmark featuring human-drawn sketches that simulate real-world use cases. Our evaluation demonstrates that ToonComposer outperforms existing methods in visual quality, motion consistency, and production efficiency, offering a superior and more flexible solution for AI-assisted cartoon production.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.10875v1" target="_blank">A Survey on Diffusion Language Models</a></h3>
                    <p><strong>Authors:</strong> Tianyi Li, Mingda Chen, Bowei Guo, Zhiqiang Shen</p>
                    <p><strong>Published:</strong> 8/14/2025</p>
                    <p><strong>Categories:</strong> cs.CL, cs.AI, cs.LG</p>
                    <p><strong>Summary:</strong> Diffusion Language Models (DLMs) are rapidly emerging as a powerful and promising alternative to the dominant autoregressive (AR) paradigm. By generating tokens in parallel through an iterative denoising process, DLMs possess inherent advantages in reducing inference latency and capturing bidirectional context, thereby enabling fine-grained control over the generation process. While achieving a several-fold speed-up, recent advancements have allowed DLMs to show performance comparable to their autoregressive counterparts, making them a compelling choice for various natural language processing tasks. In this survey, we provide a holistic overview of the current DLM landscape. We trace its evolution and relationship with other paradigms, such as autoregressive and masked language models, and cover both foundational principles and state-of-the-art models. Our work offers an up-to-date, comprehensive taxonomy and an in-depth analysis of current techniques, from pre-training strategies to advanced post-training methods. Another contribution of this survey is a thorough review of DLM inference strategies and optimizations, including improvements in decoding parallelism, caching mechanisms, and generation quality. We also highlight the latest approaches to multimodal extensions of DLMs and delineate their applications across various practical scenarios. Furthermore, our discussion addresses the limitations and challenges of DLMs, including efficiency, long-sequence handling, and infrastructure requirements, while outlining future research directions to sustain progress in this rapidly evolving field. Project GitHub is available at https://github.com/VILA-Lab/Awesome-DLMs.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.10869v1" target="_blank">Medico 2025: Visual Question Answering for Gastrointestinal Imaging</a></h3>
                    <p><strong>Authors:</strong> Sushant Gautam, Vajira Thambawita, Michael Riegler, PÃ¥l Halvorsen, Steven Hicks</p>
                    <p><strong>Published:</strong> 8/14/2025</p>
                    <p><strong>Categories:</strong> cs.CV, cs.AI, 68T45, 92C55, I.2.10; I.4.9</p>
                    <p><strong>Summary:</strong> The Medico 2025 challenge addresses Visual Question Answering (VQA) for Gastrointestinal (GI) imaging, organized as part of the MediaEval task series. The challenge focuses on developing Explainable Artificial Intelligence (XAI) models that answer clinically relevant questions based on GI endoscopy images while providing interpretable justifications aligned with medical reasoning. It introduces two subtasks: (1) answering diverse types of visual questions using the Kvasir-VQA-x1 dataset, and (2) generating multimodal explanations to support clinical decision-making. The Kvasir-VQA-x1 dataset, created from 6,500 images and 159,549 complex question-answer (QA) pairs, serves as the benchmark for the challenge. By combining quantitative performance metrics and expert-reviewed explainability assessments, this task aims to advance trustworthy Artificial Intelligence (AI) in medical image analysis. Instructions, data access, and an updated guide for participation are available in the official competition repository: https://github.com/simula/MediaEval-Medico-2025</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.10860v1" target="_blank">From Black Box to Transparency: Enhancing Automated Interpreting Assessment with Explainable AI in College Classrooms</a></h3>
                    <p><strong>Authors:</strong> Zhaokun Jiang, Ziyin Zhang</p>
                    <p><strong>Published:</strong> 8/14/2025</p>
                    <p><strong>Categories:</strong> cs.CL, cs.AI</p>
                    <p><strong>Summary:</strong> Recent advancements in machine learning have spurred growing interests in automated interpreting quality assessment. Nevertheless, existing research suffers from insufficient examination of language use quality, unsatisfactory modeling effectiveness due to data scarcity and imbalance, and a lack of efforts to explain model predictions. To address these gaps, we propose a multi-dimensional modeling framework that integrates feature engineering, data augmentation, and explainable machine learning. This approach prioritizes explainability over ``black box predictions by utilizing only construct-relevant, transparent features and conducting Shapley Value (SHAP) analysis. Our results demonstrate strong predictive performance on a novel English-Chinese consecutive interpreting dataset, identifying BLEURT and CometKiwi scores to be the strongest predictive features for fidelity, pause-related features for fluency, and Chinese-specific phraseological diversity metrics for language use. Overall, by placing particular emphasis on explainability, we present a scalable, reliable, and transparent alternative to traditional human evaluation, facilitating the provision of detailed diagnostic feedback for learners and supporting self-regulated learning advantages not afforded by automated scores in isolation.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.10852v1" target="_blank">EVOSCAT: Exploring Software Change Dynamics in Large-Scale Historical Datasets</a></h3>
                    <p><strong>Authors:</strong> Souhaila Serbout, Diana Carolina MuÃ±oz Hurtado, Hassan Atwi, Edoardo Riggio, Cesare Pautasso</p>
                    <p><strong>Published:</strong> 8/14/2025</p>
                    <p><strong>Categories:</strong> cs.SE</p>
                    <p><strong>Summary:</strong> Long lived software projects encompass a large number of artifacts, which undergo many revisions throughout their history. Empirical software engineering researchers studying software evolution gather and collect datasets with millions of events, representing changes introduced to specific artifacts. In this paper, we propose EvoScat, a tool that attempts addressing temporal scalability through the usage of interactive density scatterplot to provide a global overview of large historical datasets mined from open source repositories in a single visualization. EvoScat intents to provide researchers with a mean to produce scalable visualizations that can help them explore and characterize evolution datasets, as well as comparing the histories of individual artifacts, both in terms of 1) observing how rapidly different artifacts age over multiple-year-long time spans 2) how often metrics associated with each artifacts tend towards an improvement or worsening. The paper shows how the tool can be tailored to specific analysis needs (pace of change comparison, clone detection, freshness assessment) thanks to its support for flexible configuration of history scaling and alignment along the time axis, artifacts sorting and interactive color mapping, enabling the analysis of millions of events obtained by mining the histories of tens of thousands of software artifacts. We include in this paper a gallery showcasing datasets gathering specific artifacts (OpenAPI descriptions, GitHub workflow definitions) across multiple repositories, as well as diving into the history of specific popular open source projects.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.10849v1" target="_blank">Integrating Terrestrial and Non-Terrestrial Networks for Sustainable 6G Operations: A Latency-Aware Multi-Tier Cell-Switching Approach</a></h3>
                    <p><strong>Authors:</strong> Metin Ozturk, Maryam Salamatmoghadasi, Halim Yanikomeroglu</p>
                    <p><strong>Published:</strong> 8/14/2025</p>
                    <p><strong>Categories:</strong> eess.SY, cs.SY</p>
                    <p><strong>Summary:</strong> Sustainability is paramount in modern cellular networks, which face significant energy consumption challenges from rising mobile traffic and advancements in wireless technology. Cell-switching, well-established in literature as an effective solution, encounters limitations such as inadequate capacity and limited coverage when implemented through terrestrial networks (TN). This study enhances cell-switching by integrating non-terrestrial networks (NTN), including satellites (used for cell-switching for the first time), high altitude platform stations (HAPS), and uncrewed aerial vehicles (UAVs) into TN. This integration significantly boosts energy savings by expanding capacity, enhancing coverage, and increasing operational flexibility. We introduce a multi-tier cell-switching approach that dynamically offloads users across network layers to manage energy effectively and minimize delays, accommodating diverse user demands with a context aware strategy. Additionally, we explore the role of artificial intelligence (AI), particularly generative AI, in optimizing network efficiency through data compression, handover optimization between different network layers, and enhancing device compatibility, further improving the adaptability and energy efficiency of cell-switching operations. A case study confirms substantial improvements in network power consumption and user satisfaction, demonstrating the potential of our approach for future networks.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.10848v1" target="_blank">Psyche-R1: Towards Reliable Psychological LLMs through Unified Empathy, Expertise, and Reasoning</a></h3>
                    <p><strong>Authors:</strong> Chongyuan Dai, Jinpeng Hu, Hongchang Shi, Zhuo Li, Xun Yang, Meng Wang</p>
                    <p><strong>Published:</strong> 8/14/2025</p>
                    <p><strong>Categories:</strong> cs.CL</p>
                    <p><strong>Summary:</strong> Amidst a shortage of qualified mental health professionals, the integration of large language models (LLMs) into psychological applications offers a promising way to alleviate the growing burden of mental health disorders. Recent reasoning-augmented LLMs have achieved remarkable performance in mathematics and programming, while research in the psychological domain has predominantly emphasized emotional support and empathetic dialogue, with limited attention to reasoning mechanisms that are beneficial to generating reliable responses. Therefore, in this paper, we propose Psyche-R1, the first Chinese psychological LLM that jointly integrates empathy, psychological expertise, and reasoning, built upon a novel data curation pipeline. Specifically, we design a comprehensive data synthesis pipeline that produces over 75k high-quality psychological questions paired with detailed rationales, generated through chain-of-thought (CoT) reasoning and iterative prompt-rationale optimization, along with 73k empathetic dialogues. Subsequently, we employ a hybrid training strategy wherein challenging samples are identified through a multi-LLM cross-selection strategy for group relative policy optimization (GRPO) to improve reasoning ability, while the remaining data is used for supervised fine-tuning (SFT) to enhance empathetic response generation and psychological domain knowledge. Extensive experiment results demonstrate the effectiveness of the Psyche-R1 across several psychological benchmarks, where our 7B Psyche-R1 achieves comparable results to 671B DeepSeek-R1.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.10836v1" target="_blank">SoK: Data Minimization in Machine Learning</a></h3>
                    <p><strong>Authors:</strong> Robin Staab, Nikola JovanoviÄ‡, Kimberly Mai, Prakhar Ganesh, Martin Vechev, Ferdinando Fioretto, Matthew Jagielski</p>
                    <p><strong>Published:</strong> 8/14/2025</p>
                    <p><strong>Categories:</strong> cs.LG, cs.CR</p>
                    <p><strong>Summary:</strong> Data minimization (DM) describes the principle of collecting only the data strictly necessary for a given task. It is a foundational principle across major data protection regulations like GDPR and CPRA. Violations of this principle have substantial real-world consequences, with regulatory actions resulting in fines reaching hundreds of millions of dollars. Notably, the relevance of data minimization is particularly pronounced in machine learning (ML) applications, which typically rely on large datasets, resulting in an emerging research area known as Data Minimization in Machine Learning (DMML). At the same time, existing work on other ML privacy and security topics often addresses concerns relevant to DMML without explicitly acknowledging the connection. This disconnect leads to confusion among practitioners, complicating their efforts to implement DM principles and interpret the terminology, metrics, and evaluation criteria used across different research communities. To address this gap, our work introduces a comprehensive framework for DMML, including a unified data pipeline, adversaries, and points of minimization. This framework allows us to systematically review the literature on data minimization and \emph{DM-adjacent} methodologies, for the first time presenting a structured overview designed to help practitioners and researchers effectively apply DM principles. Our work facilitates a unified DM-centric understanding and broader adoption of data minimization strategies in AI/ML.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.10833v1" target="_blank">UI-Venus Technical Report: Building High-performance UI Agents with RFT</a></h3>
                    <p><strong>Authors:</strong> Zhangxuan Gu, Zhengwen Zeng, Zhenyu Xu, Xingran Zhou, Shuheng Shen, Yunfei Liu, Beitong Zhou, Changhua Meng, Tianyu Xia, Weizhi Chen, Yue Wen, Jingya Dou, Fei Tang, Jinzhen Lin, Yulin Liu, Zhenlin Guo, Yichen Gong, Heng Jia, Changlong Gao, Yuan Guo, Yong Deng, Zhenyu Guo, Liang Chen, Weiqiang Wang</p>
                    <p><strong>Published:</strong> 8/14/2025</p>
                    <p><strong>Categories:</strong> cs.CV</p>
                    <p><strong>Summary:</strong> We present UI-Venus, a native UI agent that takes only screenshots as input based on a multimodal large language model. UI-Venus achieves SOTA performance on both UI grounding and navigation tasks using only several hundred thousand high-quality training samples through reinforcement finetune (RFT) based on Qwen2.5-VL. Specifically, the 7B and 72B variants of UI-Venus obtain 94.1% / 50.8% and 95.3% / 61.9% on the standard grounding benchmarks, i.e., Screenspot-V2 / Pro, surpassing the previous SOTA baselines including open-source GTA1 and closed-source UI-TARS-1.5.To show UI-Venuss summary and planing ability, we also evaluate it on the AndroidWorld, an online UI navigation arena, on which our 7B and 72B variants achieve 49.1% and 65.9% success rate, also beating existing models.To achieve this, we introduce carefully designed reward functions for both UI grounding and navigation tasks and corresponding efficient data cleaning strategies.To further boost navigation performance, we propose Self-Evolving Trajectory History Alignment \ Sparse Action Enhancement that refine historical reasoning traces and balances the distribution of sparse but critical actions, leading to more coherent planning and better generalization in complex UI tasks. Our contributions include the publish of SOTA open-source UI agents, comprehensive data cleaning protocols and a novel self-evolving framework for improving navigation performance, which encourage further research and development in the community. Code is available at https://github.com/antgroup/UI-Venus.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.10830v1" target="_blank">Advances in Speech Separation: Techniques, Challenges, and Future Trends</a></h3>
                    <p><strong>Authors:</strong> Kai Li, Guo Chen, Wendi Sang, Yi Luo, Zhuo Chen, Shuai Wang, Shulin He, Zhong-Qiu Wang, Andong Li, Zhiyong Wu, Xiaolin Hu</p>
                    <p><strong>Published:</strong> 8/14/2025</p>
                    <p><strong>Categories:</strong> cs.SD, eess.AS</p>
                    <p><strong>Summary:</strong> The field of speech separation, addressing the cocktail party problem, has seen revolutionary advances with DNNs. Speech separation enhances clarity in complex acoustic environments and serves as crucial pre-processing for speech recognition and speaker recognition. However, current literature focuses narrowly on specific architectures or isolated approaches, creating fragmented understanding. This survey addresses this gap by providing systematic examination of DNN-based speech separation techniques. Our work differentiates itself through: (I) Comprehensive perspective: We systematically investigate learning paradigms, separation scenarios with known/unknown speakers, comparative analysis of supervised/self-supervised/unsupervised frameworks, and architectural components from encoders to estimation strategies. (II) Timeliness: Coverage of cutting-edge developments ensures access to current innovations and benchmarks. (III) Unique insights: Beyond summarization, we evaluate technological trajectories, identify emerging patterns, and highlight promising directions including domain-robust frameworks, efficient architectures, multimodal integration, and novel self-supervised paradigms. (IV) Fair evaluation: We provide quantitative evaluations on standard datasets, revealing true capabilities and limitations of different methods. This comprehensive survey serves as an accessible reference for experienced researchers and newcomers navigating speech separations complex landscape.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.10806v1" target="_blank">Who Benefits from AI Explanations? Towards Accessible and Interpretable Systems</a></h3>
                    <p><strong>Authors:</strong> Maria J. P. Peixoto, Akriti Pandey, Ahsan Zaman, Peter R. Lewis</p>
                    <p><strong>Published:</strong> 8/14/2025</p>
                    <p><strong>Categories:</strong> cs.AI</p>
                    <p><strong>Summary:</strong> As AI systems are increasingly deployed to support decision-making in critical domains, explainability has become a means to enhance the understandability of these outputs and enable users to make more informed and conscious choices. However, despite growing interest in the usability of eXplainable AI (XAI), the accessibility of these methods, particularly for users with vision impairments, remains underexplored. This paper investigates accessibility gaps in XAI through a two-pronged approach. First, a literature review of 79 studies reveals that evaluations of XAI techniques rarely include disabled users, with most explanations relying on inherently visual formats. Second, we present a four-part methodological proof of concept that operationalizes inclusive XAI design: (1) categorization of AI systems, (2) persona definition and contextualization, (3) prototype design and implementation, and (4) expert and user assessment of XAI techniques for accessibility. Preliminary findings suggest that simplified explanations are more comprehensible for non-visual users than detailed ones, and that multimodal presentation is required for more equitable interpretability.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.10798v1" target="_blank">The SET Perceptual Factors Framework: Towards Assured Perception for Autonomous Systems</a></h3>
                    <p><strong>Authors:</strong> Troi Williams</p>
                    <p><strong>Published:</strong> 8/14/2025</p>
                    <p><strong>Categories:</strong> cs.RO, cs.AI</p>
                    <p><strong>Summary:</strong> Future autonomous systems promise significant societal benefits, yet their deployment raises concerns about safety and trustworthiness. A key concern is assuring the reliability of robot perception, as perception seeds safe decision-making. Failures in perception are often due to complex yet common environmental factors and can lead to accidents that erode public trust. To address this concern, we introduce the SET (Self, Environment, and Target) Perceptual Factors Framework. We designed the framework to systematically analyze how factors such as weather, occlusion, or sensor limitations negatively impact perception. To achieve this, the framework employs SET State Trees to categorize where such factors originate and SET Factor Trees to model how these sources and factors impact perceptual tasks like object detection or pose estimation. Next, we develop Perceptual Factor Models using both trees to quantify the uncertainty for a given task. Our framework aims to promote rigorous safety assurances and cultivate greater public understanding and trust in autonomous systems by offering a transparent and standardized method for identifying, modeling, and communicating perceptual risks.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.10780v1" target="_blank">Learning Task Execution Hierarchies for Redundant Robots</a></h3>
                    <p><strong>Authors:</strong> Alessandro Adami, Aris Synodinos, Matteo Iovino, Ruggero Carli, Pietro Falco</p>
                    <p><strong>Published:</strong> 8/14/2025</p>
                    <p><strong>Categories:</strong> cs.RO, cs.SY, eess.SY</p>
                    <p><strong>Summary:</strong> Modern robotic systems, such as mobile manipulators, humanoids, and aerial robots with arms, often possess high redundancy, enabling them to perform multiple tasks simultaneously. Managing this redundancy is key to achieving reliable and flexible behavior. A widely used approach is the Stack of Tasks (SoT), which organizes control objectives by priority within a unified framework. However, traditional SoTs are manually designed by experts, limiting their adaptability and accessibility. This paper introduces a novel framework that automatically learns both the hierarchy and parameters of a SoT from user-defined objectives. By combining Reinforcement Learning and Genetic Programming, the system discovers task priorities and control strategies without manual intervention. A cost function based on intuitive metrics such as precision, safety, and execution time guides the learning process. We validate our method through simulations and experiments on the mobile-YuMi platform, a dual-arm mobile manipulator with high redundancy. Results show that the learned SoTs enable the robot to dynamically adapt to changing environments and inputs, balancing competing objectives while maintaining robust task execution. This approach provides a general and user-friendly solution for redundancy management in complex robots, advancing human-centered robot programming and reducing the need for expert design.</p>
                
            
                
                    <h3><a href="http://dx.doi.org/10.1145/3746027.3758295" target="_blank">AEGIS: Authenticity Evaluation Benchmark for AI-Generated Video Sequences</a></h3>
                    <p><strong>Authors:</strong> Jieyu Li, Xin Zhang, Joey Tianyi Zhou</p>
                    <p><strong>Published:</strong> 8/14/2025</p>
                    <p><strong>Categories:</strong> cs.CV, cs.AI</p>
                    <p><strong>Summary:</strong> Recent advances in AI-generated content have fueled the rise of highly realistic synthetic videos, posing severe risks to societal trust and digital integrity. Existing benchmarks for video authenticity detection typically suffer from limited realism, insufficient scale, and inadequate complexity, failing to effectively evaluate modern vision-language models against sophisticated forgeries. To address this critical gap, we introduce AEGIS, a novel large-scale benchmark explicitly targeting the detection of hyper-realistic and semantically nuanced AI-generated videos. AEGIS comprises over 10,000 rigorously curated real and synthetic videos generated by diverse, state-of-the-art generative models, including Stable Video Diffusion, CogVideoX-5B, KLing, and Sora, encompassing open-source and proprietary architectures. In particular, AEGIS features specially constructed challenging subsets enhanced with robustness evaluation. Furthermore, we provide multimodal annotations spanning Semantic-Authenticity Descriptions, Motion Features, and Low-level Visual Features, facilitating authenticity detection and supporting downstream tasks such as multimodal fusion and forgery localization. Extensive experiments using advanced vision-language models demonstrate limited detection capabilities on the most challenging subsets of AEGIS, highlighting the datasets unique complexity and realism beyond the current generalization capabilities of existing models. In essence, AEGIS establishes an indispensable evaluation benchmark, fundamentally advancing research toward developing genuinely robust, reliable, broadly generalizable video authenticity detection methodologies capable of addressing real-world forgery threats. Our dataset is available on https://huggingface.co/datasets/Clarifiedfish/AEGIS.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.10769v1" target="_blank">Modeling Human Responses to Multimodal AI Content</a></h3>
                    <p><strong>Authors:</strong> Zhiqi Shen, Shaojing Fan, Danni Xu, Terence Sim, Mohan Kankanhalli</p>
                    <p><strong>Published:</strong> 8/14/2025</p>
                    <p><strong>Categories:</strong> cs.AI, cs.MM</p>
                    <p><strong>Summary:</strong> As AI-generated content becomes widespread, so does the risk of misinformation. While prior research has primarily focused on identifying whether content is authentic, much less is known about how such content influences human perception and behavior. In domains like trading or the stock market, predicting how people react (e.g., whether a news post will go viral), can be more critical than verifying its factual accuracy. To address this, we take a human-centered approach and introduce the MhAIM Dataset, which contains 154,552 online posts (111,153 of them AI-generated), enabling large-scale analysis of how people respond to AI-generated content. Our human study reveals that people are better at identifying AI content when posts include both text and visuals, particularly when inconsistencies exist between the two. We propose three new metrics: trustworthiness, impact, and openness, to quantify how users judge and engage with online content. We present T-Lens, an LLM-based agent system designed to answer user queries by incorporating predicted human responses to multimodal information. At its core is HR-MCP (Human Response Model Context Protocol), built on the standardized Model Context Protocol (MCP), enabling seamless integration with any LLM. This integration allows T-Lens to better align with human reactions, enhancing both interpretability and interaction capabilities. Our work provides empirical insights and practical tools to equip LLMs with human-awareness capabilities. By highlighting the complex interplay among AI, human cognition, and information reception, our findings suggest actionable strategies for mitigating the risks of AI-driven misinformation.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.10766v1" target="_blank">Frechet and Mordukhovich Derivative (Coderivative) and Covering Constant for Single-Valued Mapping in Euclidean Space with Applications (I)</a></h3>
                    <p><strong>Authors:</strong> Jinlu Li</p>
                    <p><strong>Published:</strong> 8/14/2025</p>
                    <p><strong>Categories:</strong> math.FA, 49J52, 49J53, 47H10, 90C31</p>
                    <p><strong>Summary:</strong> In this paper, we study Frechet derivatives and Mordukhovich derivatives (or coderivatives) of single-valued mappings in Euclidean spaces. At first, we prove the guideline for calculating the Frechet derivatives of single-valued mappings by their partial derivatives. Then, by using the connections between Frechet derivatives and Mordukhovich derivatives (or coderivatives) of single-valued mappings in Banach spaces, we derive the useful rules for calculating the Mordukhovich derivatives of single-valued mappings in Euclidean spaces. For practicing these rules, we find the precise solutions of the Frechet derivatives and Mordukhovich derivatives for some single-valued mappings in Euclidean spaces (in R^2, it can be extended to R^n). By using these solutions, we will find the covering constants for the considered mappings. As applications of the results about the covering constants and by applying the Arutyunov Mordukhovich and Zhukovskiy Parameterized Coincidence Point Theorem, we solve some parameterized equations.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.10765v1" target="_blank">Memorisation and forgetting in a learning Hopfield neural network: bifurcation mechanisms, attractors and basins</a></h3>
                    <p><strong>Authors:</strong> Adam E. Essex, Natalia B. Janson, Rachel A. Norris, Alexander G. Balanov</p>
                    <p><strong>Published:</strong> 8/14/2025</p>
                    <p><strong>Categories:</strong> math.DS, cs.LG, nlin.AO, 37N99 (primary) 68T07, 68T05 (secondary)</p>
                    <p><strong>Summary:</strong> Despite explosive expansion of artificial intelligence based on artificial neural networks (ANNs), these are employed as black boxes, as it is unclear how, during learning, they form memories or develop unwanted features, including spurious memories and catastrophic forgetting. Much research is available on isolated aspects of learning ANNs, but due to their high dimensionality and non-linearity, their comprehensive analysis remains a challenge. In ANNs, knowledge is thought to reside in connection weights or in attractor basins, but these two paradigms are not linked explicitly. Here we comprehensively analyse mechanisms of memory formation in an 81-neuron Hopfield network undergoing Hebbian learning by revealing bifurcations leading to formation and destruction of attractors and their basin boundaries. We show that, by affecting evolution of connection weights, the applied stimuli induce a pitchfork and then a cascade of saddle-node bifurcations creating new attractors with their basins that can code true or spurious memories, and an abrupt disappearance of old memories (catastrophic forgetting). With successful learning, new categories are represented by the basins of newly born point attractors, and their boundaries by the stable manifolds of new saddles. With this, memorisation and forgetting represent two manifestations of the same mechanism. Our strategy to analyse high-dimensional learning ANNs is universal and applicable to recurrent ANNs of any form. The demonstrated mechanisms of memory formation and of catastrophic forgetting shed light on the operation of a wider class of recurrent ANNs and could aid the development of approaches to mitigate their flaws.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.10760v1" target="_blank">FROGENT: An End-to-End Full-process Drug Design Agent</a></h3>
                    <p><strong>Authors:</strong> Qihua Pan, Dong Xu, Jenna Xinyi Yao, Lijia Ma, Zexuan Zhu, Junkai Ji</p>
                    <p><strong>Published:</strong> 8/14/2025</p>
                    <p><strong>Categories:</strong> q-bio.BM, cs.AI</p>
                    <p><strong>Summary:</strong> Powerful AI tools for drug discovery reside in isolated web apps, desktop programs, and code libraries. Such fragmentation forces scientists to manage incompatible interfaces and specialized scripts, which can be a cumbersome and repetitive process. To address this issue, a Full-pROcess druG dEsign ageNT, named FROGENT, has been proposed. Specifically, FROGENT utilizes a Large Language Model and the Model Context Protocol to integrate multiple dynamic biochemical databases, extensible tool libraries, and task-specific AI models. This agentic framework allows FROGENT to execute complicated drug discovery workflows dynamically, including component tasks such as target identification, molecule generation and retrosynthetic planning. FROGENT has been evaluated on eight benchmarks that cover various aspects of drug discovery, such as knowledge retrieval, property prediction, virtual screening, mechanistic analysis, molecular design, and synthesis. It was compared against six increasingly advanced ReAct-style agents that support code execution and literature searches. Empirical results demonstrated that FROGENT triples the best baseline performance in hit-finding and doubles it in interaction profiling, significantly outperforming both the open-source model Qwen3-32B and the commercial model GPT-4o. In addition, real-world cases have been utilized to validate the practicability and generalization of FROGENT. This development suggests that streamlining the agentic drug discovery pipeline can significantly enhance researcher productivity.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.10749v1" target="_blank">OpenSWI: A Massive-Scale Benchmark Dataset for Surface Wave Dispersion Curve Inversion</a></h3>
                    <p><strong>Authors:</strong> Feng Liu, Sijie Zhao, Xinyu Gu, Fenghua Ling, Peiqin Zhuang, Yaxing Li, Rui Su, Lihua Fang, Lianqing Zhou, Jianping Huang, Lei Bai</p>
                    <p><strong>Published:</strong> 8/14/2025</p>
                    <p><strong>Categories:</strong> physics.geo-ph</p>
                    <p><strong>Summary:</strong> Surface wave dispersion curve inversion plays a critical role in both shallow resource exploration and deep geological studies, yet it remains hindered by sensitivity to initial models and low computational efficiency. Recently, data-driven deep learning methods, inspired by advances in computer vision, have shown promising potential to address these challenges. However, the lack of large-scale, diverse benchmark datasets remains a major obstacle to their development and evaluation. To bridge this gap, we present OpenSWI, a comprehensive benchmark dataset generated through the Surface Wave Inversion Dataset Preparation (SWIDP) pipeline. OpenSWI includes two synthetic datasets tailored to different research scales and scenarios, OpenSWI-shallow and OpenSWI-deep, and an AI-ready real-world dataset for generalization evaluation, OpenSWI-real. OpenSWI-shallow, derived from the 2-D OpenFWI geological model dataset, contains over 22 million 1-D velocity profiles paired with fundamental-mode phase and group velocity dispersion curves, spanning a wide range of shallow geological structures (e.g., flat layers, faults, folds, realistic stratigraphy). OpenSWI-deep, built from 14 global and regional 3-D geological models, comprises 1.26 million high-fidelity 1-D velocity-dispersion pairs for deep-Earth studies. OpenSWI-real, compiled from open-source projects, contains two sets of observed dispersion curves with corresponding reference models, serving as a benchmark for evaluating model generalization. To demonstrate utility, we trained models on OpenSWI-shallow and -deep and evaluated them on OpenSWI-real, demonstrating strong agreement between predictions and references, which confirms the diversity and representativeness of the dataset. To advance intelligent surface wave inversion, we release the SWIDP toolbox, OpenSWI datasets, and trained models for the research community.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.10745v1" target="_blank">Agentic Design Review System</a></h3>
                    <p><strong>Authors:</strong> Sayan Nag, K J Joseph, Koustava Goswami, Vlad I Morariu, Balaji Vasan Srinivasan</p>
                    <p><strong>Published:</strong> 8/14/2025</p>
                    <p><strong>Categories:</strong> cs.AI, cs.CV, cs.LG, cs.MA, cs.MM</p>
                    <p><strong>Summary:</strong> Evaluating graphic designs involves assessing it from multiple facets like alignment, composition, aesthetics and color choices. Evaluating designs in a holistic way involves aggregating feedback from individual expert reviewers. Towards this, we propose an Agentic Design Review System (AgenticDRS), where multiple agents collaboratively analyze a design, orchestrated by a meta-agent. A novel in-context exemplar selection approach based on graph matching and a unique prompt expansion method plays central role towards making each agent design aware. Towards evaluating this framework, we propose DRS-BENCH benchmark. Thorough experimental evaluation against state-of-the-art baselines adapted to the problem setup, backed-up with critical ablation experiments brings out the efficacy of Agentic-DRS in evaluating graphic designs and generating actionable feedback. We hope that this work will attract attention to this pragmatic, yet under-explored research direction.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.10741v1" target="_blank">Forgery Guided Learning Strategy with Dual Perception Network for Deepfake Cross-domain Detection</a></h3>
                    <p><strong>Authors:</strong> Lixin Jia, Zhiqing Guo, Gaobo Yang, Liejun Wang, Keqin Li</p>
                    <p><strong>Published:</strong> 8/14/2025</p>
                    <p><strong>Categories:</strong> cs.CV</p>
                    <p><strong>Summary:</strong> The emergence of deepfake technology has introduced a range of societal problems, garnering considerable attention. Current deepfake detection methods perform well on specific datasets, but exhibit poor performance when applied to datasets with unknown forgery techniques. Moreover, as the gap between emerging and traditional forgery techniques continues to widen, cross-domain detection methods that rely on common forgery traces are becoming increasingly ineffective. This situation highlights the urgency of developing deepfake detection technology with strong generalization to cope with fast iterative forgery techniques. To address these challenges, we propose a Forgery Guided Learning (FGL) strategy designed to enable detection networks to continuously adapt to unknown forgery techniques. Specifically, the FGL strategy captures the differential information between known and unknown forgery techniques, allowing the model to dynamically adjust its learning process in real time. To further improve the ability to perceive forgery traces, we design a Dual Perception Network (DPNet) that captures both differences and relationships among forgery traces. In the frequency stream, the network dynamically perceives and extracts discriminative features across various forgery techniques, establishing essential detection cues. These features are then integrated with spatial features and projected into the embedding space. In addition, graph convolution is employed to perceive relationships across the entire feature space, facilitating a more comprehensive understanding of forgery trace correlations. Extensive experiments show that our approach generalizes well across different scenarios and effectively handles unknown forgery challenges, providing robust support for deepfake detection. Our code is available on https://github.com/vpsg-research/FGL.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.10737v1" target="_blank">Privacy-enhancing Sclera Segmentation Benchmarking Competition: SSBC 2025</a></h3>
                    <p><strong>Authors:</strong> Matej Vitek, Darian TomaÅ¡eviÄ‡, Abhijit Das, Sabari Nathan, GÃ¶khan Ã–zbulak, GÃ¶zde AyÅŸe TataroÄŸlu Ã–zbulak, Jean-Paul Calbimonte, AndrÃ© Anjos, Hariohm Hemant Bhatt, Dhruv Dhirendra Premani, Jay Chaudhari, Caiyong Wang, Jian Jiang, Chi Zhang, Qi Zhang, Iyyakutti Iyappan Ganapathi, Syed Sadaf Ali, Divya Velayudan, Maregu Assefa, Naoufel Werghi, Zachary A. Daniels, Leeon John, Ritesh Vyas, Jalil Nourmohammadi Khiarak, Taher Akbari Saeed, Mahsa Nasehi, Ali Kianfar, Mobina Pashazadeh Panahi, Geetanjali Sharma, Pushp Raj Panth, Raghavendra Ramachandra, Aditya Nigam, Umapada Pal, Peter Peer, Vitomir Å truc</p>
                    <p><strong>Published:</strong> 8/14/2025</p>
                    <p><strong>Categories:</strong> cs.CV</p>
                    <p><strong>Summary:</strong> This paper presents a summary of the 2025 Sclera Segmentation Benchmarking Competition (SSBC), which focused on the development of privacy-preserving sclera-segmentation models trained using synthetically generated ocular images. The goal of the competition was to evaluate how well models trained on synthetic data perform in comparison to those trained on real-world datasets. The competition featured two tracks: $(i)$ one relying solely on synthetic data for model development, and $(ii)$ one combining/mixing synthetic with (a limited amount of) real-world data. A total of nine research groups submitted diverse segmentation models, employing a variety of architectural designs, including transformer-based solutions, lightweight models, and segmentation networks guided by generative frameworks. Experiments were conducted across three evaluation datasets containing both synthetic and real-world images, collected under diverse conditions. Results show that models trained entirely on synthetic data can achieve competitive performance, particularly when dedicated training strategies are employed, as evidenced by the top performing models that achieved $F_1$ scores of over $0.8$ in the synthetic data track. Moreover, performance gains in the mixed track were often driven more by methodological choices rather than by the inclusion of real data, highlighting the promise of synthetic data for privacy-aware biometric development. The code and data for the competition is available at: https://github.com/dariant/SSBC_2025.</p>
                
            
                
                    <h3><a href="http://dx.doi.org/10.1109/DCOSS-IoT65416.2025.00096" target="_blank">Traffic Intersection Simulation Using Turning Movement Count Data in SUMO: A Case Study of Toronto Intersections</a></h3>
                    <p><strong>Authors:</strong> Harshit Maheshwari, Li Yang, Richard W Pazzi</p>
                    <p><strong>Published:</strong> 8/14/2025</p>
                    <p><strong>Categories:</strong> cs.CY, cs.SY, eess.SY, 90B20, 90B06, 68U20, I.6.4; I.6.5</p>
                    <p><strong>Summary:</strong> Urban traffic simulation is vital in planning, modeling, and analyzing road networks. However, the realism of a simulation depends extensively on the quality of input data. This paper presents an intersection traffic simulation tool that leverages real-world vehicle turning movement count (TMC) data from the City of Toronto to model traffic in an urban environment at an individual or multiple intersections using Simulation of Urban MObility (SUMO). The simulation performed in this research focuses specifically on intersection-level traffic generation without creating full vehicle routes through the network. This also helps keep the networks complexity to a minimum. The simulated traffic is evaluated against actual data to show that the simulation closely reproduces real intersection flows. This validates that the real data can drive practical simulations, and these scenarios can replace synthetic or random generated data, which is prominently used in developing new traffic-related methodologies. This is the first tool to integrate TMC data from Toronto into SUMO via an easy-to-use Graphical User Interface. This work contributes to the research and traffic planning community on data-driven traffic simulation. It provides transportation engineers with a framework to evaluate intersection design and traffic signal optimization strategies using readily available aggregate traffic data.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.10716v1" target="_blank">Revisiting Cross-View Localization from Image Matching</a></h3>
                    <p><strong>Authors:</strong> Panwang Xia, Qiong Wu, Lei Yu, Yi Liu, Mingtao Xiong, Lei Liang, Yongjun Zhang, Yi Wan</p>
                    <p><strong>Published:</strong> 8/14/2025</p>
                    <p><strong>Categories:</strong> cs.CV</p>
                    <p><strong>Summary:</strong> Cross-view localization aims to estimate the 3 degrees of freedom pose of a ground-view image by registering it to aerial or satellite imagery. It is essential in GNSS-denied environments such as urban canyons and disaster zones. Existing methods either regress poses directly or align features in a shared birds-eye view (BEV) space, both built upon accurate spatial correspondences between perspectives. However, these methods fail to establish strict cross-view correspondences, yielding only coarse or geometrically inconsistent matches. Consequently, fine-grained image matching between ground and aerial views remains an unsolved problem, which in turn constrains the interpretability of localization results. In this paper, we revisit cross-view localization from the perspective of cross-view image matching and propose a novel framework that improves both matching and localization. Specifically, we introduce a Surface Model to model visible regions for accurate BEV projection, and a SimRefiner module to refine the similarity matrix through local-global residual correction, eliminating the reliance on post-processing like RANSAC. To further support research in this area, we introduce CVFM, the first benchmark with 32,509 cross-view image pairs annotated with pixel-level correspondences. Extensive experiments demonstrate that our approach substantially improves both localization accuracy and image matching quality, setting new baselines under extreme viewpoint disparity.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.10711v1" target="_blank">NextStep-1: Toward Autoregressive Image Generation with Continuous Tokens at Scale</a></h3>
                    <p><strong>Authors:</strong> NextStep Team, Chunrui Han, Guopeng Li, Jingwei Wu, Quan Sun, Yan Cai, Yuang Peng, Zheng Ge, Deyu Zhou, Haomiao Tang, Hongyu Zhou, Kenkun Liu, Ailin Huang, Bin Wang, Changxin Miao, Deshan Sun, En Yu, Fukun Yin, Gang Yu, Hao Nie, Haoran Lv, Hanpeng Hu, Jia Wang, Jian Zhou, Jianjian Sun, Kaijun Tan, Kang An, Kangheng Lin, Liang Zhao, Mei Chen, Peng Xing, Rui Wang, Shiyu Liu, Shutao Xia, Tianhao You, Wei Ji, Xianfang Zeng, Xin Han, Xuelin Zhang, Yana Wei, Yanming Xu, Yimin Jiang, Yingming Wang, Yu Zhou, Yucheng Han, Ziyang Meng, Binxing Jiao, Daxin Jiang, Xiangyu Zhang, Yibo Zhu</p>
                    <p><strong>Published:</strong> 8/14/2025</p>
                    <p><strong>Categories:</strong> cs.CV</p>
                    <p><strong>Summary:</strong> Prevailing autoregressive (AR) models for text-to-image generation either rely on heavy, computationally-intensive diffusion models to process continuous image tokens, or employ vector quantization (VQ) to obtain discrete tokens with quantization loss. In this paper, we push the autoregressive paradigm forward with NextStep-1, a 14B autoregressive model paired with a 157M flow matching head, training on discrete text tokens and continuous image tokens with next-token prediction objectives. NextStep-1 achieves state-of-the-art performance for autoregressive models in text-to-image generation tasks, exhibiting strong capabilities in high-fidelity image synthesis. Furthermore, our method shows strong performance in image editing, highlighting the power and versatility of our unified approach. To facilitate open research, we will release our code and models to the community.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.10691v1" target="_blank">THERMOS: Thermally-Aware Multi-Objective Scheduling of AI Workloads on Heterogeneous Multi-Chiplet PIM Architectures</a></h3>
                    <p><strong>Authors:</strong> Alish Kanani, Lukas Pfromm, Harsh Sharma, Janardhan Rao Doppa, Partha Pratim Pande, Umit Y. Ogras</p>
                    <p><strong>Published:</strong> 8/14/2025</p>
                    <p><strong>Categories:</strong> cs.AR</p>
                    <p><strong>Summary:</strong> Chiplet-based integration enables large-scale systems that combine diverse technologies, enabling higher yield, lower costs, and scalability, making them well-suited to AI workloads. Processing-in-Memory (PIM) has emerged as a promising solution for AI inference, leveraging technologies such as ReRAM, SRAM, and FeFET, each offering unique advantages and trade-offs. A heterogeneous chiplet-based PIM architecture can harness the complementary strengths of these technologies to enable higher performance and energy efficiency. However, scheduling AI workloads across such a heterogeneous system is challenging due to competing performance objectives, dynamic workload characteristics, and power and thermal constraints. To address this need, we propose THERMOS, a thermally-aware, multi-objective scheduling framework for AI workloads on heterogeneous multi-chiplet PIM architectures. THERMOS trains a single multi-objective reinforcement learning (MORL) policy that is capable of achieving Pareto-optimal execution time, energy, or a balanced objective at runtime, depending on the target preferences. Comprehensive evaluations show that THERMOS achieves up to 89% faster average execution time and 57% lower average energy consumption than baseline AI workload scheduling algorithms with only 0.14% runtime and 0.022% energy overhead.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.10687v1" target="_blank">Continuous Bangla Sign Language Translation: Mitigating the Expense of Gloss Annotation with the Assistance of Graph</a></h3>
                    <p><strong>Authors:</strong> Safaeid Hossain Arib, Rabeya Akter, Sejuti Rahman</p>
                    <p><strong>Published:</strong> 8/14/2025</p>
                    <p><strong>Categories:</strong> cs.CL, cs.AI</p>
                    <p><strong>Summary:</strong> Millions of individuals worldwide are affected by deafness and hearing impairment. Sign language serves as a sophisticated means of communication for the deaf and hard of hearing. However, in societies that prioritize spoken languages, sign language often faces underestimation, leading to communication barriers and social exclusion. The Continuous Bangla Sign Language Translation project aims to address this gap by enhancing translation methods. While recent approaches leverage transformer architecture for state-of-the-art results, our method integrates graph-based methods with the transformer architecture. This fusion, combining transformer and STGCN-LSTM architectures, proves more effective in gloss-free translation. Our contributions include architectural fusion, exploring various fusion strategies, and achieving a new state-of-the-art performance on diverse sign language datasets, namely RWTH-PHOENIX-2014T, CSL-Daily, How2Sign, and BornilDB v1.0. Our approach demonstrates superior performance compared to current translation outcomes across all datasets, showcasing notable improvements of BLEU-4 scores of 4.01, 2.07, and 0.5, surpassing those of GASLT, GASLT and slt_how2sign in RWTH-PHOENIX-2014T, CSL-Daily, and How2Sign, respectively. Also, we introduce benchmarking on the BornilDB v1.0 dataset for the first time. Our method sets a benchmark for future research, emphasizing the importance of gloss-free translation to improve communication accessibility for the deaf and hard of hearing.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.10686v1" target="_blank">An Open-Source User-Friendly Interface for Simulating Magnetic Soft Robots using Simulation Open Framework Architecture (SOFA)</a></h3>
                    <p><strong>Authors:</strong> Carla Wehner, Finn Schubert, Heiko Hellkamp, Julius Hahnewald, Kilian Scheafer, Muhammad Bilal Khan, Oliver Gutfleisch</p>
                    <p><strong>Published:</strong> 8/14/2025</p>
                    <p><strong>Categories:</strong> cs.RO, cond-mat.mtrl-sci</p>
                    <p><strong>Summary:</strong> Soft robots, particularly magnetic soft robots, require specialized simulation tools to accurately model their deformation under external magnetic fields. However, existing platforms often lack dedicated support for magnetic materials, making them difficult to use for researchers at different expertise levels. This work introduces an open-source, user-friendly simulation interface using the Simulation Open Framework Architecture (SOFA), specifically designed to model magnetic soft robots. The tool enables users to define material properties, apply magnetic fields, and observe resulting deformations in real time. By integrating intuitive controls and stress analysis capabilities, it aims to bridge the gap between theoretical modeling and practical design. Four benchmark models - a beam, three- and four-finger grippers, and a butterfly - demonstrate its functionality. The softwares ease of use makes it accessible to both beginners and advanced researchers. Future improvements will refine accuracy through experimental validation and comparison with industry-standard finite element solvers, ensuring realistic and predictive simulations of magnetic soft robots.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.10671v1" target="_blank">AEGISS -- Atomic orbital and Entropy-based Guided Inference for Space Selection -- A novel semi-automated active space selection workflow for quantum chemistry and quantum computing applications</a></h3>
                    <p><strong>Authors:</strong> Fabio Tarocco, Pi A. B. Haase, Fabijan PavoÅ¡eviÄ‡, Vijay Krishna, Leonardo Guidoni, Stefan Knecht, Martina Stella</p>
                    <p><strong>Published:</strong> 8/14/2025</p>
                    <p><strong>Categories:</strong> physics.chem-ph, cond-mat.str-el, physics.comp-ph, quant-ph</p>
                    <p><strong>Summary:</strong> The selection of a balanced active space is a critical step in multi-reference quantum chemistry calculations, particularly for systems with strong electron correlation. Likewise, active space selection is a key to unlock the potential of contemporary quantum computing in quantum chemistry. Albeit recent progress, there remains a lack of a unified, robust, and fully automated framework for active space selection that performs reliably across a wide range of molecular systems. In this work, we present a novel approach inspired by both the AVAS (Atomic Valence Active Space) and AutoCAS methods. Our method unifies orbital entropy analysis with atomic orbital projections to guide the construction of chemically and physically meaningful active spaces. This integrated scheme enables a more consistent and flexible selection of active orbitals while retaining automation and scalability. We validate our approach on a set of molecular systems relevant to photodynamic therapy, in particular a set of Ru(II)-complexes, selected to span increasing levels of electron correlation and structural complexity. These molecules serve as challenging test cases due to the presence of strong static correlation and the need for highly accurate electronic structure descriptions. Our results demonstrate that the method can reliably identify compact, chemically intuitive active spaces that capture the essential physics, making it suitable for both classical and quantum computational frameworks. Furthermore, we have developed this approach in a package that is intuitive to use for users and can be interfaced with both standard quantum chemistry and quantum computing applications, making it accessible to a broad research community.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.10666v1" target="_blank">Deep Learning in Classical and Quantum Physics</a></h3>
                    <p><strong>Authors:</strong> Timothy Heightman, Marcin PÅ‚odzieÅ„</p>
                    <p><strong>Published:</strong> 8/14/2025</p>
                    <p><strong>Categories:</strong> quant-ph, cs.AI, cs.NE, physics.comp-ph</p>
                    <p><strong>Summary:</strong> Scientific progress is tightly coupled to the emergence of new research tools. Today, machine learning (ML)-especially deep learning (DL)-has become a transformative instrument for quantum science and technology. Owing to the intrinsic complexity of quantum systems, DL enables efficient exploration of large parameter spaces, extraction of patterns from experimental data, and data-driven guidance for research directions. These capabilities already support tasks such as refining quantum control protocols and accelerating the discovery of materials with targeted quantum properties, making ML/DL literacy an essential skill for the next generation of quantum scientists. At the same time, DLs power brings risks: models can overfit noisy data, obscure causal structure, and yield results with limited physical interpretability. Recognizing these limitations and deploying mitigation strategies is crucial for scientific rigor. These lecture notes provide a comprehensive, graduate-level introduction to DL for quantum applications, combining conceptual exposition with hands-on examples. Organized as a progressive sequence, they aim to equip readers to decide when and how to apply DL effectively, to understand its practical constraints, and to adapt AI methods responsibly to problems across quantum physics, chemistry, and engineering.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.10655v1" target="_blank">Serial Over Parallel: Learning Continual Unification for Multi-Modal Visual Object Tracking and Benchmarking</a></h3>
                    <p><strong>Authors:</strong> Zhangyong Tang, Tianyang Xu, Xuefeng Zhu, Chunyang Cheng, Tao Zhou, Xiaojun Wu, Josef Kittler</p>
                    <p><strong>Published:</strong> 8/14/2025</p>
                    <p><strong>Categories:</strong> cs.CV, cs.AI</p>
                    <p><strong>Summary:</strong> Unifying multiple multi-modal visual object tracking (MMVOT) tasks draws increasing attention due to the complementary nature of different modalities in building robust tracking systems. Existing practices mix all data sensor types in a single training procedure, structuring a parallel paradigm from the data-centric perspective and aiming for a global optimum on the joint distribution of the involved tasks. However, the absence of a unified benchmark where all types of data coexist forces evaluations on separated benchmarks, causing \textit{inconsistency} between training and testing, thus leading to performance \textit{degradation}. To address these issues, this work advances in two aspects: \ding{182} A unified benchmark, coined as UniBench300, is introduced to bridge the inconsistency by incorporating multiple task data, reducing inference passes from three to one and cutting time consumption by 27\%. \ding{183} The unification process is reformulated in a serial format, progressively integrating new tasks. In this way, the performance degradation can be specified as knowledge forgetting of previous tasks, which naturally aligns with the philosophy of continual learning (CL), motivating further exploration of injecting CL into the unification process. Extensive experiments conducted on two baselines and four benchmarks demonstrate the significance of UniBench300 and the superiority of CL in supporting a stable unification process. Moreover, while conducting dedicated analyses, the performance degradation is found to be negatively correlated with network capacity. Additionally, modality discrepancies contribute to varying degradation levels across tasks (RGBT  RGBD  RGBE in MMVOT), offering valuable insights for future multi-modal vision research. Source codes and the proposed benchmark is available at \textit{https://github.com/Zhangyong-Tang/UniBench300}.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.10652v1" target="_blank">A Novel Study on Intelligent Methods and Explainable AI for Dynamic Malware Analysis</a></h3>
                    <p><strong>Authors:</strong> Richa Dasila, Vatsala Upadhyay, Samo Bobek, Abhishek Vaish</p>
                    <p><strong>Published:</strong> 8/14/2025</p>
                    <p><strong>Categories:</strong> cs.CR, cs.IT, math.IT</p>
                    <p><strong>Summary:</strong> Deep learning models are one of the security strategies, trained on extensive datasets, and play a critical role in detecting and responding to these threats by recognizing complex patterns in malicious code. However, the opaque nature of these models-often described as black boxes-makes their decision-making processes difficult to understand, even for their creators. This research addresses these challenges by integrating Explainable AI (XAI) techniques to enhance the interpretability and trustworthiness of malware detection models. In this research, the use of Multi-Layer Perceptrons (MLP) for dynamic malware analysis has been considered, a less explored area, and its efficacy in detecting Metamorphic Malware, and further the effectiveness and transparency of MLPs, CNNs, RNNs, and CNN-LSTM models in malware classification, evaluating these models through the lens of Explainable AI (XAI). This comprehensive approach aims to demystify the internal workings of deep learning models, promoting a better understanding and trust in their predictive capabilities in cybersecurity contexts. Such in-depth analysis and implementation havent been done to the best of our knowledge.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.10649v1" target="_blank">Geospatial Diffusion for Land Cover Imperviousness Change Forecasting</a></h3>
                    <p><strong>Authors:</strong> Debvrat Varshney, Vibhas Vats, Bhartendu Pandey, Christa Brelsford, Philipe Dias</p>
                    <p><strong>Published:</strong> 8/14/2025</p>
                    <p><strong>Categories:</strong> cs.LG, cs.CV</p>
                    <p><strong>Summary:</strong> Land cover, both present and future, has a significant effect on several important Earth system processes. For example, impervious surfaces heat up and speed up surface water runoff and reduce groundwater infiltration, with concomitant effects on regional hydrology and flood risk. While regional Earth System models have increasing skill at forecasting hydrologic and atmospheric processes at high resolution in future climate scenarios, our ability to forecast land-use and land-cover change (LULC), a critical input to risk and consequences assessment for these scenarios, has lagged behind. In this paper, we propose a new paradigm exploiting Generative AI (GenAI) for land cover change forecasting by framing LULC forecasting as a data synthesis problem conditioned on historical and auxiliary data-sources. We discuss desirable properties of generative models that fundament our research premise, and demonstrate the feasibility of our methodology through experiments on imperviousness forecasting using historical data covering the entire conterminous United States. Specifically, we train a diffusion model for decadal forecasting of imperviousness and compare its performance to a baseline that assumes no change at all. Evaluation across 12 metropolitan areas for a year held-out during training indicate that for average resolutions $\geq 0.7\times0.7km^2$ our model yields MAE lower than such a baseline. This finding corroborates that such a generative model can capture spatiotemporal patterns from historical data that are significant for projecting future change. Finally, we discuss future research to incorporate auxiliary information on physical properties about the Earth, as well as supporting simulation of different scenarios by means of driver variables.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.10648v1" target="_blank">Isogeometric multi-patch shell analysis using the Geometry + Simulation Modules</a></h3>
                    <p><strong>Authors:</strong> Hugo M. Verhelst, Angelos Mantzaflaris, Matthias MÃ¶ller</p>
                    <p><strong>Published:</strong> 8/14/2025</p>
                    <p><strong>Categories:</strong> math.NA, cs.NA</p>
                    <p><strong>Summary:</strong> Isogeometric Analysis (IGA) bridges Computer-Aided Design (CAD) and Finite Element Analysis (FEA) by employing splines as a common basis for geometry and analysis. One of the advantages of IGA is in the realm of thin shell analysis: due to the arbitrary continuity of the spline basis, Kirchhoff-Love shells can be modeled without the need to introduce unknowns for the mid-plane rotations, leading to a reduction in the number of unknowns. In this paper, we provide the background of an implementation of Isogeometric Kirchhoff--Love shells within the Geometry + Simulation Modules (G+Smo). This paper accompanies multiple previous publications and elaborates on the design of the software used in these papers, rather than the novelty of the methods presented therein. The presented implementation provides patch coupling via penalty methods and unstructured splines, goal-oriented error estimators, several algorithms for structural analysis and advanced algorithms for the modeling of wrinkling in hyperelastic membranes. These methods are all contained in three new modules in G+Smo: a module for Kirchhoff-Love shells, a module for structural analysis, and a module for unstructured spline constructions. As motivated in this paper, the modules are implemented to be compatible with future developments. For example, by providing base implementations of material laws, by using black-box functions for the structural analysis module, or by providing a standardized approach for the implementation of unstructured spline constructions. Overall, this paper demonstrates that the new modules contribute to a versatile ecosystem for the modeling of multi-patch shell problems through fast off-the-shelf solvers with a simple interface, designed to be extended in future research.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.10645v1" target="_blank">SemPT: Semantic Prompt Tuning for Vision-Language Models</a></h3>
                    <p><strong>Authors:</strong> Xiao Shi, Yangjun Ou, Zhenzhong Chen</p>
                    <p><strong>Published:</strong> 8/14/2025</p>
                    <p><strong>Categories:</strong> cs.CV</p>
                    <p><strong>Summary:</strong> Visual transfer learning for unseen categories presents an active research topic yet a challenging task, due to the inherent conflict between preserving category-specific representations and acquiring transferable knowledge. Vision-Language Models (VLMs) pre-trained on large amounts of image-text pairs offer a promising solution. However, existing prompt tuning methods rely on sparse category labels or disparate LLM-generated descriptions, which fragment knowledge representation and hinder transferability. To address this limitation, we introduce Semantic Prompt Tuning (SemPT), a novel framework that tackles the generalization challenge by leveraging shared attribute-level knowledge across categories. Specifically, SemPT adopts a two-step prompting strategy to guide LLM in extracting shared visual attributes and generating attribute-level descriptions, capturing transferable semantic cues beyond labels while ensuring coherent structure. Then, visually guided weighting is applied to the embeddings of attribute-level descriptions to reduce noise from irrelevant attributes and enhance the text embeddings. Additionally, image embeddings are jointly aligned with both label and attribute-enhanced text embeddings, balancing discrimination for seen categories and transferability to unseen ones. Considering the availability of category exposure, our inference dynamically selects between standard label embeddings for seen categories and attribute-enhanced embeddings for unseen ones to ensure effective adaptation. Extensive experiments on 15 benchmark datasets demonstrate that SemPT achieves state-of-the-art performance across various settings, including base-to-novel generalization, cross-dataset transfer, cross-domain transfer, and few-shot learning.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.10642v1" target="_blank">A Guide to Bayesian Optimization in Bioprocess Engineering</a></h3>
                    <p><strong>Authors:</strong> Maximilian Siska, Emma Pajak, Katrin Rosenthal, Antonio del Rio Chanona, Eric von Lieres, Laura Marie Helleckes</p>
                    <p><strong>Published:</strong> 8/14/2025</p>
                    <p><strong>Categories:</strong> q-bio.OT, stat.ML</p>
                    <p><strong>Summary:</strong> Bayesian optimization has become widely popular across various experimental sciences due to its favorable attributes: it can handle noisy data, perform well with relatively small datasets, and provide adaptive suggestions for sequential experimentation. While still in its infancy, Bayesian optimization has recently gained traction in bioprocess engineering. However, experimentation with biological systems is highly complex and the resulting experimental uncertainty requires specific extensions to classical Bayesian optimization. Moreover, current literature often targets readers with a strong statistical background, limiting its accessibility for practitioners. In light of these developments, this review has two aims: first, to provide an intuitive and practical introduction to Bayesian optimization; and second, to outline promising application areas and open algorithmic challenges, thereby highlighting opportunities for future research in machine learning.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.10634v1" target="_blank">Synthesis of Deep Neural Networks with Safe Robust Adaptive Control for Reliable Operation of Wheeled Mobile Robots</a></h3>
                    <p><strong>Authors:</strong> Mehdi Heydari Shahna, Jouni Mattila</p>
                    <p><strong>Published:</strong> 8/14/2025</p>
                    <p><strong>Categories:</strong> cs.RO, cs.SY, eess.SY</p>
                    <p><strong>Summary:</strong> Deep neural networks (DNNs) can enable precise control while maintaining low computational costs by circumventing the need for dynamic modeling. However, the deployment of such black-box approaches remains challenging for heavy-duty wheeled mobile robots (WMRs), which are subject to strict international standards and prone to faults and disturbances. We designed a hierarchical control policy for heavy-duty WMRs, monitored by two safety layers with differing levels of authority. To this end, a DNN policy was trained and deployed as the primary control strategy, providing high-precision performance under nominal operating conditions. When external disturbances arise and reach a level of intensity such that the system performance falls below a predefined threshold, a low-level safety layer intervenes by deactivating the primary control policy and activating a model-free robust adaptive control (RAC) policy. This transition enables the system to continue operating while ensuring stability by effectively managing the inherent trade-off between system robustness and responsiveness. Regardless of the control policy in use, a high-level safety layer continuously monitors system performance during operation. It initiates a shutdown only when disturbances become sufficiently severe such that compensation is no longer viable and continued operation would jeopardize the system or its environment. The proposed synthesis of DNN and RAC policy guarantees uniform exponential stability of the entire WMR system while adhering to safety standards to some extent. The effectiveness of the proposed approach was further validated through real-time experiments using a 6,000 kg WMR.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.10627v1" target="_blank">Probing ultrafast heating and ionization dynamics in solid density plasmas with time-resolved resonant X-ray absorption and emission</a></h3>
                    <p><strong>Authors:</strong> Lingen Huang, Mikhail Mishchenko, Michal Å mÃ­d, Oliver Humphries, Thomas R. Preston, Xiayun Pan, Long Yang, Johannes Hagemann, Thea Engler, Yangzhe Cui, Thomas Kluge, Carsten Baehtz, Erik Brambrink, Alejandro Laso Garcia, Sebastian GÃ¶de, Christian Gutt, Mohamed Hassan, Hauke HÃ¶ppner, Michaela Kozlova, Josefine Metzkes-Ng, Masruri Masruri, Motoaki Nakatsutsumi, Masato Ota, Ã–zgÃ¼l Ã–ztÃ¼rk, Alexander Pelka, Irene Prencipe, Lisa Randolph, Martin Rehwald, Hans-Peter Schlenvoigt, Ulrich Schramm, Jan-Patrick Schwinkendorf, Monika Toncian, Toma Toncian, Jan Vorberger, Karl Zeil, Ulf Zastrau, Thomas E. Cowan</p>
                    <p><strong>Published:</strong> 8/14/2025</p>
                    <p><strong>Categories:</strong> physics.plasm-ph</p>
                    <p><strong>Summary:</strong> Heating and ionization are among the most fundamental processes in ultra-short, relativistic laser-solid interactions. However, capturing their spatiotemporal evolution experimentally is challenging due to the inherently transient and non-local thermodynamic equilibrium (NLTE) nature. Here, time-resolved resonant X-ray emission spectroscopy, in conjunction with simultaneous X-ray absorption imaging, is employed to investigate such complex dynamics in a thin copper wire driven by an optical high-intensity laser pulse, with sub-picosecond temporal resolution. The diagnostic leverages the high brightness and narrow spectral bandwidth of an X-ray free-electron laser, to selectively excite resonant transitions of highly charged ions within the hot dense plasma generated by the optical laser. The measurements reveal a distinct rise-and-fall temporal evolution of the resonant X-ray emission yield-and consequently the selected ion population-over a 10 ps timescale, accompanied by an inversely correlated x-ray transmission. In addition, off-resonance emissions with comparable yields on both sides of the XFEL photon energy are clearly observed, indicating balanced ionization and recombination rates. Furthermore, experimental results are compared with comprehensive simulations using atomic collisional-radiative models, PIC, and MHD codes to elucidate the underlying physics. The comparison reveals that typical models overestimate the plasma heating under the extreme conditions achieved in our experiment, highlighting the requirement for improved modeling of NLTE collisional processes for predictive capabilities. These results are of broad interest to the high-energy-density science and inertial fusion energy research, both as an experimental platform for accessing theoretically challenging conditions and as a benchmark for improving models of high-power laser-plasma interactions.</p>
                
            
                
                    <h3><a href="http://dx.doi.org/10.1145/3744335.3758480" target="_blank">DEV: A Driver-Environment-Vehicle Closed-Loop Framework for Risk-Aware Adaptive Automation of Driving</a></h3>
                    <p><strong>Authors:</strong> AnaÃ¯s Halin, Christel Devue, Marc Van Droogenbroeck</p>
                    <p><strong>Published:</strong> 8/14/2025</p>
                    <p><strong>Categories:</strong> cs.HC</p>
                    <p><strong>Summary:</strong> The increasing integration of automation in vehicles aims to enhance both safety and comfort, but it also introduces new risks, including driver disengagement, reduced situation awareness, and mode confusion. In this work, we propose the DEV framework, a closed-loop framework for risk-aware adaptive driving automation that captures the dynamic interplay between the driver, the environment, and the vehicle. The framework promotes to continuously adjusting the operational level of automation based on a risk management strategy. The real-time risk assessment supports smoother transitions and effective cooperation between the driver and the automation system. Furthermore, we introduce a nomenclature of indexes corresponding to each core component, namely driver involvement, environment complexity, and vehicle engagement, and discuss how their interaction influences driving risk. The DEV framework offers a comprehensive perspective to align multidisciplinary research efforts and guide the development of dynamic, risk-aware driving automation systems.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.10606v1" target="_blank">Bistochastically private release of longitudinal data</a></h3>
                    <p><strong>Authors:</strong> Nicolas Ruiz</p>
                    <p><strong>Published:</strong> 8/14/2025</p>
                    <p><strong>Categories:</strong> stat.ME, cs.CR</p>
                    <p><strong>Summary:</strong> Although the bulk of the research in privacy and statistical disclosure control is designed for cross-sectional data, i.e. data where individuals are observed at one single point in time, longitudinal data, i.e. individuals observed over multiple periods, are increasingly collected. Such data enhance undoubtedly the possibility of statistical analysis compared to cross-sectional data, but also come with one additional layer of information, individual trajectories, that must remain practically useful in a privacy-preserving way. Few extensions, essentially k-anonymity based, of popular privacy tools have been proposed to deal with the challenges posed by longitudinal data, and these proposals are often complex. By considering randomized response, and specifically its recent bistochastic extension, in the context of longitudinal data, this paper proposes a simple approach for their anonymization. After having characterized new results on bistochastic matrices, we show that a simple relationship exists between the protection of each data set released at each period, and the protection of individuals trajectories over time. In turn, this relationship can be tuned according to desired protection and information requirements. We illustrate the application of the proposed approach by an empirical example.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.10605v1" target="_blank">DIVA-VQA: Detecting Inter-frame Variations in UGC Video Quality</a></h3>
                    <p><strong>Authors:</strong> Xinyi Wang, Angeliki Katsenou, David Bull</p>
                    <p><strong>Published:</strong> 8/14/2025</p>
                    <p><strong>Categories:</strong> eess.IV, cs.CV, cs.MM</p>
                    <p><strong>Summary:</strong> The rapid growth of user-generated (video) content (UGC) has driven increased demand for research on no-reference (NR) perceptual video quality assessment (VQA). NR-VQA is a key component for large-scale video quality monitoring in social media and streaming applications where a pristine reference is not available. This paper proposes a novel NR-VQA model based on spatio-temporal fragmentation driven by inter-frame variations. By leveraging these inter-frame differences, the model progressively analyses quality-sensitive regions at multiple levels: frames, patches, and fragmented frames. It integrates frames, fragmented residuals, and fragmented frames aligned with residuals to effectively capture global and local information. The model extracts both 2D and 3D features in order to characterize these spatio-temporal variations. Experiments conducted on five UGC datasets and against state-of-the-art models ranked our proposed method among the top 2 in terms of average rank correlation (DIVA-VQA-L: 0.898 and DIVA-VQA-B: 0.886). The improved performance is offered at a low runtime complexity, with DIVA-VQA-B ranked top and DIVA-VQA-L third on average compared to the fastest existing NR-VQA method. Code and models are publicly available at: https://github.com/xinyiW915/DIVA-VQA.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.10603v1" target="_blank">Why Report Failed Interactions With Robots?! Towards Vignette-based Interaction Quality</a></h3>
                    <p><strong>Authors:</strong> Agnes Axelsson, Merle Reimann, Ronald Cumbal, Hannah Pelikan, Divesh Lala</p>
                    <p><strong>Published:</strong> 8/14/2025</p>
                    <p><strong>Categories:</strong> cs.RO, cs.HC</p>
                    <p><strong>Summary:</strong> Although the quality of human-robot interactions has improved with the advent of LLMs, there are still various factors that cause systems to be sub-optimal when compared to human-human interactions. The nature and criticality of failures are often dependent on the context of the interaction and so cannot be generalized across the wide range of scenarios and experiments which have been implemented in HRI research. In this work we propose the use of a technique overlooked in the field of HRI, ethnographic vignettes, to clearly highlight these failures, particularly those that are rarely documented. We describe the methodology behind the process of writing vignettes and create our own based on our personal experiences with failures in HRI systems. We emphasize the strength of vignettes as the ability to communicate failures from a multi-disciplinary perspective, promote transparency about the capabilities of robots, and document unexpected behaviours which would otherwise be omitted from research reports. We encourage the use of vignettes to augment existing interaction evaluation methods.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.10600v1" target="_blank">Towards Powerful and Practical Patch Attacks for 2D Object Detection in Autonomous Driving</a></h3>
                    <p><strong>Authors:</strong> Yuxin Cao, Yedi Zhang, Wentao He, Yifan Liao, Yan Xiao, Chang Li, Zhiyong Huang, Jin Song Dong</p>
                    <p><strong>Published:</strong> 8/14/2025</p>
                    <p><strong>Categories:</strong> cs.CV</p>
                    <p><strong>Summary:</strong> Learning-based autonomous driving systems remain critically vulnerable to adversarial patches, posing serious safety and security risks in their real-world deployment. Black-box attacks, notable for their high attack success rate without model knowledge, are especially concerning, with their transferability extensively studied to reduce computational costs compared to query-based attacks. Previous transferability-based black-box attacks typically adopt mean Average Precision (mAP) as the evaluation metric and design training loss accordingly. However, due to the presence of multiple detected bounding boxes and the relatively lenient Intersection over Union (IoU) thresholds, the attack effectiveness of these approaches is often overestimated, resulting in reduced success rates in practical attacking scenarios. Furthermore, patches trained on low-resolution data often fail to maintain effectiveness on high-resolution images, limiting their transferability to autonomous driving datasets. To fill this gap, we propose P$^3$A, a Powerful and Practical Patch Attack framework for 2D object detection in autonomous driving, specifically optimized for high-resolution datasets. First, we introduce a novel metric, Practical Attack Success Rate (PASR), to more accurately quantify attack effectiveness with greater relevance for pedestrian safety. Second, we present a tailored Localization-Confidence Suppression Loss (LCSL) to improve attack transferability under PASR. Finally, to maintain the transferability for high-resolution datasets, we further incorporate the Probabilistic Scale-Preserving Padding (PSPP) into the patch attack pipeline as a data preprocessing step. Extensive experiments show that P$^3$A outperforms state-of-the-art attacks on unseen models and unseen high-resolution datasets, both under the proposed practical IoU-based evaluation metric and the previous mAP-based metrics.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.10598v1" target="_blank">Oops!... They Stole it Again: Attacks on Split Learning</a></h3>
                    <p><strong>Authors:</strong> Tanveer Khan, Antonis Michalas</p>
                    <p><strong>Published:</strong> 8/14/2025</p>
                    <p><strong>Categories:</strong> cs.LG</p>
                    <p><strong>Summary:</strong> Split Learning (SL) is a collaborative learning approach that improves privacy by keeping data on the client-side while sharing only the intermediate output with a server. However, the distributed nature of SL introduces new security challenges, necessitating a comprehensive exploration of potential attacks. This paper systematically reviews various attacks on SL, classifying them based on factors such as the attackers role, the type of privacy risks, when data leaks occur, and where vulnerabilities exist. We also analyze existing defense methods, including cryptographic methods, data modification approaches, distributed techniques, and hybrid solutions. Our findings reveal security gaps, highlighting the effectiveness and limitations of existing defenses. By identifying open challenges and future directions, this work provides valuable information to improve SL privacy issues and guide further research.</p>
                
            
                
                    <h3><a href="http://dx.doi.org/10.1007/978-3-031-73021-4_17" target="_blank">On Spectral Properties of Gradient-based Explanation Methods</a></h3>
                    <p><strong>Authors:</strong> Amir Mehrpanah, Erik Englesson, Hossein Azizpour</p>
                    <p><strong>Published:</strong> 8/14/2025</p>
                    <p><strong>Categories:</strong> cs.LG, cs.AI, cs.CV</p>
                    <p><strong>Summary:</strong> Understanding the behavior of deep networks is crucial to increase our confidence in their results. Despite an extensive body of work for explaining their predictions, researchers have faced reliability issues, which can be attributed to insufficient formalism. In our research, we adopt novel probabilistic and spectral perspectives to formally analyze explanation methods. Our study reveals a pervasive spectral bias stemming from the use of gradient, and sheds light on some common design choices that have been discovered experimentally, in particular, the use of squared gradient and input perturbation. We further characterize how the choice of perturbation hyperparameters in explanation methods, such as SmoothGrad, can lead to inconsistent explanations and introduce two remedies based on our proposed formalism: (i) a mechanism to determine a standard perturbation scale, and (ii) an aggregation method which we call SpectralLens. Finally, we substantiate our theoretical results through quantitative evaluations.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.10586v1" target="_blank">Differential Physiological Responses to Proxemic and Facial Threats in Virtual Avatar Interactions</a></h3>
                    <p><strong>Authors:</strong> Birgit Nierula, Mustafa Tevfik Lafci, Anna Melnik, Mert AkgÃ¼l, Farelle Toumaleu Siewe, Sebastian Bosse</p>
                    <p><strong>Published:</strong> 8/14/2025</p>
                    <p><strong>Categories:</strong> cs.HC, eess.SP, q-bio.NC</p>
                    <p><strong>Summary:</strong> Proxemics, the study of spatial behavior, is fundamental to social interaction and increasingly relevant for virtual reality (VR) applications. While previous research has established that users respond to personal space violations in VR similarly as in real-world settings, phase-specific physiological responses and the modulating effects of facial expressions remain understudied. We investigated physiological and subjective responses to personal space violations by virtual avatars, to understand how threatening facial expressions and interaction phases (approach vs. standing) influence these responses. Sixteen participants experienced a 2x2 factorial design manipulating Personal Space (intrusion vs. respect) and Facial Expression (neutral vs. angry) while we recorded skin conductance response (SCR), heart rate variability (HRV), and discomfort ratings. Personal space boundaries were individually calibrated using a stop-distance procedure. Results show that SCR responses are significantly higher during the standing phase compared to the approach phase when personal space was violated, indicating that prolonged proximity within personal space boundaries is more physiologically arousing than the approach itself. Angry facial expressions significantly reduced HRV, reflecting decreased parasympathetic activity, and increased discomfort ratings, but did not amplify SCR responses. These findings demonstrate that different physiological modalities capture distinct aspects of proxemic responses: SCR primarily reflects spatial boundary violations, while HRV responds to facial threat cues. Our results provide insights for developing comprehensive multi-modal assessments of social behavior in virtual environments and inform the design of more realistic avatar interactions.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.10577v1" target="_blank">On the implications of proportional hazards assumptions for competing risks modelling</a></h3>
                    <p><strong>Authors:</strong> Simon M. S. Lo, Ralf A. Wilke, Takeshi Emura</p>
                    <p><strong>Published:</strong> 8/14/2025</p>
                    <p><strong>Categories:</strong> stat.ME, econ.EM, 62N01, 62H05</p>
                    <p><strong>Summary:</strong> The assumption of hazard rates being proportional in covariates is widely made in empirical research and extensive research has been done to develop tests of its validity. This paper does not contribute on this end. Instead, it gives new insights on the implications of proportional hazards (PH) modelling in competing risks models. It is shown that the use of a PH model for the cause-specific hazards or subdistribution hazards can strongly restrict the class of copulas and marginal hazards for being compatible with a competing risks model. The empirical researcher should be aware that working with these models can be so restrictive that only degenerate or independent risks models are compatible. Numerical results confirm that estimates of cause-specific hazards models are not informative about patterns in the data generating process.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.10899v1" target="_blank">A Dataset for Distilling Knowledge Priors from Literature for Therapeutic Design</a></h3>
                    <p><strong>Authors:</strong> Haydn Thomas Jones, Natalie Maus, Josh Magnus Ludan, Maggie Ziyu Huan, Jiaming Liang, Marcelo Der Torossian Torres, Jiatao Liang, Zachary Ives, Yoseph Barash, Cesar de la Fuente-Nunez, Jacob R. Gardner, Mark Yatskar</p>
                    <p><strong>Published:</strong> 8/14/2025</p>
                    <p><strong>Categories:</strong> cs.LG</p>
                    <p><strong>Summary:</strong> AI-driven discovery can greatly reduce design time and enhance new therapeutics effectiveness. Models using simulators explore broad design spaces but risk violating implicit constraints due to a lack of experimental priors. For example, in a new analysis we performed on a diverse set of models on the GuacaMol benchmark using supervised classifiers, over 60\% of molecules proposed had high probability of being mutagenic. In this work, we introduce \ourdataset, a dataset of priors for design problems extracted from literature describing compounds used in lab settings. It is constructed with LLM pipelines for discovering therapeutic entities in relevant paragraphs and summarizing information in concise fair-use facts. \ourdataset~ consists of 32.3 million pairs of natural language facts, and appropriate entity representations (i.e. SMILES or refseq IDs). To demonstrate the potential of the data, we train LLM, CLIP, and LLava architectures to reason jointly about text and design targets and evaluate on tasks from the Therapeutic Data Commons (TDC). \ourdataset~is highly effective for creating models with strong priors: in supervised prediction problems that use our data as pretraining, our best models with 15M learnable parameters outperform larger 2B TxGemma on both regression and classification TDC tasks, and perform comparably to 9B models on average. Models built with \ourdataset~can be used as constraints while optimizing for novel molecules in GuacaMol, resulting in proposals that are safer and nearly as effective. We release our dataset at \href{https://huggingface.co/datasets/medexanon/Medex}{huggingface.co/datasets/medexanon/Medex}, and will provide expanded versions as available literature grows.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.10898v1" target="_blank">Puppeteer: Rig and Animate Your 3D Models</a></h3>
                    <p><strong>Authors:</strong> Chaoyue Song, Xiu Li, Fan Yang, Zhongcong Xu, Jiacheng Wei, Fayao Liu, Jiashi Feng, Guosheng Lin, Jianfeng Zhang</p>
                    <p><strong>Published:</strong> 8/14/2025</p>
                    <p><strong>Categories:</strong> cs.CV, cs.GR</p>
                    <p><strong>Summary:</strong> Modern interactive applications increasingly demand dynamic 3D content, yet the transformation of static 3D models into animated assets constitutes a significant bottleneck in content creation pipelines. While recent advances in generative AI have revolutionized static 3D model creation, rigging and animation continue to depend heavily on expert intervention. We present Puppeteer, a comprehensive framework that addresses both automatic rigging and animation for diverse 3D objects. Our system first predicts plausible skeletal structures via an auto-regressive transformer that introduces a joint-based tokenization strategy for compact representation and a hierarchical ordering methodology with stochastic perturbation that enhances bidirectional learning capabilities. It then infers skinning weights via an attention-based architecture incorporating topology-aware joint attention that explicitly encodes inter-joint relationships based on skeletal graph distances. Finally, we complement these rigging advances with a differentiable optimization-based animation pipeline that generates stable, high-fidelity animations while being computationally more efficient than existing approaches. Extensive evaluations across multiple benchmarks demonstrate that our method significantly outperforms state-of-the-art techniques in both skeletal prediction accuracy and skinning quality. The system robustly processes diverse 3D content, ranging from professionally designed game assets to AI-generated shapes, producing temporally coherent animations that eliminate the jittering issues common in existing methods.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.10895v1" target="_blank">Stars Born in the Wind: M82s Outflow and Halo Star Formation</a></h3>
                    <p><strong>Authors:</strong> Vaishnav V. Rao, Adam Smercina, Eric F. Bell, Benjamin Williams, Julianne J. Dalcanton, Andrew Dolphin, Adam Leroy, Antonela Monachesi, Jeremy Bailin, Roelof S. de Jong, Fabian Walter</p>
                    <p><strong>Published:</strong> 8/14/2025</p>
                    <p><strong>Categories:</strong> astro-ph.GA</p>
                    <p><strong>Summary:</strong> Starburst galaxies, like M82, launch kiloparsec-scale galactic outflows that interact with the circumgalactic medium (CGM) in complex ways. Apart from enriching the CGM with metals and energy, these outflows may trigger star formation in the halo -- either by driving shocks into the CGM or transporting cold, star-forming gas. To investigate such processes, we analyze the star formation history (SFH) of the Southern Arcs -- arc-like stellar features located ~5 kpc from M82s star-forming disk along the minor axis -- using Hubble Space Telescope Wide Field Camera 3 photometry. From resolved stellar populations, we derive SFHs over the last ~500 Myr, finding that ~85% of the stellar mass formed between ~150 and ~70 Myr ago, followed by a brief pause, with the remaining ~15% forming since ~30 Myr ago. The two stellar populations are co-spatial on scales of at least ~200 pc. The timing of the ~100 Myr burst aligns with star formation in the M82 disk and the age distribution of its star clusters, suggesting a causal link between the disk starburst and halo star formation. We explore two mechanisms that could explain these observations. In the first, shocks driven by the interaction between hot outflowing gas and cooler CGM material compress dense clouds, triggering collapse and star formation. In the second, stars form directly within massive, cool clouds associated with the outflow. As these clouds move ballistically through the halo, subsequent interactions with tidal debris may trigger additional star formation, producing the observed episodic structure.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.10891v1" target="_blank">Fuel Consumption in Platoons: A Literature Review</a></h3>
                    <p><strong>Authors:</strong> Oumaima Barhoumi, Ghazal Farhani, Taufiq Rahman, Mohamed H. Zaki, SofiÃ¨ne Tahar, Fadi Araji</p>
                    <p><strong>Published:</strong> 8/14/2025</p>
                    <p><strong>Categories:</strong> eess.SY, cs.SY</p>
                    <p><strong>Summary:</strong> Platooning has emerged as a promising strategy for improving fuel efficiency in automated vehicle systems, with significant implications for reducing emissions and operational costs. While existing literature on vehicle platooning primarily focuses on individual aspects such as aerodynamic drag reduction or specific control strategies, this work takes a more comprehensive approach by bringing together a wide range of factors and components that contribute to fuel savings in platoons. In this literature review, we examine the impact of platooning on fuel consumption, highlighting the key components of platoon systems, the factors and actors influencing fuel savings, methods for estimating fuel use, and the effect of platoon instability on efficiency. Furthermore, we study the role of reduced aerodynamic drag, vehicle coordination, and the challenges posed by instability in real-world conditions. By compiling insights from recent studies, this work provides a comprehensive overview of the latest advancements in platooning technologies and highlights both the challenges and opportunities for future research to maximize fuel savings in real-world scenarios.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.10881v1" target="_blank">ToonComposer: Streamlining Cartoon Production with Generative Post-Keyframing</a></h3>
                    <p><strong>Authors:</strong> Lingen Li, Guangzhi Wang, Zhaoyang Zhang, Yaowei Li, Xiaoyu Li, Qi Dou, Jinwei Gu, Tianfan Xue, Ying Shan</p>
                    <p><strong>Published:</strong> 8/14/2025</p>
                    <p><strong>Categories:</strong> cs.CV, cs.AI</p>
                    <p><strong>Summary:</strong> Traditional cartoon and anime production involves keyframing, inbetweening, and colorization stages, which require intensive manual effort. Despite recent advances in AI, existing methods often handle these stages separately, leading to error accumulation and artifacts. For instance, inbetweening approaches struggle with large motions, while colorization methods require dense per-frame sketches. To address this, we introduce ToonComposer, a generative model that unifies inbetweening and colorization into a single post-keyframing stage. ToonComposer employs a sparse sketch injection mechanism to provide precise control using keyframe sketches. Additionally, it uses a cartoon adaptation method with the spatial low-rank adapter to tailor a modern video foundation model to the cartoon domain while keeping its temporal prior intact. Requiring as few as a single sketch and a colored reference frame, ToonComposer excels with sparse inputs, while also supporting multiple sketches at any temporal location for more precise motion control. This dual capability reduces manual workload and improves flexibility, empowering artists in real-world scenarios. To evaluate our model, we further created PKBench, a benchmark featuring human-drawn sketches that simulate real-world use cases. Our evaluation demonstrates that ToonComposer outperforms existing methods in visual quality, motion consistency, and production efficiency, offering a superior and more flexible solution for AI-assisted cartoon production.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.10875v1" target="_blank">A Survey on Diffusion Language Models</a></h3>
                    <p><strong>Authors:</strong> Tianyi Li, Mingda Chen, Bowei Guo, Zhiqiang Shen</p>
                    <p><strong>Published:</strong> 8/14/2025</p>
                    <p><strong>Categories:</strong> cs.CL, cs.AI, cs.LG</p>
                    <p><strong>Summary:</strong> Diffusion Language Models (DLMs) are rapidly emerging as a powerful and promising alternative to the dominant autoregressive (AR) paradigm. By generating tokens in parallel through an iterative denoising process, DLMs possess inherent advantages in reducing inference latency and capturing bidirectional context, thereby enabling fine-grained control over the generation process. While achieving a several-fold speed-up, recent advancements have allowed DLMs to show performance comparable to their autoregressive counterparts, making them a compelling choice for various natural language processing tasks. In this survey, we provide a holistic overview of the current DLM landscape. We trace its evolution and relationship with other paradigms, such as autoregressive and masked language models, and cover both foundational principles and state-of-the-art models. Our work offers an up-to-date, comprehensive taxonomy and an in-depth analysis of current techniques, from pre-training strategies to advanced post-training methods. Another contribution of this survey is a thorough review of DLM inference strategies and optimizations, including improvements in decoding parallelism, caching mechanisms, and generation quality. We also highlight the latest approaches to multimodal extensions of DLMs and delineate their applications across various practical scenarios. Furthermore, our discussion addresses the limitations and challenges of DLMs, including efficiency, long-sequence handling, and infrastructure requirements, while outlining future research directions to sustain progress in this rapidly evolving field. Project GitHub is available at https://github.com/VILA-Lab/Awesome-DLMs.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.10869v1" target="_blank">Medico 2025: Visual Question Answering for Gastrointestinal Imaging</a></h3>
                    <p><strong>Authors:</strong> Sushant Gautam, Vajira Thambawita, Michael Riegler, PÃ¥l Halvorsen, Steven Hicks</p>
                    <p><strong>Published:</strong> 8/14/2025</p>
                    <p><strong>Categories:</strong> cs.CV, cs.AI, 68T45, 92C55, I.2.10; I.4.9</p>
                    <p><strong>Summary:</strong> The Medico 2025 challenge addresses Visual Question Answering (VQA) for Gastrointestinal (GI) imaging, organized as part of the MediaEval task series. The challenge focuses on developing Explainable Artificial Intelligence (XAI) models that answer clinically relevant questions based on GI endoscopy images while providing interpretable justifications aligned with medical reasoning. It introduces two subtasks: (1) answering diverse types of visual questions using the Kvasir-VQA-x1 dataset, and (2) generating multimodal explanations to support clinical decision-making. The Kvasir-VQA-x1 dataset, created from 6,500 images and 159,549 complex question-answer (QA) pairs, serves as the benchmark for the challenge. By combining quantitative performance metrics and expert-reviewed explainability assessments, this task aims to advance trustworthy Artificial Intelligence (AI) in medical image analysis. Instructions, data access, and an updated guide for participation are available in the official competition repository: https://github.com/simula/MediaEval-Medico-2025</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.10860v1" target="_blank">From Black Box to Transparency: Enhancing Automated Interpreting Assessment with Explainable AI in College Classrooms</a></h3>
                    <p><strong>Authors:</strong> Zhaokun Jiang, Ziyin Zhang</p>
                    <p><strong>Published:</strong> 8/14/2025</p>
                    <p><strong>Categories:</strong> cs.CL, cs.AI</p>
                    <p><strong>Summary:</strong> Recent advancements in machine learning have spurred growing interests in automated interpreting quality assessment. Nevertheless, existing research suffers from insufficient examination of language use quality, unsatisfactory modeling effectiveness due to data scarcity and imbalance, and a lack of efforts to explain model predictions. To address these gaps, we propose a multi-dimensional modeling framework that integrates feature engineering, data augmentation, and explainable machine learning. This approach prioritizes explainability over ``black box predictions by utilizing only construct-relevant, transparent features and conducting Shapley Value (SHAP) analysis. Our results demonstrate strong predictive performance on a novel English-Chinese consecutive interpreting dataset, identifying BLEURT and CometKiwi scores to be the strongest predictive features for fidelity, pause-related features for fluency, and Chinese-specific phraseological diversity metrics for language use. Overall, by placing particular emphasis on explainability, we present a scalable, reliable, and transparent alternative to traditional human evaluation, facilitating the provision of detailed diagnostic feedback for learners and supporting self-regulated learning advantages not afforded by automated scores in isolation.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.10858v1" target="_blank">Hierarchical Fine-grained Preference Optimization for Physically Plausible Video Generation</a></h3>
                    <p><strong>Authors:</strong> Harold Haodong Chen, Haojian Huang, Qifeng Chen, Harry Yang, Ser-Nam Lim</p>
                    <p><strong>Published:</strong> 8/14/2025</p>
                    <p><strong>Categories:</strong> cs.CV</p>
                    <p><strong>Summary:</strong> Recent advancements in video generation have enabled the creation of high-quality, visually compelling videos. However, generating videos that adhere to the laws of physics remains a critical challenge for applications requiring realism and accuracy. In this work, we propose PhysHPO, a novel framework for Hierarchical Cross-Modal Direct Preference Optimization, to tackle this challenge by enabling fine-grained preference alignment for physically plausible video generation. PhysHPO optimizes video alignment across four hierarchical granularities: a) Instance Level, aligning the overall video content with the input prompt; b) State Level, ensuring temporal consistency using boundary frames as anchors; c) Motion Level, modeling motion trajectories for realistic dynamics; and d) Semantic Level, maintaining logical consistency between narrative and visuals. Recognizing that real-world videos are the best reflections of physical phenomena, we further introduce an automated data selection pipeline to efficiently identify and utilize good data from existing large-scale text-video datasets, thereby eliminating the need for costly and time-intensive dataset construction. Extensive experiments on both physics-focused and general capability benchmarks demonstrate that PhysHPO significantly improves physical plausibility and overall video generation quality of advanced models. To the best of our knowledge, this is the first work to explore fine-grained preference alignment and data selection for video generation, paving the way for more realistic and human-preferred video generation paradigms.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.10852v1" target="_blank">EVOSCAT: Exploring Software Change Dynamics in Large-Scale Historical Datasets</a></h3>
                    <p><strong>Authors:</strong> Souhaila Serbout, Diana Carolina MuÃ±oz Hurtado, Hassan Atwi, Edoardo Riggio, Cesare Pautasso</p>
                    <p><strong>Published:</strong> 8/14/2025</p>
                    <p><strong>Categories:</strong> cs.SE</p>
                    <p><strong>Summary:</strong> Long lived software projects encompass a large number of artifacts, which undergo many revisions throughout their history. Empirical software engineering researchers studying software evolution gather and collect datasets with millions of events, representing changes introduced to specific artifacts. In this paper, we propose EvoScat, a tool that attempts addressing temporal scalability through the usage of interactive density scatterplot to provide a global overview of large historical datasets mined from open source repositories in a single visualization. EvoScat intents to provide researchers with a mean to produce scalable visualizations that can help them explore and characterize evolution datasets, as well as comparing the histories of individual artifacts, both in terms of 1) observing how rapidly different artifacts age over multiple-year-long time spans 2) how often metrics associated with each artifacts tend towards an improvement or worsening. The paper shows how the tool can be tailored to specific analysis needs (pace of change comparison, clone detection, freshness assessment) thanks to its support for flexible configuration of history scaling and alignment along the time axis, artifacts sorting and interactive color mapping, enabling the analysis of millions of events obtained by mining the histories of tens of thousands of software artifacts. We include in this paper a gallery showcasing datasets gathering specific artifacts (OpenAPI descriptions, GitHub workflow definitions) across multiple repositories, as well as diving into the history of specific popular open source projects.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.10849v1" target="_blank">Integrating Terrestrial and Non-Terrestrial Networks for Sustainable 6G Operations: A Latency-Aware Multi-Tier Cell-Switching Approach</a></h3>
                    <p><strong>Authors:</strong> Metin Ozturk, Maryam Salamatmoghadasi, Halim Yanikomeroglu</p>
                    <p><strong>Published:</strong> 8/14/2025</p>
                    <p><strong>Categories:</strong> eess.SY, cs.SY</p>
                    <p><strong>Summary:</strong> Sustainability is paramount in modern cellular networks, which face significant energy consumption challenges from rising mobile traffic and advancements in wireless technology. Cell-switching, well-established in literature as an effective solution, encounters limitations such as inadequate capacity and limited coverage when implemented through terrestrial networks (TN). This study enhances cell-switching by integrating non-terrestrial networks (NTN), including satellites (used for cell-switching for the first time), high altitude platform stations (HAPS), and uncrewed aerial vehicles (UAVs) into TN. This integration significantly boosts energy savings by expanding capacity, enhancing coverage, and increasing operational flexibility. We introduce a multi-tier cell-switching approach that dynamically offloads users across network layers to manage energy effectively and minimize delays, accommodating diverse user demands with a context aware strategy. Additionally, we explore the role of artificial intelligence (AI), particularly generative AI, in optimizing network efficiency through data compression, handover optimization between different network layers, and enhancing device compatibility, further improving the adaptability and energy efficiency of cell-switching operations. A case study confirms substantial improvements in network power consumption and user satisfaction, demonstrating the potential of our approach for future networks.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.10848v1" target="_blank">Psyche-R1: Towards Reliable Psychological LLMs through Unified Empathy, Expertise, and Reasoning</a></h3>
                    <p><strong>Authors:</strong> Chongyuan Dai, Jinpeng Hu, Hongchang Shi, Zhuo Li, Xun Yang, Meng Wang</p>
                    <p><strong>Published:</strong> 8/14/2025</p>
                    <p><strong>Categories:</strong> cs.CL</p>
                    <p><strong>Summary:</strong> Amidst a shortage of qualified mental health professionals, the integration of large language models (LLMs) into psychological applications offers a promising way to alleviate the growing burden of mental health disorders. Recent reasoning-augmented LLMs have achieved remarkable performance in mathematics and programming, while research in the psychological domain has predominantly emphasized emotional support and empathetic dialogue, with limited attention to reasoning mechanisms that are beneficial to generating reliable responses. Therefore, in this paper, we propose Psyche-R1, the first Chinese psychological LLM that jointly integrates empathy, psychological expertise, and reasoning, built upon a novel data curation pipeline. Specifically, we design a comprehensive data synthesis pipeline that produces over 75k high-quality psychological questions paired with detailed rationales, generated through chain-of-thought (CoT) reasoning and iterative prompt-rationale optimization, along with 73k empathetic dialogues. Subsequently, we employ a hybrid training strategy wherein challenging samples are identified through a multi-LLM cross-selection strategy for group relative policy optimization (GRPO) to improve reasoning ability, while the remaining data is used for supervised fine-tuning (SFT) to enhance empathetic response generation and psychological domain knowledge. Extensive experiment results demonstrate the effectiveness of the Psyche-R1 across several psychological benchmarks, where our 7B Psyche-R1 achieves comparable results to 671B DeepSeek-R1.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.10836v1" target="_blank">SoK: Data Minimization in Machine Learning</a></h3>
                    <p><strong>Authors:</strong> Robin Staab, Nikola JovanoviÄ‡, Kimberly Mai, Prakhar Ganesh, Martin Vechev, Ferdinando Fioretto, Matthew Jagielski</p>
                    <p><strong>Published:</strong> 8/14/2025</p>
                    <p><strong>Categories:</strong> cs.LG, cs.CR</p>
                    <p><strong>Summary:</strong> Data minimization (DM) describes the principle of collecting only the data strictly necessary for a given task. It is a foundational principle across major data protection regulations like GDPR and CPRA. Violations of this principle have substantial real-world consequences, with regulatory actions resulting in fines reaching hundreds of millions of dollars. Notably, the relevance of data minimization is particularly pronounced in machine learning (ML) applications, which typically rely on large datasets, resulting in an emerging research area known as Data Minimization in Machine Learning (DMML). At the same time, existing work on other ML privacy and security topics often addresses concerns relevant to DMML without explicitly acknowledging the connection. This disconnect leads to confusion among practitioners, complicating their efforts to implement DM principles and interpret the terminology, metrics, and evaluation criteria used across different research communities. To address this gap, our work introduces a comprehensive framework for DMML, including a unified data pipeline, adversaries, and points of minimization. This framework allows us to systematically review the literature on data minimization and \emph{DM-adjacent} methodologies, for the first time presenting a structured overview designed to help practitioners and researchers effectively apply DM principles. Our work facilitates a unified DM-centric understanding and broader adoption of data minimization strategies in AI/ML.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.10833v1" target="_blank">UI-Venus Technical Report: Building High-performance UI Agents with RFT</a></h3>
                    <p><strong>Authors:</strong> Zhangxuan Gu, Zhengwen Zeng, Zhenyu Xu, Xingran Zhou, Shuheng Shen, Yunfei Liu, Beitong Zhou, Changhua Meng, Tianyu Xia, Weizhi Chen, Yue Wen, Jingya Dou, Fei Tang, Jinzhen Lin, Yulin Liu, Zhenlin Guo, Yichen Gong, Heng Jia, Changlong Gao, Yuan Guo, Yong Deng, Zhenyu Guo, Liang Chen, Weiqiang Wang</p>
                    <p><strong>Published:</strong> 8/14/2025</p>
                    <p><strong>Categories:</strong> cs.CV</p>
                    <p><strong>Summary:</strong> We present UI-Venus, a native UI agent that takes only screenshots as input based on a multimodal large language model. UI-Venus achieves SOTA performance on both UI grounding and navigation tasks using only several hundred thousand high-quality training samples through reinforcement finetune (RFT) based on Qwen2.5-VL. Specifically, the 7B and 72B variants of UI-Venus obtain 94.1% / 50.8% and 95.3% / 61.9% on the standard grounding benchmarks, i.e., Screenspot-V2 / Pro, surpassing the previous SOTA baselines including open-source GTA1 and closed-source UI-TARS-1.5.To show UI-Venuss summary and planing ability, we also evaluate it on the AndroidWorld, an online UI navigation arena, on which our 7B and 72B variants achieve 49.1% and 65.9% success rate, also beating existing models.To achieve this, we introduce carefully designed reward functions for both UI grounding and navigation tasks and corresponding efficient data cleaning strategies.To further boost navigation performance, we propose Self-Evolving Trajectory History Alignment \ Sparse Action Enhancement that refine historical reasoning traces and balances the distribution of sparse but critical actions, leading to more coherent planning and better generalization in complex UI tasks. Our contributions include the publish of SOTA open-source UI agents, comprehensive data cleaning protocols and a novel self-evolving framework for improving navigation performance, which encourage further research and development in the community. Code is available at https://github.com/antgroup/UI-Venus.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.10830v1" target="_blank">Advances in Speech Separation: Techniques, Challenges, and Future Trends</a></h3>
                    <p><strong>Authors:</strong> Kai Li, Guo Chen, Wendi Sang, Yi Luo, Zhuo Chen, Shuai Wang, Shulin He, Zhong-Qiu Wang, Andong Li, Zhiyong Wu, Xiaolin Hu</p>
                    <p><strong>Published:</strong> 8/14/2025</p>
                    <p><strong>Categories:</strong> cs.SD, eess.AS</p>
                    <p><strong>Summary:</strong> The field of speech separation, addressing the cocktail party problem, has seen revolutionary advances with DNNs. Speech separation enhances clarity in complex acoustic environments and serves as crucial pre-processing for speech recognition and speaker recognition. However, current literature focuses narrowly on specific architectures or isolated approaches, creating fragmented understanding. This survey addresses this gap by providing systematic examination of DNN-based speech separation techniques. Our work differentiates itself through: (I) Comprehensive perspective: We systematically investigate learning paradigms, separation scenarios with known/unknown speakers, comparative analysis of supervised/self-supervised/unsupervised frameworks, and architectural components from encoders to estimation strategies. (II) Timeliness: Coverage of cutting-edge developments ensures access to current innovations and benchmarks. (III) Unique insights: Beyond summarization, we evaluate technological trajectories, identify emerging patterns, and highlight promising directions including domain-robust frameworks, efficient architectures, multimodal integration, and novel self-supervised paradigms. (IV) Fair evaluation: We provide quantitative evaluations on standard datasets, revealing true capabilities and limitations of different methods. This comprehensive survey serves as an accessible reference for experienced researchers and newcomers navigating speech separations complex landscape.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.10826v1" target="_blank">The Future is Fluid: Revolutionizing DOA Estimation with Sparse Fluid Antennas</a></h3>
                    <p><strong>Authors:</strong> He Xu, Tuo Wu, Ye Tian, Ming Jin, Wei Liu, Qinghua Guo, Maged Elkashlan, Matthew C. Valenti, Chan-Byoung Chae, Kin-Fai Tong, Kai-Kit Wong</p>
                    <p><strong>Published:</strong> 8/14/2025</p>
                    <p><strong>Categories:</strong> eess.SP</p>
                    <p><strong>Summary:</strong> This paper investigates a design framework for sparse fluid antenna systems (FAS) enabling high-performance direction-of-arrival (DOA) estimation, particularly in challenging millimeter-wave (mmWave) environments. By ingeniously harnessing the mobility of fluid antenna (FA) elements, the proposed architectures achieve an extended range of spatial degrees of freedom (DoF) compared to conventional fixed-position antenna (FPA) arrays. This innovation not only facilitates the seamless application of super-resolution DOA estimators but also enables robust DOA estimation, accurately localizing more sources than the number of physical antenna elements. We introduce two bespoke FA array structures and mobility strategies tailored to scenarios with aligned and misaligned received signals, respectively, demonstrating a hardware-driven approach to overcoming complexities typically addressed by intricate algorithms. A key contribution is a light-of-sight (LoS)-centric, closed-form DOA estimator, which first employs an eigenvalue-ratio test for precise LoS path number detection, followed by a polynomial root-finding procedure. This method distinctly showcases the unique advantages of FAS by simplifying the estimation process while enhancing accuracy. Numerical results compellingly verify that the proposed FA array designs and estimation techniques yield an extended DoF range, deliver superior DOA accuracy, and maintain robustness across diverse signal conditions.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.10820v1" target="_blank">Fluid Antenna Enabled Direction-of-Arrival Estimation Under Time-Constrained Mobility</a></h3>
                    <p><strong>Authors:</strong> He Xu, Tuo Wu, Ye Tian, Kangda Zhi, Wei Liu, Baiyang Liu, Hing Cheung So, Naofal Al-Dhahir, Kin-Fai Tong, Chan-Byoung Chae, Kai-Kit Wong</p>
                    <p><strong>Published:</strong> 8/14/2025</p>
                    <p><strong>Categories:</strong> eess.SP</p>
                    <p><strong>Summary:</strong> Fluid antenna (FA) technology has emerged as a promising approach in wireless communications due to its capability of providing increased degrees of freedom (DoFs) and exceptional design flexibility. This paper addresses the challenge of direction-of-arrival (DOA) estimation for aligned received signals (ARS) and non-aligned received signals (NARS) by designing two specialized uniform FA structures under time-constrained mobility. For ARS scenarios, we propose a fully movable antenna configuration that maximizes the virtual array aperture, whereas for NARS scenarios, we design a structure incorporating a fixed reference antenna to reliably extract phase information from the signal covariance. To overcome the limitations of large virtual arrays and limited sample data inherent in time-varying channels (TVC), we introduce two novel DOA estimation methods: TMRLS-MUSIC for ARS, combining Toeplitz matrix reconstruction (TMR) with linear shrinkage (LS) estimation, and TMR-MUSIC for NARS, utilizing sub-covariance matrices to construct virtual array responses. Both methods employ Nystrom approximation to significantly reduce computational complexity while maintaining estimation accuracy. Theoretical analyses and extensive simulation results demonstrate that the proposed methods achieve underdetermined DOA estimation using minimal FA elements, outperform conventional methods in estimation accuracy, and substantially reduce computational complexity.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.10806v1" target="_blank">Who Benefits from AI Explanations? Towards Accessible and Interpretable Systems</a></h3>
                    <p><strong>Authors:</strong> Maria J. P. Peixoto, Akriti Pandey, Ahsan Zaman, Peter R. Lewis</p>
                    <p><strong>Published:</strong> 8/14/2025</p>
                    <p><strong>Categories:</strong> cs.AI</p>
                    <p><strong>Summary:</strong> As AI systems are increasingly deployed to support decision-making in critical domains, explainability has become a means to enhance the understandability of these outputs and enable users to make more informed and conscious choices. However, despite growing interest in the usability of eXplainable AI (XAI), the accessibility of these methods, particularly for users with vision impairments, remains underexplored. This paper investigates accessibility gaps in XAI through a two-pronged approach. First, a literature review of 79 studies reveals that evaluations of XAI techniques rarely include disabled users, with most explanations relying on inherently visual formats. Second, we present a four-part methodological proof of concept that operationalizes inclusive XAI design: (1) categorization of AI systems, (2) persona definition and contextualization, (3) prototype design and implementation, and (4) expert and user assessment of XAI techniques for accessibility. Preliminary findings suggest that simplified explanations are more comprehensible for non-visual users than detailed ones, and that multimodal presentation is required for more equitable interpretability.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.10795v1" target="_blank">Beyond Not Novel Enough: Enriching Scholarly Critique with LLM-Assisted Feedback</a></h3>
                    <p><strong>Authors:</strong> Osama Mohammed Afzal, Preslav Nakov, Tom Hope, Iryna Gurevych</p>
                    <p><strong>Published:</strong> 8/14/2025</p>
                    <p><strong>Categories:</strong> cs.CL</p>
                    <p><strong>Summary:</strong> Novelty assessment is a central yet understudied aspect of peer review, particularly in high volume fields like NLP where reviewer capacity is increasingly strained. We present a structured approach for automated novelty evaluation that models expert reviewer behavior through three stages: content extraction from submissions, retrieval and synthesis of related work, and structured comparison for evidence based assessment. Our method is informed by a large scale analysis of human written novelty reviews and captures key patterns such as independent claim verification and contextual reasoning. Evaluated on 182 ICLR 2025 submissions with human annotated reviewer novelty assessments, the approach achieves 86.5% alignment with human reasoning and 75.3% agreement on novelty conclusions - substantially outperforming existing LLM based baselines. The method produces detailed, literature aware analyses and improves consistency over ad hoc reviewer judgments. These results highlight the potential for structured LLM assisted approaches to support more rigorous and transparent peer review without displacing human expertise. Data and code are made available.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.10791v1" target="_blank">MapLibre Tile: A Next Generation Vector Tile Format</a></h3>
                    <p><strong>Authors:</strong> Markus Tremmel, Roland Zink</p>
                    <p><strong>Published:</strong> 8/14/2025</p>
                    <p><strong>Categories:</strong> cs.IT, math.IT</p>
                    <p><strong>Summary:</strong> The Mapbox Vector Tile (MVT) format is widely considered the leading open standard for large-scale map visualization, as evidenced by its widespread adoption by major technology companies such as AWS, Meta, and Microsoft for their products and services. However, MVT was developed nearly a decade ago and, consequently, does not fully align with the capabilities of new geospatial data sources that are characterized by rapidly increasing data volumes due to advancements in geospatial sensors and automated detection through artificial intelligence. In this paper, we introduce the MapLibre Tile (MLT) format, a novel vector tile specification designed from the ground up to address the limitations of MVT. Our experiments, simulating user sessions on widely used basemap datasets, demonstrate that MLT achieves up to three times better compression ratios compared to MVT on encoded tilesets, with over six times better on certain large tiles. Additionally, MLT offers decoding speeds that are up to three times faster and significantly enhances processing performance. MLT also introduces new functionalities and is specifically designed to lay the foundation for the next generation of map renderers, which we expect to entirely offload processing to the GPU, thereby overcoming the stagnation of Moore`s law.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.10779v1" target="_blank">Ultra-High-Definition Reference-Based Landmark Image Super-Resolution with Generative Diffusion Prior</a></h3>
                    <p><strong>Authors:</strong> Zhenning Shi, Zizheng Yan, Yuhang Yu, Clara Xue, Jingyu Zhuang, Qi Zhang, Jinwei Chen, Tao Li, Qingnan Fan</p>
                    <p><strong>Published:</strong> 8/14/2025</p>
                    <p><strong>Categories:</strong> cs.CV, cs.AI</p>
                    <p><strong>Summary:</strong> Reference-based Image Super-Resolution (RefSR) aims to restore a low-resolution (LR) image by utilizing the semantic and texture information from an additional reference high-resolution (reference HR) image. Existing diffusion-based RefSR methods are typically built upon ControlNet, which struggles to effectively align the information between the LR image and the reference HR image. Moreover, current RefSR datasets suffer from limited resolution and poor image quality, resulting in the reference images lacking sufficient fine-grained details to support high-quality restoration. To overcome the limitations above, we propose TriFlowSR, a novel framework that explicitly achieves pattern matching between the LR image and the reference HR image. Meanwhile, we introduce Landmark-4K, the first RefSR dataset for Ultra-High-Definition (UHD) landmark scenarios. Considering the UHD scenarios with real-world degradation, in TriFlowSR, we design a Reference Matching Strategy to effectively match the LR image with the reference HR image. Experimental results show that our approach can better utilize the semantic and texture information of the reference HR image compared to previous methods. To the best of our knowledge, we propose the first diffusion-based RefSR pipeline for ultra-high definition landmark scenarios under real-world degradation. Our code and model will be available at https://github.com/nkicsl/TriFlowSR.</p>
                
            
                
                    <h3><a href="http://dx.doi.org/10.1145/3746027.3758295" target="_blank">AEGIS: Authenticity Evaluation Benchmark for AI-Generated Video Sequences</a></h3>
                    <p><strong>Authors:</strong> Jieyu Li, Xin Zhang, Joey Tianyi Zhou</p>
                    <p><strong>Published:</strong> 8/14/2025</p>
                    <p><strong>Categories:</strong> cs.CV, cs.AI</p>
                    <p><strong>Summary:</strong> Recent advances in AI-generated content have fueled the rise of highly realistic synthetic videos, posing severe risks to societal trust and digital integrity. Existing benchmarks for video authenticity detection typically suffer from limited realism, insufficient scale, and inadequate complexity, failing to effectively evaluate modern vision-language models against sophisticated forgeries. To address this critical gap, we introduce AEGIS, a novel large-scale benchmark explicitly targeting the detection of hyper-realistic and semantically nuanced AI-generated videos. AEGIS comprises over 10,000 rigorously curated real and synthetic videos generated by diverse, state-of-the-art generative models, including Stable Video Diffusion, CogVideoX-5B, KLing, and Sora, encompassing open-source and proprietary architectures. In particular, AEGIS features specially constructed challenging subsets enhanced with robustness evaluation. Furthermore, we provide multimodal annotations spanning Semantic-Authenticity Descriptions, Motion Features, and Low-level Visual Features, facilitating authenticity detection and supporting downstream tasks such as multimodal fusion and forgery localization. Extensive experiments using advanced vision-language models demonstrate limited detection capabilities on the most challenging subsets of AEGIS, highlighting the datasets unique complexity and realism beyond the current generalization capabilities of existing models. In essence, AEGIS establishes an indispensable evaluation benchmark, fundamentally advancing research toward developing genuinely robust, reliable, broadly generalizable video authenticity detection methodologies capable of addressing real-world forgery threats. Our dataset is available on https://huggingface.co/datasets/Clarifiedfish/AEGIS.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.10769v1" target="_blank">Modeling Human Responses to Multimodal AI Content</a></h3>
                    <p><strong>Authors:</strong> Zhiqi Shen, Shaojing Fan, Danni Xu, Terence Sim, Mohan Kankanhalli</p>
                    <p><strong>Published:</strong> 8/14/2025</p>
                    <p><strong>Categories:</strong> cs.AI, cs.MM</p>
                    <p><strong>Summary:</strong> As AI-generated content becomes widespread, so does the risk of misinformation. While prior research has primarily focused on identifying whether content is authentic, much less is known about how such content influences human perception and behavior. In domains like trading or the stock market, predicting how people react (e.g., whether a news post will go viral), can be more critical than verifying its factual accuracy. To address this, we take a human-centered approach and introduce the MhAIM Dataset, which contains 154,552 online posts (111,153 of them AI-generated), enabling large-scale analysis of how people respond to AI-generated content. Our human study reveals that people are better at identifying AI content when posts include both text and visuals, particularly when inconsistencies exist between the two. We propose three new metrics: trustworthiness, impact, and openness, to quantify how users judge and engage with online content. We present T-Lens, an LLM-based agent system designed to answer user queries by incorporating predicted human responses to multimodal information. At its core is HR-MCP (Human Response Model Context Protocol), built on the standardized Model Context Protocol (MCP), enabling seamless integration with any LLM. This integration allows T-Lens to better align with human reactions, enhancing both interpretability and interaction capabilities. Our work provides empirical insights and practical tools to equip LLMs with human-awareness capabilities. By highlighting the complex interplay among AI, human cognition, and information reception, our findings suggest actionable strategies for mitigating the risks of AI-driven misinformation.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.10766v1" target="_blank">Frechet and Mordukhovich Derivative (Coderivative) and Covering Constant for Single-Valued Mapping in Euclidean Space with Applications (I)</a></h3>
                    <p><strong>Authors:</strong> Jinlu Li</p>
                    <p><strong>Published:</strong> 8/14/2025</p>
                    <p><strong>Categories:</strong> math.FA, 49J52, 49J53, 47H10, 90C31</p>
                    <p><strong>Summary:</strong> In this paper, we study Frechet derivatives and Mordukhovich derivatives (or coderivatives) of single-valued mappings in Euclidean spaces. At first, we prove the guideline for calculating the Frechet derivatives of single-valued mappings by their partial derivatives. Then, by using the connections between Frechet derivatives and Mordukhovich derivatives (or coderivatives) of single-valued mappings in Banach spaces, we derive the useful rules for calculating the Mordukhovich derivatives of single-valued mappings in Euclidean spaces. For practicing these rules, we find the precise solutions of the Frechet derivatives and Mordukhovich derivatives for some single-valued mappings in Euclidean spaces (in R^2, it can be extended to R^n). By using these solutions, we will find the covering constants for the considered mappings. As applications of the results about the covering constants and by applying the Arutyunov Mordukhovich and Zhukovskiy Parameterized Coincidence Point Theorem, we solve some parameterized equations.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.10765v1" target="_blank">Memorisation and forgetting in a learning Hopfield neural network: bifurcation mechanisms, attractors and basins</a></h3>
                    <p><strong>Authors:</strong> Adam E. Essex, Natalia B. Janson, Rachel A. Norris, Alexander G. Balanov</p>
                    <p><strong>Published:</strong> 8/14/2025</p>
                    <p><strong>Categories:</strong> math.DS, cs.LG, nlin.AO, 37N99 (primary) 68T07, 68T05 (secondary)</p>
                    <p><strong>Summary:</strong> Despite explosive expansion of artificial intelligence based on artificial neural networks (ANNs), these are employed as black boxes, as it is unclear how, during learning, they form memories or develop unwanted features, including spurious memories and catastrophic forgetting. Much research is available on isolated aspects of learning ANNs, but due to their high dimensionality and non-linearity, their comprehensive analysis remains a challenge. In ANNs, knowledge is thought to reside in connection weights or in attractor basins, but these two paradigms are not linked explicitly. Here we comprehensively analyse mechanisms of memory formation in an 81-neuron Hopfield network undergoing Hebbian learning by revealing bifurcations leading to formation and destruction of attractors and their basin boundaries. We show that, by affecting evolution of connection weights, the applied stimuli induce a pitchfork and then a cascade of saddle-node bifurcations creating new attractors with their basins that can code true or spurious memories, and an abrupt disappearance of old memories (catastrophic forgetting). With successful learning, new categories are represented by the basins of newly born point attractors, and their boundaries by the stable manifolds of new saddles. With this, memorisation and forgetting represent two manifestations of the same mechanism. Our strategy to analyse high-dimensional learning ANNs is universal and applicable to recurrent ANNs of any form. The demonstrated mechanisms of memory formation and of catastrophic forgetting shed light on the operation of a wider class of recurrent ANNs and could aid the development of approaches to mitigate their flaws.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.10760v1" target="_blank">FROGENT: An End-to-End Full-process Drug Design Agent</a></h3>
                    <p><strong>Authors:</strong> Qihua Pan, Dong Xu, Jenna Xinyi Yao, Lijia Ma, Zexuan Zhu, Junkai Ji</p>
                    <p><strong>Published:</strong> 8/14/2025</p>
                    <p><strong>Categories:</strong> q-bio.BM, cs.AI</p>
                    <p><strong>Summary:</strong> Powerful AI tools for drug discovery reside in isolated web apps, desktop programs, and code libraries. Such fragmentation forces scientists to manage incompatible interfaces and specialized scripts, which can be a cumbersome and repetitive process. To address this issue, a Full-pROcess druG dEsign ageNT, named FROGENT, has been proposed. Specifically, FROGENT utilizes a Large Language Model and the Model Context Protocol to integrate multiple dynamic biochemical databases, extensible tool libraries, and task-specific AI models. This agentic framework allows FROGENT to execute complicated drug discovery workflows dynamically, including component tasks such as target identification, molecule generation and retrosynthetic planning. FROGENT has been evaluated on eight benchmarks that cover various aspects of drug discovery, such as knowledge retrieval, property prediction, virtual screening, mechanistic analysis, molecular design, and synthesis. It was compared against six increasingly advanced ReAct-style agents that support code execution and literature searches. Empirical results demonstrated that FROGENT triples the best baseline performance in hit-finding and doubles it in interaction profiling, significantly outperforming both the open-source model Qwen3-32B and the commercial model GPT-4o. In addition, real-world cases have been utilized to validate the practicability and generalization of FROGENT. This development suggests that streamlining the agentic drug discovery pipeline can significantly enhance researcher productivity.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.10753v1" target="_blank">Hypercomplex Prompt-aware Multimodal Recommendation</a></h3>
                    <p><strong>Authors:</strong> Zheyu Chen, Jinfeng Xu, Hewei Wang, Shuo Yang, Zitong Wan, Haibo Hu</p>
                    <p><strong>Published:</strong> 8/14/2025</p>
                    <p><strong>Categories:</strong> cs.IR</p>
                    <p><strong>Summary:</strong> Modern recommender systems face critical challenges in handling information overload while addressing the inherent limitations of multimodal representation learning. Existing methods suffer from three fundamental limitations: (1) restricted ability to represent rich multimodal features through a single representation, (2) existing linear modality fusion strategies ignore the deep nonlinear correlations between modalities, and (3) static optimization methods failing to dynamically mitigate the over-smoothing problem in graph convolutional network (GCN). To overcome these limitations, we propose HPMRec, a novel Hypercomplex Prompt-aware Multimodal Recommendation framework, which utilizes hypercomplex embeddings in the form of multi-components to enhance the representation diversity of multimodal features. HPMRec adopts the hypercomplex multiplication to naturally establish nonlinear cross-modality interactions to bridge semantic gaps, which is beneficial to explore the cross-modality features. HPMRec also introduces the prompt-aware compensation mechanism to aid the misalignment between components and modality-specific features loss, and this mechanism fundamentally alleviates the over-smoothing problem. It further designs self-supervised learning tasks that enhance representation diversity and align different modalities. Extensive experiments on four public datasets show that HPMRec achieves state-of-the-art recommendation performance.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.10749v1" target="_blank">OpenSWI: A Massive-Scale Benchmark Dataset for Surface Wave Dispersion Curve Inversion</a></h3>
                    <p><strong>Authors:</strong> Feng Liu, Sijie Zhao, Xinyu Gu, Fenghua Ling, Peiqin Zhuang, Yaxing Li, Rui Su, Lihua Fang, Lianqing Zhou, Jianping Huang, Lei Bai</p>
                    <p><strong>Published:</strong> 8/14/2025</p>
                    <p><strong>Categories:</strong> physics.geo-ph</p>
                    <p><strong>Summary:</strong> Surface wave dispersion curve inversion plays a critical role in both shallow resource exploration and deep geological studies, yet it remains hindered by sensitivity to initial models and low computational efficiency. Recently, data-driven deep learning methods, inspired by advances in computer vision, have shown promising potential to address these challenges. However, the lack of large-scale, diverse benchmark datasets remains a major obstacle to their development and evaluation. To bridge this gap, we present OpenSWI, a comprehensive benchmark dataset generated through the Surface Wave Inversion Dataset Preparation (SWIDP) pipeline. OpenSWI includes two synthetic datasets tailored to different research scales and scenarios, OpenSWI-shallow and OpenSWI-deep, and an AI-ready real-world dataset for generalization evaluation, OpenSWI-real. OpenSWI-shallow, derived from the 2-D OpenFWI geological model dataset, contains over 22 million 1-D velocity profiles paired with fundamental-mode phase and group velocity dispersion curves, spanning a wide range of shallow geological structures (e.g., flat layers, faults, folds, realistic stratigraphy). OpenSWI-deep, built from 14 global and regional 3-D geological models, comprises 1.26 million high-fidelity 1-D velocity-dispersion pairs for deep-Earth studies. OpenSWI-real, compiled from open-source projects, contains two sets of observed dispersion curves with corresponding reference models, serving as a benchmark for evaluating model generalization. To demonstrate utility, we trained models on OpenSWI-shallow and -deep and evaluated them on OpenSWI-real, demonstrating strong agreement between predictions and references, which confirms the diversity and representativeness of the dataset. To advance intelligent surface wave inversion, we release the SWIDP toolbox, OpenSWI datasets, and trained models for the research community.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.10745v1" target="_blank">Agentic Design Review System</a></h3>
                    <p><strong>Authors:</strong> Sayan Nag, K J Joseph, Koustava Goswami, Vlad I Morariu, Balaji Vasan Srinivasan</p>
                    <p><strong>Published:</strong> 8/14/2025</p>
                    <p><strong>Categories:</strong> cs.AI, cs.CV, cs.LG, cs.MA, cs.MM</p>
                    <p><strong>Summary:</strong> Evaluating graphic designs involves assessing it from multiple facets like alignment, composition, aesthetics and color choices. Evaluating designs in a holistic way involves aggregating feedback from individual expert reviewers. Towards this, we propose an Agentic Design Review System (AgenticDRS), where multiple agents collaboratively analyze a design, orchestrated by a meta-agent. A novel in-context exemplar selection approach based on graph matching and a unique prompt expansion method plays central role towards making each agent design aware. Towards evaluating this framework, we propose DRS-BENCH benchmark. Thorough experimental evaluation against state-of-the-art baselines adapted to the problem setup, backed-up with critical ablation experiments brings out the efficacy of Agentic-DRS in evaluating graphic designs and generating actionable feedback. We hope that this work will attract attention to this pragmatic, yet under-explored research direction.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.10741v1" target="_blank">Forgery Guided Learning Strategy with Dual Perception Network for Deepfake Cross-domain Detection</a></h3>
                    <p><strong>Authors:</strong> Lixin Jia, Zhiqing Guo, Gaobo Yang, Liejun Wang, Keqin Li</p>
                    <p><strong>Published:</strong> 8/14/2025</p>
                    <p><strong>Categories:</strong> cs.CV</p>
                    <p><strong>Summary:</strong> The emergence of deepfake technology has introduced a range of societal problems, garnering considerable attention. Current deepfake detection methods perform well on specific datasets, but exhibit poor performance when applied to datasets with unknown forgery techniques. Moreover, as the gap between emerging and traditional forgery techniques continues to widen, cross-domain detection methods that rely on common forgery traces are becoming increasingly ineffective. This situation highlights the urgency of developing deepfake detection technology with strong generalization to cope with fast iterative forgery techniques. To address these challenges, we propose a Forgery Guided Learning (FGL) strategy designed to enable detection networks to continuously adapt to unknown forgery techniques. Specifically, the FGL strategy captures the differential information between known and unknown forgery techniques, allowing the model to dynamically adjust its learning process in real time. To further improve the ability to perceive forgery traces, we design a Dual Perception Network (DPNet) that captures both differences and relationships among forgery traces. In the frequency stream, the network dynamically perceives and extracts discriminative features across various forgery techniques, establishing essential detection cues. These features are then integrated with spatial features and projected into the embedding space. In addition, graph convolution is employed to perceive relationships across the entire feature space, facilitating a more comprehensive understanding of forgery trace correlations. Extensive experiments show that our approach generalizes well across different scenarios and effectively handles unknown forgery challenges, providing robust support for deepfake detection. Our code is available on https://github.com/vpsg-research/FGL.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.10740v1" target="_blank">Axis-level Symmetry Detection with Group-Equivariant Representation</a></h3>
                    <p><strong>Authors:</strong> Wongyun Yu, Ahyun Seo, Minsu Cho</p>
                    <p><strong>Published:</strong> 8/14/2025</p>
                    <p><strong>Categories:</strong> cs.CV</p>
                    <p><strong>Summary:</strong> Symmetry is a fundamental concept that has been extensively studied, yet detecting it in complex scenes remains a significant challenge in computer vision. Recent heatmap-based approaches can localize potential regions of symmetry axes but often lack precision in identifying individual axes. In this work, we propose a novel framework for axis-level detection of the two most common symmetry types-reflection and rotation-by representing them as explicit geometric primitives, i.e. lines and points. Our method employs a dual-branch architecture that is equivariant to the dihedral group, with each branch specialized to exploit the structure of dihedral group-equivariant features for its respective symmetry type. For reflection symmetry, we introduce orientational anchors, aligned with group components, to enable orientation-specific detection, and a reflectional matching that measures similarity between patterns and their mirrored counterparts across candidate axes. For rotational symmetry, we propose a rotational matching that compares patterns at fixed angular intervals to identify rotational centers. Extensive experiments demonstrate that our method achieves state-of-the-art performance, outperforming existing approaches.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.10737v1" target="_blank">Privacy-enhancing Sclera Segmentation Benchmarking Competition: SSBC 2025</a></h3>
                    <p><strong>Authors:</strong> Matej Vitek, Darian TomaÅ¡eviÄ‡, Abhijit Das, Sabari Nathan, GÃ¶khan Ã–zbulak, GÃ¶zde AyÅŸe TataroÄŸlu Ã–zbulak, Jean-Paul Calbimonte, AndrÃ© Anjos, Hariohm Hemant Bhatt, Dhruv Dhirendra Premani, Jay Chaudhari, Caiyong Wang, Jian Jiang, Chi Zhang, Qi Zhang, Iyyakutti Iyappan Ganapathi, Syed Sadaf Ali, Divya Velayudan, Maregu Assefa, Naoufel Werghi, Zachary A. Daniels, Leeon John, Ritesh Vyas, Jalil Nourmohammadi Khiarak, Taher Akbari Saeed, Mahsa Nasehi, Ali Kianfar, Mobina Pashazadeh Panahi, Geetanjali Sharma, Pushp Raj Panth, Raghavendra Ramachandra, Aditya Nigam, Umapada Pal, Peter Peer, Vitomir Å truc</p>
                    <p><strong>Published:</strong> 8/14/2025</p>
                    <p><strong>Categories:</strong> cs.CV</p>
                    <p><strong>Summary:</strong> This paper presents a summary of the 2025 Sclera Segmentation Benchmarking Competition (SSBC), which focused on the development of privacy-preserving sclera-segmentation models trained using synthetically generated ocular images. The goal of the competition was to evaluate how well models trained on synthetic data perform in comparison to those trained on real-world datasets. The competition featured two tracks: $(i)$ one relying solely on synthetic data for model development, and $(ii)$ one combining/mixing synthetic with (a limited amount of) real-world data. A total of nine research groups submitted diverse segmentation models, employing a variety of architectural designs, including transformer-based solutions, lightweight models, and segmentation networks guided by generative frameworks. Experiments were conducted across three evaluation datasets containing both synthetic and real-world images, collected under diverse conditions. Results show that models trained entirely on synthetic data can achieve competitive performance, particularly when dedicated training strategies are employed, as evidenced by the top performing models that achieved $F_1$ scores of over $0.8$ in the synthetic data track. Moreover, performance gains in the mixed track were often driven more by methodological choices rather than by the inclusion of real data, highlighting the promise of synthetic data for privacy-aware biometric development. The code and data for the competition is available at: https://github.com/dariant/SSBC_2025.</p>
                
            
                
                    <h3><a href="http://dx.doi.org/10.1109/DCOSS-IoT65416.2025.00096" target="_blank">Traffic Intersection Simulation Using Turning Movement Count Data in SUMO: A Case Study of Toronto Intersections</a></h3>
                    <p><strong>Authors:</strong> Harshit Maheshwari, Li Yang, Richard W Pazzi</p>
                    <p><strong>Published:</strong> 8/14/2025</p>
                    <p><strong>Categories:</strong> cs.CY, cs.SY, eess.SY, 90B20, 90B06, 68U20, I.6.4; I.6.5</p>
                    <p><strong>Summary:</strong> Urban traffic simulation is vital in planning, modeling, and analyzing road networks. However, the realism of a simulation depends extensively on the quality of input data. This paper presents an intersection traffic simulation tool that leverages real-world vehicle turning movement count (TMC) data from the City of Toronto to model traffic in an urban environment at an individual or multiple intersections using Simulation of Urban MObility (SUMO). The simulation performed in this research focuses specifically on intersection-level traffic generation without creating full vehicle routes through the network. This also helps keep the networks complexity to a minimum. The simulated traffic is evaluated against actual data to show that the simulation closely reproduces real intersection flows. This validates that the real data can drive practical simulations, and these scenarios can replace synthetic or random generated data, which is prominently used in developing new traffic-related methodologies. This is the first tool to integrate TMC data from Toronto into SUMO via an easy-to-use Graphical User Interface. This work contributes to the research and traffic planning community on data-driven traffic simulation. It provides transportation engineers with a framework to evaluate intersection design and traffic signal optimization strategies using readily available aggregate traffic data.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.10727v1" target="_blank">Run-and-Tumble Escape in Pursuit-Evasion Dynamics of Intelligent Active Particles</a></h3>
                    <p><strong>Authors:</strong> Segun Goh, Dennis Haustein, Gerhard Gompper</p>
                    <p><strong>Published:</strong> 8/14/2025</p>
                    <p><strong>Categories:</strong> physics.bio-ph, cond-mat.stat-mech, physics.comp-ph</p>
                    <p><strong>Summary:</strong> The pursuit-evasion game is studied for two adversarial active agents, modelled as a deterministic self-steering pursuer and a stochastic, cognitive evader. The pursuer chases the evader by reorienting its propulsion direction with limited maneuverability, while the evader escapes by executing sharp, unpredictable turns, whose timing and direction the pursuer cannot anticipate. To make the target responsive and agile when the threat level is high, the tumbling frequency is set to increase with decreasing distance from the pursuer; furthermore, the range of preferred tumbling directions is varied. Numerical simulations of such a pursuit-target pair in two spatial dimensions reveal two important scenarios. For dominant pursuers, the evader is compelled to adopt a high-risk strategy that allows the pursuer to approach closely before the evader executes a potentially game-changing backward maneuver to pull away from the pursuer. Otherwise, a strategy where the evader tumbles forward with continuous slight adjustments of the propulsion direction can significantly increase the capture time by preventing the pursuer from aligning with the target propulsion direction, while maintaining the persistence of the target motion. Our results can guide the design of bioinspired robotic systems with efficient evasion capabilities.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.10716v1" target="_blank">Revisiting Cross-View Localization from Image Matching</a></h3>
                    <p><strong>Authors:</strong> Panwang Xia, Qiong Wu, Lei Yu, Yi Liu, Mingtao Xiong, Lei Liang, Yongjun Zhang, Yi Wan</p>
                    <p><strong>Published:</strong> 8/14/2025</p>
                    <p><strong>Categories:</strong> cs.CV</p>
                    <p><strong>Summary:</strong> Cross-view localization aims to estimate the 3 degrees of freedom pose of a ground-view image by registering it to aerial or satellite imagery. It is essential in GNSS-denied environments such as urban canyons and disaster zones. Existing methods either regress poses directly or align features in a shared birds-eye view (BEV) space, both built upon accurate spatial correspondences between perspectives. However, these methods fail to establish strict cross-view correspondences, yielding only coarse or geometrically inconsistent matches. Consequently, fine-grained image matching between ground and aerial views remains an unsolved problem, which in turn constrains the interpretability of localization results. In this paper, we revisit cross-view localization from the perspective of cross-view image matching and propose a novel framework that improves both matching and localization. Specifically, we introduce a Surface Model to model visible regions for accurate BEV projection, and a SimRefiner module to refine the similarity matrix through local-global residual correction, eliminating the reliance on post-processing like RANSAC. To further support research in this area, we introduce CVFM, the first benchmark with 32,509 cross-view image pairs annotated with pixel-level correspondences. Extensive experiments demonstrate that our approach substantially improves both localization accuracy and image matching quality, setting new baselines under extreme viewpoint disparity.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.10711v1" target="_blank">NextStep-1: Toward Autoregressive Image Generation with Continuous Tokens at Scale</a></h3>
                    <p><strong>Authors:</strong> NextStep Team, Chunrui Han, Guopeng Li, Jingwei Wu, Quan Sun, Yan Cai, Yuang Peng, Zheng Ge, Deyu Zhou, Haomiao Tang, Hongyu Zhou, Kenkun Liu, Ailin Huang, Bin Wang, Changxin Miao, Deshan Sun, En Yu, Fukun Yin, Gang Yu, Hao Nie, Haoran Lv, Hanpeng Hu, Jia Wang, Jian Zhou, Jianjian Sun, Kaijun Tan, Kang An, Kangheng Lin, Liang Zhao, Mei Chen, Peng Xing, Rui Wang, Shiyu Liu, Shutao Xia, Tianhao You, Wei Ji, Xianfang Zeng, Xin Han, Xuelin Zhang, Yana Wei, Yanming Xu, Yimin Jiang, Yingming Wang, Yu Zhou, Yucheng Han, Ziyang Meng, Binxing Jiao, Daxin Jiang, Xiangyu Zhang, Yibo Zhu</p>
                    <p><strong>Published:</strong> 8/14/2025</p>
                    <p><strong>Categories:</strong> cs.CV</p>
                    <p><strong>Summary:</strong> Prevailing autoregressive (AR) models for text-to-image generation either rely on heavy, computationally-intensive diffusion models to process continuous image tokens, or employ vector quantization (VQ) to obtain discrete tokens with quantization loss. In this paper, we push the autoregressive paradigm forward with NextStep-1, a 14B autoregressive model paired with a 157M flow matching head, training on discrete text tokens and continuous image tokens with next-token prediction objectives. NextStep-1 achieves state-of-the-art performance for autoregressive models in text-to-image generation tasks, exhibiting strong capabilities in high-fidelity image synthesis. Furthermore, our method shows strong performance in image editing, highlighting the power and versatility of our unified approach. To facilitate open research, we will release our code and models to the community.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.10710v1" target="_blank">CountCluster: Training-Free Object Quantity Guidance with Cross-Attention Map Clustering for Text-to-Image Generation</a></h3>
                    <p><strong>Authors:</strong> Joohyeon Lee, Jin-Seop Lee, Jee-Hyong Lee</p>
                    <p><strong>Published:</strong> 8/14/2025</p>
                    <p><strong>Categories:</strong> cs.CV</p>
                    <p><strong>Summary:</strong> Diffusion-based text-to-image generation models have demonstrated strong performance in terms of image quality and diversity. However, they still struggle to generate images that accurately reflect the number of objects specified in the input prompt. Several approaches have been proposed that rely on either external counting modules for iterative refinement or quantity representations derived from learned tokens or latent features. However, they still have limitations in accurately reflecting the specified number of objects and overlook an important structural characteristic--The number of object instances in the generated image is largely determined in the early timesteps of the denoising process. To correctly reflect the object quantity for image generation, the highly activated regions in the object cross-attention map at the early timesteps should match the input object quantity, while each region should be clearly separated. To address this issue, we propose \textit{CountCluster}, a method that guides the object cross-attention map to be clustered according to the specified object count in the input, without relying on any external tools or additional training. The proposed method partitions the object cross-attention map into $k$ clusters at inference time based on attention scores, defines an ideal distribution in which each cluster is spatially well-separated, and optimizes the latent to align with this target distribution. Our method achieves an average improvement of 18.5\%p in object count accuracy compared to existing methods, and demonstrates superior quantity control performance across a variety of prompts. Code will be released at: https://github.com/JoohyeonL22/CountCluster .</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.10709v1" target="_blank">Forgotten treasures in the HST/FOC UV imaging polarimetric archives of active galactic nuclei -- IV. 5 Orphaned AGNs</a></h3>
                    <p><strong>Authors:</strong> Thibault Barnouin, FrÃ©dÃ©ric Marin, Enrique Lopez-Rodriguez</p>
                    <p><strong>Published:</strong> 8/14/2025</p>
                    <p><strong>Categories:</strong> astro-ph.GA, astro-ph.HE, 85-04 (Primary), 85-08, 85A25 (Secondary), J.2; I.4.1; I.4.7; I.6.4</p>
                    <p><strong>Summary:</strong> The Faint Object Camera (FOC) onboard the Hubble Space Telescope (HST) acquired high-resolution, spatially resolved polarimetric images of nearby Active Galactic Nuclei (AGNs) in the near ultraviolet (near-UV) band. Among the 25 individual targets in the polarized archives, 8 had no published analysis until the beginning of this series of papers. We tackle the last 5 targets in the following. In this paper, we finalize the publication of near-UV imaging polarimetry of AGNs in the HST/FOC archives. We render available spatially resolved polarization maps of the [OIII] emission lines for Mrk 3 and Mrk 78, as well as near-UV continuum polarization maps for Mrk 3, NGC 3862, Cygnus A and 3C 109. We make use of the generalized reduction pipeline presented in the first paper in this series to homogeneously analyze the five remaining polarized observations of AGNs in the FOC archives. In Mrk 3 and Mrk 78, we find a polarization pattern in the narrow-line regions consistent with scattering from an obscured nucleus. For NGC 3862, we confirm marginal UV polarization parallel with the inner radio jet, related to synchrotron emission. In Cygnus A, we report spatially resolved centro-symmetric polarization patterns in both opposite outflows, highlighting the scattering origin of the polarized light. Finally, 3C 109 shows high nuclear polarization, consistent with AGN-dominated emission and parallel with the radio axis, but in tension with polarization arising from dichroic absorption invoked by previous authors. The imaging polarimetry obtained for the narrow-line region and the extended scattering medium surrounding the obscured AGNs is aligned with the predictions of the unified AGN model and demonstrates the power of spatially-resolved polarimetric observation to decipher the complex morphologies at work in AGNs.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.10704v1" target="_blank">Beyond conventional vision: RGB-event fusion for robust object detection in dynamic traffic scenarios</a></h3>
                    <p><strong>Authors:</strong> Zhanwen Liu, Yujing Sun, Yang Wang, Nan Yang, Shengbo Eben Li, Xiangmo Zhao</p>
                    <p><strong>Published:</strong> 8/14/2025</p>
                    <p><strong>Categories:</strong> cs.CV</p>
                    <p><strong>Summary:</strong> The dynamic range limitation of conventional RGB cameras reduces global contrast and causes loss of high-frequency details such as textures and edges in complex traffic environments (e.g., nighttime driving, tunnels), hindering discriminative feature extraction and degrading frame-based object detection. To address this, we integrate a bio-inspired event camera with an RGB camera to provide high dynamic range information and propose a motion cue fusion network (MCFNet), which achieves optimal spatiotemporal alignment and adaptive cross-modal feature fusion under challenging lighting. Specifically, an event correction module (ECM) temporally aligns asynchronous event streams with image frames via optical-flow-based warping, jointly optimized with the detection network to learn task-aware event representations. The event dynamic upsampling module (EDUM) enhances spatial resolution of event frames to match image structures, ensuring precise spatiotemporal alignment. The cross-modal mamba fusion module (CMM) uses adaptive feature fusion with a novel interlaced scanning mechanism, effectively integrating complementary information for robust detection. Experiments conducted on the DSEC-Det and PKU-DAVIS-SOD datasets demonstrate that MCFNet significantly outperforms existing methods in various poor lighting and fast moving traffic scenarios. Notably, on the DSEC-Det dataset, MCFNet achieves a remarkable improvement, surpassing the best existing methods by 7.4% in mAP50 and 1.7% in mAP metrics, respectively. The code is available at https://github.com/Charm11492/MCFNet.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.10703v1" target="_blank">GenOM: Ontology Matching with Description Generation and Large Language Model</a></h3>
                    <p><strong>Authors:</strong> Yiping Song, Jiaoyan Chen, Renate A. Schmidt</p>
                    <p><strong>Published:</strong> 8/14/2025</p>
                    <p><strong>Categories:</strong> cs.AI</p>
                    <p><strong>Summary:</strong> Ontology matching (OM) plays an essential role in enabling semantic interoperability and integration across heterogeneous knowledge sources, particularly in the biomedical domain which contains numerous complex concepts related to diseases and pharmaceuticals. This paper introduces GenOM, a large language model (LLM)-based ontology alignment framework, which enriches the semantic representations of ontology concepts via generating textual definitions, retrieves alignment candidates with an embedding model, and incorporates exact matching-based tools to improve precision. Extensive experiments conducted on the OAEI Bio-ML track demonstrate that GenOM can often achieve competitive performance, surpassing many baselines including traditional OM systems and recent LLM-based methods. Further ablation studies confirm the effectiveness of semantic enrichment and few-shot prompting, highlighting the frameworks robustness and adaptability.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.10691v1" target="_blank">THERMOS: Thermally-Aware Multi-Objective Scheduling of AI Workloads on Heterogeneous Multi-Chiplet PIM Architectures</a></h3>
                    <p><strong>Authors:</strong> Alish Kanani, Lukas Pfromm, Harsh Sharma, Janardhan Rao Doppa, Partha Pratim Pande, Umit Y. Ogras</p>
                    <p><strong>Published:</strong> 8/14/2025</p>
                    <p><strong>Categories:</strong> cs.AR</p>
                    <p><strong>Summary:</strong> Chiplet-based integration enables large-scale systems that combine diverse technologies, enabling higher yield, lower costs, and scalability, making them well-suited to AI workloads. Processing-in-Memory (PIM) has emerged as a promising solution for AI inference, leveraging technologies such as ReRAM, SRAM, and FeFET, each offering unique advantages and trade-offs. A heterogeneous chiplet-based PIM architecture can harness the complementary strengths of these technologies to enable higher performance and energy efficiency. However, scheduling AI workloads across such a heterogeneous system is challenging due to competing performance objectives, dynamic workload characteristics, and power and thermal constraints. To address this need, we propose THERMOS, a thermally-aware, multi-objective scheduling framework for AI workloads on heterogeneous multi-chiplet PIM architectures. THERMOS trains a single multi-objective reinforcement learning (MORL) policy that is capable of achieving Pareto-optimal execution time, energy, or a balanced objective at runtime, depending on the target preferences. Comprehensive evaluations show that THERMOS achieves up to 89% faster average execution time and 57% lower average energy consumption than baseline AI workload scheduling algorithms with only 0.14% runtime and 0.022% energy overhead.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.10687v1" target="_blank">Continuous Bangla Sign Language Translation: Mitigating the Expense of Gloss Annotation with the Assistance of Graph</a></h3>
                    <p><strong>Authors:</strong> Safaeid Hossain Arib, Rabeya Akter, Sejuti Rahman</p>
                    <p><strong>Published:</strong> 8/14/2025</p>
                    <p><strong>Categories:</strong> cs.CL, cs.AI</p>
                    <p><strong>Summary:</strong> Millions of individuals worldwide are affected by deafness and hearing impairment. Sign language serves as a sophisticated means of communication for the deaf and hard of hearing. However, in societies that prioritize spoken languages, sign language often faces underestimation, leading to communication barriers and social exclusion. The Continuous Bangla Sign Language Translation project aims to address this gap by enhancing translation methods. While recent approaches leverage transformer architecture for state-of-the-art results, our method integrates graph-based methods with the transformer architecture. This fusion, combining transformer and STGCN-LSTM architectures, proves more effective in gloss-free translation. Our contributions include architectural fusion, exploring various fusion strategies, and achieving a new state-of-the-art performance on diverse sign language datasets, namely RWTH-PHOENIX-2014T, CSL-Daily, How2Sign, and BornilDB v1.0. Our approach demonstrates superior performance compared to current translation outcomes across all datasets, showcasing notable improvements of BLEU-4 scores of 4.01, 2.07, and 0.5, surpassing those of GASLT, GASLT and slt_how2sign in RWTH-PHOENIX-2014T, CSL-Daily, and How2Sign, respectively. Also, we introduce benchmarking on the BornilDB v1.0 dataset for the first time. Our method sets a benchmark for future research, emphasizing the importance of gloss-free translation to improve communication accessibility for the deaf and hard of hearing.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.10686v1" target="_blank">An Open-Source User-Friendly Interface for Simulating Magnetic Soft Robots using Simulation Open Framework Architecture (SOFA)</a></h3>
                    <p><strong>Authors:</strong> Carla Wehner, Finn Schubert, Heiko Hellkamp, Julius Hahnewald, Kilian Scheafer, Muhammad Bilal Khan, Oliver Gutfleisch</p>
                    <p><strong>Published:</strong> 8/14/2025</p>
                    <p><strong>Categories:</strong> cs.RO, cond-mat.mtrl-sci</p>
                    <p><strong>Summary:</strong> Soft robots, particularly magnetic soft robots, require specialized simulation tools to accurately model their deformation under external magnetic fields. However, existing platforms often lack dedicated support for magnetic materials, making them difficult to use for researchers at different expertise levels. This work introduces an open-source, user-friendly simulation interface using the Simulation Open Framework Architecture (SOFA), specifically designed to model magnetic soft robots. The tool enables users to define material properties, apply magnetic fields, and observe resulting deformations in real time. By integrating intuitive controls and stress analysis capabilities, it aims to bridge the gap between theoretical modeling and practical design. Four benchmark models - a beam, three- and four-finger grippers, and a butterfly - demonstrate its functionality. The softwares ease of use makes it accessible to both beginners and advanced researchers. Future improvements will refine accuracy through experimental validation and comparison with industry-standard finite element solvers, ensuring realistic and predictive simulations of magnetic soft robots.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.10684v1" target="_blank">MDNS: Masked Diffusion Neural Sampler via Stochastic Optimal Control</a></h3>
                    <p><strong>Authors:</strong> Yuchen Zhu, Wei Guo, Jaemoo Choi, Guan-Horng Liu, Yongxin Chen, Molei Tao</p>
                    <p><strong>Published:</strong> 8/14/2025</p>
                    <p><strong>Categories:</strong> cs.LG, stat.ML</p>
                    <p><strong>Summary:</strong> We study the problem of learning a neural sampler to generate samples from discrete state spaces where the target probability mass function $\pi\propto\mathrm{e}^{-U}$ is known up to a normalizing constant, which is an important task in fields such as statistical physics, machine learning, combinatorial optimization, etc. To better address this challenging task when the state space has a large cardinality and the distribution is multi-modal, we propose $\textbf{M}$asked $\textbf{D}$iffusion $\textbf{N}$eural $\textbf{S}$ampler ($\textbf{MDNS}$), a novel framework for training discrete neural samplers by aligning two path measures through a family of learning objectives, theoretically grounded in the stochastic optimal control of the continuous-time Markov chains. We validate the efficiency and scalability of MDNS through extensive experiments on various distributions with distinct statistical properties, where MDNS learns to accurately sample from the target distributions despite the extremely high problem dimensions and outperforms other learning-based baselines by a large margin. A comprehensive study of ablations and extensions is also provided to demonstrate the efficacy and potential of the proposed framework.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.10683v1" target="_blank">Neural Machine Translation for Coptic-French: Strategies for Low-Resource Ancient Languages</a></h3>
                    <p><strong>Authors:</strong> Nasma Chaoui, Richard Khoury</p>
                    <p><strong>Published:</strong> 8/14/2025</p>
                    <p><strong>Categories:</strong> cs.CL</p>
                    <p><strong>Summary:</strong> This paper presents the first systematic study of strategies for translating Coptic into French. Our comprehensive pipeline systematically evaluates: pivot versus direct translation, the impact of pre-training, the benefits of multi-version fine-tuning, and model robustness to noise. Utilizing aligned biblical corpora, we demonstrate that fine-tuning with a stylistically-varied and noise-aware training corpus significantly enhances translation quality. Our findings provide crucial practical insights for developing translation tools for historical languages in general.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.10678v1" target="_blank">HyperTea: A Hypergraph-based Temporal Enhancement and Alignment Network for Moving Infrared Small Target Detection</a></h3>
                    <p><strong>Authors:</strong> Zhaoyuan Qi, Weihua Gao, Wenlong Niu, Jie Tang, Yun Li, Xiaodong Peng</p>
                    <p><strong>Published:</strong> 8/14/2025</p>
                    <p><strong>Categories:</strong> cs.CV</p>
                    <p><strong>Summary:</strong> In practical application scenarios, moving infrared small target detection (MIRSTD) remains highly challenging due to the targets small size, weak intensity, and complex motion pattern. Existing methods typically only model low-order correlations between feature nodes and perform feature extraction and enhancement within a single temporal scale. Although hypergraphs have been widely used for high-order correlation learning, they have received limited attention in MIRSTD. To explore the potential of hypergraphs and enhance multi-timescale feature representation, we propose HyperTea, which integrates global and local temporal perspectives to effectively model high-order spatiotemporal correlations of features. HyperTea consists of three modules: the global temporal enhancement module (GTEM) realizes global temporal context enhancement through semantic aggregation and propagation; the local temporal enhancement module (LTEM) is designed to capture local motion patterns between adjacent frames and then enhance local temporal context; additionally, we further develop a temporal alignment module (TAM) to address potential cross-scale feature misalignment. To our best knowledge, HyperTea is the first work to integrate convolutional neural networks (CNNs), recurrent neural networks (RNNs), and hypergraph neural networks (HGNNs) for MIRSTD, significantly improving detection performance. Experiments on DAUB and IRDST demonstrate its state-of-the-art (SOTA) performance. Our source codes are available at https://github.com/Lurenjia-LRJ/HyperTea.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.10671v1" target="_blank">AEGISS -- Atomic orbital and Entropy-based Guided Inference for Space Selection -- A novel semi-automated active space selection workflow for quantum chemistry and quantum computing applications</a></h3>
                    <p><strong>Authors:</strong> Fabio Tarocco, Pi A. B. Haase, Fabijan PavoÅ¡eviÄ‡, Vijay Krishna, Leonardo Guidoni, Stefan Knecht, Martina Stella</p>
                    <p><strong>Published:</strong> 8/14/2025</p>
                    <p><strong>Categories:</strong> physics.chem-ph, cond-mat.str-el, physics.comp-ph, quant-ph</p>
                    <p><strong>Summary:</strong> The selection of a balanced active space is a critical step in multi-reference quantum chemistry calculations, particularly for systems with strong electron correlation. Likewise, active space selection is a key to unlock the potential of contemporary quantum computing in quantum chemistry. Albeit recent progress, there remains a lack of a unified, robust, and fully automated framework for active space selection that performs reliably across a wide range of molecular systems. In this work, we present a novel approach inspired by both the AVAS (Atomic Valence Active Space) and AutoCAS methods. Our method unifies orbital entropy analysis with atomic orbital projections to guide the construction of chemically and physically meaningful active spaces. This integrated scheme enables a more consistent and flexible selection of active orbitals while retaining automation and scalability. We validate our approach on a set of molecular systems relevant to photodynamic therapy, in particular a set of Ru(II)-complexes, selected to span increasing levels of electron correlation and structural complexity. These molecules serve as challenging test cases due to the presence of strong static correlation and the need for highly accurate electronic structure descriptions. Our results demonstrate that the method can reliably identify compact, chemically intuitive active spaces that capture the essential physics, making it suitable for both classical and quantum computational frameworks. Furthermore, we have developed this approach in a package that is intuitive to use for users and can be interfaced with both standard quantum chemistry and quantum computing applications, making it accessible to a broad research community.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.10669v1" target="_blank">STEP: Stepwise Curriculum Learning for Context-Knowledge Fusion in Conversational Recommendation</a></h3>
                    <p><strong>Authors:</strong> Zhenye Yang, Jinpeng Chen, Huan Li, Xiongnan Jin, Xuanyang Li, Junwei Zhang, Hongbo Gao, Kaimin Wei, Senzhang Wang</p>
                    <p><strong>Published:</strong> 8/14/2025</p>
                    <p><strong>Categories:</strong> cs.AI, cs.IR, H.3.3; I.2.7; H.2.8</p>
                    <p><strong>Summary:</strong> Conversational recommender systems (CRSs) aim to proactively capture user preferences through natural language dialogue and recommend high-quality items. To achieve this, CRS gathers user preferences via a dialog module and builds user profiles through a recommendation module to generate appropriate recommendations. However, existing CRS faces challenges in capturing the deep semantics of user preferences and dialogue context. In particular, the efficient integration of external knowledge graph (KG) information into dialogue generation and recommendation remains a pressing issue. Traditional approaches typically combine KG information directly with dialogue content, which often struggles with complex semantic relationships, resulting in recommendations that may not align with user expectations. To address these challenges, we introduce STEP, a conversational recommender centered on pre-trained language models that combines curriculum-guided context-knowledge fusion with lightweight task-specific prompt tuning. At its heart, an F-Former progressively aligns the dialogue context with knowledge-graph entities through a three-stage curriculum, thus resolving fine-grained semantic mismatches. The fused representation is then injected into the frozen language model via two minimal yet adaptive prefix prompts: a conversation prefix that steers response generation toward user intent and a recommendation prefix that biases item ranking toward knowledge-consistent candidates. This dual-prompt scheme allows the model to share cross-task semantics while respecting the distinct objectives of dialogue and recommendation. Experimental results show that STEP outperforms mainstream methods in the precision of recommendation and dialogue quality in two public datasets.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.10667v1" target="_blank">AddressVLM: Cross-view Alignment Tuning for Image Address Localization using Large Vision-Language Models</a></h3>
                    <p><strong>Authors:</strong> Shixiong Xu, Chenghao Zhang, Lubin Fan, Yuan Zhou, Bin Fan, Shiming Xiang, Gaofeng Meng, Jieping Ye</p>
                    <p><strong>Published:</strong> 8/14/2025</p>
                    <p><strong>Categories:</strong> cs.CV, cs.AI</p>
                    <p><strong>Summary:</strong> Large visual language models (LVLMs) have demonstrated impressive performance in coarse-grained geo-localization at the country or city level, but they struggle with fine-grained street-level localization within urban areas. In this paper, we explore integrating city-wide address localization capabilities into LVLMs, facilitating flexible address-related question answering using street-view images. A key challenge is that the street-view visual question-and-answer (VQA) data provides only microscopic visual cues, leading to subpar performance in fine-tuned models. To tackle this issue, we incorporate perspective-invariant satellite images as macro cues and propose cross-view alignment tuning including a satellite-view and street-view image grafting mechanism, along with an automatic label generation mechanism. Then LVLMs global understanding of street distribution is enhanced through cross-view matching. Our proposed model, named AddressVLM, consists of two-stage training protocols: cross-view alignment tuning and address localization tuning. Furthermore, we have constructed two street-view VQA datasets based on image address localization datasets from Pittsburgh and San Francisco. Qualitative and quantitative evaluations demonstrate that AddressVLM outperforms counterpart LVLMs by over 9% and 12% in average address localization accuracy on these two datasets, respectively.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.10666v1" target="_blank">Deep Learning in Classical and Quantum Physics</a></h3>
                    <p><strong>Authors:</strong> Timothy Heightman, Marcin PÅ‚odzieÅ„</p>
                    <p><strong>Published:</strong> 8/14/2025</p>
                    <p><strong>Categories:</strong> quant-ph, cs.AI, cs.NE, physics.comp-ph</p>
                    <p><strong>Summary:</strong> Scientific progress is tightly coupled to the emergence of new research tools. Today, machine learning (ML)-especially deep learning (DL)-has become a transformative instrument for quantum science and technology. Owing to the intrinsic complexity of quantum systems, DL enables efficient exploration of large parameter spaces, extraction of patterns from experimental data, and data-driven guidance for research directions. These capabilities already support tasks such as refining quantum control protocols and accelerating the discovery of materials with targeted quantum properties, making ML/DL literacy an essential skill for the next generation of quantum scientists. At the same time, DLs power brings risks: models can overfit noisy data, obscure causal structure, and yield results with limited physical interpretability. Recognizing these limitations and deploying mitigation strategies is crucial for scientific rigor. These lecture notes provide a comprehensive, graduate-level introduction to DL for quantum applications, combining conceptual exposition with hands-on examples. Organized as a progressive sequence, they aim to equip readers to decide when and how to apply DL effectively, to understand its practical constraints, and to adapt AI methods responsibly to problems across quantum physics, chemistry, and engineering.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.10901v1" target="_blank">Exceptional flat bands in bipartite non-Hermitian quantum crystals</a></h3>
                    <p><strong>Authors:</strong> Juan Pablo Esparza, Vladimir Juricic</p>
                    <p><strong>Published:</strong> 8/14/2025</p>
                    <p><strong>Categories:</strong> cond-mat.mes-hall</p>
                    <p><strong>Summary:</strong> Flat bands, in which kinetic energy is quenched and quantum states become macroscopically degenerate, host a rich variety of correlated and topological phases, from unconventional superconductors to fractional Chern insulators. In Hermitian lattices, their formation mechanisms are now well understood, but whether such states persist, and acquire new features in non-Hermitian (NH) quantum crystals, relevant to open and driven systems, has remained an open question. Here we show that the Hermitian principle for flat-band formation in bipartite lattices, based on a sublattice degeneracy mismatch, extends directly to the NH regime: whenever one sublattice hosts a momentum-independent eigenvalue with degeneracy exceeding that of its partner on the other sublattice, flat bands arise regardless of gain, loss, or complex couplings. Strikingly, at exceptional points, dispersive bands coalesce to form \emph{exceptional flat bands} that persist beyond these singularities, exhibiting biorthogonal eigenmodes spanning both sublattices, with energies and lifetimes tunable via sublattice asymmetry and non-reciprocal couplings. This general framework unifies Hermitian and NH flat-band constructions, and reveals dispersionless states with no closed-system analogue. The proposed construction is applicable to synthetic platforms, from classical metamaterials, where flat bands can be directly emulated, to quantum-engineered systems such as photonic crystals and ultracold atom arrays, which should host correlated and topological phases emerging from such exceptional flat bands.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.10900v1" target="_blank">Quantum Visual Fields with Neural Amplitude Encoding</a></h3>
                    <p><strong>Authors:</strong> Shuteng Wang, Christian Theobalt, Vladislav Golyanik</p>
                    <p><strong>Published:</strong> 8/14/2025</p>
                    <p><strong>Categories:</strong> cs.CV</p>
                    <p><strong>Summary:</strong> Quantum Implicit Neural Representations (QINRs) include components for learning and execution on gate-based quantum computers. While QINRs recently emerged as a promising new paradigm, many challenges concerning their architecture and ansatz design, the utility of quantum-mechanical properties, training efficiency and the interplay with classical modules remain. This paper advances the field by introducing a new type of QINR for 2D image and 3D geometric field learning, which we collectively refer to as Quantum Visual Field (QVF). QVF encodes classical data into quantum statevectors using neural amplitude encoding grounded in a learnable energy manifold, ensuring meaningful Hilbert space embeddings. Our ansatz follows a fully entangled design of learnable parametrised quantum circuits, with quantum (unitary) operations performed in the real Hilbert space, resulting in numerically stable training with fast convergence. QVF does not rely on classical post-processing -- in contrast to the previous QINR learning approach -- and directly employs projective measurement to extract learned signals encoded in the ansatz. Experiments on a quantum hardware simulator demonstrate that QVF outperforms the existing quantum approach and widely used classical foundational baselines in terms of visual representation accuracy across various metrics and model characteristics, such as learning of high-frequency details. We also show applications of QVF in 2D and 3D field completion and 3D shape interpolation, highlighting its practical potential.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.10898v1" target="_blank">Puppeteer: Rig and Animate Your 3D Models</a></h3>
                    <p><strong>Authors:</strong> Chaoyue Song, Xiu Li, Fan Yang, Zhongcong Xu, Jiacheng Wei, Fayao Liu, Jiashi Feng, Guosheng Lin, Jianfeng Zhang</p>
                    <p><strong>Published:</strong> 8/14/2025</p>
                    <p><strong>Categories:</strong> cs.CV, cs.GR</p>
                    <p><strong>Summary:</strong> Modern interactive applications increasingly demand dynamic 3D content, yet the transformation of static 3D models into animated assets constitutes a significant bottleneck in content creation pipelines. While recent advances in generative AI have revolutionized static 3D model creation, rigging and animation continue to depend heavily on expert intervention. We present Puppeteer, a comprehensive framework that addresses both automatic rigging and animation for diverse 3D objects. Our system first predicts plausible skeletal structures via an auto-regressive transformer that introduces a joint-based tokenization strategy for compact representation and a hierarchical ordering methodology with stochastic perturbation that enhances bidirectional learning capabilities. It then infers skinning weights via an attention-based architecture incorporating topology-aware joint attention that explicitly encodes inter-joint relationships based on skeletal graph distances. Finally, we complement these rigging advances with a differentiable optimization-based animation pipeline that generates stable, high-fidelity animations while being computationally more efficient than existing approaches. Extensive evaluations across multiple benchmarks demonstrate that our method significantly outperforms state-of-the-art techniques in both skeletal prediction accuracy and skinning quality. The system robustly processes diverse 3D content, ranging from professionally designed game assets to AI-generated shapes, producing temporally coherent animations that eliminate the jittering issues common in existing methods.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.10897v1" target="_blank">Human-in-Context: Unified Cross-Domain 3D Human Motion Modeling via In-Context Learning</a></h3>
                    <p><strong>Authors:</strong> Mengyuan Liu, Xinshun Wang, Zhongbin Fang, Deheng Ye, Xia Li, Tao Tang, Songtao Wu, Xiangtai Li, Ming-Hsuan Yang</p>
                    <p><strong>Published:</strong> 8/14/2025</p>
                    <p><strong>Categories:</strong> cs.CV</p>
                    <p><strong>Summary:</strong> This paper aims to model 3D human motion across domains, where a single model is expected to handle multiple modalities, tasks, and datasets. Existing cross-domain models often rely on domain-specific components and multi-stage training, which limits their practicality and scalability. To overcome these challenges, we propose a new setting to train a unified cross-domain model through a single process, eliminating the need for domain-specific components and multi-stage training. We first introduce Pose-in-Context (PiC), which leverages in-context learning to create a pose-centric cross-domain model. While PiC generalizes across multiple pose-based tasks and datasets, it encounters difficulties with modality diversity, prompting strategy, and contextual dependency handling. We thus propose Human-in-Context (HiC), an extension of PiC that broadens generalization across modalities, tasks, and datasets. HiC combines pose and mesh representations within a unified framework, expands task coverage, and incorporates larger-scale datasets. Additionally, HiC introduces a max-min similarity prompt sampling strategy to enhance generalization across diverse domains and a network architecture with dual-branch context injection for improved handling of contextual dependencies. Extensive experimental results show that HiC performs better than PiC in terms of generalization, data scale, and performance across a wide range of domains. These results demonstrate the potential of HiC for building a unified cross-domain 3D human motion model with improved flexibility and scalability. The source codes and models are available at https://github.com/BradleyWang0416/Human-in-Context.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.10896v1" target="_blank">ESSENTIAL: Episodic and Semantic Memory Integration for Video Class-Incremental Learning</a></h3>
                    <p><strong>Authors:</strong> Jongseo Lee, Kyungho Bae, Kyle Min, Gyeong-Moon Park, Jinwoo Choi</p>
                    <p><strong>Published:</strong> 8/14/2025</p>
                    <p><strong>Categories:</strong> cs.CV</p>
                    <p><strong>Summary:</strong> In this work, we tackle the problem of video classincremental learning (VCIL). Many existing VCIL methods mitigate catastrophic forgetting by rehearsal training with a few temporally dense samples stored in episodic memory, which is memory-inefficient. Alternatively, some methods store temporally sparse samples, sacrificing essential temporal information and thereby resulting in inferior performance. To address this trade-off between memory-efficiency and performance, we propose EpiSodic and SEmaNTIc memory integrAtion for video class-incremental Learning (ESSENTIAL). ESSENTIAL consists of episodic memory for storing temporally sparse features and semantic memory for storing general knowledge represented by learnable prompts. We introduce a novel memory retrieval (MR) module that integrates episodic memory and semantic prompts through cross-attention, enabling the retrieval of temporally dense features from temporally sparse features. We rigorously validate ESSENTIAL on diverse datasets: UCF-101, HMDB51, and Something-Something-V2 from the TCD benchmark and UCF-101, ActivityNet, and Kinetics-400 from the vCLIMB benchmark. Remarkably, with significantly reduced memory, ESSENTIAL achieves favorable performance on the benchmarks.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.10894v1" target="_blank">MAESTRO: Masked AutoEncoders for Multimodal, Multitemporal, and Multispectral Earth Observation Data</a></h3>
                    <p><strong>Authors:</strong> Antoine Labatie, Michael Vaccaro, Nina Lardiere, Anatol Garioud, Nicolas Gonthier</p>
                    <p><strong>Published:</strong> 8/14/2025</p>
                    <p><strong>Categories:</strong> cs.CV</p>
                    <p><strong>Summary:</strong> Self-supervised learning holds great promise for remote sensing, but standard self-supervised methods must be adapted to the unique characteristics of Earth observation data. We take a step in this direction by conducting a comprehensive benchmark of fusion strategies and reconstruction target normalization schemes for multimodal, multitemporal, and multispectral Earth observation data. Based on our findings, we propose MAESTRO, a novel adaptation of the Masked Autoencoder, featuring optimized fusion strategies and a tailored target normalization scheme that introduces a spectral prior as a self-supervisory signal. Evaluated on four Earth observation datasets, MAESTRO sets a new state-of-the-art on tasks that strongly rely on multitemporal dynamics, while remaining highly competitive on tasks dominated by a single mono-temporal modality. Code to reproduce all our experiments is available at https://github.com/ignf/maestro.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.10893v1" target="_blank">STream3R: Scalable Sequential 3D Reconstruction with Causal Transformer</a></h3>
                    <p><strong>Authors:</strong> Yushi Lan, Yihang Luo, Fangzhou Hong, Shangchen Zhou, Honghua Chen, Zhaoyang Lyu, Shuai Yang, Bo Dai, Chen Change Loy, Xingang Pan</p>
                    <p><strong>Published:</strong> 8/14/2025</p>
                    <p><strong>Categories:</strong> cs.CV</p>
                    <p><strong>Summary:</strong> We present STream3R, a novel approach to 3D reconstruction that reformulates pointmap prediction as a decoder-only Transformer problem. Existing state-of-the-art methods for multi-view reconstruction either depend on expensive global optimization or rely on simplistic memory mechanisms that scale poorly with sequence length. In contrast, STream3R introduces an streaming framework that processes image sequences efficiently using causal attention, inspired by advances in modern language modeling. By learning geometric priors from large-scale 3D datasets, STream3R generalizes well to diverse and challenging scenarios, including dynamic scenes where traditional methods often fail. Extensive experiments show that our method consistently outperforms prior work across both static and dynamic scene benchmarks. Moreover, STream3R is inherently compatible with LLM-style training infrastructure, enabling efficient large-scale pretraining and fine-tuning for various downstream 3D tasks. Our results underscore the potential of causal Transformer models for online 3D perception, paving the way for real-time 3D understanding in streaming environments. More details can be found in our project page: https://nirvanalan.github.io/projects/stream3r.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.10890v1" target="_blank">Random Permutation Circuits are Quantum Chaotic</a></h3>
                    <p><strong>Authors:</strong> Bruno Bertini, Katja Klobas, Pavel Kos, Daniel Malz</p>
                    <p><strong>Published:</strong> 8/14/2025</p>
                    <p><strong>Categories:</strong> cond-mat.stat-mech, math-ph, math.MP, nlin.CD, nlin.CG, quant-ph</p>
                    <p><strong>Summary:</strong> Random permutation circuits were recently introduced as minimal models for local many-body dynamics that can be interpreted both as classical and quantum. Standard indicators of chaos such as damage spreading, show that these systems exhibit sensitivity to initial conditions in the classical setting. Here, we address their quantum chaoticity by studying the time evolution of local operator entanglement (LOE). We show that the behaviour of LOE in random permutation circuits depends on the dimension of the local configuration space q. When q = 2, i.e. the circuits act on qubits, random permutations are Clifford and the LOE of any local operator is bounded by a constant, indicating that they are not truly chaotic. On the other hand, when the dimension of the local configuration space exceeds two, the LOE grows linearly in time. We prove this in the limit of large dimensions and present numerical evidence that a three-dimensional local configuration space is sufficient for a linear growth of LOE. Our findings highlight that quantum chaos can be produced by essentially classical dynamics. Moreover, we show that LOE can be defined also in the classical realm and put it forward as a universal indicator chaos, both quantum and classical.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.10889v1" target="_blank">Discovery of Niobium Hydride Precipitates in Superconducting Qubits</a></h3>
                    <p><strong>Authors:</strong> Zuhawn Sung, Daniel Bafia, Arely Cano, Akshay Murthy, Jaeyel Lee, Matthew J Reagor, Juan Rubio-Zuazo, Anna Grassellino, Alexander Romanenko</p>
                    <p><strong>Published:</strong> 8/14/2025</p>
                    <p><strong>Categories:</strong> cond-mat.supr-con</p>
                    <p><strong>Summary:</strong> We report the evidence of the formation of niobium hydride phase within niobium films on silicon substrates in superconducting qubits fabricated at Rigetti Computing. For this study, we combined complementary techniques, including room-temperature and cryogenic atomic force microscopy (AFM), synchrotron Xray diffraction, and time of flight secondary ion mass spectroscopy (ToF-SIMS), to directly reveal the existence of niobium hydride precipitates in the Rigetti chip area. Upon cryogenic cooling, we observed variation in the size and morphology of the hydrides, ranging from small (5 nm) irregular shapes to large (~10-100 nm) domain within the Nb grains, fully converted to niobium hydrides. Since niobium hydrides are non-superconducting and can easily change in size and location upon different cooldowns to cryogenic temperature, our finding highlights a new and previously unknown source of decoherence in superconducting qubits. This contributes to both quasiparticle and two level system (TLS) losses, offering a potential explanation for changes in qubit performance upon cooldowns. Finally, by leveraging the RF performance of a 3D bulk Nb resonator, we can quantify RF dissipation on a superconducting qubit, caused by hydrogen concentration variation, and are able to propose a practical engineering pathway to mitigate the formation of the Nb hydrides for superconducting qubit applications.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.10888v1" target="_blank">Conic Formulations of Transport Metrics for Unbalanced Measure Networks and Hypernetworks</a></h3>
                    <p><strong>Authors:</strong> Mary Chriselda Antony Oliver, Emmanuel Hartman, Tom Needham</p>
                    <p><strong>Published:</strong> 8/14/2025</p>
                    <p><strong>Categories:</strong> stat.ML, math.MG</p>
                    <p><strong>Summary:</strong> The Gromov-Wasserstein (GW) variant of optimal transport, designed to compare probability densities defined over distinct metric spaces, has emerged as an important tool for the analysis of data with complex structure, such as ensembles of point clouds or networks. To overcome certain limitations, such as the restriction to comparisons of measures of equal mass and sensitivity to outliers, several unbalanced or partial transport relaxations of the GW distance have been introduced in the recent literature. This paper is concerned with the Conic Gromov-Wasserstein (CGW) distance introduced by S\{e}journ\{e}, Vialard, and Peyr\{e}. We provide a novel formulation in terms of semi-couplings, and extend the framework beyond the metric measure space setting, to compare more general network and hypernetwork structures. With this new formulation, we establish several fundamental properties of the CGW metric, including its scaling behavior under dilation, variational convergence in the limit of volume growth constraints, and comparison bounds with established optimal transport metrics. We further derive quantitative bounds that characterize the robustness of the CGW metric to perturbations in the underlying measures. The hypernetwork formulation of CGW admits a simple and provably convergent block coordinate ascent algorithm for its estimation, and we demonstrate the computational tractability and scalability of our approach through experiments on synthetic and real-world high-dimensional and structured datasets.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.10887v1" target="_blank">Empirical Investigation into Configuring Echo State Networks for Representative Benchmark Problem Domains</a></h3>
                    <p><strong>Authors:</strong> Brooke R. Weborg, Gursel Serpen</p>
                    <p><strong>Published:</strong> 8/14/2025</p>
                    <p><strong>Categories:</strong> cs.NE, cs.AI, cs.LG</p>
                    <p><strong>Summary:</strong> This paper examines Echo State Network, a reservoir computer, performance using four different benchmark problems, then proposes heuristics or rules of thumb for configuring the architecture, as well as the selection of parameters and their values, which are applicable to problems within the same domain, to help serve to fill the experience gap needed by those entering this field of study. The influence of various parameter selections and their value adjustments, as well as architectural changes made to an Echo State Network, a powerful recurrent neural network configured as a reservoir computer, can be challenging to fully comprehend without experience in the field, and even some hyperparameter optimization algorithms may have difficulty adjusting parameter values without proper manual selections made first. Therefore, it is imperative to understand the effects of parameters and their value selection on Echo State Network architecture performance for a successful build. Thus, to address the requirement for an extensive background in Echo State Network architecture, as well as examine how Echo State Network performance is affected with respect to variations in architecture, design, and parameter selection and values, a series of benchmark tasks representing different problem domains, including time series prediction, pattern generation, chaotic system prediction, and time series classification, were modeled and experimented on to show the impact on the performance of Echo State Network.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.10885v1" target="_blank">New Materials, New Functionalities: Molecular Beam Epitaxy of Ultra-High Conductivity Oxides</a></h3>
                    <p><strong>Authors:</strong> Gaurab Rimal, Tanzila Tasnim, Brian Opatosky, Ryan B. Comes, Debarghya Mallick, Simon Kim, Rob G. Moore, Seongshik Oh, Matthew Brahlek</p>
                    <p><strong>Published:</strong> 8/14/2025</p>
                    <p><strong>Categories:</strong> cond-mat.mtrl-sci, cond-mat.mes-hall, cond-mat.str-el, cond-mat.supr-con</p>
                    <p><strong>Summary:</strong> Understanding fundamental properties of materials is necessary for all modern electronic technologies. Toward this end, the fabrication of new ultrapure thin film materials is critical to discover and understand novel properties that can allow further development of technology. Oxide materials are a vast material class abound with diverse properties, and, therefore, harnessing such phases is critical for realizing emerging technologies. Pushing forward, however, requires understanding basic properties of insulating, semiconducting and metallic oxides, as well as the more complex phases that arise out of strong electronic correlations unique to this class of materials. In this review, we will focus on one of the unique aspects of oxides: the ultra-high conductivity metallic state, which can be a critical component for future all-oxide microelectronics such as low-loss interconnects and gate-metals, spintronics, as well as future quantum technologies that employ emergent magnetic or superconducting ground states. Like most oxides, a critical challenge to understanding and ultimately integrating high-conductivity metals into new technologies is the ability to synthesize high-quality materials. Therefore, we will frame the discussions in the context of epitaxial film growth via molecular beam epitaxy (MBE), which has provided insights into the electronic behavior of specific materials while providing samples with unprecedented quality. We will highlight and underscore how MBE has enabled developments and deeper understanding of their properties and how it plays a critical role in the future of this unique class of materials.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.10883v1" target="_blank">Exchange-driven self-diffusion of nanoscale crystalline parahydrogen clusters on graphite</a></h3>
                    <p><strong>Authors:</strong> K. M. Kolevski, M. Boninsegni</p>
                    <p><strong>Published:</strong> 8/14/2025</p>
                    <p><strong>Categories:</strong> cond-mat.other</p>
                    <p><strong>Summary:</strong> Computer simulations yield evidence of superfluid behavior of nanoscale size clusters of parahydrogen adsorbed on a graphite substrate at low temperature ($T\lesssim 0.25 \text{ K}$). Clusters with a number of molecules between 7 and 12 display concurrent superfluidity and crystalline order, reflecting the corrugation of the substrate. Remarkably, it is found that specific clusters with a number of molecules ranging between 7 and 12 self-diffuse on the surface like free particles, despite the strong pinning effect of the substrate. This effect is underlain by coordinated quantum-mechanical exchanges of groups of identical molecules, i.e., it has no classical counterpart.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.10882v1" target="_blank">Bicharacter twists of quantum groups</a></h3>
                    <p><strong>Authors:</strong> Ian Martin, Alexander Tsymbaliuk</p>
                    <p><strong>Published:</strong> 8/14/2025</p>
                    <p><strong>Categories:</strong> math.RT, hep-th, math-ph, math.MP, math.QA, nlin.SI</p>
                    <p><strong>Summary:</strong> We apply the general construction of a twist of bigraded Hopf algebras by skew bicharacters to obtain two-parameter quantum groups in the Drinfeld-Jimbo, new Drinfeld (for affine types), and FRT (for both finite and affine) presentations from their standard one-parameter versions. This yields new elementary proofs of the fundamental results on two-parameter quantum groups that appeared in the literature over the last two decades, and also leads to natural generalizations in the super and multiparameter cases.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.10881v1" target="_blank">ToonComposer: Streamlining Cartoon Production with Generative Post-Keyframing</a></h3>
                    <p><strong>Authors:</strong> Lingen Li, Guangzhi Wang, Zhaoyang Zhang, Yaowei Li, Xiaoyu Li, Qi Dou, Jinwei Gu, Tianfan Xue, Ying Shan</p>
                    <p><strong>Published:</strong> 8/14/2025</p>
                    <p><strong>Categories:</strong> cs.CV, cs.AI</p>
                    <p><strong>Summary:</strong> Traditional cartoon and anime production involves keyframing, inbetweening, and colorization stages, which require intensive manual effort. Despite recent advances in AI, existing methods often handle these stages separately, leading to error accumulation and artifacts. For instance, inbetweening approaches struggle with large motions, while colorization methods require dense per-frame sketches. To address this, we introduce ToonComposer, a generative model that unifies inbetweening and colorization into a single post-keyframing stage. ToonComposer employs a sparse sketch injection mechanism to provide precise control using keyframe sketches. Additionally, it uses a cartoon adaptation method with the spatial low-rank adapter to tailor a modern video foundation model to the cartoon domain while keeping its temporal prior intact. Requiring as few as a single sketch and a colored reference frame, ToonComposer excels with sparse inputs, while also supporting multiple sketches at any temporal location for more precise motion control. This dual capability reduces manual workload and improves flexibility, empowering artists in real-world scenarios. To evaluate our model, we further created PKBench, a benchmark featuring human-drawn sketches that simulate real-world use cases. Our evaluation demonstrates that ToonComposer outperforms existing methods in visual quality, motion consistency, and production efficiency, offering a superior and more flexible solution for AI-assisted cartoon production.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.10880v1" target="_blank">Searching for Privacy Risks in LLM Agents via Simulation</a></h3>
                    <p><strong>Authors:</strong> Yanzhe Zhang, Diyi Yang</p>
                    <p><strong>Published:</strong> 8/14/2025</p>
                    <p><strong>Categories:</strong> cs.CR, cs.AI, cs.CL</p>
                    <p><strong>Summary:</strong> The widespread deployment of LLM-based agents is likely to introduce a critical privacy threat: malicious agents that proactively engage others in multi-turn interactions to extract sensitive information. These dynamic dialogues enable adaptive attack strategies that can cause severe privacy violations, yet their evolving nature makes it difficult to anticipate and discover sophisticated vulnerabilities manually. To tackle this problem, we present a search-based framework that alternates between improving attacker and defender instructions by simulating privacy-critical agent interactions. Each simulation involves three roles: data subject, data sender, and data recipient. While the data subjects behavior is fixed, the attacker (data recipient) attempts to extract sensitive information from the defender (data sender) through persistent and interactive exchanges. To explore this interaction space efficiently, our search algorithm employs LLMs as optimizers, using parallel search with multiple threads and cross-thread propagation to analyze simulation trajectories and iteratively propose new instructions. Through this process, we find that attack strategies escalate from simple direct requests to sophisticated multi-turn tactics such as impersonation and consent forgery, while defenses advance from rule-based constraints to identity-verification state machines. The discovered attacks and defenses transfer across diverse scenarios and backbone models, demonstrating strong practical utility for building privacy-aware agents.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.10876v1" target="_blank">Accelerating cosmological inference of interacting dark energy with neural emulators</a></h3>
                    <p><strong>Authors:</strong> Karim Carrion</p>
                    <p><strong>Published:</strong> 8/14/2025</p>
                    <p><strong>Categories:</strong> astro-ph.CO</p>
                    <p><strong>Summary:</strong> The present thesis aims to tackle two critical aspects of present and future cosmological analysis of Large-Scale Structure (LSS): accurate modelling of the nonlinear matter power spectrum beyond $\Lambda$CDM, and efficient computational techniques for Bayesian parameter estimation. Both are crucial for testing alternative cosmologies and avoiding spurious results. We focus on the Dark Scattering (DS) model, describing pure momentum transfer between dark matter -- dark energy through the parameter $A_{\rm ds}$. To capture DS effects, we adopt the halo model reaction framework within $\tt{ReACT}$, compute the nonlinear DS spectrum, and validate it against $N$-body simulations. We further include baryonic feedback and massive neutrinos, finding degeneracies between DS and baryonic effects but not with neutrinos. We then constrain DS using cosmic shear from KiDS-1000, accelerated by neural emulators from $\tt{CosmoPower}$, which speed up predictions by $\mathcal{O}(10^4)$. Our DS emulator, trained on halo model reaction outputs, preserves percent-level accuracy and incorporates baryonic feedback. Analysing KiDS shear statistics, we obtain $\vert A_{\rm ds}\vert \lesssim 20$ b/GeV at $68 \%$ C.L. Combining KiDS with Planck CMB and BAO data, we find $A_{\rm ds}=10.6^{+4.5}_{-7.3}$ b/GeV at $68 \%$ C.L., suggesting the DS model as a promising resolution to the $S_8$ tension. Finally, we present weak lensing forecasts for Stage IV surveys using an automatically differentiable pipeline with $\tt{jax-cosmo}$ and gradient-based samplers in $\tt{NumPyro}$, reducing computational cost from months on CPUs to days on GPUs. Model evidence is evaluated with $\tt{harmonic}$ under multiple scale cuts. To put things into perspective, the modelling strategies and machine learning accelerations developed here provide powerful tools for the next generation of LSS cosmology.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.10875v1" target="_blank">A Survey on Diffusion Language Models</a></h3>
                    <p><strong>Authors:</strong> Tianyi Li, Mingda Chen, Bowei Guo, Zhiqiang Shen</p>
                    <p><strong>Published:</strong> 8/14/2025</p>
                    <p><strong>Categories:</strong> cs.CL, cs.AI, cs.LG</p>
                    <p><strong>Summary:</strong> Diffusion Language Models (DLMs) are rapidly emerging as a powerful and promising alternative to the dominant autoregressive (AR) paradigm. By generating tokens in parallel through an iterative denoising process, DLMs possess inherent advantages in reducing inference latency and capturing bidirectional context, thereby enabling fine-grained control over the generation process. While achieving a several-fold speed-up, recent advancements have allowed DLMs to show performance comparable to their autoregressive counterparts, making them a compelling choice for various natural language processing tasks. In this survey, we provide a holistic overview of the current DLM landscape. We trace its evolution and relationship with other paradigms, such as autoregressive and masked language models, and cover both foundational principles and state-of-the-art models. Our work offers an up-to-date, comprehensive taxonomy and an in-depth analysis of current techniques, from pre-training strategies to advanced post-training methods. Another contribution of this survey is a thorough review of DLM inference strategies and optimizations, including improvements in decoding parallelism, caching mechanisms, and generation quality. We also highlight the latest approaches to multimodal extensions of DLMs and delineate their applications across various practical scenarios. Furthermore, our discussion addresses the limitations and challenges of DLMs, including efficiency, long-sequence handling, and infrastructure requirements, while outlining future research directions to sustain progress in this rapidly evolving field. Project GitHub is available at https://github.com/VILA-Lab/Awesome-DLMs.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.10874v1" target="_blank">SSRL: Self-Search Reinforcement Learning</a></h3>
                    <p><strong>Authors:</strong> Yuchen Fan, Kaiyan Zhang, Heng Zhou, Yuxin Zuo, Yanxu Chen, Yu Fu, Xinwei Long, Xuekai Zhu, Che Jiang, Yuchen Zhang, Li Kang, Gang Chen, Cheng Huang, Zhizhou He, Bingning Wang, Lei Bai, Ning Ding, Bowen Zhou</p>
                    <p><strong>Published:</strong> 8/14/2025</p>
                    <p><strong>Categories:</strong> cs.CL</p>
                    <p><strong>Summary:</strong> We investigate the potential of large language models (LLMs) to serve as efficient simulators for agentic search tasks in reinforcement learning (RL), thereby reducing dependence on costly interactions with external search engines. To this end, we first quantify the intrinsic search capability of LLMs via structured prompting and repeated sampling, which we term Self-Search. Our results reveal that LLMs exhibit strong scaling behavior with respect to the inference budget, achieving high pass@k on question-answering benchmarks, including the challenging BrowseComp task. Building on these observations, we introduce Self-Search RL (SSRL), which enhances LLMs Self-Search capability through format-based and rule-based rewards. SSRL enables models to iteratively refine their knowledge utilization internally, without requiring access to external tools. Empirical evaluations demonstrate that SSRL-trained policy models provide a cost-effective and stable environment for search-driven RL training, reducing reliance on external search engines and facilitating robust sim-to-real transfer. We draw the following conclusions: 1) LLMs possess world knowledge that can be effectively elicited to achieve high performance; 2) SSRL demonstrates the potential of leveraging internal knowledge to reduce hallucination; 3) SSRL-trained models integrate seamlessly with external search engines without additional effort. Our findings highlight the potential of LLMs to support more scalable RL agent training.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.10873v1" target="_blank">QB Ground State Energy Estimation Benchmark</a></h3>
                    <p><strong>Authors:</strong> Nicole Bellonzi, Joshua T. Cantin, Mohammad Reza Jangrouei, Alexander Kunitsa, Jason Necaise, Nam Nguyen, John Penuel, Maxwell D. Radin, Jhonathan Romero Fontalvo, Rashmi Sundareswara, Linjun Wang, Thomas Watts, Yanbing Zhou, Michael C. Garrett, Adam Holmes, Artur F. Izmaylov, Matthew Otten</p>
                    <p><strong>Published:</strong> 8/14/2025</p>
                    <p><strong>Categories:</strong> quant-ph</p>
                    <p><strong>Summary:</strong> Ground State Energy Estimation (GSEE) is a central problem in quantum chemistry and condensed matter physics, demanding efficient algorithms to solve complex electronic structure calculations. This work introduces a structured benchmarking framework for evaluating the performance of both classical and quantum solvers on diverse GSEE problem instances. We assess three prominent methods -- Semistochastic Heat-Bath Configuration Interaction (SHCI), Density Matrix Renormalization Group (DMRG), and Double-Factorized Quantum Phase Estimation (DF QPE) -- ighlighting their respective strengths and limitations. Our results show that fully optimized SHCI achieves near-universal solvability on the benchmark set, DMRG excels for low-entanglement systems, and DF QPE is currently constrained by hardware and algorithmic limitations. However, we observe that many benchmark Hamiltonians are drawn from datasets tailored to SHCI and related approaches, introducing a bias that favors classical solvers. To mitigate this, we propose expanding the benchmark suite to include more challenging, strongly correlated systems to enable a more balanced and forward-looking evaluation of solver capabilities. As quantum hardware and algorithms improve, this benchmarking framework will serve as a vital tool for tracking progress and identifying domains where quantum methods may surpass classical techniques. The QB-GSEE benchmark repository is openly available at https://github.com/isi-usc-edu/qb-gsee-benchmark [1]. By maintaining a scalable and open resource, we aim to accelerate innovation in computational quantum chemistry and quantum computing.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.10872v1" target="_blank">TLE-Based A2C Agent for Terrestrial Coverage Orbital Path Planning</a></h3>
                    <p><strong>Authors:</strong> Anantha Narayanan, Battu Bhanu Teja, Pruthwik Mishra</p>
                    <p><strong>Published:</strong> 8/14/2025</p>
                    <p><strong>Categories:</strong> cs.RO, cs.AI</p>
                    <p><strong>Summary:</strong> The increasing congestion of Low Earth Orbit (LEO) poses persistent challenges to the efficient deployment and safe operation of Earth observation satellites. Mission planners must now account not only for mission-specific requirements but also for the increasing collision risk with active satellites and space debris. This work presents a reinforcement learning framework using the Advantage Actor-Critic (A2C) algorithm to optimize satellite orbital parameters for precise terrestrial coverage within predefined surface radii. By formulating the problem as a Markov Decision Process (MDP) within a custom OpenAI Gymnasium environment, our method simulates orbital dynamics using classical Keplerian elements. The agent progressively learns to adjust five of the orbital parameters - semi-major axis, eccentricity, inclination, right ascension of ascending node, and the argument of perigee-to achieve targeted terrestrial coverage. Comparative evaluation against Proximal Policy Optimization (PPO) demonstrates A2Cs superior performance, achieving 5.8x higher cumulative rewards (10.0 vs 9.263025) while converging in 31.5x fewer timesteps (2,000 vs 63,000). The A2C agent consistently meets mission objectives across diverse target coordinates while maintaining computational efficiency suitable for real-time mission planning applications. Key contributions include: (1) a TLE-based orbital simulation environment incorporating physics constraints, (2) validation of actor-critic methods superiority over trust region approaches in continuous orbital control, and (3) demonstration of rapid convergence enabling adaptive satellite deployment. This approach establishes reinforcement learning as a computationally efficient alternative for scalable and intelligent LEO mission planning.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.10869v1" target="_blank">Medico 2025: Visual Question Answering for Gastrointestinal Imaging</a></h3>
                    <p><strong>Authors:</strong> Sushant Gautam, Vajira Thambawita, Michael Riegler, PÃ¥l Halvorsen, Steven Hicks</p>
                    <p><strong>Published:</strong> 8/14/2025</p>
                    <p><strong>Categories:</strong> cs.CV, cs.AI, 68T45, 92C55, I.2.10; I.4.9</p>
                    <p><strong>Summary:</strong> The Medico 2025 challenge addresses Visual Question Answering (VQA) for Gastrointestinal (GI) imaging, organized as part of the MediaEval task series. The challenge focuses on developing Explainable Artificial Intelligence (XAI) models that answer clinically relevant questions based on GI endoscopy images while providing interpretable justifications aligned with medical reasoning. It introduces two subtasks: (1) answering diverse types of visual questions using the Kvasir-VQA-x1 dataset, and (2) generating multimodal explanations to support clinical decision-making. The Kvasir-VQA-x1 dataset, created from 6,500 images and 159,549 complex question-answer (QA) pairs, serves as the benchmark for the challenge. By combining quantitative performance metrics and expert-reviewed explainability assessments, this task aims to advance trustworthy Artificial Intelligence (AI) in medical image analysis. Instructions, data access, and an updated guide for participation are available in the official competition repository: https://github.com/simula/MediaEval-Medico-2025</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.10868v1" target="_blank">TexVerse: A Universe of 3D Objects with High-Resolution Textures</a></h3>
                    <p><strong>Authors:</strong> Yibo Zhang, Li Zhang, Rui Ma, Nan Cao</p>
                    <p><strong>Published:</strong> 8/14/2025</p>
                    <p><strong>Categories:</strong> cs.CV</p>
                    <p><strong>Summary:</strong> We introduce TexVerse, a large-scale 3D dataset featuring high-resolution textures. While recent advances in large-scale 3D datasets have enhanced high-resolution geometry generation, creating high-resolution textures end-to-end remains underexplored due to the lack of suitable datasets. TexVerse fills this gap with a curated collection of over 858K unique high-resolution 3D models sourced from Sketchfab, including more than 158K models with physically based rendering (PBR) materials. Each model encompasses all of its high-resolution variants, bringing the total to 1.6M 3D instances. TexVerse also includes specialized subsets: TexVerse-Skeleton, with 69K rigged models, and TexVerse-Animation, with 54K animated models, both preserving original skeleton and animation data uploaded by the user. We also provide detailed model annotations describing overall characteristics, structural components, and intricate features. TexVerse offers a high-quality data resource with wide-ranging potential applications in texture synthesis, PBR material development, animation, and various 3D vision and graphics tasks.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.10866v1" target="_blank">Efficiently Verifiable Proofs of Data Attribution</a></h3>
                    <p><strong>Authors:</strong> Ari Karchmer, Seth Neel, Martin Pawelczyk</p>
                    <p><strong>Published:</strong> 8/14/2025</p>
                    <p><strong>Categories:</strong> cs.LG</p>
                    <p><strong>Summary:</strong> Data attribution methods aim to answer useful counterfactual questions like what would a ML models prediction be if it were trained on a different dataset? However, estimation of data attribution models through techniques like empirical influence or datamodeling remains very computationally expensive. This causes a critical trust issue: if only a few computationally rich parties can obtain data attributions, how can resource-constrained parties trust that the provided attributions are indeed good, especially when they are used for important downstream applications (e.g., data pricing)? In this paper, we address this trust issue by proposing an interactive verification paradigm for data attribution. An untrusted and computationally powerful Prover learns data attributions, and then engages in an interactive proof with a resource-constrained Verifier. Our main result is a protocol that provides formal completeness, soundness, and efficiency guarantees in the sense of Probably-Approximately-Correct (PAC) verification. Specifically, if both Prover and Verifier follow the protocol, the Verifier accepts data attributions that are {\epsilon}-close to the optimal data attributions (in terms of the Mean Squared Error) with probability 1-{\delta}. Conversely, if the Prover arbitrarily deviates from the protocol, even with infinite compute, then this is detected (or it still yields data attributions to the Verifier) except with probability {\delta}. Importantly, our protocol ensures the Verifiers workload, measured by the number of independent model retrainings it must perform, scales only as O(1/{\epsilon}); i.e., independently of the dataset size. At a technical level, our results apply to efficiently verifying any linear function over the boolean hypercube computed by the Prover, making them broadly applicable to various attribution tasks.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.10865v1" target="_blank">Performance of GPT-5 in Brain Tumor MRI Reasoning</a></h3>
                    <p><strong>Authors:</strong> Mojtaba Safari, Shansong Wang, Mingzhe Hu, Zach Eidex, Qiang Li, Xiaofeng Yang</p>
                    <p><strong>Published:</strong> 8/14/2025</p>
                    <p><strong>Categories:</strong> cs.CV, cs.AI</p>
                    <p><strong>Summary:</strong> Accurate differentiation of brain tumor types on magnetic resonance imaging (MRI) is critical for guiding treatment planning in neuro-oncology. Recent advances in large language models (LLMs) have enabled visual question answering (VQA) approaches that integrate image interpretation with natural language reasoning. In this study, we evaluated GPT-4o, GPT-5-nano, GPT-5-mini, and GPT-5 on a curated brain tumor VQA benchmark derived from 3 Brain Tumor Segmentation (BraTS) datasets - glioblastoma (GLI), meningioma (MEN), and brain metastases (MET). Each case included multi-sequence MRI triplanar mosaics and structured clinical features transformed into standardized VQA items. Models were assessed in a zero-shot chain-of-thought setting for accuracy on both visual and reasoning tasks. Results showed that GPT-5-mini achieved the highest macro-average accuracy (44.19%), followed by GPT-5 (43.71%), GPT-4o (41.49%), and GPT-5-nano (35.85%). Performance varied by tumor subtype, with no single model dominating across all cohorts. These findings suggest that GPT-5 family models can achieve moderate accuracy in structured neuro-oncological VQA tasks, but not at a level acceptable for clinical use.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.10864v1" target="_blank">Statistics in 3d gravity from knots and links</a></h3>
                    <p><strong>Authors:</strong> Jeevan Chandra</p>
                    <p><strong>Published:</strong> 8/14/2025</p>
                    <p><strong>Categories:</strong> hep-th</p>
                    <p><strong>Summary:</strong> In recent years, there has been remarkable progress in evaluating wormhole amplitudes in 3d Einstein gravity with negative cosmological constant and matching them to statistics of 2d CFT data. In this work, we compute non-perturbative Gaussian and non-Gaussian gravitational contributions to the OPE statistics using a framework that can systematically generate a class of such non-perturbative effects - \textit{Fragmentation of knots and links by Wilson lines}. We illustrate this idea by constructing multi-boundary wormholes from fragmentation diagrams of prime knots and links with upto five crossings. We discuss fragmentations of hyperbolic knots and links like the figure-eight knot, the three-twist knot and the Whitehead link; and non-hyperbolic ones like the Hopf link, the trefoil knot, the Solomons knot and the Cinquefoil knot. Using Virasoro TQFT, we show how the partition functions on wormholes constructed from different fragmentations of the same knot or link are closely related. Using these fragmentations, we compute gravitational contributions to the variance, a two-point non-Gaussianity, two structures of four-point non-Gaussianities called the `pillow contraction and the `$6j$-contraction, and some six-point non-Gaussianities. We also check the consistency of some of these non-Gaussianities with the extended Gaussian ensemble of OPE data that incorporates the Gaussian corrections to the variance from knots.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.10862v1" target="_blank">Minimmit: Fast Finality with Even Faster Blocks</a></h3>
                    <p><strong>Authors:</strong> Brendan Kobayashi Chou, Andrew Lewis-Pye, Patrick OGrady</p>
                    <p><strong>Published:</strong> 8/14/2025</p>
                    <p><strong>Categories:</strong> cs.DC</p>
                    <p><strong>Summary:</strong> Minimmit is a new protocol for State-Machine-Replication (SMR) that extends the 2-round finality approach of protocols such as Alpenglow to further reduce latency, by allowing for faster progression through views. This preliminary draft provides motivation and pseudocode, together with proofs of consistency and liveness. An updated draft with a proof of optimistic responsiveness, suggested optimizations, and experiments, is to follow.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.10860v1" target="_blank">From Black Box to Transparency: Enhancing Automated Interpreting Assessment with Explainable AI in College Classrooms</a></h3>
                    <p><strong>Authors:</strong> Zhaokun Jiang, Ziyin Zhang</p>
                    <p><strong>Published:</strong> 8/14/2025</p>
                    <p><strong>Categories:</strong> cs.CL, cs.AI</p>
                    <p><strong>Summary:</strong> Recent advancements in machine learning have spurred growing interests in automated interpreting quality assessment. Nevertheless, existing research suffers from insufficient examination of language use quality, unsatisfactory modeling effectiveness due to data scarcity and imbalance, and a lack of efforts to explain model predictions. To address these gaps, we propose a multi-dimensional modeling framework that integrates feature engineering, data augmentation, and explainable machine learning. This approach prioritizes explainability over ``black box predictions by utilizing only construct-relevant, transparent features and conducting Shapley Value (SHAP) analysis. Our results demonstrate strong predictive performance on a novel English-Chinese consecutive interpreting dataset, identifying BLEURT and CometKiwi scores to be the strongest predictive features for fidelity, pause-related features for fluency, and Chinese-specific phraseological diversity metrics for language use. Overall, by placing particular emphasis on explainability, we present a scalable, reliable, and transparent alternative to traditional human evaluation, facilitating the provision of detailed diagnostic feedback for learners and supporting self-regulated learning advantages not afforded by automated scores in isolation.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.10858v1" target="_blank">Hierarchical Fine-grained Preference Optimization for Physically Plausible Video Generation</a></h3>
                    <p><strong>Authors:</strong> Harold Haodong Chen, Haojian Huang, Qifeng Chen, Harry Yang, Ser-Nam Lim</p>
                    <p><strong>Published:</strong> 8/14/2025</p>
                    <p><strong>Categories:</strong> cs.CV</p>
                    <p><strong>Summary:</strong> Recent advancements in video generation have enabled the creation of high-quality, visually compelling videos. However, generating videos that adhere to the laws of physics remains a critical challenge for applications requiring realism and accuracy. In this work, we propose PhysHPO, a novel framework for Hierarchical Cross-Modal Direct Preference Optimization, to tackle this challenge by enabling fine-grained preference alignment for physically plausible video generation. PhysHPO optimizes video alignment across four hierarchical granularities: a) Instance Level, aligning the overall video content with the input prompt; b) State Level, ensuring temporal consistency using boundary frames as anchors; c) Motion Level, modeling motion trajectories for realistic dynamics; and d) Semantic Level, maintaining logical consistency between narrative and visuals. Recognizing that real-world videos are the best reflections of physical phenomena, we further introduce an automated data selection pipeline to efficiently identify and utilize good data from existing large-scale text-video datasets, thereby eliminating the need for costly and time-intensive dataset construction. Extensive experiments on both physics-focused and general capability benchmarks demonstrate that PhysHPO significantly improves physical plausibility and overall video generation quality of advanced models. To the best of our knowledge, this is the first work to explore fine-grained preference alignment and data selection for video generation, paving the way for more realistic and human-preferred video generation paradigms.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.10854v1" target="_blank">Introducing CQ: A C-like API for Quantum Accelerated HPC</a></h3>
                    <p><strong>Authors:</strong> Oliver Thomson Brown, Mateusz Meller, James Richings</p>
                    <p><strong>Published:</strong> 8/14/2025</p>
                    <p><strong>Categories:</strong> cs.DC, quant-ph</p>
                    <p><strong>Summary:</strong> In this paper we present CQ, a specification for a C-like API for quantum accelerated HPC, as well as CQ-SimBE, a reference implementation of CQ written in C99, and built on top of the statevector simulator QuEST. CQ focuses on enabling the incremental integration of quantum computing into classical HPC codes by supporting runtime offloading from languages such as C and Fortran. It provides a way of describing and offloading quantum computations which is compatible with strictly and strongly typed compiled languages, and gives the programmer fine-grained control over classical data movement. The CQ Simulated Backend (CQ-SimBE) provides both a way to demonstrate the usage and utility of CQ, and a space to experiment with new features such as support for analogue quantum computing. Both the CQ specification and CQ-SimBE are open-source, and available in public repositories.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.10851v1" target="_blank">CrossDenoise: Denoising Implicit Feedback via a Lightweight Entity-Aware Synergistic Framework</a></h3>
                    <p><strong>Authors:</strong> Ze Liu, Xianquan Wang, Shuochen Liu, Jie Ma, Huibo Xu, Yupeng Han, Zhe Yang, Kai Zhang, Longfei Li, Jun Zhou</p>
                    <p><strong>Published:</strong> 8/14/2025</p>
                    <p><strong>Categories:</strong> cs.IR, cs.LG</p>
                    <p><strong>Summary:</strong> Recommender systems heavily rely on implicit feedback, which is inherently noisy due to false positives and negatives, severely degrading recommendation accuracy. Existing denoising strategies often overlook entity-aware modeling, suffer from high computational overhead, or demand excessive hyperparameter tuning, limiting their real-world applicability. We propose CrossDenoise, a novel and lightweight framework that addresses these challenges by disentangling noise estimation into user-, item-, and interaction-specific factors. Leveraging empirical observations that show significant heterogeneity in user and item noise propensities, CrossDenoise computes entity reputation factors (user/item reliability) via a rank-based linear mapping of average training losses. These are fused with interaction-level weights derived from an empirical cumulative distribution function (ECDF) of individual losses. This design is model-agnostic, computationally efficient, and requires only two intuitive hyperparameters. Extensive experiments on ML-1M, Yelp, and Amazon-book datasets, across GMF, NeuMF, and CDAE backbones, demonstrate that CrossDenoise consistently and significantly outperforms state-of-the-art baselines. For instance, it achieves up to 27.01% NDCG@50 gain on Yelp with NeuMF, while incurring negligible computational and memory overhead. Our analysis confirms that CrossDenoise effectively separates clean from noisy samples and remains robust under varied hyperparameter settings. It offers a practical and scalable solution for denoising implicit feedback.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.10850v1" target="_blank">Scalable platform for qudit-based quantum computing using polar molecules</a></h3>
                    <p><strong>Authors:</strong> Soleh Kh. Muminov, Evgeniy O. Kiktenko, Anastasiia S. Nikolaeva, Denis A. Drozhzhin, Sergey I. Matveenko, Aleksey K. Fedorov, Georgy V. Shlyapnikov</p>
                    <p><strong>Published:</strong> 8/14/2025</p>
                    <p><strong>Categories:</strong> quant-ph</p>
                    <p><strong>Summary:</strong> We propose a model of a scalable qudit-based quantum processor that uses rotational degrees of freedom of polar molecules. Entangling gates between qudits are implemented via moving molecule in optical traps by exploiting dipole-dipole interactions to mediate coherent coupling. We develop encoding schemes that map single qubits (d=2) into qudits of dimensions 2 = d = 5, and pairs of qubits into higher-dimensional qudits with d=4, 5. This approach enables the realization of a universal set of quantum gates. In particular, we exploit additional levels that are present in the d=3 and d=5 qudits, which allows one to simplify the decomposition of multiqubit gates. We then analyze the relevant experimental parameters to realize our schemes with SrF and 87Rb133Cs molecules. The proposed approach offers a promising route towards scalable and versatile quantum information processing with multilevel systems, which can be potentially realized using currently available experimental facilities.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.10848v1" target="_blank">Psyche-R1: Towards Reliable Psychological LLMs through Unified Empathy, Expertise, and Reasoning</a></h3>
                    <p><strong>Authors:</strong> Chongyuan Dai, Jinpeng Hu, Hongchang Shi, Zhuo Li, Xun Yang, Meng Wang</p>
                    <p><strong>Published:</strong> 8/14/2025</p>
                    <p><strong>Categories:</strong> cs.CL</p>
                    <p><strong>Summary:</strong> Amidst a shortage of qualified mental health professionals, the integration of large language models (LLMs) into psychological applications offers a promising way to alleviate the growing burden of mental health disorders. Recent reasoning-augmented LLMs have achieved remarkable performance in mathematics and programming, while research in the psychological domain has predominantly emphasized emotional support and empathetic dialogue, with limited attention to reasoning mechanisms that are beneficial to generating reliable responses. Therefore, in this paper, we propose Psyche-R1, the first Chinese psychological LLM that jointly integrates empathy, psychological expertise, and reasoning, built upon a novel data curation pipeline. Specifically, we design a comprehensive data synthesis pipeline that produces over 75k high-quality psychological questions paired with detailed rationales, generated through chain-of-thought (CoT) reasoning and iterative prompt-rationale optimization, along with 73k empathetic dialogues. Subsequently, we employ a hybrid training strategy wherein challenging samples are identified through a multi-LLM cross-selection strategy for group relative policy optimization (GRPO) to improve reasoning ability, while the remaining data is used for supervised fine-tuning (SFT) to enhance empathetic response generation and psychological domain knowledge. Extensive experiment results demonstrate the effectiveness of the Psyche-R1 across several psychological benchmarks, where our 7B Psyche-R1 achieves comparable results to 671B DeepSeek-R1.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.10841v1" target="_blank">Performance of universal machine-learned potentials with explicit long-range interactions in biomolecular simulations</a></h3>
                    <p><strong>Authors:</strong> Viktor Zaverkin, Matheus Ferraz, Francesco Alesiani, Mathias Niepert</p>
                    <p><strong>Published:</strong> 8/14/2025</p>
                    <p><strong>Categories:</strong> physics.chem-ph, cond-mat.soft, cs.LG, physics.comp-ph</p>
                    <p><strong>Summary:</strong> Universal machine-learned potentials promise transferable accuracy across compositional and vibrational degrees of freedom, yet their application to biomolecular simulations remains underexplored. This work systematically evaluates equivariant message-passing architectures trained on the SPICE-v2 dataset with and without explicit long-range dispersion and electrostatics. We assess the impact of model size, training data composition, and electrostatic treatment across in- and out-of-distribution benchmark datasets, as well as molecular simulations of bulk liquid water, aqueous NaCl solutions, and biomolecules, including alanine tripeptide, the mini-protein Trp-cage, and Crambin. While larger models improve accuracy on benchmark datasets, this trend does not consistently extend to properties obtained from simulations. Predicted properties also depend on the composition of the training dataset. Long-range electrostatics show no systematic impact across systems. However, for Trp-cage, their inclusion yields increased conformational variability. Our results suggest that imbalanced datasets and immature evaluation practices currently challenge the applicability of universal machine-learned potentials to biomolecular simulations.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.10840v1" target="_blank">Generalizable Federated Learning using Client Adaptive Focal Modulation</a></h3>
                    <p><strong>Authors:</strong> Tajamul Ashraf, Iqra Altaf Gillani</p>
                    <p><strong>Published:</strong> 8/14/2025</p>
                    <p><strong>Categories:</strong> cs.CV</p>
                    <p><strong>Summary:</strong> Federated learning (FL) has proven essential for privacy-preserving, collaborative training across distributed clients. Our prior work, TransFed, introduced a robust transformer-based FL framework that leverages a learn-to-adapt hypernetwork to generate personalized focal modulation layers per client, outperforming traditional methods in non-IID and cross-domain settings. In this extended version, we propose AdaptFED, where we deepen the investigation of focal modulation in generalizable FL by incorporating: (1) a refined adaptation strategy that integrates task-aware client embeddings to personalize modulation dynamics further, (2) enhanced theoretical bounds on adaptation performance, and (3) broader empirical validation across additional modalities, including time-series and multilingual data. We also introduce an efficient variant of TransFed that reduces server-client communication overhead via low-rank hypernetwork conditioning, enabling scalable deployment in resource-constrained environments. Extensive experiments on eight diverse datasets reaffirm the superiority of our method over state-of-the-art baselines, particularly in source-free and cross-task federated setups. Our findings not only extend the capabilities of focal modulation in FL but also pave the way for more adaptive, scalable, and generalizable transformer-based federated systems. The code is available at http://github.com/Tajamul21/TransFed</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.10839v1" target="_blank">Reinforced Language Models for Sequential Decision Making</a></h3>
                    <p><strong>Authors:</strong> Jim Dilkes, Vahid Yazdanpanah, Sebastian Stein</p>
                    <p><strong>Published:</strong> 8/14/2025</p>
                    <p><strong>Categories:</strong> cs.CL, cs.AI, cs.LG, I.2.7; I.2.8</p>
                    <p><strong>Summary:</strong> Large Language Models (LLMs) show potential as sequential decision-making agents, but their application is often limited due to a reliance on large, computationally expensive models. This creates a need to improve smaller models, yet existing post-training methods are designed for single-turn interactions and cannot handle credit assignment in multi-step agentic tasks. To address this, we introduce Multi-Step Group-Relative Policy Optimization (MS-GRPO), a new algorithm for post-training LLM agents, grounded in formal Text-Mediated Stochastic Game (TSMG) and Language-Agent Policy (LAP) frameworks. For credit assignment, MS-GRPO attributes the entire cumulative episode reward to each individual episode step. We supplement this algorithm with a novel absolute-advantage-weighted episode sampling strategy that we show improves training performance. We evaluate our approach by post-training a 3-billion parameter model on Snake and Frozen Lake. Our experiments demonstrate that the method is effective in improving decision-making performance: our post-trained 3B parameter model outperforms a 72B parameter baseline by 50% on the Frozen Lake task. This work demonstrates that targeted post-training is a practical and efficient alternative to relying on model scale for creating sequential decision-making agents using LLMs.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.10838v1" target="_blank">Self-Supervised Stereo Matching with Multi-Baseline Contrastive Learning</a></h3>
                    <p><strong>Authors:</strong> Peng Xu, Zhiyu Xiang, Jingyun Fu, Tianyu Pu, Kai Wang, Chaojie Ji, Tingming Bai, Eryun Liu</p>
                    <p><strong>Published:</strong> 8/14/2025</p>
                    <p><strong>Categories:</strong> cs.CV</p>
                    <p><strong>Summary:</strong> Current self-supervised stereo matching relies on the photometric consistency assumption, which breaks down in occluded regions due to ill-posed correspondences. To address this issue, we propose BaCon-Stereo, a simple yet effective contrastive learning framework for self-supervised stereo network training in both non-occluded and occluded regions. We adopt a teacher-student paradigm with multi-baseline inputs, in which the stereo pairs fed into the teacher and student share the same reference view but differ in target views. Geometrically, regions occluded in the students target view are often visible in the teachers, making it easier for the teacher to predict in these regions. The teachers prediction is rescaled to match the students baseline and then used to supervise the student. We also introduce an occlusion-aware attention map to better guide the student in learning occlusion completion. To support training, we synthesize a multi-baseline dataset BaCon-20k. Extensive experiments demonstrate that BaCon-Stereo improves prediction in both occluded and non-occluded regions, achieves strong generalization and robustness, and outperforms state-of-the-art self-supervised methods on both KITTI 2015 and 2012 benchmarks. Our code and dataset will be released upon paper acceptance.</p>
                
            
                
                    <h3><a href="http://dx.doi.org/10.1088/1361-6382/adf40a" target="_blank">Unraveling the Hubble tension with warm inflation</a></h3>
                    <p><strong>Authors:</strong> Anupama B, P K Suresh</p>
                    <p><strong>Published:</strong> 8/14/2025</p>
                    <p><strong>Categories:</strong> gr-qc</p>
                    <p><strong>Summary:</strong> The validity of warm inflation is investigated in the light of recent CMB missions in both strong and weak dissipative regimes. The tensor to scalar ratio of various inflationary models is found to be consistent with the recent CMB results for different models of warm inflation. The role of dissipation on the popular models of warm inflation in the context of supersymmetry and string theory is investigated. Further, the effect of dissipation coefficient of warm inflation on the Hubble parameter and its role in accounting the Hubble tension is examined. Warm inflation embodies superstring theory and can provide a platform to test quantum gravity in multi field scenario.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.10833v1" target="_blank">UI-Venus Technical Report: Building High-performance UI Agents with RFT</a></h3>
                    <p><strong>Authors:</strong> Zhangxuan Gu, Zhengwen Zeng, Zhenyu Xu, Xingran Zhou, Shuheng Shen, Yunfei Liu, Beitong Zhou, Changhua Meng, Tianyu Xia, Weizhi Chen, Yue Wen, Jingya Dou, Fei Tang, Jinzhen Lin, Yulin Liu, Zhenlin Guo, Yichen Gong, Heng Jia, Changlong Gao, Yuan Guo, Yong Deng, Zhenyu Guo, Liang Chen, Weiqiang Wang</p>
                    <p><strong>Published:</strong> 8/14/2025</p>
                    <p><strong>Categories:</strong> cs.CV</p>
                    <p><strong>Summary:</strong> We present UI-Venus, a native UI agent that takes only screenshots as input based on a multimodal large language model. UI-Venus achieves SOTA performance on both UI grounding and navigation tasks using only several hundred thousand high-quality training samples through reinforcement finetune (RFT) based on Qwen2.5-VL. Specifically, the 7B and 72B variants of UI-Venus obtain 94.1% / 50.8% and 95.3% / 61.9% on the standard grounding benchmarks, i.e., Screenspot-V2 / Pro, surpassing the previous SOTA baselines including open-source GTA1 and closed-source UI-TARS-1.5.To show UI-Venuss summary and planing ability, we also evaluate it on the AndroidWorld, an online UI navigation arena, on which our 7B and 72B variants achieve 49.1% and 65.9% success rate, also beating existing models.To achieve this, we introduce carefully designed reward functions for both UI grounding and navigation tasks and corresponding efficient data cleaning strategies.To further boost navigation performance, we propose Self-Evolving Trajectory History Alignment \ Sparse Action Enhancement that refine historical reasoning traces and balances the distribution of sparse but critical actions, leading to more coherent planning and better generalization in complex UI tasks. Our contributions include the publish of SOTA open-source UI agents, comprehensive data cleaning protocols and a novel self-evolving framework for improving navigation performance, which encourage further research and development in the community. Code is available at https://github.com/antgroup/UI-Venus.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.10832v1" target="_blank">Field-free superconducting diode effect in two-dimensional Shiba lattices</a></h3>
                    <p><strong>Authors:</strong> Sayak Bhowmik, Dibyendu Samanta, Ashis K. Nandy, Arijit Saha, Sudeep Kumar Ghosh</p>
                    <p><strong>Published:</strong> 8/14/2025</p>
                    <p><strong>Categories:</strong> cond-mat.supr-con, cond-mat.mes-hall, cond-mat.mtrl-sci</p>
                    <p><strong>Summary:</strong> The superconducting diode effect (SDE) refers to non-reciprocal transport, where current flows without resistance in one direction but becomes resistive in the opposite direction, but its typical reliance on magnetic field hinders scalability and device integration. In this article, we present a theoretical framework for realizing a field-free SDE based on a two-dimensional (2D) Shiba lattice featuring a conical spin texture. Using the real-space Bogoliubov-de Gennes (BdG) calculations, we illustrate that the conical spin configuration alone is sufficient to break the necessary inversion and time reversal symmetries, enabling nonreciprocal supercurrent flow without any external magnetic field, yielding diode efficiency exceeding 40%. Furthermore, we find that the efficiency of such a diode effect becomes strongly dependent on the direction of current flow, revealing a pronounced angular dependence that can be tuned by varying the pitches of the spin texture along the two spatial lattice directions. Our findings offer a pathway toward scalable, field-free superconducting components for non-dissipative electronics and quantum technologies.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.10831v1" target="_blank">Scalable FAS: A New Paradigm for Array Signal Processing</a></h3>
                    <p><strong>Authors:</strong> Tuo Wu, Ye Tian, Jie Tang, Kangda Zhi, Maged Elkashlan, Kin-Fai Tong, Naofal Al-Dhahir, Chan-Byoung Chae, Matthew C. Valenti, George K. Karagiannidis, Kwai-Man Luk</p>
                    <p><strong>Published:</strong> 8/14/2025</p>
                    <p><strong>Categories:</strong> eess.SP</p>
                    <p><strong>Summary:</strong> Most existing antenna array-based source localization methods rely on fixed-position arrays (FPAs) and strict assumptions about source field conditions (near-field or far-field), which limits their effectiveness in complex, dynamic real-world scenarios where high-precision localization is required. In contrast, this paper introduces a novel scalable fluid antenna system (SFAS) that can dynamically adjust its aperture configuration to optimize performance for different localization tasks. Within this framework, we develop a two-stage source localization strategy based on the exact spatial geometry (ESG) model: the first stage uses a compact aperture configuration for initial direction-of-arrival (DOA) estimation, while the second stage employs an expanded aperture for enhanced DOA and range estimation. The proposed approach eliminates the traditional need for signal separation or isolation to classify source types and enables a single SFAS array to achieve high localization accuracy without field-specific assumptions, model simplifications, or approximations, representing a new paradigm in array-based source localization. Extensive simulations demonstrate the superiority of the proposed method in terms of localization accuracy, computational efficiency, and robustness to different source types.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.10828v1" target="_blank">A Multimodal Neural Network for Recognizing Subjective Self-Disclosure Towards Social Robots</a></h3>
                    <p><strong>Authors:</strong> Henry Powell, Guy Laban, Emily S. Cross</p>
                    <p><strong>Published:</strong> 8/14/2025</p>
                    <p><strong>Categories:</strong> cs.RO, cs.AI</p>
                    <p><strong>Summary:</strong> Subjective self-disclosure is an important feature of human social interaction. While much has been done in the social and behavioural literature to characterise the features and consequences of subjective self-disclosure, little work has been done thus far to develop computational systems that are able to accurately model it. Even less work has been done that attempts to model specifically how human interactants self-disclose with robotic partners. It is becoming more pressing as we require social robots to work in conjunction with and establish relationships with humans in various social settings. In this paper, our aim is to develop a custom multimodal attention network based on models from the emotion recognition literature, training this model on a large self-collected self-disclosure video corpus, and constructing a new loss function, the scale preserving cross entropy loss, that improves upon both classification and regression versions of this problem. Our results show that the best performing model, trained with our novel loss function, achieves an F1 score of 0.83, an improvement of 0.48 from the best baseline model. This result makes significant headway in the aim of allowing social robots to pick up on an interaction partners self-disclosures, an ability that will be essential in social robots with social cognition.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.10824v1" target="_blank">Memory-Augmented Transformers: A Systematic Review from Neuroscience Principles to Technical Solutions</a></h3>
                    <p><strong>Authors:</strong> Parsa Omidi, Xingshuai Huang, Axel Laborieux, Bahareh Nikpour, Tianyu Shi, Armaghan Eshaghi</p>
                    <p><strong>Published:</strong> 8/14/2025</p>
                    <p><strong>Categories:</strong> cs.LG, cs.CL</p>
                    <p><strong>Summary:</strong> Memory is fundamental to intelligence, enabling learning, reasoning, and adaptability across biological and artificial systems. While Transformer architectures excel at sequence modeling, they face critical limitations in long-range context retention, continual learning, and knowledge integration. This review presents a unified framework bridging neuroscience principles, including dynamic multi-timescale memory, selective attention, and consolidation, with engineering advances in Memory-Augmented Transformers. We organize recent progress through three taxonomic dimensions: functional objectives (context extension, reasoning, knowledge integration, adaptation), memory representations (parameter-encoded, state-based, explicit, hybrid), and integration mechanisms (attention fusion, gated control, associative retrieval). Our analysis of core memory operations (reading, writing, forgetting, and capacity management) reveals a shift from static caches toward adaptive, test-time learning systems. We identify persistent challenges in scalability and interference, alongside emerging solutions including hierarchical buffering and surprise-gated updates. This synthesis provides a roadmap toward cognitively-inspired, lifelong-learning Transformer architectures.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.10822v1" target="_blank">Gauging the variational optimization of projected entangled-pair states</a></h3>
                    <p><strong>Authors:</strong> Wei Tang, Laurens Vanderstraeten, Jutho Haegeman</p>
                    <p><strong>Published:</strong> 8/14/2025</p>
                    <p><strong>Categories:</strong> cond-mat.str-el, quant-ph</p>
                    <p><strong>Summary:</strong> Projected entangled-pair states (PEPS) constitute a powerful variational ansatz for capturing ground state physics of two-dimensional quantum systems. However, accurately computing and minimizing the energy expectation value remains challenging, in part because the impact of the gauge degrees of freedom that are present in the tensor network representation is poorly understood. We analyze the role of gauge transformations for the case of a U(1)-symmetric PEPS with point group symmetry, thereby reducing the gauge degrees of freedom to a single class. We show how gradient-based optimization strategies exploit the gauge freedom, causing the tensor network contraction to become increasingly inaccurate and to produce artificially low variational energies. Furthermore, we develop a gauge-fixed optimization strategy that largely suppresses this effect, resulting in a more robust optimization. Our study underscores the need for gauge-aware optimization strategies to guarantee reliability of variational PEPS in general settings.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.10820v1" target="_blank">Fluid Antenna Enabled Direction-of-Arrival Estimation Under Time-Constrained Mobility</a></h3>
                    <p><strong>Authors:</strong> He Xu, Tuo Wu, Ye Tian, Kangda Zhi, Wei Liu, Baiyang Liu, Hing Cheung So, Naofal Al-Dhahir, Kin-Fai Tong, Chan-Byoung Chae, Kai-Kit Wong</p>
                    <p><strong>Published:</strong> 8/14/2025</p>
                    <p><strong>Categories:</strong> eess.SP</p>
                    <p><strong>Summary:</strong> Fluid antenna (FA) technology has emerged as a promising approach in wireless communications due to its capability of providing increased degrees of freedom (DoFs) and exceptional design flexibility. This paper addresses the challenge of direction-of-arrival (DOA) estimation for aligned received signals (ARS) and non-aligned received signals (NARS) by designing two specialized uniform FA structures under time-constrained mobility. For ARS scenarios, we propose a fully movable antenna configuration that maximizes the virtual array aperture, whereas for NARS scenarios, we design a structure incorporating a fixed reference antenna to reliably extract phase information from the signal covariance. To overcome the limitations of large virtual arrays and limited sample data inherent in time-varying channels (TVC), we introduce two novel DOA estimation methods: TMRLS-MUSIC for ARS, combining Toeplitz matrix reconstruction (TMR) with linear shrinkage (LS) estimation, and TMR-MUSIC for NARS, utilizing sub-covariance matrices to construct virtual array responses. Both methods employ Nystrom approximation to significantly reduce computational complexity while maintaining estimation accuracy. Theoretical analyses and extensive simulation results demonstrate that the proposed methods achieve underdetermined DOA estimation using minimal FA elements, outperform conventional methods in estimation accuracy, and substantially reduce computational complexity.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.10819v1" target="_blank">Concentration-Free Quantum Kernel Learning in the Rydberg Blockade</a></h3>
                    <p><strong>Authors:</strong> Ayana Sarkar, Martin Schnee, Roya Radgohar, Mojde Fadaie, Victor Drouin-Touchette, Stefanos Kourtis</p>
                    <p><strong>Published:</strong> 8/14/2025</p>
                    <p><strong>Categories:</strong> cond-mat.str-el, quant-ph</p>
                    <p><strong>Summary:</strong> Quantum kernel methods (QKMs) offer an appealing framework for machine learning on near-term quantum computers. However, QKMs generically suffer from exponential concentration, requiring an exponential number of measurements to resolve the kernel values, with the exception of trivial (i.e., classically simulable) kernels. Here we propose a QKM that is free of exponential concentration, yet remains hard to simulate classically. Our QKM utilizes the weak ergodicity-breaking many-body dynamics in the Rydberg blockade of coherently driven neutral atom arrays. We demonstrate the fundamental properties of our QKM by analytically solving an approximate toy model of its underpinning quantum dynamics, as well as by extensive numerical simulations on randomly generated datasets. We further show that the proposed kernel exhibits effective learning on real data. The proposed QKM can be implemented in current neutral atom quantum computers.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.10817v1" target="_blank">Mobile-Friendly Deep Learning for Plant Disease Detection: A Lightweight CNN Benchmark Across 101 Classes of 33 Crops</a></h3>
                    <p><strong>Authors:</strong> Anand Kumar, Harminder Pal Monga, Tapasi Brahma, Satyam Kalra, Navas Sherif</p>
                    <p><strong>Published:</strong> 8/14/2025</p>
                    <p><strong>Categories:</strong> cs.CV, cs.LG</p>
                    <p><strong>Summary:</strong> Plant diseases are a major threat to food security globally. It is important to develop early detection systems which can accurately detect. The advancement in computer vision techniques has the potential to solve this challenge. We have developed a mobile-friendly solution which can accurately classify 101 plant diseases across 33 crops. We built a comprehensive dataset by combining different datasets, Plant Doc, PlantVillage, and PlantWild, all of which are for the same purpose. We evaluated performance across several lightweight architectures - MobileNetV2, MobileNetV3, MobileNetV3-Large, and EfficientNet-B0, B1 - specifically chosen for their efficiency on resource-constrained devices. The results were promising, with EfficientNet-B1 delivering our best performance at 94.7% classification accuracy. This architecture struck an optimal balance between accuracy and computational efficiency, making it well-suited for real-world deployment on mobile devices.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.10816v1" target="_blank">Unified Theory of Dark Count Rate and System Detection Efficiency for NbN, WSi Based Superconducting Single Photon Detectors</a></h3>
                    <p><strong>Authors:</strong> Daien He, Leif Bauer, Sathwik Bharadwaj, Zubin Jacob</p>
                    <p><strong>Published:</strong> 8/14/2025</p>
                    <p><strong>Categories:</strong> cond-mat.supr-con, quant-ph</p>
                    <p><strong>Summary:</strong> Predicting the behavior of superconducting nanowire single photon detectors (SNSPDs) is important as their use becomes more widespread in fields ranging from quantum computing to quantum remote sensing. Here, we present a vortex crossing theory of photon detection which provides a unified definition of system detection efficiency and dark count rates. Our approach quantitatively captures the plateau region of system detection efficiency for NbN and WSi based SNSPDs. We concurrently predict the temperature dependence of dark count rates and the intrinsic timing jitter of SNSPDs. We extensively benchmark our model against various experiments to aid in the design of the next generation of SNSPDs.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.10815v1" target="_blank">Comparison of Data Reduction Criteria for Online Gaussian Processes</a></h3>
                    <p><strong>Authors:</strong> Thore Wietzke, Knut Graichen</p>
                    <p><strong>Published:</strong> 8/14/2025</p>
                    <p><strong>Categories:</strong> cs.LG, stat.ML</p>
                    <p><strong>Summary:</strong> Gaussian Processes (GPs) are widely used for regression and system identification due to their flexibility and ability to quantify uncertainty. However, their computational complexity limits their applicability to small datasets. Moreover in a streaming scenario, more and more datapoints accumulate which is intractable even for Sparse GPs. Online GPs aim to alleviate this problem by e.g. defining a maximum budget of datapoints and removing redundant datapoints. This work provides a unified comparison of several reduction criteria, analyzing both their computational complexity and reduction behavior. The criteria are evaluated on benchmark functions and real-world datasets, including dynamic system identification tasks. Additionally, acceptance criteria are proposed to further filter out redundant datapoints. This work yields practical guidelines for choosing a suitable criterion for an online GP algorithm.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.10814v1" target="_blank">A Suspended 4H-Silicon Carbide Membrane Platform for Defect Integration into Quantum Devices</a></h3>
                    <p><strong>Authors:</strong> Amberly H. Xie, Aaron M. Day, Jonathan R. Dietz, Chang Jin, Chaoshen Zhang, Eliana Mann, Zhujing Xu, Marko Loncar, Evelyn L. Hu</p>
                    <p><strong>Published:</strong> 8/14/2025</p>
                    <p><strong>Categories:</strong> physics.app-ph</p>
                    <p><strong>Summary:</strong> 4H-silicon carbide is a promising platform for solid-state quantum technology due to its commercial availability as a wide bandgap semiconductor and ability to host numerous spin-active color centers. Integrating color centers into suspended nanodevices enhances defect control and readout--key advances needed to fully harness their potential. However, challenges in developing robust fabrication processes for 4H-SiC thin films--due to the materials chemical and mechanical stability--limit their implementation in quantum applications. Here, we report on a new fabrication approach that first synthesizes suspended thin films from a monolithic platform, then patterns devices. With this technique, we fabricate and characterize structures tailored for defect integration, demonstrating 1D photonic crystal cavities, with and without waveguide interfaces, and lithium niobate on 4H-SiC acoustic cavities. This approach allows for greater fabrication flexibility--supporting high temperature annealing and heterogeneous material platform compatibility--providing a versatile platform for scalable fabrication of 4H-SiC devices for quantum technologies.</p>
                
            
        
        
            <p><em>Generated by AI News Agent</em></p>
        
    

