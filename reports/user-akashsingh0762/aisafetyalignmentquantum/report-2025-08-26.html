
    
        <h1>ðŸ¤– AI Research Report</h1>
        
            <strong>Date:</strong> 2025-08-26<br>
            <strong>Topics:</strong> ai safety research, ai alignment research, quantum computing<br>
            <strong>Papers Found:</strong> 60
        
        
        
            
                <h2>ðŸ¤– AI Summary</h2>
                <p>## ai safety research

The recent body of research on AI safety encompasses a diverse array of studies addressing both the physical and algorithmic dimensions of safety in AI systems. A notable trend involves the integration of safety constraints within AI models to mitigate risks associated with physical systems. For instance, the SafeBimanual framework enhances bimanual robot manipulation by imposing trajectory optimization constraints to prevent collisions and damage, achieving significant improvements in both safety and task success rates. This highlights a growing emphasis on embedding safety constraints directly into AI systems to ensure safer interactions with the physical environment.

Additionally, the exploration of AIs ability to assist in safety-critical applications is evident in studies like the BirdRecorder system, which employs AI to detect and prevent bird collisions with wind turbines. Such applications underscore AIs potential to balance technological advancement with environmental and wildlife conservation, thereby addressing broader safety and ethical concerns. Furthermore, the development of frameworks like MIRAGE for medical question-answering emphasizes the importance of accuracy and traceability, key components of safety in AI-driven decision-making processes. This diverse research landscape illustrates ongoing efforts to ensure that AI systems are not only effective but also align with safety, ethical, and environmental standards.

*Based on 50 research papers*

---

## ai alignment research

The provided research papers, although diverse in their focus, collectively highlight advancements in AI technology that bear implications for AI alignmentâ€”a field concerned with ensuring AI systems act in accordance with human intentions and values. Notably, the papers explore improvements in AI reasoning, adaptability, and interaction with human users, all of which are crucial for alignment.

InternVL3.5 and MIRAGE present significant breakthroughs in enhancing AI reasoning capabilities and efficiency, which are pivotal for alignment as they improve AIs ability to understand and respond to complex human instructions accurately. InternVL3.5 introduces a Cascade Reinforcement Learning framework that allows for more refined alignment in reasoning tasks, while MIRAGE leverages structured knowledge graphs for more accurate and interpretable reasoning in medical contexts. These innovations contribute to alignment by ensuring AI systems can not only perform tasks effectively but also do so in a way that is interpretable and traceable, addressing a key challenge in AI alignment.

Similarly, the development of AI systems like those evaluated in the Caregiver-in-the-Loop AI study emphasizes the role of human feedback in refining AI behavior, aligning it more closely with human expectations and needs. This integration of human feedback loops into AI systems, as demonstrated in the study, is a critical aspect of alignment, ensuring that AI systems remain responsive and adaptable to their users. Collectively, these papers underscore the importance of enhancing AIâ€™s reasoning, feedback integration, and transparency to advance AI alignment, making AI systems more trustworthy and reliable in complex, real-world applications.

*Based on 5 research papers*

---

## quantum computing

The provided research papers primarily focus on advancements in multimodal models, large reasoning models, and optimization techniques, none of which directly address quantum computing. However, trends and breakthroughs in these areas can indirectly impact quantum computing, particularly through improvements in computational efficiency, reasoning capabilities, and optimization methods.

For instance, the development of advanced multimodal models, such as InternVL3.5, which enhances reasoning capabilities and efficiency through novel frameworks like Cascade Reinforcement Learning and Visual Resolution Router, could inspire similar strategies in quantum algorithms to optimize resource usage and improve problem-solving effectiveness. Similarly, MMToks approach to maximizing coverage in vision-language models by leveraging multimodal information highlights the importance of integrating diverse data sources, a concept that could be applied to quantum computing to enhance data processing and interpretation.

Furthermore, breakthroughs in reasoning frameworks like MIRAGE, which employs structured graph-based reasoning, could inform the design of quantum algorithms that require complex data analysis and decision-making processes. As quantum computing evolves, the adoption of advanced optimization techniques, such as those proposed with Ano, could enhance quantum algorithm performance in noisy environments, potentially improving the robustness and efficiency of quantum systems in real-world applications.

Overall, while these papers do not address quantum computing directly, the innovations they present in computational efficiency, reasoning, and optimization offer valuable insights and methodologies that could be adapted to advance the field of quantum computing.

*Based on 5 research papers*</p>
            
        
        
        <h2>ðŸ“š Research Papers</h2>
        
                
                    <h3><a href="http://arxiv.org/abs/2508.18270v1" target="_blank">Proving it is impossible; on ErdÅ‘s problem $\# 278$</a></h3>
                    <p><strong>Authors:</strong> Stijn Cambie</p>
                    <p><strong>Published:</strong> 8/25/2025</p>
                    <p><strong>Categories:</strong> math.CO, 05-01</p>
                    <p><strong>Summary:</strong> Erd\H{o}s asked many mathematical questions. Some lead to exciting research, others turned out to be easily solved. In this article, we provide evidence that one of his questions, Erd\H{o}s problem \#278 , has no general answer. We do so by relating it with a hard knapsack problem instance,and by demonstrating that different, non-equivalent formulas arise depending on the structure of the moduli.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.18268v1" target="_blank">SafeBimanual: Diffusion-based Trajectory Optimization for Safe Bimanual Manipulation</a></h3>
                    <p><strong>Authors:</strong> Haoyuan Deng, Wenkai Guo, Qianzhun Wang, Zhenyu Wu, Ziwei Wang</p>
                    <p><strong>Published:</strong> 8/25/2025</p>
                    <p><strong>Categories:</strong> cs.RO, cs.AI</p>
                    <p><strong>Summary:</strong> Bimanual manipulation has been widely applied in household services and manufacturing, which enables the complex task completion with coordination requirements. Recent diffusion-based policy learning approaches have achieved promising performance in modeling action distributions for bimanual manipulation. However, they ignored the physical safety constraints of bimanual manipulation, which leads to the dangerous behaviors with damage to robots and objects. To this end, we propose a test-time trajectory optimization framework named SafeBimanual for any pre-trained diffusion-based bimanual manipulation policies, which imposes the safety constraints on bimanual actions to avoid dangerous robot behaviors with improved success rate. Specifically, we design diverse cost functions for safety constraints in different dual-arm cooperation patterns including avoidance of tearing objects and collision between arms and objects, which optimizes the manipulator trajectories with guided sampling of diffusion denoising process. Moreover, we employ a vision-language model (VLM) to schedule the cost functions by specifying keypoints and corresponding pairwise relationship, so that the optimal safety constraint is dynamically generated in the entire bimanual manipulation process. SafeBimanual demonstrates superiority on 8 simulated tasks in RoboTwin with a 13.7% increase in success rate and a 18.8% reduction in unsafe interactions over state-of-the-art diffusion-based methods. Extensive experiments on 4 real-world tasks further verify its practical value by improving the success rate by 32.5%.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.18267v1" target="_blank">Caregiver-in-the-Loop AI: A Simulation-Based Feasibility Study for Dementia Task Verification</a></h3>
                    <p><strong>Authors:</strong> Joy Lai, David Black, Kelly Beaton, Bing Ye, Alex Mihailidis</p>
                    <p><strong>Published:</strong> 8/25/2025</p>
                    <p><strong>Categories:</strong> cs.HC</p>
                    <p><strong>Summary:</strong> Caregivers of people living with dementia (PLwD) experience stress when verifying whether tasks are truly completed, even with digital reminder systems. Generative AI, such as GPT-4, may help by automating task verification through follow-up questioning and decision support. This feasibility study evaluates an AI-powered task verification system integrated with digital reminders for PLwD. It examines (1) GPT-4s ability to generate effective follow-up questions, (2) the accuracy of an AI-driven response flagging mechanism, and (3) the role of caregiver feedback in refining system adaptability. A simulated pipeline was tested on 64 anonymized reminders. GPT-4 generated follow-up questions with and without contextual information about PLwD routines. Responses were classified into High, Medium, or Low concern, and simulated caregiver feedback was used to refine outputs. Results show that contextual information and caregiver input improved the clarity and relevance of AI-generated questions. The flagging system accurately identified concerns, particularly for safety-critical tasks, though subjective or non-urgent tasks remained challenging. Findings demonstrate the feasibility of AI-assisted task verification in dementia care. Context-aware AI prompts and caregiver feedback can enhance task monitoring, reduce caregiver stress, and strengthen PLwD support. Future work should focus on real-world validation and scalability.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.18260v1" target="_blank">MIRAGE: Scaling Test-Time Inference with Parallel Graph-Retrieval-Augmented Reasoning Chains</a></h3>
                    <p><strong>Authors:</strong> Kaiwen Wei, Rui Shan, Dongsheng Zou, Jianzhong Yang, Bi Zhao, Junnan Zhu, Jiang Zhong</p>
                    <p><strong>Published:</strong> 8/25/2025</p>
                    <p><strong>Categories:</strong> cs.CL, I.2.3; I.2.4; I.2.7</p>
                    <p><strong>Summary:</strong> Large reasoning models (LRMs) have shown significant progress in test-time scaling through chain-of-thought prompting. Current approaches like search-o1 integrate retrieval augmented generation (RAG) into multi-step reasoning processes but rely on a single, linear reasoning chain while incorporating unstructured textual information in a flat, context-agnostic manner. As a result, these approaches can lead to error accumulation throughout the reasoning chain, which significantly limits its effectiveness in medical question-answering (QA) tasks where both accuracy and traceability are critical requirements. To address these challenges, we propose MIRAGE (Multi-chain Inference with Retrieval-Augmented Graph Exploration), a novel test-time scalable reasoning framework that performs dynamic multi-chain inference over structured medical knowledge graphs. Specifically, MIRAGE 1) decomposes complex queries into entity-grounded sub-questions, 2) executes parallel inference chains, 3) retrieves evidence adaptively via neighbor expansion and multi-hop traversal, and 4) integrates answers using cross-chain verification to resolve contradictions. Experiments on three medical QA benchmarks (GenMedGPT-5k, CMCQA, and ExplainCPE) show that MIRAGE consistently outperforms GPT-4o, Tree-of-Thought variants, and other retrieval-augmented baselines in both automatic and human evaluations. Additionally, MIRAGE improves interpretability by generating explicit reasoning chains that trace each factual claim to concrete chains within the knowledge graph, making it well-suited for complex medical reasoning scenarios. The code will be available for further research.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.18255v1" target="_blank">Hermes 4 Technical Report</a></h3>
                    <p><strong>Authors:</strong> Ryan Teknium, Roger Jin, Jai Suphavadeeprasit, Dakota Mahan, Jeffrey Quesnelle, Joe Li, Chen Guang, Shannon Sands, Karan Malhotra</p>
                    <p><strong>Published:</strong> 8/25/2025</p>
                    <p><strong>Categories:</strong> cs.AI</p>
                    <p><strong>Summary:</strong> We present Hermes 4, a family of hybrid reasoning models that combine structured, multi-turn reasoning with broad instruction-following ability. We describe the challenges encountered during data curation, synthesis, training, and evaluation, and outline the solutions employed to address these challenges at scale. We comprehensively evaluate across mathematical reasoning, coding, knowledge, comprehension, and alignment benchmarks, and we report both quantitative performance and qualitative behavioral analysis. To support open research, all model weights are published publicly at https://huggingface.co/collections/NousResearch/hermes-4-collection-68a731bfd452e20816725728</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.18243v1" target="_blank">Enhanced Tritium Production in Irradiated TiD2 from Collisional Fusion in the Solid-State</a></h3>
                    <p><strong>Authors:</strong> Andrew K. Gillespie, Cuikun Lin, Ian Jones, Brad Jeffries, Joseph Caleb Philipps, Sandeep Puri, John Gahl, John Brockman, R. V. Duncan</p>
                    <p><strong>Published:</strong> 8/25/2025</p>
                    <p><strong>Categories:</strong> nucl-ex</p>
                    <p><strong>Summary:</strong> Ongoing research in new nuclear mechanisms hold the potential for beneficial developments in nuclear power cycle designs. Recent reports investigated the possibility of lattice dynamics to influence nuclear processes in metals. Results from Steinetz et al., at the NASA Glenn Research Center indicated that it may be feasible to initiate deuterium deuterium fusion reactions that are enhanced using electron screening to reduce the deuterium deuterium fusion barrier. This article presents tritium production results from both simulations and experiments targeting specific nuclear processes in an effort to identify the source of higher energy neutrons observed in those results. We explore two pathways of tritium generation in TiD2 through this fusion cycle. Tritium production from TiD2 in the University of Missouri Research Reactor, where the neutron spectrum was approximately 90 percent thermal, was within 25 percent of the predicted amount from simulations, and well explained by known nuclear reactions without invoking screening enhanced recoil-induced fusion. Tritium production from TiD2 in the cyclotron vault at MURR, where the neutron spectrum was completely energetic with almost no thermal neutrons, was a factor of 2.9 to 5.1 times higher than predicted from simulations using known nuclear reactions. This indicates the likelihood of an additional mechanism, such as collision-induced fusion in the solid state, increasing the credibility in the results from Steinetz et al.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.18226v1" target="_blank">Disentangling the Factors of Convergence between Brains and Computer Vision Models</a></h3>
                    <p><strong>Authors:</strong> JosÃ©phine Raugel, Marc Szafraniec, Huy V. Vo, Camille Couprie, Patrick Labatut, Piotr Bojanowski, Valentin Wyart, Jean-RÃ©mi King</p>
                    <p><strong>Published:</strong> 8/25/2025</p>
                    <p><strong>Categories:</strong> cs.AI, q-bio.NC</p>
                    <p><strong>Summary:</strong> Many AI models trained on natural images develop representations that resemble those of the human brain. However, the factors that drive this brain-model similarity remain poorly understood. To disentangle how the model, training and data independently lead a neural network to develop brain-like representations, we trained a family of self-supervised vision transformers (DINOv3) that systematically varied these different factors. We compare their representations of images to those of the human brain recorded with both fMRI and MEG, providing high resolution in spatial and temporal analyses. We assess the brain-model similarity with three complementary metrics focusing on overall representational similarity, topographical organization, and temporal dynamics. We show that all three factors - model size, training amount, and image type - independently and interactively impact each of these brain similarity metrics. In particular, the largest DINOv3 models trained with the most human-centric images reach the highest brain-similarity. This emergence of brain-like representations in AI models follows a specific chronology during training: models first align with the early representations of the sensory cortices, and only align with the late and prefrontal representations of the brain with considerably more training. Finally, this developmental trajectory is indexed by both structural and functional properties of the human cortex: the representations that are acquired last by the models specifically align with the cortical areas with the largest developmental expansion, thickness, least myelination, and slowest timescales. Overall, these findings disentangle the interplay between architecture and experience in shaping how artificial neural networks come to see the world as humans do, thus offering a promising framework to understand how the human brain comes to represent its visual world.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.18214v1" target="_blank">AI Data Centers Need Pioneers to Deliver Scalable Power via Offgrid AI</a></h3>
                    <p><strong>Authors:</strong> Steven P. Reinhardt</p>
                    <p><strong>Published:</strong> 8/25/2025</p>
                    <p><strong>Categories:</strong> eess.SY, cs.CY, cs.SY, physics.soc-ph</p>
                    <p><strong>Summary:</strong> The scalable computing revolution of the late 80s through mid- 00s forged a new technical and economic model for computing that delivered massive societal impact, but its economic benefit has driven scalability to sizes that are now exhausting the energy grids capacity. Our time demands a new revolution in scalable energy, mirroring in key ways the scalable computing revolution; e.g., compelling economic forces, use of mass-market components, overcoming foibles of those components, judicious use of physical locality, and the the difficult integration into an effective system. The offgrid AI approach closely fits this mold, combining local mostly renewable generation and storage to power an AI data center, starting offgrid. Obstacles to delivering this approach are social, technical, and project, but the potential is massive. I argue that the offgrid-AI approach needs pioneers among both system developers and AI-data-center operators to move it quickly from concept to large-scale deployment.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.18212v1" target="_blank">Better Language Model-Based Judging Reward Modeling through Scaling Comprehension Boundaries</a></h3>
                    <p><strong>Authors:</strong> Meiling Ning, Zhongbao Zhang, Junda Ye, Jiabao Guo, Qingyuan Guan</p>
                    <p><strong>Published:</strong> 8/25/2025</p>
                    <p><strong>Categories:</strong> cs.CL</p>
                    <p><strong>Summary:</strong> The emergence of LM-based judging reward modeling, represented by generative reward models, has successfully made reinforcement learning from AI feedback (RLAIF) efficient and scalable. To further advance this paradigm, we propose a core insight: this form of reward modeling shares fundamental formal consistency with natural language inference (NLI), a core task in natural language understanding. This reframed perspective points to a key path for building superior reward models: scaling the models comprehension boundaries. Pursuing this path, exploratory experiments on NLI tasks demonstrate that the slot prediction masked language models (MLMs) incorporating contextual explanations achieve significantly better performance compared to mainstream autoregressive models. Based on this key finding, we propose ESFP-RM, a two-stage LM-based judging reward model that utilizes an explanation based slot framework for prediction to fully leverage the advantages of MLMs. Extensive experiments demonstrate that in both reinforcement learning from human feedback (RLHF) and out-of-distribution (OOD) scenarios, the ESFP-RM framework delivers more stable and generalizable reward signals compared to generative reward models.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.18208v1" target="_blank">Exploring the Interplay between Musical Preferences and Personality through the Lens of Language</a></h3>
                    <p><strong>Authors:</strong> Eliran Shem-Tov, Ella Rabinovich</p>
                    <p><strong>Published:</strong> 8/25/2025</p>
                    <p><strong>Categories:</strong> cs.CL</p>
                    <p><strong>Summary:</strong> Music serves as a powerful reflection of individual identity, often aligning with deeper psychological traits. Prior research has established correlations between musical preferences and personality traits, while separate studies have demonstrated that personality is detectable through linguistic analysis. Our study bridges these two research domains by investigating whether individuals musical preferences are recognizable in their spontaneous language through the lens of the Big Five personality traits (Openness, Conscientiousness, Extroversion, Agreeableness, and Neuroticism). Using a carefully curated dataset of over 500,000 text samples from nearly 5,000 authors with reliably identified musical preferences, we build advanced models to assess personality characteristics. Our results reveal significant personality differences across fans of five musical genres. We release resources for future research at the intersection of computational linguistics, music psychology and personality analysis.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.18203v1" target="_blank">Tractable Stochastic Hybrid Model Predictive Control using Gaussian Processes for Repetitive Tasks in Unseen Environments</a></h3>
                    <p><strong>Authors:</strong> Leroy DSouza, Yash Vardhan Pant, Sebastian Fischmeister</p>
                    <p><strong>Published:</strong> 8/25/2025</p>
                    <p><strong>Categories:</strong> eess.SY, cs.SY, math.OC</p>
                    <p><strong>Summary:</strong> Improving the predictive accuracy of a dynamics model is crucial to obtaining good control performance and safety from Model Predictive Controllers (MPC). One approach involves learning unmodelled (residual) dynamics, in addition to nominal models derived from first principles. Varying residual models across an environment manifest as modes of a piecewise residual (PWR) model that requires a) identifying how modes are distributed across the environment and b) solving a computationally intensive Mixed Integer Nonlinear Program (MINLP) problem for control. We develop an iterative mapping algorithm capable of predicting time-varying mode distributions. We then develop and solve two tractable approximations of the MINLP to combine with the predictor in closed-loop to solve the overall control problem. In simulation, we first demonstrate how the approximations improve performance by 4-18% in comparison to the MINLP while achieving significantly lower computation times (upto 250x faster). We then demonstrate how the proposed mapping algorithm incrementally improves controller performance (upto 3x) over multiple iterations of a trajectory tracking control task even when the mode distributions change over time.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.18202v1" target="_blank">Uncertain data assimilation for urban wind flow simulations with OpenLB-UQ</a></h3>
                    <p><strong>Authors:</strong> Mingliang Zhong, Dennis Teutscher, Adrian KummerlÃ¤nder, Mathias J. Krause, Martin Frank, Stephan Simonis</p>
                    <p><strong>Published:</strong> 8/25/2025</p>
                    <p><strong>Categories:</strong> physics.flu-dyn, cs.MS, cs.NA, math.NA, physics.comp-ph</p>
                    <p><strong>Summary:</strong> Accurate prediction of urban wind flow is essential for urban planning, pedestrian safety, and environmental management. Yet, it remains challenging due to uncertain boundary conditions and the high cost of conventional CFD simulations. This paper presents the use of the modular and efficient uncertainty quantification (UQ) framework OpenLB-UQ for urban wind flow simulations. We specifically use the lattice Boltzmann method (LBM) coupled with a stochastic collocation (SC) approach based on generalized polynomial chaos (gPC). The framework introduces a relative-error noise model for inflow wind speeds based on real measurements. The model is propagated through a non-intrusive SC LBM pipeline using sparse-grid quadrature. Key quantities of interest, including mean flow fields, standard deviations, and vertical profiles with confidence intervals, are efficiently computed without altering the underlying deterministic solver. We demonstrate this on a real urban scenario, highlighting how uncertainty localizes in complex flow regions such as wakes and shear layers. The results show that the SC LBM approach provides accurate, uncertainty-aware predictions with significant computational efficiency, making OpenLB-UQ a practical tool for real-time urban wind analysis.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.18188v1" target="_blank">Explain and Monitor Deep Learning Models for Computer Vision using Obz AI</a></h3>
                    <p><strong>Authors:</strong> Neo Christopher Chung, Jakub Binda</p>
                    <p><strong>Published:</strong> 8/25/2025</p>
                    <p><strong>Categories:</strong> cs.CV, cs.AI, cs.HC, cs.SE</p>
                    <p><strong>Summary:</strong> Deep learning has transformed computer vision (CV), achieving outstanding performance in classification, segmentation, and related tasks. Such AI-based CV systems are becoming prevalent, with applications spanning from medical imaging to surveillance. State of the art models such as convolutional neural networks (CNNs) and vision transformers (ViTs) are often regarded as ``black boxes, offering limited transparency into their decision-making processes. Despite a recent advancement in explainable AI (XAI), explainability remains underutilized in practical CV deployments. A primary obstacle is the absence of integrated software solutions that connect XAI techniques with robust knowledge management and monitoring frameworks. To close this gap, we have developed Obz AI, a comprehensive software ecosystem designed to facilitate state-of-the-art explainability and observability for vision AI systems. Obz AI provides a seamless integration pipeline, from a Python client library to a full-stack analytics dashboard. With Obz AI, a machine learning engineer can easily incorporate advanced XAI methodologies, extract and analyze features for outlier detection, and continuously monitor AI models in real time. By making the decision-making mechanisms of deep models interpretable, Obz AI promotes observability and responsible deployment of computer vision systems.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.18177v1" target="_blank">Scene-Aware Vectorized Memory Multi-Agent Framework with Cross-Modal Differentiated Quantization VLMs for Visually Impaired Assistance</a></h3>
                    <p><strong>Authors:</strong> Xiangxiang Wang, Xuanyu Wang, YiJia Luo, Yongbin Yu, Manping Fan, Jingtao Zhang, Liyong Ren</p>
                    <p><strong>Published:</strong> 8/25/2025</p>
                    <p><strong>Categories:</strong> cs.CV, cs.LG, cs.MA</p>
                    <p><strong>Summary:</strong> This study proposes the dual technological innovation framework, including a cross-modal differ entiated quantization framework for vision-language models (VLMs) and a scene-aware vectorized memory multi-agent system for visually impaired assistance. The modular framework was developed implementing differentiated processing strategies, effectively reducing memory requirements from 38GB to 16GB while maintaining model performance. The multi-agent architecture combines scene classification, vectorized memory, and multimodal interaction, enabling persistent storage and efficient retrieval of scene memories. Through perception-memory-reasoning workflows, the system provides environmental information beyond the current view using historical memories. Experiments show the quantized 19B-parameter model only experiences a 2.05% performance drop on MMBench and maintains 63.7 accuracy on OCR-VQA (original: 64.9), outperforming smaller models with equivalent memory requirements like the Molmo-7B series. The system maintains response latency between 2.83-3.52 seconds from scene analysis to initial speech output, substantially faster than non-streaming methods. This research advances computational efficiency and assistive technology, offering visually impaired users comprehensive real-time assistance in scene perception, text recognition, and navigation.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.18175v1" target="_blank">Amortized Sampling with Transferable Normalizing Flows</a></h3>
                    <p><strong>Authors:</strong> Charlie B. Tan, Majdi Hassan, Leon Klein, Saifuddin Syed, Dominique Beaini, Michael M. Bronstein, Alexander Tong, Kirill Neklyudov</p>
                    <p><strong>Published:</strong> 8/25/2025</p>
                    <p><strong>Categories:</strong> cs.LG, cs.AI</p>
                    <p><strong>Summary:</strong> Efficient equilibrium sampling of molecular conformations remains a core challenge in computational chemistry and statistical inference. Classical approaches such as molecular dynamics or Markov chain Monte Carlo inherently lack amortization; the computational cost of sampling must be paid in-full for each system of interest. The widespread success of generative models has inspired interest into overcoming this limitation through learning sampling algorithms. Despite performing on par with conventional methods when trained on a single system, learned samplers have so far demonstrated limited ability to transfer across systems. We prove that deep learning enables the design of scalable and transferable samplers by introducing Prose, a 280 million parameter all-atom transferable normalizing flow trained on a corpus of peptide molecular dynamics trajectories up to 8 residues in length. Prose draws zero-shot uncorrelated proposal samples for arbitrary peptide systems, achieving the previously intractable transferability across sequence length, whilst retaining the efficient likelihood evaluation of normalizing flows. Through extensive empirical evaluation we demonstrate the efficacy of Prose as a proposal for a variety of sampling algorithms, finding a simple importance sampling-based finetuning procedure to achieve superior performance to established methods such as sequential Monte Carlo on unseen tetrapeptides. We open-source the Prose codebase, model weights, and training dataset, to further stimulate research into amortized sampling methods and finetuning objectives.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.18173v1" target="_blank">Unveiling the Actual Performance of Neural-based Models for Equation Discovery on Graph Dynamical Systems</a></h3>
                    <p><strong>Authors:</strong> Riccardo Cappi, Paolo Frazzetto, NicolÃ² Navarin, Alessandro Sperduti</p>
                    <p><strong>Published:</strong> 8/25/2025</p>
                    <p><strong>Categories:</strong> cs.LG</p>
                    <p><strong>Summary:</strong> The ``black-box nature of deep learning models presents a significant barrier to their adoption for scientific discovery, where interpretability is paramount. This challenge is especially pronounced in discovering the governing equations of dynamical processes on networks or graphs, since even their topological structure further affects the processes behavior. This paper provides a rigorous, comparative assessment of state-of-the-art symbolic regression techniques for this task. We evaluate established methods, including sparse regression and MLP-based architectures, and introduce a novel adaptation of Kolmogorov-Arnold Networks (KANs) for graphs, designed to exploit their inherent interpretability. Across a suite of synthetic and real-world dynamical systems, our results demonstrate that both MLP and KAN-based architectures can successfully identify the underlying symbolic equations, significantly surpassing existing baselines. Critically, we show that KANs achieve this performance with greater parsimony and transparency, as their learnable activation functions provide a clearer mapping to the true physical dynamics. This study offers a practical guide for researchers, clarifying the trade-offs between model expressivity and interpretability, and establishes the viability of neural-based architectures for robust scientific discovery on complex systems.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.18167v1" target="_blank">DiscussLLM: Teaching Large Language Models When to Speak</a></h3>
                    <p><strong>Authors:</strong> Deep Anil Patel, Iain Melvin, Christopher Malon, Martin Renqiang Min</p>
                    <p><strong>Published:</strong> 8/25/2025</p>
                    <p><strong>Categories:</strong> cs.CL, cs.HC</p>
                    <p><strong>Summary:</strong> Large Language Models (LLMs) have demonstrated remarkable capabilities in understanding and generating human-like text, yet they largely operate as reactive agents, responding only when directly prompted. This passivity creates an awareness gap, limiting their potential as truly collaborative partners in dynamic human discussions. We introduce $\textit{DiscussLLM}$, a framework designed to bridge this gap by training models to proactively decide not just $\textit{what}$ to say, but critically, $\textit{when}$ to speak. Our primary contribution is a scalable two-stage data generation pipeline that synthesizes a large-scale dataset of realistic multi-turn human discussions. Each discussion is annotated with one of five intervention types (e.g., Factual Correction, Concept Definition) and contains an explicit conversational trigger where an AI intervention adds value. By training models to predict a special silent token when no intervention is needed, they learn to remain quiet until a helpful contribution can be made. We explore two architectural baselines: an integrated end-to-end model and a decoupled classifier-generator system optimized for low-latency inference. We evaluate these models on their ability to accurately time interventions and generate helpful responses, paving the way for more situationally aware and proactive conversational AI.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.18149v1" target="_blank">First-Order LTLf Synthesis with Lookback (Extended Version)</a></h3>
                    <p><strong>Authors:</strong> Sarah Winkler</p>
                    <p><strong>Published:</strong> 8/25/2025</p>
                    <p><strong>Categories:</strong> cs.LO</p>
                    <p><strong>Summary:</strong> Reactive synthesis addresses the problem of generating a controller for a temporal specification in an adversarial environment; it was typically studied for LTL. Driven by applications ranging from AI to business process management, LTL modulo first order-theories over finite traces (LTLfMT) has recently gained traction, where propositional variables in properties are replaced by first-order constraints. Though reactive synthesis for LTLf with some first-order features has been addressed, existing work in this direction strongly restricts or excludes the possibility to compare variables across instants, a limitation that severely restricts expressiveness and applicability. In this work we present a reactive synthesis procedure for LTLfMT, where properties support lookback to model cross-instant comparison of variables. Our procedure works for full LTLfMT with lookback, subsuming the fragments of LTLfMT for which realizability was studied earlier. However, the setting with cross-instant comparison is inherently highly complex, as realizability is undecidable even over decidable background theories. Hence termination of our approach is in general not guaranteed. Nevertheless, we prove its soundness, and show that it is complete if a bound on the strategy length exists. Finally, we show that our approach constitutes a decision procedure for several relevant fragments of LTLfMT, at once re-proving known decidability results and identifying new decidable classes.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.18142v1" target="_blank">Mirroring Users: Towards Building Preference-aligned User Simulator with User Feedback in Recommendation</a></h3>
                    <p><strong>Authors:</strong> Tianjun Wei, Huizhong Guo, Yingpeng Du, Zhu Sun, Chen Huang, Dongxia Wang, Jie Zhang</p>
                    <p><strong>Published:</strong> 8/25/2025</p>
                    <p><strong>Categories:</strong> cs.HC, cs.CY, cs.IR</p>
                    <p><strong>Summary:</strong> User simulation is increasingly vital to develop and evaluate recommender systems (RSs). While Large Language Models (LLMs) offer promising avenues to simulate user behavior, they often struggle with the absence of specific domain alignment required for RSs and the efficiency demands of large-scale simulation. A vast yet underutilized resource for enhancing this alignment is the extensive user feedback inherent in RSs. However, directly leveraging such feedback presents two significant challenges. First, user feedback in RSs is often ambiguous and noisy, which negatively impacts effective preference alignment. Second, the massive volume of feedback largely hinders the efficiency of preference alignment, necessitating an efficient filtering mechanism to identify more informative samples. To overcome these hurdles, we introduce a novel data construction framework that leverages user feedback in RSs with advanced LLM capabilities to generate high-quality simulation data. Our framework unfolds in two key phases: (1) employing LLMs to generate cognitive decision-making processes on constructed simulation samples, reducing ambiguity in raw user feedback; (2) data distillation based on uncertainty estimation and behavior sampling to filter challenging yet denoised simulation samples. Accordingly, we fine-tune lightweight LLMs, as user simulators, using such high-quality dataset with corresponding decision-making processes. Extensive experiments verify that our framework significantly boosts the alignment with human preferences and in-domain reasoning capabilities of fine-tuned LLMs, and provides more insightful and interpretable signals when interacting with RSs. We believe our work will advance the RS community and offer valuable insights for broader human-centric AI research.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.18139v1" target="_blank">Analysis of Harpys Constrained Trotting and Jumping Maneuver</a></h3>
                    <p><strong>Authors:</strong> Prathima Ananda Kumar</p>
                    <p><strong>Published:</strong> 8/25/2025</p>
                    <p><strong>Categories:</strong> cs.RO</p>
                    <p><strong>Summary:</strong> This study presents an analysis of experimental data from Harpy, a thruster-assisted bipedal robot developed at Northeastern University. The study examines data sets from trotting and jumping experiments to understand the fundamental principles governing hybrid leg-thruster locomotion. Through data analysis across multiple locomotion modes, this research reveals that Harpy achieves stable locomotion with bounded trajectories and consistent foot placement through strategic leg-thruster synergy. The results demonstrate controlled joint behavior with low torques and symmetric tracking, accurate foot placement within kinematic constraints despite phase-transition perturbations, and underactuated degree-of-freedom stability without divergence. Energy level analysis reveals that legs provide primary propulsion, while the thrusters enable additional aerial phase control. The analysis identifies critical body-leg coupling dynamics during aerial phases that require phase-specific control strategies. Consistent repeatability and symmetry across experiments validate the robustness of the hybrid actuation approach.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.18136v1" target="_blank">BirdRecorders AI on Sky: Safeguarding birds of prey by detection and classification of tiny objects around wind turbines</a></h3>
                    <p><strong>Authors:</strong> Nico Klar, Nizam Gifary, Felix P. G. Ziegler, Frank Sehnke, Anton Kaifel, Eric Price, Aamir Ahmad</p>
                    <p><strong>Published:</strong> 8/25/2025</p>
                    <p><strong>Categories:</strong> cs.CV, cs.LG, cs.RO, cs.SY, eess.SY</p>
                    <p><strong>Summary:</strong> The urgent need for renewable energy expansion, particularly wind power, is hindered by conflicts with wildlife conservation. To address this, we developed BirdRecorder, an advanced AI-based anti-collision system to protect endangered birds, especially the red kite (Milvus milvus). Integrating robotics, telemetry, and high-performance AI algorithms, BirdRecorder aims to detect, track, and classify avian species within a range of 800 m to minimize bird-turbine collisions. BirdRecorder integrates advanced AI methods with optimized hardware and software architectures to enable real-time image processing. Leveraging Single Shot Detector (SSD) for detection, combined with specialized hardware acceleration and tracking algorithms, our system achieves high detection precision while maintaining the speed necessary for real-time decision-making. By combining these components, BirdRecorder outperforms existing approaches in both accuracy and efficiency. In this paper, we summarize results on field tests and performance of the BirdRecorder system. By bridging the gap between renewable energy expansion and wildlife conservation, BirdRecorder contributes to a more sustainable coexistence of technology and nature.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.18127v1" target="_blank">An Introduction to Silent Paralinguistics</a></h3>
                    <p><strong>Authors:</strong> Zhao Ren, Simon Pistrosch, Buket CoÅŸkun, Kevin Scheck, Anton Batliner, BjÃ¶rn W. Schuller, Tanja Schultz</p>
                    <p><strong>Published:</strong> 8/25/2025</p>
                    <p><strong>Categories:</strong> cs.HC</p>
                    <p><strong>Summary:</strong> The ability to speak is an inherent part of human nature and fundamental to our existence as a social species. Unfortunately, this ability can be restricted in certain situations, such as for individuals who have lost their voice or in environments where speaking aloud is unsuitable. Additionally, some people may prefer not to speak audibly due to privacy concerns. For such cases, silent speech interfaces have been proposed, which focus on processing biosignals corresponding to silently produced speech. These interfaces enable synthesis of audible speech from biosignals that are produced when speaking silently and recognition aka decoding of biosignals into text that corresponds to the silently produced speech. While recognition and synthesis of silent speech has been a prominent focus in many research studies, there is a significant gap in deriving paralinguistic information such as affective states from silent speech. To fill this gap, we propose Silent Paralinguistics, aiming to predict paralinguistic information from silent speech and ultimately integrate it into the reconstructed audible voice for natural communication. This survey provides a comprehensive look at methods, research strategies, and objectives within the emerging field of silent paralinguistics.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.18123v1" target="_blank">Views: A Hardware-friendly Graph Database Model For Storing Semantic Information</a></h3>
                    <p><strong>Authors:</strong> Yanjun Yang, Adrian Wheeldon, Yihan Pan, Alex Serb</p>
                    <p><strong>Published:</strong> 8/25/2025</p>
                    <p><strong>Categories:</strong> cs.DB, cs.AR, cs.DC, cs.SC</p>
                    <p><strong>Summary:</strong> The graph database (GDB) is an increasingly common storage model for data involving relationships between entries. Beyond its widespread usage in database industries, the advantages of GDBs indicate a strong potential in constructing symbolic artificial intelligences (AIs) and retrieval-augmented generation (RAG), where knowledge of data inter-relationships takes a critical role in implementation. However, current GDB models are not optimised for hardware acceleration, leading to bottlenecks in storage capacity and computational efficiency. In this paper, we propose a hardware-friendly GDB model, called Views. We show its data structure and organisation tailored for efficient storage and retrieval of graph data and demonstrate its equivalence to represent traditional graph representations. We further demonstrate its symbolic processing abilities in semantic reasoning and cognitive modelling with practical examples and provide a short perspective on future developments.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.18118v1" target="_blank">HLLM-Creator: Hierarchical LLM-based Personalized Creative Generation</a></h3>
                    <p><strong>Authors:</strong> Junyi Chen, Lu Chi, Siliang Xu, Shiwei Ran, Bingyue Peng, Zehuan Yuan</p>
                    <p><strong>Published:</strong> 8/25/2025</p>
                    <p><strong>Categories:</strong> cs.IR, cs.CL</p>
                    <p><strong>Summary:</strong> AI-generated content technologies are widely used in content creation. However, current AIGC systems rely heavily on creators inspiration, rarely generating truly user-personalized content. In real-world applications such as online advertising, a single product may have multiple selling points, with different users focusing on different features. This underscores the significant value of personalized, user-centric creative generation. Effective personalized content generation faces two main challenges: (1) accurately modeling user interests and integrating them into the content generation process while adhering to factual constraints, and (2) ensuring high efficiency and scalability to handle the massive user base in industrial scenarios. Additionally, the scarcity of personalized creative data in practice complicates model training, making data construction another key hurdle. We propose HLLM-Creator, a hierarchical LLM framework for efficient user interest modeling and personalized content generation. During inference, a combination of user clustering and a user-ad-matching-prediction based pruning strategy is employed to significantly enhance generation efficiency and reduce computational overhead, making the approach suitable for large-scale deployment. Moreover, we design a data construction pipeline based on chain-of-thought reasoning, which generates high-quality, user-specific creative titles and ensures factual consistency despite limited personalized data. This pipeline serves as a critical foundation for the effectiveness of our model. Extensive experiments on personalized title generation for Douyin Search Ads show the effectiveness of HLLM-Creator. Online A/B test shows a 0.476% increase on Adss, paving the way for more effective and efficient personalized generation in industrial scenarios. Codes for academic dataset are available at https://github.com/bytedance/HLLM.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.18113v1" target="_blank">The AI Data Scientist</a></h3>
                    <p><strong>Authors:</strong> Farkhad Akimov, Munachiso Samuel Nwadike, Zangir Iklassov, Martin TakÃ¡Ä</p>
                    <p><strong>Published:</strong> 8/25/2025</p>
                    <p><strong>Categories:</strong> cs.AI, cs.CL, cs.LG</p>
                    <p><strong>Summary:</strong> Imagine decision-makers uploading data and, within minutes, receiving clear, actionable insights delivered straight to their fingertips. That is the promise of the AI Data Scientist, an autonomous Agent powered by large language models (LLMs) that closes the gap between evidence and action. Rather than simply writing code or responding to prompts, it reasons through questions, tests ideas, and delivers end-to-end insights at a pace far beyond traditional workflows. Guided by the scientific tenet of the hypothesis, this Agent uncovers explanatory patterns in data, evaluates their statistical significance, and uses them to inform predictive modeling. It then translates these results into recommendations that are both rigorous and accessible. At the core of the AI Data Scientist is a team of specialized LLM Subagents, each responsible for a distinct task such as data cleaning, statistical testing, validation, and plain-language communication. These Subagents write their own code, reason about causality, and identify when additional data is needed to support sound conclusions. Together, they achieve in minutes what might otherwise take days or weeks, enabling a new kind of interaction that makes deep data science both accessible and actionable.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.18112v1" target="_blank">Forward-Backward Quantization of Scenario Processes in Multi-Stage Stochastic Optimization</a></h3>
                    <p><strong>Authors:</strong> Anna Timonina-Farkas</p>
                    <p><strong>Published:</strong> 8/25/2025</p>
                    <p><strong>Categories:</strong> math.OC</p>
                    <p><strong>Summary:</strong> Multi-stage stochastic optimization lies at the core of decision-making under uncertainty. As the analytical solution is available only in exceptional cases, dynamic optimization aims to efficiently find approximations but often neglects non-Markovian time-interdependencies. Methods on scenario trees can represent such interdependencies but are subject to the curse of dimensionality. To ease this problem, researchers typically approximate the uncertainty by smaller but more accurate trees. In this article, we focus on multi-stage optimal tree quantization methods of time-interdependent stochastic processes, for which we develop novel bounds and demonstrate that the upper bound can be minimized via projected gradient descent incorporating the tree structure as linear constraints. Consequently, we propose an efficient quantization procedure, which improves forward-looking samples using a backward step on the tree. We apply the results to the multi-stage inventory control with time-interdependent demand. For the case with one product, we benchmark the approximation because the problem allows a solution in closed-form. For the multi-dimensional problem, our solution found by optimal discrete approximation demonstrates the importance of holding mitigation inventory in different phases of the product life cycle.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.18109v1" target="_blank">Aligning Core Aspects: Improving Vulnerability Proof-of-Concepts via Cross-Source Insights</a></h3>
                    <p><strong>Authors:</strong> Lingxiao Wang, Wenjing Dang, Mengyao Zhang, Yue Wang, Xianzong Wu, Sen Chen</p>
                    <p><strong>Published:</strong> 8/25/2025</p>
                    <p><strong>Categories:</strong> cs.CR</p>
                    <p><strong>Summary:</strong> For vulnerabilities, Proof-of-Concept (PoC) plays an irreplaceable role in demonstrating the exploitability. PoC reports may include critical information such as specific usage, test platforms, and more, providing essential insights for researchers. However, in reality, due to various PoC templates across PoC platforms, PoC reports extensively suffer from information deficiency, leading the suboptimal quality and limited usefulness. Fortunately, we found that information deficiency of PoC reports could be mitigated by the completion from multiple sources given the same referred vulnerability. In this paper, we conduct the first study on the deficiency of information in PoC reports across public platforms. We began by collecting 173,170 PoC reports from 4 different platforms and defined 8 key aspects that PoCs should contain. By integrating rule-based matching and a fine-tuned BERT-NER model for extraction of key aspects, we discovered that all PoC reports available on public platforms have at least one missing key aspect. Subsequently, we developed a multi-source information fusion method to complete the missing aspect information in PoC reports by leveraging CVE entries and related PoC reports from different sources. Finally, we successfully completed 69,583 PoC reports (40.18% of all reports).</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.18106v1" target="_blank">A.S.E: A Repository-Level Benchmark for Evaluating Security in AI-Generated Code</a></h3>
                    <p><strong>Authors:</strong> Keke Lian, Bin Wang, Lei Zhang, Libo Chen, Junjie Wang, Ziming Zhao, Yujiu Yang, Haotong Duan, Haoran Zhao, Shuang Liao, Mingda Guo, Jiazheng Quan, Yilu Zhong, Chenhao He, Zichuan Chen, Jie Wu, Haoling Li, Zhaoxuan Li, Jiongchi Yu, Hui Li, Dong Zhang</p>
                    <p><strong>Published:</strong> 8/25/2025</p>
                    <p><strong>Categories:</strong> cs.SE, cs.AI</p>
                    <p><strong>Summary:</strong> The increasing adoption of large language models (LLMs) in software engineering necessitates rigorous security evaluation of their generated code. However, existing benchmarks are inadequate, as they focus on isolated code snippets, employ unstable evaluation methods that lack reproducibility, and fail to connect the quality of input context with the security of the output. To address these gaps, we introduce A.S.E (AI Code Generation Security Evaluation), a benchmark for repository-level secure code generation. A.S.E constructs tasks from real-world repositories with documented CVEs, preserving full repository context like build systems and cross-file dependencies. Its reproducible, containerized evaluation framework uses expert-defined rules to provide stable, auditable assessments of security, build quality, and generation stability. Our evaluation of leading LLMs on A.S.E reveals three key findings: (1) Claude-3.7-Sonnet achieves the best overall performance. (2) The security gap between proprietary and open-source models is narrow; Qwen3-235B-A22B-Instruct attains the top security score. (3) Concise, ``fast-thinking decoding strategies consistently outperform complex, ``slow-thinking reasoning for security patching.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.18100v1" target="_blank">Analysis and Detection of RIS-based Spoofing in Integrated Sensing and Communication (ISAC)</a></h3>
                    <p><strong>Authors:</strong> Tingyu Shui, Po-Heng Chou, Walid Saad, Mingzhe Chen</p>
                    <p><strong>Published:</strong> 8/25/2025</p>
                    <p><strong>Categories:</strong> cs.IT, math.IT</p>
                    <p><strong>Summary:</strong> Integrated sensing and communication (ISAC) is a key feature of next-generation 6G wireless systems, allowing them to achieve high data rates and sensing accuracy. While prior research has primarily focused on addressing communication safety in ISAC systems, the equally critical issue of sensing safety remains largely under-explored. In this paper, the possibility of spoofing the sensing function of ISAC in vehicle networks is examined, whereby a malicious reconfigurable intelligent surface (RIS) is deployed to compromise the sensing functionality of a roadside unit (RSU). For this scenario, the requirements on the malicious RIS phase shifts design and number of reflecting elements are analyzed. Under such spoofing, the practical estimation bias of the vehicular user (VU)s Doppler shift and angle-of-departure (AoD) for an arbitrary time slot is analytically derived. Moreover, from the attackers view, a Markov decision process (MDP) is formulated to optimize the RIS phase shifts design. The goal of this MDP is to generate complete and plausible fake trajectories by incorporating the concept of spatial-temporal consistency. To defend against this sensing spoofing attack, a signal temporal logic (STL)-based neuro-symbolic attack detection framework is proposed and shown to learn interoperable formulas for identifying spoofed trajectories.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.18095v1" target="_blank">Incorporating Pre-trained Diffusion Models in Solving the SchrÃ¶dinger Bridge Problem</a></h3>
                    <p><strong>Authors:</strong> Zhicong Tang, Tiankai Hang, Shuyang Gu, Dong Chen, Baining Guo</p>
                    <p><strong>Published:</strong> 8/25/2025</p>
                    <p><strong>Categories:</strong> cs.CV, cs.LG</p>
                    <p><strong>Summary:</strong> This paper aims to unify Score-based Generative Models (SGMs), also known as Diffusion models, and the Schr\odinger Bridge (SB) problem through three reparameterization techniques: Iterative Proportional Mean-Matching (IPMM), Iterative Proportional Terminus-Matching (IPTM), and Iterative Proportional Flow-Matching (IPFM). These techniques significantly accelerate and stabilize the training of SB-based models. Furthermore, the paper introduces novel initialization strategies that use pre-trained SGMs to effectively train SB-based models. By using SGMs as initialization, we leverage the advantages of both SB-based models and SGMs, ensuring efficient training of SB-based models and further improving the performance of SGMs. Extensive experiments demonstrate the significant effectiveness and improvements of the proposed methods. We believe this work contributes to and paves the way for future research on generative models.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.18094v1" target="_blank">TOI-2322: two transiting rocky planets close to the stellar rotation period and its first harmonic</a></h3>
                    <p><strong>Authors:</strong> M. J. Hobson, A. SuÃ¡rez MascareÃ±o, C. Lovis, F. Bouchy, B. Lavie, M. Cretignier, A. M. Silva, S. G. Sousa, H. M. Tabernero, V. Adibekyan, C. Allende Prieto, Y. Alibert, S. C. C. Barros, A. Castro-GonzÃ¡lez, K. A. Collins, S. Cristiani, V. DOdorico, M. Damasso, D. Dragomir, X. Dumusque, D. Ehrenreich, P. Figueira, R. GÃ©nova Santos, B. Goeke, J. I. GonzÃ¡lez HernÃ¡ndez, K. Hesse, J. Lillo-Box, G. Lo Curto, C. J. A. P. Martins, A. Mehner, G. Micela, P. Molaro, N. J. Nunes, E. Palle, V. M. Passegger, F. Pepe, R. Rebolo, J. Rodrigues, N. Santos, A. Sozzetti, B. M. Tofflemire, S. Udry, C. Watkins, M. -R. Zapatero Osorio, C. Ziegler</p>
                    <p><strong>Published:</strong> 8/25/2025</p>
                    <p><strong>Categories:</strong> astro-ph.EP</p>
                    <p><strong>Summary:</strong> Context. Active regions on the stellar surface can induce quasi-periodic radial velocity (RV) variations that can mimic planets and mask true planetary signals. These spurious signals can be problematic for RV surveys such as those carried out by the ESPRESSO consortium. Aims. Using ESPRESSO and HARPS RVs and activity indicators, we aim to confirm and characterize two candidate transiting planets from TESS orbiting a K4 star with strong activity signals. Methods. From the ESPRESSO FWHM, TESS photometry, and ASAS-SN photometry, we measure a stellar rotation period of 21.28 $\pm$ 0.08 d. We jointly model the TESS photometry, ESPRESSO and HARPS RVs, and activity indicators, applying a multivariate Gaussian Process (GP) framework to the spectroscopic data. Results. We are able to disentangle the planetary and activity components, finding that TOI-2322 b has a $11.307170^{+0.000085}_{-0.000079}$ d period, close to the first harmonic of the rotation period, a $\leq 2.03 M_\oplus$ mass upper limit and a $0.994^{+0.057}_{-0.059}$ $\mathrm{R_\oplus}$ radius. TOI-2322 c orbits close to the stellar rotation period, with a $20.225528^{+0.000039}_{-0.000044}$ d period; it has a $18.10^{+4.34}_{-5.36}$ $\mathrm{M_\oplus}$ mass and a $1.874^{+0.066}_{-0.057}$ $\mathrm{R_\oplus}$ radius. Conclusions. The multivariate GP framework is crucial to separating the stellar and planetary signals, significantly outperforming a one-dimensional GP. Likewise, the transit data is fundamental to constraining the periods and epochs, enabling the retrieval of the planetary signals in the RVs. The internal structure of TOI-2322 c is very similar to that of Earth, making it one of the most massive planets with an Earth-like composition known.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.18091v1" target="_blank">Teaching LLMs to Think Mathematically: A Critical Study of Decision-Making via Optimization</a></h3>
                    <p><strong>Authors:</strong> Mohammad J. Abdel-Rahman, Yasmeen Alslman, Dania Refai, Amro Saleh, Malik A. Abu Loha, Mohammad Yahya Hamed</p>
                    <p><strong>Published:</strong> 8/25/2025</p>
                    <p><strong>Categories:</strong> cs.AI, math.OC</p>
                    <p><strong>Summary:</strong> This paper investigates the capabilities of large language models (LLMs) in formulating and solving decision-making problems using mathematical programming. We first conduct a systematic review and meta-analysis of recent literature to assess how well LLMs understand, structure, and solve optimization problems across domains. The analysis is guided by critical review questions focusing on learning approaches, dataset designs, evaluation metrics, and prompting strategies. Our systematic evidence is complemented by targeted experiments designed to evaluate the performance of state-of-the-art LLMs in automatically generating optimization models for problems in computer networks. Using a newly constructed dataset, we apply three prompting strategies: Act-as-expert, chain-of-thought, and self-consistency, and evaluate the obtained outputs based on optimality gap, token-level F1 score, and compilation accuracy. Results show promising progress in LLMs ability to parse natural language and represent symbolic formulations, but also reveal key limitations in accuracy, scalability, and interpretability. These empirical gaps motivate several future research directions, including structured datasets, domain-specific fine-tuning, hybrid neuro-symbolic approaches, modular multi-agent architectures, and dynamic retrieval via chain-of-RAGs. This paper contributes a structured roadmap for advancing LLM capabilities in mathematical programming.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.18089v1" target="_blank">LLM-Guided Genetic Improvement: Envisioning Semantic Aware Automated Software Evolution</a></h3>
                    <p><strong>Authors:</strong> Karine Even-Mendoza, Alexander Brownlee, Alina Geiger, Carol Hanna, Justyna Petke, Federica Sarro, Dominik Sobania</p>
                    <p><strong>Published:</strong> 8/25/2025</p>
                    <p><strong>Categories:</strong> cs.SE</p>
                    <p><strong>Summary:</strong> Genetic Improvement (GI) of software automatically creates alternative software versions that are improved according to certain properties of interests (e.g., running-time). Search-based GI excels at navigating large program spaces, but operates primarily at the syntactic level. In contrast, Large Language Models (LLMs) offer semantic-aware edits, yet lack goal-directed feedback and control (which is instead a strength of GI). As such, we propose the investigation of a new research line on AI-powered GI aimed at incorporating semantic aware search. We take a first step at it by augmenting GI with the use of automated clustering of LLM edits. We provide initial empirical evidence that our proposal, dubbed PatchCat, allows us to automatically and effectively categorize LLM-suggested patches. PatchCat identified 18 different types of software patches and categorized newly suggested patches with high accuracy. It also enabled detecting NoOp edits in advance and, prospectively, to skip test suite execution to save resources in many cases. These results, coupled with the fact that PatchCat works with small, local LLMs, are a promising step toward interpretable, efficient, and green GI. We outline a rich agenda of future work and call for the community to join our vision of building a principled understanding of LLM-driven mutations, guiding the GI search process with semantic signals.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.18076v1" target="_blank">Neither Valid nor Reliable? Investigating the Use of LLMs as Judges</a></h3>
                    <p><strong>Authors:</strong> Khaoula Chehbouni, Mohammed Haddou, Jackie Chi Kit Cheung, Golnoosh Farnadi</p>
                    <p><strong>Published:</strong> 8/25/2025</p>
                    <p><strong>Categories:</strong> cs.CL, I.2.7</p>
                    <p><strong>Summary:</strong> Evaluating natural language generation (NLG) systems remains a core challenge of natural language processing (NLP), further complicated by the rise of large language models (LLMs) that aims to be general-purpose. Recently, large language models as judges (LLJs) have emerged as a promising alternative to traditional metrics, but their validity remains underexplored. This position paper argues that the current enthusiasm around LLJs may be premature, as their adoption has outpaced rigorous scrutiny of their reliability and validity as evaluators. Drawing on measurement theory from the social sciences, we identify and critically assess four core assumptions underlying the use of LLJs: their ability to act as proxies for human judgment, their capabilities as evaluators, their scalability, and their cost-effectiveness. We examine how each of these assumptions may be challenged by the inherent limitations of LLMs, LLJs, or current practices in NLG evaluation. To ground our analysis, we explore three applications of LLJs: text summarization, data annotation, and safety alignment. Finally, we highlight the need for more responsible evaluation practices in LLJs evaluation, to ensure that their growing role in the field supports, rather than undermines, progress in NLG.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.18074v1" target="_blank">The Effects of Communication Delay on Human Performance and Neurocognitive Responses in Mobile Robot Teleoperation</a></h3>
                    <p><strong>Authors:</strong> Zhaokun Chen, Wenshuo Wang, Wenzhuo Liu, Yichen Liu, Junqiang Xi</p>
                    <p><strong>Published:</strong> 8/25/2025</p>
                    <p><strong>Categories:</strong> cs.RO</p>
                    <p><strong>Summary:</strong> Communication delays in mobile robot teleoperation adversely affect human-machine collaboration. Understanding delay effects on human operational performance and neurocognition is essential for resolving this issue. However, no previous research has explored this. To fill this gap, we conduct a human-in-the-loop experiment involving 10 participants, integrating electroencephalography (EEG) and robot behavior data under varying delays (0-500 ms in 100 ms increments) to systematically investigate these effects. Behavior analysis reveals significant performance degradation at 200-300 ms delays, affecting both task efficiency and accuracy. EEG analysis discovers features with significant delay dependence: frontal $\theta/\beta$-band and parietal $\alpha$-band power. We also identify a threshold window (100-200 ms) for early perception of delay in humans, during which these EEG features first exhibit significant differences. When delay exceeds 400 ms, all features plateau, indicating saturation of cognitive resource allocation at physiological limits. These findings provide the first evidence of perceptual and cognitive delay thresholds during teleoperation tasks in humans, offering critical neurocognitive insights for the design of delay compensation strategies.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.18073v1" target="_blank">Debian in the Research Software Ecosystem: A Bibliometric Analysis</a></h3>
                    <p><strong>Authors:</strong> Joenio Marques da Costa, Christina von Flach</p>
                    <p><strong>Published:</strong> 8/25/2025</p>
                    <p><strong>Categories:</strong> cs.SE, cs.DL</p>
                    <p><strong>Summary:</strong> Context: The Debian system has historically participated in academic works and scientific projects, with well-known examples including NeuroDebian, Debian Med, Debsources, Debian Science, and Debian GIS, where the scientific relevance of Debian and its contribution to the Research Software ecosystem are evident. Objective: The objective of this study is to investigate the Debian system through academic publications, with the aim of classifying articles, mapping research, identifying trends, and finding opportunities. Method: The study is based on a bibliometric analysis starting with an initial search for the term Debian in the titles, abstracts, or keywords of academic publications, using the Scopus database. This analysis calculates metrics of co-citation, co-authorship, and word co-occurrence, and is guided by a set of research questions and criteria for inclusion and exclusion to conduct the bibliometric analysis. Results: The study includes a set of articles published across various fields of knowledge, providing a map of the academic publication space about Debian. The studys data will be available in a public repository, reporting demographic and bibliometric trends, including the most cited articles, active countries, researchers, and popular conferences. Conclusion: Results includes a bibliometric and demographic analysis identified in publications about Debian, shedding light on the intellectual structure of academic research. The results of the analyses can help researchers gain an overview of existing trends in publications about Debian and identify areas that require more attention from the scientific community.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.18071v1" target="_blank">EventTracer: Fast Path Tracing-based Event Stream Rendering</a></h3>
                    <p><strong>Authors:</strong> Zhenyang Li, Xiaoyang Bai, Jinfan Lu, Pengfei Shen, Edmund Y. Lam, Yifan Peng</p>
                    <p><strong>Published:</strong> 8/25/2025</p>
                    <p><strong>Categories:</strong> cs.CV</p>
                    <p><strong>Summary:</strong> Simulating event streams from 3D scenes has become a common practice in event-based vision research, as it meets the demand for large-scale, high temporal frequency data without setting up expensive hardware devices or undertaking extensive data collections. Yet existing methods in this direction typically work with noiseless RGB frames that are costly to render, and therefore they can only achieve a temporal resolution equivalent to 100-300 FPS, far lower than that of real-world event data. In this work, we propose EventTracer, a path tracing-based rendering pipeline that simulates high-fidelity event sequences from complex 3D scenes in an efficient and physics-aware manner. Specifically, we speed up the rendering process via low sample-per-pixel (SPP) path tracing, and train a lightweight event spiking network to denoise the resulting RGB videos into realistic event sequences. To capture the physical properties of event streams, the network is equipped with a bipolar leaky integrate-and-fired (BiLIF) spiking unit and trained with a bidirectional earth mover distance (EMD) loss. Our EventTracer pipeline runs at a speed of about 4 minutes per second of 720p video, and it inherits the merit of accurate spatiotemporal modeling from its path tracing backbone. We show in two downstream tasks that EventTracer captures better scene details and demonstrates a greater similarity to real-world event data than other event simulators, which establishes it as a promising tool for creating large-scale event-RGB datasets at a low cost, narrowing the sim-to-real gap in event-based vision, and boosting various application scenarios such as robotics, autonomous driving, and VRAR.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.18043v1" target="_blank">Anatomy of the gem5 Simulator: AtomicSimpleCPU, TimingSimpleCPU, O3CPU, and Their Interaction with the Ruby Memory System</a></h3>
                    <p><strong>Authors:</strong> Johan SÃ¶derstrÃ¶m, Yuan Yao</p>
                    <p><strong>Published:</strong> 8/25/2025</p>
                    <p><strong>Categories:</strong> cs.AR</p>
                    <p><strong>Summary:</strong> gem5 is a popular modular-based computer system simulator, widely used in computer architecture research and known for its long simulation time and steep learning curve. This report examines its three major CPU models: the AtomicSimpleCPU (AS CPU), the TimingSimpleCPU (TS CPU), the Out-of-order (O3) CPU, and their interactions with the memory subsystem. We provide a detailed anatomical overview of each CPUs function call-chains and present how gem5 partitions its execution time for each simulated hardware layer. We perform our analysis using a lightweight profiler built on Linuxs perf_event interface, with user-configurable options to target specific functions and examine their interactions in detail. By profiling each CPU across a wide selection of benchmarks, we identify their software bottlenecks. Our results show that the Ruby memory subsystem consistently accounts for the largest share of execution time in the sequential AS and TS CPUs, primarily during the instruction fetch stage. In contrast, the O3 CPU spends a relatively smaller fraction of time in Ruby, with most of its time devoted to constructing instruction instances and the various pipeline stages of the CPU. We believe that the anatomical view of each CPUs execution flow is valuable for educational purposes, as it clearly illustrates the interactions among simulated components. These insights form a foundation for optimizing gem5s performance, particularly for the AS, TS, and O3 CPUs. Moreover, our framework can be readily applied to analyze other gem5 components or to develop and evaluate new models.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.18040v1" target="_blank">PerPilot: Personalizing VLM-based Mobile Agents via Memory and Exploration</a></h3>
                    <p><strong>Authors:</strong> Xin Wang, Zhiyao Cui, Hao Li, Ya Zeng, Chenxu Wang, Ruiqi Song, Yihang Chen, Kun Shao, Qiaosheng Zhang, Jinzhuo Liu, Siyue Ren, Shuyue Hu, Zhen Wang</p>
                    <p><strong>Published:</strong> 8/25/2025</p>
                    <p><strong>Categories:</strong> cs.AI</p>
                    <p><strong>Summary:</strong> Vision language model (VLM)-based mobile agents show great potential for assisting users in performing instruction-driven tasks. However, these agents typically struggle with personalized instructions -- those containing ambiguous, user-specific context -- a challenge that has been largely overlooked in previous research. In this paper, we define personalized instructions and introduce PerInstruct, a novel human-annotated dataset covering diverse personalized instructions across various mobile scenarios. Furthermore, given the limited personalization capabilities of existing mobile agents, we propose PerPilot, a plug-and-play framework powered by large language models (LLMs) that enables mobile agents to autonomously perceive, understand, and execute personalized user instructions. PerPilot identifies personalized elements and autonomously completes instructions via two complementary approaches: memory-based retrieval and reasoning-based exploration. Experimental results demonstrate that PerPilot effectively handles personalized tasks with minimal user intervention and progressively improves its performance with continued use, underscoring the importance of personalization-aware reasoning for next-generation mobile agents. The dataset and code are available at: https://github.com/xinwang-nwpu/PerPilot</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.18031v1" target="_blank">FCR: Investigating Generative AI models for Forensic Craniofacial Reconstruction</a></h3>
                    <p><strong>Authors:</strong> Ravi Shankar Prasad, Dinesh Singh</p>
                    <p><strong>Published:</strong> 8/25/2025</p>
                    <p><strong>Categories:</strong> cs.CV</p>
                    <p><strong>Summary:</strong> Craniofacial reconstruction in forensics is one of the processes to identify victims of crime and natural disasters. Identifying an individual from their remains plays a crucial role when all other identification methods fail. Traditional methods for this task, such as clay-based craniofacial reconstruction, require expert domain knowledge and are a time-consuming process. At the same time, other probabilistic generative models like the statistical shape model or the Basel face model fail to capture the skull and face cross-domain attributes. Looking at these limitations, we propose a generic framework for craniofacial reconstruction from 2D X-ray images. Here, we used various generative models (i.e., CycleGANs, cGANs, etc) and fine-tune the generator and discriminator parts to generate more realistic images in two distinct domains, which are the skull and face of an individual. This is the first time where 2D X-rays are being used as a representation of the skull by generative models for craniofacial reconstruction. We have evaluated the quality of generated faces using FID, IS, and SSIM scores. Finally, we have proposed a retrieval framework where the query is the generated face image and the gallery is the database of real faces. By experimental results, we have found that this can be an effective tool for forensic science.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.18025v1" target="_blank">AQ-PCDSys: An Adaptive Quantized Planetary Crater Detection System for Autonomous Space Exploration</a></h3>
                    <p><strong>Authors:</strong> Aditri Paul, Archan Paul</p>
                    <p><strong>Published:</strong> 8/25/2025</p>
                    <p><strong>Categories:</strong> cs.LG, cs.AI, cs.CV, cs.ET, cs.SY, eess.SY, 68T07(2020), 68T45(2020), 68T10(2020), 90C90(2020), I.2.10; I.2.6; I.2.9; J.2</p>
                    <p><strong>Summary:</strong> Autonomous planetary exploration missions are critically dependent on real-time, accurate environmental perception for navigation and hazard avoidance. However, deploying deep learning models on the resource-constrained computational hardware of planetary exploration platforms remains a significant challenge. This paper introduces the Adaptive Quantized Planetary Crater Detection System (AQ-PCDSys), a novel framework specifically engineered for real-time, onboard deployment in the computationally constrained environments of space exploration missions. AQ-PCDSys synergistically integrates a Quantized Neural Network (QNN) architecture, trained using Quantization-Aware Training (QAT), with an Adaptive Multi-Sensor Fusion (AMF) module. The QNN architecture significantly optimizes model size and inference latency suitable for real-time onboard deployment in space exploration missions, while preserving high accuracy. The AMF module intelligently fuses data from Optical Imagery (OI) and Digital Elevation Models (DEMs) at the feature level, utilizing an Adaptive Weighting Mechanism (AWM) to dynamically prioritize the most relevant and reliable sensor modality based on planetary ambient conditions. This approach enhances detection robustness across diverse planetary landscapes. Paired with Multi-Scale Detection Heads specifically designed for robust and efficient detection of craters across a wide range of sizes, AQ-PCDSys provides a computationally efficient, reliable and accurate solution for planetary crater detection, a critical capability for enabling the next generation of autonomous planetary landing, navigation, and scientific exploration.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.18012v1" target="_blank">Development of a Neural Network Model for Currency Detection to aid visually impaired people in Nigeria</a></h3>
                    <p><strong>Authors:</strong> Sochukwuma Nwokoye, Desmond Moru</p>
                    <p><strong>Published:</strong> 8/25/2025</p>
                    <p><strong>Categories:</strong> cs.CV, cs.LG</p>
                    <p><strong>Summary:</strong> Neural networks in assistive technology for visually impaired leverage artificial intelligences capacity to recognize patterns in complex data. They are used for converting visual data into auditory or tactile representations, helping the visually impaired understand their surroundings. The primary aim of this research is to explore the potential of artificial neural networks to facilitate the differentiation of various forms of cash for individuals with visual impairments. In this study, we built a custom dataset of 3,468 images, which was subsequently used to train an SSD neural network model. The proposed system can accurately identify Nigerian cash, thereby streamlining commercial transactions. The performance of the system in terms of accuracy was assessed, and the Mean Average Precision score was over 90%. We believe that our system has the potential to make a substantial contribution to the field of assistive technology while also improving the quality of life of visually challenged persons in Nigeria and beyond.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.18003v1" target="_blank">Previously on... Automating Code Review</a></h3>
                    <p><strong>Authors:</strong> Robert HeumÃ¼ller, Frank Ortmeier</p>
                    <p><strong>Published:</strong> 8/25/2025</p>
                    <p><strong>Categories:</strong> cs.SE, cs.AI</p>
                    <p><strong>Summary:</strong> Modern Code Review (MCR) is a standard practice in software engineering, yet it demands substantial time and resource investments. Recent research has increasingly explored automating core review tasks using machine learning (ML) and deep learning (DL). As a result, there is substantial variability in task definitions, datasets, and evaluation procedures. This study provides the first comprehensive analysis of MCR automation research, aiming to characterize the fields evolution, formalize learning tasks, highlight methodological challenges, and offer actionable recommendations to guide future research. Focusing on the primary code review tasks, we systematically surveyed 691 publications and identified 24 relevant studies published between May 2015 and April 2024. Each study was analyzed in terms of tasks, models, metrics, baselines, results, validity concerns, and artifact availability. In particular, our analysis reveals significant potential for standardization, including 48 task metric combinations, 22 of which were unique to their original paper, and limited dataset reuse. We highlight challenges and derive concrete recommendations for examples such as the temporal bias threat, which are rarely addressed so far. Our work contributes to a clearer overview of the field, supports the framing of new research, helps to avoid pitfalls, and promotes greater standardization in evaluation practices.</p>
                
            
                
                    <h3><a href="http://dx.doi.org/10.1021/acsnano.5c07687" target="_blank">Crystalline-to-Crystalline Phase Transition between Germanium Selenide Polymorphs with High Resistance Contrast</a></h3>
                    <p><strong>Authors:</strong> Joonho Kim, Kihyun Lee, Joong-Eon Jung, Han Joo Lee, Seongil Im, Kwanpyo Kim</p>
                    <p><strong>Published:</strong> 8/25/2025</p>
                    <p><strong>Categories:</strong> cond-mat.mtrl-sci, physics.app-ph</p>
                    <p><strong>Summary:</strong> Understanding phase transitions between crystalline phases of a material is crucial for both fundamental research and potential applications such as phase-change memory. In this study, we investigate the phase transition between GeSe crystalline polymorphs induced by either global annealing at moderate temperatures or localized laser-induced heating. The highly conductive gamma-GeSe transforms into semiconducting, single-crystalline alpha-GeSe while preserving a well-aligned crystal orientation. The distinct structural and electronic properties at the gamma-GeSe/alpha-GeSe interface were investigated by transmission electron microscopy analysis. We propose that the clustering of Ge vacancies in the gamma-GeSe phase at elevated temperatures is a key mechanism driving the transition, leading to the formation of alpha-GeSe through the segregation of a minor GeSe2 phase. Furthermore, we observe a high electrical resistance contrast of approximately 10^7 between gamma-GeSe and alpha-GeSe, underscoring the potential of GeSe as a model polymorphic system for electronic applications, including phase-change memory.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.17992v1" target="_blank">A Model of Triple-Channel Interaction Dynamics in Pharmaceutical Retailing in Emerging Economies</a></h3>
                    <p><strong>Authors:</strong> Koushik Mondal, Balagopal G Menon, Sunil Sahadev</p>
                    <p><strong>Published:</strong> 8/25/2025</p>
                    <p><strong>Categories:</strong> econ.GN, q-fin.EC</p>
                    <p><strong>Summary:</strong> The survival of unorganized pharmacies is increasingly challenging in the face of growing competition from organized and e-pharmaceutical retail channels in emerging economies. A theoretical model is developed to capture the triple-channel interactions among unorganized, organized and e-retailing in emerging markets, taking into account the essential features of the pharmaceutical retail landscape, consumers, retailers and pharmaceutical products. Given the retailer and customer-specific factors, the price-setting game between the triple-channel retailers yielded the optimal prices for these retailers. The analysis found that the product category level demand has no influence on optimal pricing strategies of the retailers. The analysis also reveals counterintuitive results, for instance, (i) an increase in customer acceptance of unorganized retailers will result in a decrease in profits of both unorganized and organized retailers; (ii) as the distance and transportation cost to unorganized retailers increases for the consumers, the profit of the unorganized retailer increases; and (iii) consumers marginal utility of money has no influence on the optimal price, but have an influence on the profit of the three retail channels. Our research findings offer valuable insights for policymakers facing challenges in achieving a balanced growth among the organized, unorganized, and e-pharmaceutical retail sectors in emerging economies. Keywords: Unorganized, Organized, and Online E-Retail; Nanostores; Emerging Markets; Game Theory.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.17985v1" target="_blank">Integration of Computer Vision with Adaptive Control for Autonomous Driving Using ADORE</a></h3>
                    <p><strong>Authors:</strong> Abu Shad Ahammed, Md Shahi Amran Hossain, Sayeri Mukherjee, Roman Obermaisser, Md. Ziaur Rahman</p>
                    <p><strong>Published:</strong> 8/25/2025</p>
                    <p><strong>Categories:</strong> cs.RO</p>
                    <p><strong>Summary:</strong> Ensuring safety in autonomous driving requires a seamless integration of perception and decision making under uncertain conditions. Although computer vision (CV) models such as YOLO achieve high accuracy in detecting traffic signs and obstacles, their performance degrades in drift scenarios caused by weather variations or unseen objects. This work presents a simulated autonomous driving system that combines a context aware CV model with adaptive control using the ADORE framework. The CARLA simulator was integrated with ADORE via the ROS bridge, allowing real-time communication between perception, decision, and control modules. A simulated test case was designed in both clear and drift weather conditions to demonstrate the robust detection performance of the perception model while ADORE successfully adapted vehicle behavior to speed limits and obstacles with low response latency. The findings highlight the potential of coupling deep learning-based perception with rule-based adaptive decision making to improve automotive safety critical system.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.17975v1" target="_blank">Enhanced Drift-Aware Computer Vision Architecture for Autonomous Driving</a></h3>
                    <p><strong>Authors:</strong> Md Shahi Amran Hossain, Abu Shad Ahammed, Sayeri Mukherjee, Roman Obermaisser</p>
                    <p><strong>Published:</strong> 8/25/2025</p>
                    <p><strong>Categories:</strong> cs.CV, math.LO</p>
                    <p><strong>Summary:</strong> The use of computer vision in automotive is a trending research in which safety and security are a primary concern. In particular, for autonomous driving, preventing road accidents requires highly accurate object detection under diverse conditions. To address this issue, recently the International Organization for Standardization (ISO) released the 8800 norm, providing structured frameworks for managing associated AI relevant risks. However, challenging scenarios such as adverse weather or low lighting often introduce data drift, leading to degraded model performance and potential safety violations. In this work, we present a novel hybrid computer vision architecture trained with thousands of synthetic image data from the road environment to improve robustness in unseen drifted environments. Our dual mode framework utilized YOLO version 8 for swift detection and incorporated a five-layer CNN for verification. The system functioned in sequence and improved the detection accuracy by more than 90\% when tested with drift-augmented road images. The focus was to demonstrate how such a hybrid model can provide better road safety when working together in a hybrid structure.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.17973v1" target="_blank">German4All - A Dataset and Model for Readability-Controlled Paraphrasing in German</a></h3>
                    <p><strong>Authors:</strong> Miriam AnschÃ¼tz, Thanh Mai Pham, Eslam Nasrallah, Maximilian MÃ¼ller, Cristian-George Craciun, Georg Groh</p>
                    <p><strong>Published:</strong> 8/25/2025</p>
                    <p><strong>Categories:</strong> cs.CL</p>
                    <p><strong>Summary:</strong> The ability to paraphrase texts across different complexity levels is essential for creating accessible texts that can be tailored toward diverse reader groups. Thus, we introduce German4All, the first large-scale German dataset of aligned readability-controlled, paragraph-level paraphrases. It spans five readability levels and comprises over 25,000 samples. The dataset is automatically synthesized using GPT-4 and rigorously evaluated through both human and LLM-based judgments. Using German4All, we train an open-source, readability-controlled paraphrasing model that achieves state-of-the-art performance in German text simplification, enabling more nuanced and reader-specific adaptations. We opensource both the dataset and the model to encourage further research on multi-level paraphrasing</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.17969v1" target="_blank">A holistic perception system of internal and external monitoring for ground autonomous vehicles: AutoTRUST paradigm</a></h3>
                    <p><strong>Authors:</strong> Alexandros Gkillas, Christos Anagnostopoulos, Nikos Piperigkos, Dimitris Tsiktsiris, Theofilos Christodoulou, Theofanis Siamatras, Dimitrios Triantafyllou, Christos Basdekis, Theoktisti Marinopoulou, Panagiotis Lepentsiotis, Elefterios Blitsis, Aggeliki Zacharaki, Nearchos Stylianidis, Leonidas Katelaris, Lamberto Salvan, Aris S. Lalos, Christos Laoudias, Antonios Lalas, Konstantinos Votis</p>
                    <p><strong>Published:</strong> 8/25/2025</p>
                    <p><strong>Categories:</strong> cs.RO, cs.CV</p>
                    <p><strong>Summary:</strong> This paper introduces a holistic perception system for internal and external monitoring of autonomous vehicles, with the aim of demonstrating a novel AI-leveraged self-adaptive framework of advanced vehicle technologies and solutions that optimize perception and experience on-board. Internal monitoring system relies on a multi-camera setup designed for predicting and identifying driver and occupant behavior through facial recognition, exploiting in addition a large language model as virtual assistant. Moreover, the in-cabin monitoring system includes AI-empowered smart sensors that measure air-quality and perform thermal comfort analysis for efficient on and off-boarding. On the other hand, external monitoring system perceives the surrounding environment of vehicle, through a LiDAR-based cost-efficient semantic segmentation approach, that performs highly accurate and efficient super-resolution on low-quality raw 3D point clouds. The holistic perception framework is developed in the context of EUs Horizon Europe programm AutoTRUST, and has been integrated and deployed on a real electric vehicle provided by ALKE. Experimental validation and evaluation at the integration site of Joint Research Centre at Ispra, Italy, highlights increased performance and efficiency of the modular blocks of the proposed perception architecture.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.17961v1" target="_blank">Beam Geometry and Input Dimensionality: Impact on Sparse-Sampling Artifact Correction for Clinical CT with U-Nets</a></h3>
                    <p><strong>Authors:</strong> Tina Dorosti, Johannes Thalhammer, Sebastian Peterhansl, Daniela Pfeiffer, Franz Pfeiffer, Florian Schaff</p>
                    <p><strong>Published:</strong> 8/25/2025</p>
                    <p><strong>Categories:</strong> cs.CV, physics.med-ph</p>
                    <p><strong>Summary:</strong> This study aims to investigate the effect of various beam geometries and dimensions of input data on the sparse-sampling streak artifact correction task with U-Nets for clinical CT scans as a means of incorporating the volumetric context into artifact reduction tasks to improve model performance. A total of 22 subjects were retrospectively selected (01.2016-12.2018) from the Technical University of Munichs research hospital, TUM Klinikum rechts der Isar. Sparsely-sampled CT volumes were simulated with the Astra toolbox for parallel, fan, and cone beam geometries. 2048 views were taken as full-view scans. 2D and 3D U-Nets were trained and validated on 14, and tested on 8 subjects, respectively. For the dimensionality study, in addition to the 512x512 2D CT images, the CT scans were further pre-processed to generate a so-called 2.5D, and 3D data: Each CT volume was divided into 64x64x64 voxel blocks. The 3D data refers to individual 64-voxel blocks. An axial, coronal, and sagittal cut through the center of each block resulted in three 64x64 2D patches that were rearranged as a single 64x64x3 image, proposed as 2.5D data. Model performance was assessed with the mean squared error (MSE) and structural similarity index measure (SSIM). For all geometries, the 2D U-Net trained on axial 2D slices results in the best MSE and SSIM values, outperforming the 2.5D and 3D input data dimensions.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.18270v1" target="_blank">Proving it is impossible; on ErdÅ‘s problem $\# 278$</a></h3>
                    <p><strong>Authors:</strong> Stijn Cambie</p>
                    <p><strong>Published:</strong> 8/25/2025</p>
                    <p><strong>Categories:</strong> math.CO, 05-01</p>
                    <p><strong>Summary:</strong> Erd\H{o}s asked many mathematical questions. Some lead to exciting research, others turned out to be easily solved. In this article, we provide evidence that one of his questions, Erd\H{o}s problem \#278 , has no general answer. We do so by relating it with a hard knapsack problem instance,and by demonstrating that different, non-equivalent formulas arise depending on the structure of the moduli.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.18267v1" target="_blank">Caregiver-in-the-Loop AI: A Simulation-Based Feasibility Study for Dementia Task Verification</a></h3>
                    <p><strong>Authors:</strong> Joy Lai, David Black, Kelly Beaton, Bing Ye, Alex Mihailidis</p>
                    <p><strong>Published:</strong> 8/25/2025</p>
                    <p><strong>Categories:</strong> cs.HC</p>
                    <p><strong>Summary:</strong> Caregivers of people living with dementia (PLwD) experience stress when verifying whether tasks are truly completed, even with digital reminder systems. Generative AI, such as GPT-4, may help by automating task verification through follow-up questioning and decision support. This feasibility study evaluates an AI-powered task verification system integrated with digital reminders for PLwD. It examines (1) GPT-4s ability to generate effective follow-up questions, (2) the accuracy of an AI-driven response flagging mechanism, and (3) the role of caregiver feedback in refining system adaptability. A simulated pipeline was tested on 64 anonymized reminders. GPT-4 generated follow-up questions with and without contextual information about PLwD routines. Responses were classified into High, Medium, or Low concern, and simulated caregiver feedback was used to refine outputs. Results show that contextual information and caregiver input improved the clarity and relevance of AI-generated questions. The flagging system accurately identified concerns, particularly for safety-critical tasks, though subjective or non-urgent tasks remained challenging. Findings demonstrate the feasibility of AI-assisted task verification in dementia care. Context-aware AI prompts and caregiver feedback can enhance task monitoring, reduce caregiver stress, and strengthen PLwD support. Future work should focus on real-world validation and scalability.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.18265v1" target="_blank">InternVL3.5: Advancing Open-Source Multimodal Models in Versatility, Reasoning, and Efficiency</a></h3>
                    <p><strong>Authors:</strong> Weiyun Wang, Zhangwei Gao, Lixin Gu, Hengjun Pu, Long Cui, Xingguang Wei, Zhaoyang Liu, Linglin Jing, Shenglong Ye, Jie Shao, Zhaokai Wang, Zhe Chen, Hongjie Zhang, Ganlin Yang, Haomin Wang, Qi Wei, Jinhui Yin, Wenhao Li, Erfei Cui, Guanzhou Chen, Zichen Ding, Changyao Tian, Zhenyu Wu, Jingjing Xie, Zehao Li, Bowen Yang, Yuchen Duan, Xuehui Wang, Songze Li, Xiangyu Zhao, Haodong Duan, Nianchen Deng, Bin Fu, Yinan He, Yi Wang, Conghui He, Botian Shi, Junjun He, Yingtong Xiong, Han Lv, Lijun Wu, Wenqi Shao, Kaipeng Zhang, Huipeng Deng, Biqing Qi, Jiaye Ge, Qipeng Guo, Wenwei Zhang, Wanli Ouyang, Limin Wang, Min Dou, Xizhou Zhu, Tong Lu, Dahua Lin, Jifeng Dai, Bowen Zhou, Weijie Su, Kai Chen, Yu Qiao, Wenhai Wang, Gen Luo</p>
                    <p><strong>Published:</strong> 8/25/2025</p>
                    <p><strong>Categories:</strong> cs.CV</p>
                    <p><strong>Summary:</strong> We introduce InternVL 3.5, a new family of open-source multimodal models that significantly advances versatility, reasoning capability, and inference efficiency along the InternVL series. A key innovation is the Cascade Reinforcement Learning (Cascade RL) framework, which enhances reasoning through a two-stage process: offline RL for stable convergence and online RL for refined alignment. This coarse-to-fine training strategy leads to substantial improvements on downstream reasoning tasks, e.g., MMMU and MathVista. To optimize efficiency, we propose a Visual Resolution Router (ViR) that dynamically adjusts the resolution of visual tokens without compromising performance. Coupled with ViR, our Decoupled Vision-Language Deployment (DvD) strategy separates the vision encoder and language model across different GPUs, effectively balancing computational load. These contributions collectively enable InternVL3.5 to achieve up to a +16.0\% gain in overall reasoning performance and a 4.05$\times$ inference speedup compared to its predecessor, i.e., InternVL3. In addition, InternVL3.5 supports novel capabilities such as GUI interaction and embodied agency. Notably, our largest model, i.e., InternVL3.5-241B-A28B, attains state-of-the-art results among open-source MLLMs across general multimodal, reasoning, text, and agentic tasks -- narrowing the performance gap with leading commercial models like GPT-5. All models and code are publicly released.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.18260v1" target="_blank">MIRAGE: Scaling Test-Time Inference with Parallel Graph-Retrieval-Augmented Reasoning Chains</a></h3>
                    <p><strong>Authors:</strong> Kaiwen Wei, Rui Shan, Dongsheng Zou, Jianzhong Yang, Bi Zhao, Junnan Zhu, Jiang Zhong</p>
                    <p><strong>Published:</strong> 8/25/2025</p>
                    <p><strong>Categories:</strong> cs.CL, I.2.3; I.2.4; I.2.7</p>
                    <p><strong>Summary:</strong> Large reasoning models (LRMs) have shown significant progress in test-time scaling through chain-of-thought prompting. Current approaches like search-o1 integrate retrieval augmented generation (RAG) into multi-step reasoning processes but rely on a single, linear reasoning chain while incorporating unstructured textual information in a flat, context-agnostic manner. As a result, these approaches can lead to error accumulation throughout the reasoning chain, which significantly limits its effectiveness in medical question-answering (QA) tasks where both accuracy and traceability are critical requirements. To address these challenges, we propose MIRAGE (Multi-chain Inference with Retrieval-Augmented Graph Exploration), a novel test-time scalable reasoning framework that performs dynamic multi-chain inference over structured medical knowledge graphs. Specifically, MIRAGE 1) decomposes complex queries into entity-grounded sub-questions, 2) executes parallel inference chains, 3) retrieves evidence adaptively via neighbor expansion and multi-hop traversal, and 4) integrates answers using cross-chain verification to resolve contradictions. Experiments on three medical QA benchmarks (GenMedGPT-5k, CMCQA, and ExplainCPE) show that MIRAGE consistently outperforms GPT-4o, Tree-of-Thought variants, and other retrieval-augmented baselines in both automatic and human evaluations. Additionally, MIRAGE improves interpretability by generating explicit reasoning chains that trace each factual claim to concrete chains within the knowledge graph, making it well-suited for complex medical reasoning scenarios. The code will be available for further research.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.18255v1" target="_blank">Hermes 4 Technical Report</a></h3>
                    <p><strong>Authors:</strong> Ryan Teknium, Roger Jin, Jai Suphavadeeprasit, Dakota Mahan, Jeffrey Quesnelle, Joe Li, Chen Guang, Shannon Sands, Karan Malhotra</p>
                    <p><strong>Published:</strong> 8/25/2025</p>
                    <p><strong>Categories:</strong> cs.AI</p>
                    <p><strong>Summary:</strong> We present Hermes 4, a family of hybrid reasoning models that combine structured, multi-turn reasoning with broad instruction-following ability. We describe the challenges encountered during data curation, synthesis, training, and evaluation, and outline the solutions employed to address these challenges at scale. We comprehensively evaluate across mathematical reasoning, coding, knowledge, comprehension, and alignment benchmarks, and we report both quantitative performance and qualitative behavioral analysis. To support open research, all model weights are published publicly at https://huggingface.co/collections/NousResearch/hermes-4-collection-68a731bfd452e20816725728</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.18271v1" target="_blank">ObjFiller-3D: Consistent Multi-view 3D Inpainting via Video Diffusion Models</a></h3>
                    <p><strong>Authors:</strong> Haitang Feng, Jie Liu, Jie Tang, Gangshan Wu, Beiqi Chen, Jianhuang Lai, Guangcong Wang</p>
                    <p><strong>Published:</strong> 8/25/2025</p>
                    <p><strong>Categories:</strong> cs.CV</p>
                    <p><strong>Summary:</strong> 3D inpainting often relies on multi-view 2D image inpainting, where the inherent inconsistencies across different inpainted views can result in blurred textures, spatial discontinuities, and distracting visual artifacts. These inconsistencies pose significant challenges when striving for accurate and realistic 3D object completion, particularly in applications that demand high fidelity and structural coherence. To overcome these limitations, we propose ObjFiller-3D, a novel method designed for the completion and editing of high-quality and consistent 3D objects. Instead of employing a conventional 2D image inpainting model, our approach leverages a curated selection of state-of-the-art video editing model to fill in the masked regions of 3D objects. We analyze the representation gap between 3D and videos, and propose an adaptation of a video inpainting model for 3D scene inpainting. In addition, we introduce a reference-based 3D inpainting method to further enhance the quality of reconstruction. Experiments across diverse datasets show that compared to previous methods, ObjFiller-3D produces more faithful and fine-grained reconstructions (PSNR of 26.6 vs. NeRFiller (15.9) and LPIPS of 0.19 vs. Instant3dit (0.25)). Moreover, it demonstrates strong potential for practical deployment in real-world 3D editing applications. Project page: https://objfiller3d.github.io/ Code: https://github.com/objfiller3d/ObjFiller-3D .</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.18265v1" target="_blank">InternVL3.5: Advancing Open-Source Multimodal Models in Versatility, Reasoning, and Efficiency</a></h3>
                    <p><strong>Authors:</strong> Weiyun Wang, Zhangwei Gao, Lixin Gu, Hengjun Pu, Long Cui, Xingguang Wei, Zhaoyang Liu, Linglin Jing, Shenglong Ye, Jie Shao, Zhaokai Wang, Zhe Chen, Hongjie Zhang, Ganlin Yang, Haomin Wang, Qi Wei, Jinhui Yin, Wenhao Li, Erfei Cui, Guanzhou Chen, Zichen Ding, Changyao Tian, Zhenyu Wu, Jingjing Xie, Zehao Li, Bowen Yang, Yuchen Duan, Xuehui Wang, Songze Li, Xiangyu Zhao, Haodong Duan, Nianchen Deng, Bin Fu, Yinan He, Yi Wang, Conghui He, Botian Shi, Junjun He, Yingtong Xiong, Han Lv, Lijun Wu, Wenqi Shao, Kaipeng Zhang, Huipeng Deng, Biqing Qi, Jiaye Ge, Qipeng Guo, Wenwei Zhang, Wanli Ouyang, Limin Wang, Min Dou, Xizhou Zhu, Tong Lu, Dahua Lin, Jifeng Dai, Bowen Zhou, Weijie Su, Kai Chen, Yu Qiao, Wenhai Wang, Gen Luo</p>
                    <p><strong>Published:</strong> 8/25/2025</p>
                    <p><strong>Categories:</strong> cs.CV</p>
                    <p><strong>Summary:</strong> We introduce InternVL 3.5, a new family of open-source multimodal models that significantly advances versatility, reasoning capability, and inference efficiency along the InternVL series. A key innovation is the Cascade Reinforcement Learning (Cascade RL) framework, which enhances reasoning through a two-stage process: offline RL for stable convergence and online RL for refined alignment. This coarse-to-fine training strategy leads to substantial improvements on downstream reasoning tasks, e.g., MMMU and MathVista. To optimize efficiency, we propose a Visual Resolution Router (ViR) that dynamically adjusts the resolution of visual tokens without compromising performance. Coupled with ViR, our Decoupled Vision-Language Deployment (DvD) strategy separates the vision encoder and language model across different GPUs, effectively balancing computational load. These contributions collectively enable InternVL3.5 to achieve up to a +16.0\% gain in overall reasoning performance and a 4.05$\times$ inference speedup compared to its predecessor, i.e., InternVL3. In addition, InternVL3.5 supports novel capabilities such as GUI interaction and embodied agency. Notably, our largest model, i.e., InternVL3.5-241B-A28B, attains state-of-the-art results among open-source MLLMs across general multimodal, reasoning, text, and agentic tasks -- narrowing the performance gap with leading commercial models like GPT-5. All models and code are publicly released.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.18264v1" target="_blank">MMTok: Multimodal Coverage Maximization for Efficient Inference of VLMs</a></h3>
                    <p><strong>Authors:</strong> Sixun Dong, Juhua Hu, Mian Zhang, Ming Yin, Yanjie Fu, Qi Qian</p>
                    <p><strong>Published:</strong> 8/25/2025</p>
                    <p><strong>Categories:</strong> cs.CV</p>
                    <p><strong>Summary:</strong> Vision-Language Models (VLMs) demonstrate impressive performance in understanding visual content with language instruction by converting visual input to vision tokens. However, redundancy in vision tokens results in the degenerated inference efficiency of VLMs. While many algorithms have been proposed to reduce the number of vision tokens, most of them apply only unimodal information (i.e., vision/text) for pruning and ignore the inherent multimodal property of vision-language tasks. Moreover, it lacks a generic criterion that can be applied to different modalities. To mitigate this limitation, in this work, we propose to leverage both vision and text tokens to select informative vision tokens by the criterion of coverage. We first formulate the subset selection problem as a maximum coverage problem. Afterward, a subset of vision tokens is optimized to cover the text tokens and the original set of vision tokens, simultaneously. Finally, a VLM agent can be adopted to further improve the quality of text tokens for guiding vision pruning. The proposed method MMTok is extensively evaluated on benchmark datasets with different VLMs. The comparison illustrates that vision and text information are complementary, and combining multimodal information can surpass the unimodal baseline with a clear margin. Moreover, under the maximum coverage criterion on the POPE dataset, our method achieves a 1.87x speedup while maintaining 98.7% of the original performance on LLaVA-NeXT-13B. Furthermore, with only four vision tokens, it still preserves 87.7% of the original performance on LLaVA-1.5-7B. These results highlight the effectiveness of coverage in token selection.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.18260v1" target="_blank">MIRAGE: Scaling Test-Time Inference with Parallel Graph-Retrieval-Augmented Reasoning Chains</a></h3>
                    <p><strong>Authors:</strong> Kaiwen Wei, Rui Shan, Dongsheng Zou, Jianzhong Yang, Bi Zhao, Junnan Zhu, Jiang Zhong</p>
                    <p><strong>Published:</strong> 8/25/2025</p>
                    <p><strong>Categories:</strong> cs.CL, I.2.3; I.2.4; I.2.7</p>
                    <p><strong>Summary:</strong> Large reasoning models (LRMs) have shown significant progress in test-time scaling through chain-of-thought prompting. Current approaches like search-o1 integrate retrieval augmented generation (RAG) into multi-step reasoning processes but rely on a single, linear reasoning chain while incorporating unstructured textual information in a flat, context-agnostic manner. As a result, these approaches can lead to error accumulation throughout the reasoning chain, which significantly limits its effectiveness in medical question-answering (QA) tasks where both accuracy and traceability are critical requirements. To address these challenges, we propose MIRAGE (Multi-chain Inference with Retrieval-Augmented Graph Exploration), a novel test-time scalable reasoning framework that performs dynamic multi-chain inference over structured medical knowledge graphs. Specifically, MIRAGE 1) decomposes complex queries into entity-grounded sub-questions, 2) executes parallel inference chains, 3) retrieves evidence adaptively via neighbor expansion and multi-hop traversal, and 4) integrates answers using cross-chain verification to resolve contradictions. Experiments on three medical QA benchmarks (GenMedGPT-5k, CMCQA, and ExplainCPE) show that MIRAGE consistently outperforms GPT-4o, Tree-of-Thought variants, and other retrieval-augmented baselines in both automatic and human evaluations. Additionally, MIRAGE improves interpretability by generating explicit reasoning chains that trace each factual claim to concrete chains within the knowledge graph, making it well-suited for complex medical reasoning scenarios. The code will be available for further research.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.18258v1" target="_blank">ANO : Faster is Better in Noisy Landscape</a></h3>
                    <p><strong>Authors:</strong> Adrien Kegreisz</p>
                    <p><strong>Published:</strong> 8/25/2025</p>
                    <p><strong>Categories:</strong> cs.LG, cs.AI</p>
                    <p><strong>Summary:</strong> Stochastic optimizers are central to deep learning, yet widely used methods such as Adam and Adan can degrade in non-stationary or noisy environments, partly due to their reliance on momentum-based magnitude estimates. We introduce Ano, a novel optimizer that decouples direction and magnitude: momentum is used for directional smoothing, while instantaneous gradient magnitudes determine step size. This design improves robustness to gradient noise while retaining the simplicity and efficiency of first-order methods. We further propose Anolog, which removes sensitivity to the momentum coefficient by expanding its window over time via a logarithmic schedule. We establish non-convex convergence guarantees with a convergence rate similar to other sign-based methods, and empirically show that Ano provides substantial gains in noisy and non-stationary regimes such as reinforcement learning, while remaining competitive on low-noise tasks such as standard computer vision benchmarks.</p>
                
            
        
        
            <p><em>Generated by AI News Agent</em></p>
        
    

