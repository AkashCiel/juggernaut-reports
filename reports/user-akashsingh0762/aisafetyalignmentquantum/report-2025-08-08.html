
    
        <h1>ðŸ¤– AI Research Report</h1>
        
            <strong>Date:</strong> 2025-08-08<br>
            <strong>Topics:</strong> ai safety research, ai alignment research, quantum computing<br>
            <strong>Papers Found:</strong> 150
        
        
        
            
                <h2>ðŸ¤– AI Summary</h2>
                <p>## ai safety research

The landscape of AI safety research is rapidly evolving, with a focus on creating robust and generalizable systems that can operate safely under a variety of conditions and disruptions. In Towards Generalizable Safety in Crowd Navigation via Conformal Uncertainty Handling, the authors tackle the challenge of mobile robots navigating unpredictable environments. By integrating adaptive conformal inference with reinforcement learning, the study demonstrates significant improvements in safety and robustness under both in-distribution and out-of-distribution scenarios, highlighting a trend toward leveraging uncertainty to enhance AI safety in real-world applications.

Another significant advancement is seen in The Missing Reward: Active Inference in the Era of Experience, which addresses the issue of AI systems reliance on human-engineered rewards for learning, proposing Active Inference (AIF) as a scalable solution. AIF aims to replace external rewards with an intrinsic drive to minimize free energy, offering a path to autonomous AI agents capable of learning from experience while adhering to human values. This paper underscores a broader trend towards developing AI systems that can autonomously navigate complex environments with minimal human intervention, a critical aspect of ensuring AI systems remain aligned with human safety and ethical standards. These advancements collectively reflect a growing emphasis on creating AI systems that are not only highly functional but also inherently safe and aligned with human norms.

*Based on 50 research papers*

---

## ai alignment research

Recent advancements in AI alignment research, as reflected in the provided papers, emphasize the increasing necessity to bridge the gap between theoretical AI capabilities and practical, real-world applications, particularly in complex and dynamic environments. Notably, the Genie Envisioner platform highlights a trend towards developing comprehensive frameworks that integrate simulation, policy learning, and evaluation for embodied intelligence, underscoring the importance of instruction-driven AI alignment. This aligns with a broader push to create more adaptable, context-aware systems that can operate effectively across different scenarios with minimal supervision. Similarly, the LearnerAgent framework exemplifies the exploration of human-like learning dynamics using AI, revealing insights into cognitive growth and the challenges in achieving true understanding and adaptability akin to human learners.

Moreover, several papers underscore the challenges and gaps in current AI systems alignment with human values and expectations. For instance, The Missing Reward paper criticizes the scalability of current reward engineering paradigms and suggests Active Inference (AIF) as a means to imbue AI agents with intrinsic motivations that align more naturally with human values. This suggests a shift towards more autonomous AI systems capable of self-directed learning and decision-making. Additionally, the OmniEAR benchmark exposes the limitations of current large language models in embodied reasoning tasks, highlighting the need for enhanced alignment mechanisms that can handle nuanced, real-world interactions. These findings collectively indicate a critical need for developing robust AI alignment frameworks that not only optimize performance but also ensure that AI behaviors and objectives remain consistent with human values and societal norms.

*Based on 50 research papers*</p>
            
        
        
        <h2>ðŸ“š Research Papers</h2>
        
                
                    <h3><a href="http://arxiv.org/abs/2508.05634v1" target="_blank">Towards Generalizable Safety in Crowd Navigation via Conformal Uncertainty Handling</a></h3>
                    <p><strong>Authors:</strong> Jianpeng Yao, Xiaopan Zhang, Yu Xia, Zejin Wang, Amit K. Roy-Chowdhury, Jiachen Li</p>
                    <p><strong>Published:</strong> 8/7/2025</p>
                    <p><strong>Categories:</strong> cs.RO, cs.AI, cs.CV, cs.LG, cs.SY, eess.SY</p>
                    <p><strong>Summary:</strong> Mobile robots navigating in crowds trained using reinforcement learning are known to suffer performance degradation when faced with out-of-distribution scenarios. We propose that by properly accounting for the uncertainties of pedestrians, a robot can learn safe navigation policies that are robust to distribution shifts. Our method augments agent observations with prediction uncertainty estimates generated by adaptive conformal inference, and it uses these estimates to guide the agents behavior through constrained reinforcement learning. The system helps regulate the agents actions and enables it to adapt to distribution shifts. In the in-distribution setting, our approach achieves a 96.93% success rate, which is over 8.80% higher than the previous state-of-the-art baselines with over 3.72 times fewer collisions and 2.43 times fewer intrusions into ground-truth human future trajectories. In three out-of-distribution scenarios, our method shows much stronger robustness when facing distribution shifts in velocity variations, policy changes, and transitions from individual to group dynamics. We deploy our method on a real robot, and experiments show that the robot makes safe and robust decisions when interacting with both sparse and dense crowds. Our code and videos are available on https://gen-safe-nav.github.io/.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.05633v1" target="_blank">KuaiLive: A Real-time Interactive Dataset for Live Streaming Recommendation</a></h3>
                    <p><strong>Authors:</strong> Changle Qu, Sunhao Dai, Ke Guo, Liqin Zhao, Yanan Niu, Xiao Zhang, Jun Xu</p>
                    <p><strong>Published:</strong> 8/7/2025</p>
                    <p><strong>Categories:</strong> cs.IR, cs.AI</p>
                    <p><strong>Summary:</strong> Live streaming platforms have become a dominant form of online content consumption, offering dynamically evolving content, real-time interactions, and highly engaging user experiences. These unique characteristics introduce new challenges that differentiate live streaming recommendation from traditional recommendation settings and have garnered increasing attention from industry in recent years. However, research progress in academia has been hindered by the lack of publicly available datasets that accurately reflect the dynamic nature of live streaming environments. To address this gap, we introduce KuaiLive, the first real-time, interactive dataset collected from Kuaishou, a leading live streaming platform in China with over 400 million daily active users. The dataset records the interaction logs of 23,772 users and 452,621 streamers over a 21-day period. Compared to existing datasets, KuaiLive offers several advantages: it includes precise live room start and end timestamps, multiple types of real-time user interactions (click, comment, like, gift), and rich side information features for both users and streamers. These features enable more realistic simulation of dynamic candidate items and better modeling of user and streamer behaviors. We conduct a thorough analysis of KuaiLive from multiple perspectives and evaluate several representative recommendation methods on it, establishing a strong benchmark for future research. KuaiLive can support a wide range of tasks in the live streaming domain, such as top-K recommendation, click-through rate prediction, watch time prediction, and gift price prediction. Moreover, its fine-grained behavioral data also enables research on multi-behavior modeling, multi-task learning, and fairness-aware recommendation. The dataset and related resources are publicly available at https://imgkkk574.github.io/KuaiLive.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.05630v1" target="_blank">MOSEv2: A More Challenging Dataset for Video Object Segmentation in Complex Scenes</a></h3>
                    <p><strong>Authors:</strong> Henghui Ding, Kaining Ying, Chang Liu, Shuting He, Xudong Jiang, Yu-Gang Jiang, Philip H. S. Torr, Song Bai</p>
                    <p><strong>Published:</strong> 8/7/2025</p>
                    <p><strong>Categories:</strong> cs.CV</p>
                    <p><strong>Summary:</strong> Video object segmentation (VOS) aims to segment specified target objects throughout a video. Although state-of-the-art methods have achieved impressive performance (e.g., 90+% JF) on existing benchmarks such as DAVIS and YouTube-VOS, these datasets primarily contain salient, dominant, and isolated objects, limiting their generalization to real-world scenarios. To advance VOS toward more realistic environments, coMplex video Object SEgmentation (MOSEv1) was introduced to facilitate VOS research in complex scenes. Building on the strengths and limitations of MOSEv1, we present MOSEv2, a significantly more challenging dataset designed to further advance VOS methods under real-world conditions. MOSEv2 consists of 5,024 videos and over 701,976 high-quality masks for 10,074 objects across 200 categories. Compared to its predecessor, MOSEv2 introduces significantly greater scene complexity, including more frequent object disappearance and reappearance, severe occlusions and crowding, smaller objects, as well as a range of new challenges such as adverse weather (e.g., rain, snow, fog), low-light scenes (e.g., nighttime, underwater), multi-shot sequences, camouflaged objects, non-physical targets (e.g., shadows, reflections), scenarios requiring external knowledge, etc. We benchmark 20 representative VOS methods under 5 different settings and observe consistent performance drops. For example, SAM2 drops from 76.4% on MOSEv1 to only 50.9% on MOSEv2. We further evaluate 9 video object tracking methods and find similar declines, demonstrating that MOSEv2 presents challenges across tasks. These results highlight that despite high accuracy on existing datasets, current VOS methods still struggle under real-world complexities. MOSEv2 is publicly available at https://MOSE.video.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.05622v1" target="_blank">Simulating Human-Like Learning Dynamics with LLM-Empowered Agents</a></h3>
                    <p><strong>Authors:</strong> Yu Yuan, Lili Zhao, Wei Chen, Guangting Zheng, Kai Zhang, Mengdi Zhang, Qi Liu</p>
                    <p><strong>Published:</strong> 8/7/2025</p>
                    <p><strong>Categories:</strong> cs.AI</p>
                    <p><strong>Summary:</strong> Capturing human learning behavior based on deep learning methods has become a major research focus in both psychology and intelligent systems. Recent approaches rely on controlled experiments or rule-based models to explore cognitive processes. However, they struggle to capture learning dynamics, track progress over time, or provide explainability. To address these challenges, we introduce LearnerAgent, a novel multi-agent framework based on Large Language Models (LLMs) to simulate a realistic teaching environment. To explore human-like learning dynamics, we construct learners with psychologically grounded profiles-such as Deep, Surface, and Lazy-as well as a persona-free General Learner to inspect the base LLMs default behavior. Through weekly knowledge acquisition, monthly strategic choices, periodic tests, and peer interaction, we can track the dynamic learning progress of individual learners over a full-year journey. Our findings are fourfold: 1) Longitudinal analysis reveals that only Deep Learner achieves sustained cognitive growth. Our specially designed trap questions effectively diagnose Surface Learners shallow knowledge. 2) The behavioral and cognitive patterns of distinct learners align closely with their psychological profiles. 3) Learners self-concept scores evolve realistically, with the General Learner developing surprisingly high self-efficacy despite its cognitive limitations. 4) Critically, the default profile of base LLM is a diligent but brittle Surface Learner-an agent that mimics the behaviors of a good student but lacks true, generalizable understanding. Extensive simulation experiments demonstrate that LearnerAgent aligns well with real scenarios, yielding more insightful findings about LLMs behavior.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.05619v1" target="_blank">The Missing Reward: Active Inference in the Era of Experience</a></h3>
                    <p><strong>Authors:</strong> Bo Wen</p>
                    <p><strong>Published:</strong> 8/7/2025</p>
                    <p><strong>Categories:</strong> cs.AI, nlin.AO, physics.bio-ph, physics.comp-ph, physics.hist-ph</p>
                    <p><strong>Summary:</strong> This paper argues that Active Inference (AIF) provides a crucial foundation for developing autonomous AI agents capable of learning from experience without continuous human reward engineering. As AI systems begin to exhaust high-quality training data and rely on increasingly large human workforces for reward design, the current paradigm faces significant scalability challenges that could impede progress toward genuinely autonomous intelligence. The proposal for an ``Era of Experience, where agents learn from self-generated data, is a promising step forward. However, this vision still depends on extensive human engineering of reward functions, effectively shifting the bottleneck from data curation to reward curation. This highlights what we identify as the \textbf{grounded-agency gap}: the inability of contemporary AI systems to autonomously formulate, adapt, and pursue objectives in response to changing circumstances. We propose that AIF can bridge this gap by replacing external reward signals with an intrinsic drive to minimize free energy, allowing agents to naturally balance exploration and exploitation through a unified Bayesian objective. By integrating Large Language Models as generative world models with AIFs principled decision-making framework, we can create agents that learn efficiently from experience while remaining aligned with human values. This synthesis offers a compelling path toward AI systems that can develop autonomously while adhering to both computational and physical constraints.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.05616v1" target="_blank">TrajEvo: Trajectory Prediction Heuristics Design via LLM-driven Evolution</a></h3>
                    <p><strong>Authors:</strong> Zhikai Zhao, Chuanbo Hua, Federico Berto, Kanghoon Lee, Zihan Ma, Jiachen Li, Jinkyoo Park</p>
                    <p><strong>Published:</strong> 8/7/2025</p>
                    <p><strong>Categories:</strong> cs.LG, cs.AI, cs.NE, cs.RO</p>
                    <p><strong>Summary:</strong> Trajectory prediction is a critical task in modeling human behavior, especially in safety-critical domains such as social robotics and autonomous vehicle navigation. Traditional heuristics based on handcrafted rules often lack accuracy and generalizability. Although deep learning approaches offer improved performance, they typically suffer from high computational cost, limited explainability, and, importantly, poor generalization to out-of-distribution (OOD) scenarios. In this paper, we introduce TrajEvo, a framework that leverages Large Language Models (LLMs) to automatically design trajectory prediction heuristics. TrajEvo employs an evolutionary algorithm to generate and refine prediction heuristics from past trajectory data. We propose two key innovations: Cross-Generation Elite Sampling to encourage population diversity, and a Statistics Feedback Loop that enables the LLM to analyze and improve alternative predictions. Our evaluations demonstrate that TrajEvo outperforms existing heuristic methods across multiple real-world datasets, and notably surpasses both heuristic and deep learning methods in generalizing to an unseen OOD real-world dataset. TrajEvo marks a promising step toward the automated design of fast, explainable, and generalizable trajectory prediction heuristics. We release our source code to facilitate future research at https://github.com/ai4co/trajevo.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.05614v1" target="_blank">OmniEAR: Benchmarking Agent Reasoning in Embodied Tasks</a></h3>
                    <p><strong>Authors:</strong> Zixuan Wang, Dingming Li, Hongxing Li, Shuo Chen, Yuchen Yan, Wenqi Zhang, Yongliang Shen, Weiming Lu, Jun Xiao, Yueting Zhuang</p>
                    <p><strong>Published:</strong> 8/7/2025</p>
                    <p><strong>Categories:</strong> cs.CL, cs.AI</p>
                    <p><strong>Summary:</strong> Large language models excel at abstract reasoning but their capacity for embodied agent reasoning remains largely unexplored. We present OmniEAR, a comprehensive framework for evaluating how language models reason about physical interactions, tool usage, and multi-agent coordination in embodied tasks. Unlike existing benchmarks that provide predefined tool sets or explicit collaboration directives, OmniEAR requires agents to dynamically acquire capabilities and autonomously determine coordination strategies based on task demands. Through text-based environment representation, we model continuous physical properties and complex spatial relationships across 1,500 scenarios spanning household and industrial domains. Our systematic evaluation reveals severe performance degradation when models must reason from constraints: while achieving 85-96% success with explicit instructions, performance drops to 56-85% for tool reasoning and 63-85% for implicit collaboration, with compound tasks showing over 50% failure rates. Surprisingly, complete environmental information degrades coordination performance, indicating models cannot filter task-relevant constraints. Fine-tuning improves single-agent tasks dramatically (0.6% to 76.3%) but yields minimal multi-agent gains (1.5% to 5.5%), exposing fundamental architectural limitations. These findings demonstrate that embodied reasoning poses fundamentally different challenges than current models can address, establishing OmniEAR as a rigorous benchmark for evaluating and advancing embodied AI systems. Our code and data are included in the supplementary materials and will be open-sourced upon acceptance.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.05602v1" target="_blank">LLaVA-RE: Binary Image-Text Relevancy Evaluation with Multimodal Large Language Model</a></h3>
                    <p><strong>Authors:</strong> Tao Sun, Oliver Liu, JinJin Li, Lan Ma</p>
                    <p><strong>Published:</strong> 8/7/2025</p>
                    <p><strong>Categories:</strong> cs.CV</p>
                    <p><strong>Summary:</strong> Multimodal generative AI usually involves generating image or text responses given inputs in another modality. The evaluation of image-text relevancy is essential for measuring response quality or ranking candidate responses. In particular, binary relevancy evaluation, i.e., ``Relevant vs. ``Not Relevant, is a fundamental problem. However, this is a challenging task considering that texts have diverse formats and the definition of relevancy varies in different scenarios. We find that Multimodal Large Language Models (MLLMs) are an ideal choice to build such evaluators, as they can flexibly handle complex text formats and take in additional task information. In this paper, we present LLaVA-RE, a first attempt for binary image-text relevancy evaluation with MLLM. It follows the LLaVA architecture and adopts detailed task instructions and multimodal in-context samples. In addition, we propose a novel binary relevancy data set that covers various tasks. Experimental results validate the effectiveness of our framework.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.05584v1" target="_blank">Robust adaptive fuzzy sliding mode control for trajectory tracking for of cylindrical manipulator</a></h3>
                    <p><strong>Authors:</strong> Van Cuong Pham, Minh Hai Tran, Phuc Anh Nguyen, Ngoc Son Vu, Nga Nguyen Thi</p>
                    <p><strong>Published:</strong> 8/7/2025</p>
                    <p><strong>Categories:</strong> cs.RO</p>
                    <p><strong>Summary:</strong> This research proposes a robust adaptive fuzzy sliding mode control (AFSMC) approach to enhance the trajectory tracking performance of cylindrical robotic manipulators, extensively utilized in applications such as CNC and 3D printing. The proposed approach integrates fuzzy logic with sliding mode control (SMC) to bolster adaptability and robustness, with fuzzy logic approximating the uncertain dynamics of the system, while SMC ensures strong performance. Simulation results in MATLAB/Simulink demonstrate that AFSMC significantly improves trajectory tracking accuracy, stability, and disturbance rejection compared to traditional methods. This research underscores the effectiveness of AFSMC in controlling robotic manipulators, contributing to enhanced precision in industrial robotic applications.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.05583v1" target="_blank">Research on integrated intelligent energy management system based on big data analysis and machine learning</a></h3>
                    <p><strong>Authors:</strong> Jinzhou Xu, Yadan Zhang, Paola Tapia</p>
                    <p><strong>Published:</strong> 8/7/2025</p>
                    <p><strong>Categories:</strong> eess.SY, cs.SY</p>
                    <p><strong>Summary:</strong> The application of big data is one of the significant features of integrated smart energy. Applying it to the file management of integrated smart energy projects is of great significance for improving the efficiency of project management and control. This article first discussed the benefits and challenges of implementing big data analysis in document management and control of integrated smart energy projects. In addition, an implementation framework for big data analysis in integrated smart energy project document management was developed, and a method for optimizing the efficiency of integrated smart energy project document management through machine learning was proposed. Using various types of data and information generated during the project document management process, the efficiency of the entire process project document control through three different machine learning methods was optimized. The result of fitting a penalty linear regression model shows that when there is enough data as a training set, the accuracy of the model achieved can reach over 95\%. By using big data analysis and machine learning to analyze the efficiency of comprehensive smart energy project document management, it is possible to track the entire process of comprehensive smart energy project documents and optimize business processes, thereby strengthening project construction control and improving project construction efficiency.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.05580v1" target="_blank">Follow-Your-Instruction: A Comprehensive MLLM Agent for World Data Synthesis</a></h3>
                    <p><strong>Authors:</strong> Kunyu Feng, Yue Ma, Xinhua Zhang, Boshi Liu, Yikuang Yuluo, Yinhan Zhang, Runtao Liu, Hongyu Liu, Zhiyuan Qin, Shanhui Mo, Qifeng Chen, Zeyu Wang</p>
                    <p><strong>Published:</strong> 8/7/2025</p>
                    <p><strong>Categories:</strong> cs.CV</p>
                    <p><strong>Summary:</strong> With the growing demands of AI-generated content (AIGC), the need for high-quality, diverse, and scalable data has become increasingly crucial. However, collecting large-scale real-world data remains costly and time-consuming, hindering the development of downstream applications. While some works attempt to collect task-specific data via a rendering process, most approaches still rely on manual scene construction, limiting their scalability and accuracy. To address these challenges, we propose Follow-Your-Instruction, a Multimodal Large Language Model (MLLM)-driven framework for automatically synthesizing high-quality 2D, 3D, and 4D data. Our \textbf{Follow-Your-Instruction} first collects assets and their associated descriptions through multimodal inputs using the MLLM-Collector. Then it constructs 3D layouts, and leverages Vision-Language Models (VLMs) for semantic refinement through multi-view scenes with the MLLM-Generator and MLLM-Optimizer, respectively. Finally, it uses MLLM-Planner to generate temporally coherent future frames. We evaluate the quality of the generated data through comprehensive experiments on the 2D, 3D, and 4D generative tasks. The results show that our synthetic data significantly boosts the performance of existing baseline models, demonstrating Follow-Your-Instructions potential as a scalable and effective data engine for generative intelligence.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.05572v1" target="_blank">Discrepancy-Aware Contrastive Adaptation in Medical Time Series Analysis</a></h3>
                    <p><strong>Authors:</strong> Yifan Wang, Hongfeng Ai, Ruiqi Li, Maowei Jiang, Ruiyuan Kang, Jiahua Dong, Cheng Jiang, Chenzhong Li</p>
                    <p><strong>Published:</strong> 8/7/2025</p>
                    <p><strong>Categories:</strong> cs.HC</p>
                    <p><strong>Summary:</strong> In medical time series disease diagnosis, two key challenges are identified. First, the high annotation cost of medical data leads to overfitting in models trained on label-limited, single-center datasets. To address this, we propose incorporating external data from related tasks and leveraging AE-GAN to extract prior knowledge, providing valuable references for downstream tasks. Second, many existing studies employ contrastive learning to derive more generalized medical sequence representations for diagnostic tasks, usually relying on manually designed diverse positive and negative sample pairs. However, these approaches are complex, lack generalizability, and fail to adaptively capture disease-specific features across different conditions. To overcome this, we introduce LMCF (Learnable Multi-views Contrastive Framework), a framework that integrates a multi-head attention mechanism and adaptively learns representations from different views through inter-view and intra-view contrastive learning strategies. Additionally, the pre-trained AE-GAN is used to reconstruct discrepancies in the target data as disease probabilities, which are then integrated into the contrastive learning process. Experiments on three target datasets demonstrate that our method consistently outperforms other seven baselines, highlighting its significant impact on healthcare applications such as the diagnosis of myocardial infarction, Alzheimers disease, and Parkinsons disease. We release the source code at xxxxx.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.05571v1" target="_blank">Fairy$\pm i$: the First 2-bit Complex LLM with All Parameters in $\{\pm1, \pm i\}$</a></h3>
                    <p><strong>Authors:</strong> Feiyu Wang, Guoan Wang, Yihao Zhang, Shengfan Wang, Weitao Li, Bokai Huang, Shimao Chen, Zihan Jiang, Rui Xu, Tong Yang</p>
                    <p><strong>Published:</strong> 8/7/2025</p>
                    <p><strong>Categories:</strong> cs.LG, cs.CL</p>
                    <p><strong>Summary:</strong> Quantization-Aware Training (QAT) integrates quantization into the training loop, enabling LLMs to learn robust low-bit representations, and is widely recognized as one of the most promising research directions. All current QAT research focuses on minimizing quantization error on full-precision models, where the full-precision accuracy acts as an upper bound (accuracy ceiling). No existing method has even attempted to surpass this ceiling. To break this ceiling, we propose a new paradigm: raising the ceiling (full-precision model), and then still quantizing it efficiently into 2 bits. We propose Fairy$\pm i$, the first 2-bit quantization framework for complex-valued LLMs. Specifically, our method leverages the representational advantages of the complex domain to boost full-precision accuracy. We map weights to the fourth roots of unity $\{\pm1, \pm i\}$, forming a perfectly symmetric and information-theoretically optimal 2-bit representation. Importantly, each quantized weight has either a zero real or imaginary part, enabling multiplication-free inference using only additions and element swaps. Experimental results show that Fairy$\pm i$ outperforms the ceiling of existing 2-bit quantization approaches in terms of both PPL and downstream tasks, while maintaining strict storage and compute efficiency. This work opens a new direction for building highly accurate and practical LLMs under extremely low-bit constraints.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.05557v1" target="_blank">MV-Debate: Multi-view Agent Debate with Dynamic Reflection Gating for Multimodal Harmful Content Detection in Social Media</a></h3>
                    <p><strong>Authors:</strong> Rui Lu, Jinhe Bi, Yunpu Ma, Feng Xiao, Yuntao Du, Yijun Tian</p>
                    <p><strong>Published:</strong> 8/7/2025</p>
                    <p><strong>Categories:</strong> cs.AI</p>
                    <p><strong>Summary:</strong> Social media has evolved into a complex multimodal environment where text, images, and other signals interact to shape nuanced meanings, often concealing harmful intent. Identifying such intent, whether sarcasm, hate speech, or misinformation, remains challenging due to cross-modal contradictions, rapid cultural shifts, and subtle pragmatic cues. To address these challenges, we propose MV-Debate, a multi-view agent debate framework with dynamic reflection gating for unified multimodal harmful content detection. MV-Debate assembles four complementary debate agents, a surface analyst, a deep reasoner, a modality contrast, and a social contextualist, to analyze content from diverse interpretive perspectives. Through iterative debate and reflection, the agents refine responses under a reflection-gain criterion, ensuring both accuracy and efficiency. Experiments on three benchmark datasets demonstrate that MV-Debate significantly outperforms strong single-model and existing multi-agent debate baselines. This work highlights the promise of multi-agent debate in advancing reliable social intent detection in safety-critical online contexts.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.05554v1" target="_blank">SPGISpeech 2.0: Transcribed multi-speaker financial audio for speaker-tagged transcription</a></h3>
                    <p><strong>Authors:</strong> Raymond Grossman, Taejin Park, Kunal Dhawan, Andrew Titus, Sophia Zhi, Yulia Shchadilova, Weiqing Wang, Jagadeesh Balam, Boris Ginsburg</p>
                    <p><strong>Published:</strong> 8/7/2025</p>
                    <p><strong>Categories:</strong> cs.SD, cs.CL, eess.AS</p>
                    <p><strong>Summary:</strong> We introduce SPGISpeech 2.0, a dataset suitable for speaker-tagged transcription in the financial domain. SPGISpeech 2.0 improves the diversity of applicable modeling tasks while maintaining the core characteristic of the original SPGISpeech dataset: audio snippets and their corresponding fully formatted text transcriptions, usable for end-to-end automatic speech recognition (ASR). SPGISpeech 2.0 consists of 3,780 additional hours of professionally transcribed earnings calls. Furthermore, the dataset contains call and speaker information for each audio snippet facilitating multi-talker ASR. We validate the utility of SPGISpeech 2.0 through improvements in speaker-tagged ASR performance of popular speech recognition models after fine-tuning on SPGISpeech 2.0. Released free for non-commercial use, we expect SPGISpeech 2.0 to foster advancements in speech recognition technologies and inspire a wide range of research applications.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.05550v1" target="_blank">PhysiBoSS-Models: A database for multiscale models</a></h3>
                    <p><strong>Authors:</strong> Vincent Noel, Marco Ruscone, Randy Heiland, Arnau Montagud, Alfonso Valencia, Emmanuel Barillot, Paul Macklin, Laurence Calzone</p>
                    <p><strong>Published:</strong> 8/7/2025</p>
                    <p><strong>Categories:</strong> q-bio.QM</p>
                    <p><strong>Summary:</strong> PhysiBoSS is an open-source platform that integrates agent-based modeling of cell populations with intracellular stochastic Boolean networks, enabling multiscale simulations of complex biological behaviors. To promote model sharing and versioning, we present the PhysiBoSS-Models database: a curated repository for multiscale models built with PhysiBoSS. By providing a simple Python API, PhysiBoSS-Models provides an easy way to download and simulate preexisting models through tools such as PhysiCell Studio. By providing standardized access to validated models, PhysiBoSS-Models facilitates reuse, validation, and benchmarking, supporting research in biology.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.05547v1" target="_blank">Adapting Vision-Language Models Without Labels: A Comprehensive Survey</a></h3>
                    <p><strong>Authors:</strong> Hao Dong, Lijun Sheng, Jian Liang, Ran He, Eleni Chatzi, Olga Fink</p>
                    <p><strong>Published:</strong> 8/7/2025</p>
                    <p><strong>Categories:</strong> cs.LG, cs.AI, cs.CV</p>
                    <p><strong>Summary:</strong> Vision-Language Models (VLMs) have demonstrated remarkable generalization capabilities across a wide range of tasks. However, their performance often remains suboptimal when directly applied to specific downstream scenarios without task-specific adaptation. To enhance their utility while preserving data efficiency, recent research has increasingly focused on unsupervised adaptation methods that do not rely on labeled data. Despite the growing interest in this area, there remains a lack of a unified, task-oriented survey dedicated to unsupervised VLM adaptation. To bridge this gap, we present a comprehensive and structured overview of the field. We propose a taxonomy based on the availability and nature of unlabeled visual data, categorizing existing approaches into four key paradigms: Data-Free Transfer (no data), Unsupervised Domain Transfer (abundant data), Episodic Test-Time Adaptation (batch data), and Online Test-Time Adaptation (streaming data). Within this framework, we analyze core methodologies and adaptation strategies associated with each paradigm, aiming to establish a systematic understanding of the field. Additionally, we review representative benchmarks across diverse applications and highlight open challenges and promising directions for future research. An actively maintained repository of relevant literature is available at https://github.com/tim-learn/Awesome-LabelFree-VLMs.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.05543v1" target="_blank">CleanUpBench: Embodied Sweeping and Grasping Benchmark</a></h3>
                    <p><strong>Authors:</strong> Wenbo Li, Guanting Chen, Tao Zhao, Jiyao Wang, Tianxin Hu, Yuwen Liao, Weixiang Guo, Shenghai Yuan</p>
                    <p><strong>Published:</strong> 8/7/2025</p>
                    <p><strong>Categories:</strong> cs.RO</p>
                    <p><strong>Summary:</strong> Embodied AI benchmarks have advanced navigation, manipulation, and reasoning, but most target complex humanoid agents or large-scale simulations that are far from real-world deployment. In contrast, mobile cleaning robots with dual mode capabilities, such as sweeping and grasping, are rapidly emerging as realistic and commercially viable platforms. However, no benchmark currently exists that systematically evaluates these agents in structured, multi-target cleaning tasks, revealing a critical gap between academic research and real-world applications. We introduce CleanUpBench, a reproducible and extensible benchmark for evaluating embodied agents in realistic indoor cleaning scenarios. Built on NVIDIA Isaac Sim, CleanUpBench simulates a mobile service robot equipped with a sweeping mechanism and a six-degree-of-freedom robotic arm, enabling interaction with heterogeneous objects. The benchmark includes manually designed environments and one procedurally generated layout to assess generalization, along with a comprehensive evaluation suite covering task completion, spatial efficiency, motion quality, and control performance. To support comparative studies, we provide baseline agents based on heuristic strategies and map-based planning. CleanUpBench bridges the gap between low-level skill evaluation and full-scene testing, offering a scalable testbed for grounded, embodied intelligence in everyday settings.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.05532v1" target="_blank">Aircraft routing: periodicity and complexity</a></h3>
                    <p><strong>Authors:</strong> FrÃ©dÃ©ric Meunier, Axel Parmentier, Nour ElHouda Tellache</p>
                    <p><strong>Published:</strong> 8/7/2025</p>
                    <p><strong>Categories:</strong> cs.DM, math.OC</p>
                    <p><strong>Summary:</strong> The aircraft routing problem is one of the most studied problems of operations research applied to aircraft management. It involves assigning flights to aircraft while ensuring regular visits to maintenance bases. This paper examines two aspects of the problem. First, we explore the relationship between periodic instances, where flights are the same every day, and periodic solutions. The literature has implicitly assumed-without discussion-that periodic instances necessitate periodic solutions, and even periodic solutions in a stronger form, where every two airplanes perform either the exact same cyclic sequence of flights, or completely disjoint cyclic sequences. However, enforcing such periodicity may eliminate feasible solutions. We prove that, when regular maintenance is required at most every four days, there always exist periodic solutions of this form. Second, we consider the computational hardness of the problem. Even if many papers in this area refer to the NP-hardness of the aircraft routing problem, such a result is only available in the literature for periodic instances. We establish its NP-hardness for a non-periodic version. Polynomiality of a special but natural case is also proven.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.05527v1" target="_blank">AI vs. Human Moderators: A Comparative Evaluation of Multimodal LLMs in Content Moderation for Brand Safety</a></h3>
                    <p><strong>Authors:</strong> Adi Levi, Or Levi, Sardhendu Mishra, Jonathan Morra</p>
                    <p><strong>Published:</strong> 8/7/2025</p>
                    <p><strong>Categories:</strong> cs.CV, I.2.10; I.2.7; H.3.3; H.4.3; K.4.1</p>
                    <p><strong>Summary:</strong> As the volume of video content online grows exponentially, the demand for moderation of unsafe videos has surpassed human capabilities, posing both operational and mental health challenges. While recent studies demonstrated the merits of Multimodal Large Language Models (MLLMs) in various video understanding tasks, their application to multimodal content moderation, a domain that requires nuanced understanding of both visual and textual cues, remains relatively underexplored. In this work, we benchmark the capabilities of MLLMs in brand safety classification, a critical subset of content moderation for safe-guarding advertising integrity. To this end, we introduce a novel, multimodal and multilingual dataset, meticulously labeled by professional reviewers in a multitude of risk categories. Through a detailed comparative analysis, we demonstrate the effectiveness of MLLMs such as Gemini, GPT, and Llama in multimodal brand safety, and evaluate their accuracy and cost efficiency compared to professional human reviewers. Furthermore, we present an in-depth discussion shedding light on limitations of MLLMs and failure cases. We are releasing our dataset alongside this paper to facilitate future research on effective and responsible brand safety and content moderation.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.05526v1" target="_blank">When Deepfake Detection Meets Graph Neural Network:a Unified and Lightweight Learning Framework</a></h3>
                    <p><strong>Authors:</strong> Haoyu Liu, Chaoyu Gong, Mengke He, Jiate Li, Kai Han, Siqiang Luo</p>
                    <p><strong>Published:</strong> 8/7/2025</p>
                    <p><strong>Categories:</strong> cs.CV</p>
                    <p><strong>Summary:</strong> The proliferation of generative video models has made detecting AI-generated and manipulated videos an urgent challenge. Existing detection approaches often fail to generalize across diverse manipulation types due to their reliance on isolated spatial, temporal, or spectral information, and typically require large models to perform well. This paper introduces SSTGNN, a lightweight Spatial-Spectral-Temporal Graph Neural Network framework that represents videos as structured graphs, enabling joint reasoning over spatial inconsistencies, temporal artifacts, and spectral distortions. SSTGNN incorporates learnable spectral filters and temporal differential modeling into a graph-based architecture, capturing subtle manipulation traces more effectively. Extensive experiments on diverse benchmark datasets demonstrate that SSTGNN not only achieves superior performance in both in-domain and cross-domain settings, but also offers strong robustness against unseen manipulations. Remarkably, SSTGNN accomplishes these results with up to 42.4$\times$ fewer parameters than state-of-the-art models, making it highly lightweight and scalable for real-world deployment.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.05519v1" target="_blank">Leveraging AI to Accelerate Clinical Data Cleaning: A Comparative Study of AI-Assisted vs. Traditional Methods</a></h3>
                    <p><strong>Authors:</strong> Matthew Purri, Amit Patel, Erik Deurrell</p>
                    <p><strong>Published:</strong> 8/7/2025</p>
                    <p><strong>Categories:</strong> cs.CV</p>
                    <p><strong>Summary:</strong> Clinical trial data cleaning represents a critical bottleneck in drug development, with manual review processes struggling to manage exponentially increasing data volumes and complexity. This paper presents Octozi, an artificial intelligence-assisted platform that combines large language models with domain-specific heuristics to transform clinical data review. In a controlled experimental study with experienced clinical reviewers (n=10), we demonstrate that AI assistance increased data cleaning throughput by 6.03-fold while simultaneously decreasing cleaning errors from 54.67% to 8.48% (a 6.44-fold improvement). Crucially, the system reduced false positive queries by 15.48-fold, minimizing unnecessary site burden. These improvements were consistent across reviewers regardless of experience level, suggesting broad applicability. Our findings indicate that AI-assisted approaches can address fundamental inefficiencies in clinical trial operations, potentially accelerating drug development timelines and reducing costs while maintaining regulatory compliance. This work establishes a framework for integrating AI into safety-critical clinical workflows and demonstrates the transformative potential of human-AI collaboration in pharmaceutical clinical trials.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.05514v1" target="_blank">Head Anchor Enhanced Detection and Association for Crowded Pedestrian Tracking</a></h3>
                    <p><strong>Authors:</strong> Zewei Wu, CÃ©sar Teixeira, Wei Ke, Zhang Xiong</p>
                    <p><strong>Published:</strong> 8/7/2025</p>
                    <p><strong>Categories:</strong> cs.CV</p>
                    <p><strong>Summary:</strong> Visual pedestrian tracking represents a promising research field, with extensive applications in intelligent surveillance, behavior analysis, and human-computer interaction. However, real-world applications face significant occlusion challenges. When multiple pedestrians interact or overlap, the loss of target features severely compromises the trackers ability to maintain stable trajectories. Traditional tracking methods, which typically rely on full-body bounding box features extracted from {Re-ID} models and linear constant-velocity motion assumptions, often struggle in severe occlusion scenarios. To address these limitations, this work proposes an enhanced tracking framework that leverages richer feature representations and a more robust motion model. Specifically, the proposed method incorporates detection features from both the regression and classification branches of an object detector, embedding spatial and positional information directly into the feature representations. To further mitigate occlusion challenges, a head keypoint detection model is introduced, as the head is less prone to occlusion compared to the full body. In terms of motion modeling, we propose an iterative Kalman filtering approach designed to align with modern detector assumptions, integrating 3D priors to better complete motion trajectories in complex scenes. By combining these advancements in appearance and motion modeling, the proposed method offers a more robust solution for multi-object tracking in crowded environments where occlusions are prevalent.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.05513v1" target="_blank">Streamlining Admission with LOR Insights: AI-Based Leadership Assessment in Online Masters Program</a></h3>
                    <p><strong>Authors:</strong> Meryem Yilmaz Soylu, Adrian Gallard, Jeonghyun Lee, Gayane Grigoryan, Rushil Desai, Stephen Harmon</p>
                    <p><strong>Published:</strong> 8/7/2025</p>
                    <p><strong>Categories:</strong> cs.AI, cs.LG</p>
                    <p><strong>Summary:</strong> Letters of recommendation (LORs) provide valuable insights into candidates capabilities and experiences beyond standardized test scores. However, reviewing these text-heavy materials is time-consuming and labor-intensive. To address this challenge and support the admission committee in providing feedback for students professional growth, our study introduces LORI: LOR Insights, a novel AI-based detection tool for assessing leadership skills in LORs submitted by online masters program applicants. By employing natural language processing and leveraging large language models using RoBERTa and LLAMA, we seek to identify leadership attributes such as teamwork, communication, and innovation. Our latest RoBERTa model achieves a weighted F1 score of 91.6%, a precision of 92.4%, and a recall of 91.6%, showing a strong level of consistency in our test data. With the growing importance of leadership skills in the STEM sector, integrating LORI into the graduate admissions process is crucial for accurately assessing applicants leadership capabilities. This approach not only streamlines the admissions process but also automates and ensures a more comprehensive evaluation of candidates capabilities.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.05511v1" target="_blank">Adaptive Parallel Downloader for Large Genomic Datasets</a></h3>
                    <p><strong>Authors:</strong> Rasman Mubtasim Swargo, Engin Arslan, Md Arifuzzaman</p>
                    <p><strong>Published:</strong> 8/7/2025</p>
                    <p><strong>Categories:</strong> cs.DC</p>
                    <p><strong>Summary:</strong> Modern next-generation sequencing (NGS) projects routinely generate terabytes of data, which researchers commonly download from public repositories such as SRA or ENA. Existing download tools often employ static concurrency settings, leading to inefficient bandwidth utilization and prolonged download times due to their inability to adapt to dynamic network conditions. We introduce FastBioDL, a parallel file downloader designed for large biological datasets, featuring an adaptive concurrency controller. FastBioDL frames the download process as an online optimization problem, utilizing a utility function and gradient descent to adjust the number of concurrent socket streams in real-time dynamically. This approach maximizes download throughput while minimizing resource overhead. Comprehensive evaluations on public genomic datasets demonstrate that FastBioDL achieves up to $4x$ speedup over state-of-the-art tools. Moreover, in high-speed network experiments, its adaptive design was up to $2.1x$ faster than existing tools. By intelligently optimizing standard HTTP or FTP downloads on the client side, FastBioDL provides a robust and efficient solution for large-scale genomic data acquisition, democratizing high-performance data retrieval for researchers without requiring specialized commercial software or protocols.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.05497v1" target="_blank">Towards Human-Centric Evaluation of Interaction-Aware Automated Vehicle Controllers: A Framework and Case Study</a></h3>
                    <p><strong>Authors:</strong> Federico ScarÃ¬, Olger Siebinga, Arkady Zgonnikov</p>
                    <p><strong>Published:</strong> 8/7/2025</p>
                    <p><strong>Categories:</strong> cs.HC, cs.RO</p>
                    <p><strong>Summary:</strong> As automated vehicles (AVs) increasingly integrate into mixed-traffic environments, evaluating their interaction with human-driven vehicles (HDVs) becomes critical. In most research focused on developing new AV control algorithms (controllers), the performance of these algorithms is assessed solely based on performance metrics such as collision avoidance or lane-keeping efficiency, while largely overlooking the human-centred dimensions of interaction with HDVs. This paper proposes a structured evaluation framework that addresses this gap by incorporating metrics grounded in the human-robot interaction literature. The framework spans four key domains: a) interaction effect, b) interaction perception, c) interaction effort, and d) interaction ability. These domains capture both the performance of the AV and its impact on human drivers around it. To demonstrate the utility of the framework, we apply it to a case study evaluating how a state-of-the-art AV controller interacts with human drivers in a merging scenario in a driving simulator. Measuring HDV-HDV interactions as a baseline, this study included one representative metric per domain: a) perceived safety, b) subjective ratings, specifically how participants perceived the other vehicles driving behaviour (e.g., aggressiveness or predictability) , c) driver workload, and d) merging success. The results showed that incorporating metrics covering all four domains in the evaluation of AV controllers can illuminate critical differences in driver experience when interacting with AVs. This highlights the need for a more comprehensive evaluation approach. Our framework offers researchers, developers, and policymakers a systematic method for assessing AV behaviour beyond technical performance, fostering the development of AVs that are not only functionally capable but also understandable, acceptable, and safe from a human perspective.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.05496v1" target="_blank">InfiAlign: A Scalable and Sample-Efficient Framework for Aligning LLMs to Enhance Reasoning Capabilities</a></h3>
                    <p><strong>Authors:</strong> Shuo Cai, Su Lu, Qi Zhou, Kejing Yang, Zhijie Sang, Congkai Xie, Hongxia Yang</p>
                    <p><strong>Published:</strong> 8/7/2025</p>
                    <p><strong>Categories:</strong> cs.AI</p>
                    <p><strong>Summary:</strong> Large language models (LLMs) have exhibited impressive reasoning abilities on a wide range of complex tasks. However, enhancing these capabilities through post-training remains resource intensive, particularly in terms of data and computational cost. Although recent efforts have sought to improve sample efficiency through selective data curation, existing methods often rely on heuristic or task-specific strategies that hinder scalability. In this work, we introduce InfiAlign, a scalable and sample-efficient post-training framework that integrates supervised fine-tuning (SFT) with Direct Preference Optimization (DPO) to align LLMs for enhanced reasoning. At the core of InfiAlign is a robust data selection pipeline that automatically curates high-quality alignment data from open-source reasoning datasets using multidimensional quality metrics. This pipeline enables significant performance gains while drastically reducing data requirements and remains extensible to new data sources. When applied to the Qwen2.5-Math-7B-Base model, our SFT model achieves performance on par with DeepSeek-R1-Distill-Qwen-7B, while using only approximately 12% of the training data, and demonstrates strong generalization across diverse reasoning tasks. Additional improvements are obtained through the application of DPO, with particularly notable gains in mathematical reasoning tasks. The model achieves an average improvement of 3.89% on AIME 24/25 benchmarks. Our results highlight the effectiveness of combining principled data selection with full-stage post-training, offering a practical solution for aligning large reasoning models in a scalable and data-efficient manner. The model checkpoints are available at https://huggingface.co/InfiX-ai/InfiAlign-Qwen-7B-SFT.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.05495v1" target="_blank">A 20-Year Retrospective on Power and Thermal Modeling and Management</a></h3>
                    <p><strong>Authors:</strong> David Atienza, Kai Zhu, Darong Huang, Luis Costero</p>
                    <p><strong>Published:</strong> 8/7/2025</p>
                    <p><strong>Categories:</strong> eess.SY, cs.SY</p>
                    <p><strong>Summary:</strong> As processor performance advances, increasing power densities and complex thermal behaviors threaten both energy efficiency and system reliability. This survey covers more than two decades of research on power and thermal modeling and management in modern processors. We start by comparing analytical, regression-based, and neural network-based techniques for power estimation, then review thermal modeling methods, including finite element, finite difference, and data-driven approaches. Next, we categorize dynamic runtime management strategies that balance performance, power consumption, and reliability. Finally, we conclude with a discussion of emerging challenges and promising research directions.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.05493v1" target="_blank">Exact and Heuristic Algorithms for Constrained Biclustering</a></h3>
                    <p><strong>Authors:</strong> Antonio M. Sudoso</p>
                    <p><strong>Published:</strong> 8/7/2025</p>
                    <p><strong>Categories:</strong> math.OC, cs.LG</p>
                    <p><strong>Summary:</strong> Biclustering, also known as co-clustering or two-way clustering, simultaneously partitions the rows and columns of a data matrix to reveal submatrices with coherent patterns. Incorporating background knowledge into clustering to enhance solution quality and interpretability has attracted growing interest in mathematical optimization and machine learning research. Extending this paradigm to biclustering enables prior information to guide the joint grouping of rows and columns. We study constrained biclustering with pairwise constraints, namely must-link and cannot-link constraints, which specify whether objects should belong to the same or different biclusters. As a model problem, we address the constrained version of the k-densest disjoint biclique problem, which aims to identify k disjoint complete bipartite subgraphs (called bicliques) in a weighted complete bipartite graph, maximizing the total density while satisfying pairwise constraints. We propose both exact and heuristic algorithms. The exact approach is a tailored branch-and-cut algorithm based on a low-dimensional semidefinite programming (SDP) relaxation, strengthened with valid inequalities and solved in a cutting-plane fashion. Exploiting integer programming tools, a rounding scheme converts SDP solutions into feasible biclusterings at each node. For large-scale instances, we introduce an efficient heuristic based on the low-rank factorization of the SDP. The resulting nonlinear optimization problem is tackled with an augmented Lagrangian method, where the subproblem is solved by decomposition through a block-coordinate projected gradient algorithm. Extensive experiments on synthetic and real-world datasets show that the exact method significantly outperforms general-purpose solvers, while the heuristic achieves high-quality solutions efficiently on large instances.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.05491v1" target="_blank">Deconstructing the Crystal Ball: From Ad-Hoc Prediction to Principled Startup Evaluation with the SAISE Framework</a></h3>
                    <p><strong>Authors:</strong> Seyed Mohammad Ali Jafari, Ali Mobini Dehkordi, Ehsan Chitsaz, Yadollah Yaghoobzadeh</p>
                    <p><strong>Published:</strong> 8/7/2025</p>
                    <p><strong>Categories:</strong> cs.CE, econ.GN, q-fin.EC</p>
                    <p><strong>Summary:</strong> The integration of Artificial Intelligence (AI) into startup evaluation represents a significant technological shift, yet the academic research underpinning this transition remains methodologically fragmented. Existing studies often employ ad-hoc approaches, leading to a body of work with inconsistent definitions of success, atheoretical features, and a lack of rigorous validation. This fragmentation severely limits the comparability, reliability, and practical utility of current predictive models. To address this critical gap, this paper presents a comprehensive systematic literature review of 57 empirical studies. We deconstruct the current state-of-the-art by systematically mapping the features, algorithms, data sources, and evaluation practices that define the AI-driven startup prediction landscape. Our synthesis reveals a field defined by a central paradox: a strong convergence on a common toolkit -- venture databases and tree-based ensembles -- but a stark divergence in methodological rigor. We identify four foundational weaknesses: a fragmented definition of success, a divide between theory-informed and data-driven feature engineering, a chasm between common and best-practice model validation, and a nascent approach to data ethics and explainability. In response to these findings, our primary contribution is the proposal of the Systematic AI-driven Startup Evaluation (SAISE) Framework. This novel, five-stage prescriptive roadmap is designed to guide researchers from ad-hoc prediction toward principled evaluation. By mandating a coherent, end-to-end methodology that emphasizes stage-aware problem definition, theory-informed data synthesis, principled feature engineering, rigorous validation, and risk-aware interpretation, the SAISE framework provides a new standard for conducting more comparable, robust, and practically relevant research in this rapidly maturing domain</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.05476v1" target="_blank">MM2CT: MR-to-CT translation for multi-modal image fusion with mamba</a></h3>
                    <p><strong>Authors:</strong> Chaohui Gong, Zhiying Wu, Zisheng Huang, Gaofeng Meng, Zhen Lei, Hongbin Liu</p>
                    <p><strong>Published:</strong> 8/7/2025</p>
                    <p><strong>Categories:</strong> eess.IV</p>
                    <p><strong>Summary:</strong> Magnetic resonance (MR)-to-computed tomography (CT) translation offers significant advantages, including the elimination of radiation exposure associated with CT scans and the mitigation of imaging artifacts caused by patient motion. The existing approaches are based on single-modality MR-to-CT translation, with limited research exploring multimodal fusion. To address this limitation, we introduce Multi-modal MR to CT (MM2CT) translation method by leveraging multimodal T1- and T2-weighted MRI data, an innovative Mamba-based framework for multi-modal medical image synthesis. Mamba effectively overcomes the limited local receptive field in CNNs and the high computational complexity issues in Transformers. MM2CT leverages this advantage to maintain long-range dependencies modeling capabilities while achieving multi-modal MR feature integration. Additionally, we incorporate a dynamic local convolution module and a dynamic enhancement module to improve MRI-to-CT synthesis. The experiments on a public pelvis dataset demonstrate that MM2CT achieves state-of-the-art performance in terms of Structural Similarity Index Measure (SSIM) and Peak Signal-to-Noise Ratio (PSNR). Our code is publicly available at https://github.com/Gots-ch/MM2CT.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.05471v1" target="_blank">An Improved Approximation Algorithm for the Capacitated Arc Routing Problem</a></h3>
                    <p><strong>Authors:</strong> Jingyang Zhao, Mingyu Xiao</p>
                    <p><strong>Published:</strong> 8/7/2025</p>
                    <p><strong>Categories:</strong> cs.DS</p>
                    <p><strong>Summary:</strong> The Capacitated Arc Routing Problem (CARP), introduced by Golden and Wong in 1981, is an important arc routing problem in Operations Research, which generalizes the famous Capacitated Vehicle Routing Problem (CVRP). When every customer has a unit demand, the best known approximation ratio for CARP, given by Jansen in 1993, remains $\frac{5}{2}-\frac{1.5}{k}$, where $k$ denotes the vehicle capacity. Based on recent progress in approximating CVRP, we improve this result by proposing a $(\frac{5}{2}-\Theta(\frac{1}{\sqrt{k}}))$-approximation algorithm, which to the best of our knowledge constitutes the first improvement over Jansens bound.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.05470v1" target="_blank">Rethinking Creativity Evaluation: A Critical Analysis of Existing Creativity Evaluations</a></h3>
                    <p><strong>Authors:</strong> Li-Chun Lu, Miri Liu, Pin-Chun Lu, Yufei Tian, Shao-Hua Sun, Nanyun Peng</p>
                    <p><strong>Published:</strong> 8/7/2025</p>
                    <p><strong>Categories:</strong> cs.CL</p>
                    <p><strong>Summary:</strong> We systematically examine, analyze, and compare representative creativity measures--creativity index, perplexity, syntactic templates, and LLM-as-a-Judge--across diverse creative domains, including creative writing, unconventional problem-solving, and research ideation. Our analyses reveal that these metrics exhibit limited consistency, capturing different dimensions of creativity. We highlight key limitations, including the creativity indexs focus on lexical diversity, perplexitys sensitivity to model confidence, and syntactic templates inability to capture conceptual creativity. Additionally, LLM-as-a-Judge shows instability and bias. Our findings underscore the need for more robust, generalizable evaluation frameworks that better align with human judgments of creativity.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.05469v1" target="_blank">Lets Measure Information Step-by-Step: LLM-Based Evaluation Beyond Vibes</a></h3>
                    <p><strong>Authors:</strong> Zachary Robertson, Sanmi Koyejo</p>
                    <p><strong>Published:</strong> 8/7/2025</p>
                    <p><strong>Categories:</strong> cs.LG, cs.IT, math.IT</p>
                    <p><strong>Summary:</strong> We develop mechanisms for evaluating AI systems without ground truth by exploiting a connection between gaming resistance and output quality. The data processing inequality ensures post-hoc attempts to game a metric degrades both information content and task performance. We prove that f-mutual information measures are the unique gaming resistant mechanisms under natural conditions, with the overseer acting as an agent. While Shannon mutual information faces exponential sample complexity, bounded measures like total variation distance remain tractable. Empirically, across ten domains from translation to peer review, all information-theoretic mechanisms achieve perfect discrimination (d  0.5) between faithful and strategic agents. In contrast, LLM judges exhibit systematic evaluation inversion, preferring fabricated content over accurate summaries. Our mechanisms show 10-100x better robustness to adversarial manipulation than current practices. We also find performance follows an inverted-U curve with compression ratio, peaking at 10:1 where agent responses exhibit optimal information diversity (3 effective dimensions), giving a bias-variance perspective on when our approach is expected to be most effective.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.05465v1" target="_blank">F2PASeg: Feature Fusion for Pituitary Anatomy Segmentation in Endoscopic Surgery</a></h3>
                    <p><strong>Authors:</strong> Lumin Chen, Zhiying Wu, Tianye Lei, Xuexue Bai, Ming Feng, Yuxi Wang, Gaofeng Meng, Zhen Lei, Hongbin Liu</p>
                    <p><strong>Published:</strong> 8/7/2025</p>
                    <p><strong>Categories:</strong> cs.CV, cs.SY, eess.IV, eess.SY</p>
                    <p><strong>Summary:</strong> Pituitary tumors often cause deformation or encapsulation of adjacent vital structures. Anatomical structure segmentation can provide surgeons with early warnings of regions that pose surgical risks, thereby enhancing the safety of pituitary surgery. However, pixel-level annotated video stream datasets for pituitary surgeries are extremely rare. To address this challenge, we introduce a new dataset for Pituitary Anatomy Segmentation (PAS). PAS comprises 7,845 time-coherent images extracted from 120 videos. To mitigate class imbalance, we apply data augmentation techniques that simulate the presence of surgical instruments in the training data. One major challenge in pituitary anatomy segmentation is the inconsistency in feature representation due to occlusions, camera motion, and surgical bleeding. By incorporating a Feature Fusion module, F2PASeg is proposed to refine anatomical structure segmentation by leveraging both high-resolution image features and deep semantic embeddings, enhancing robustness against intraoperative variations. Experimental results demonstrate that F2PASeg consistently segments critical anatomical structures in real time, providing a reliable solution for intraoperative pituitary surgery planning. Code: https://github.com/paulili08/F2PASeg.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.05464v1" target="_blank">Bench-2-CoP: Can We Trust Benchmarking for EU AI Compliance?</a></h3>
                    <p><strong>Authors:</strong> Matteo Prandi, Vincenzo Suriani, Federico Pierucci, Marcello Galisai, Daniele Nardi, Piercosma Bisconti</p>
                    <p><strong>Published:</strong> 8/7/2025</p>
                    <p><strong>Categories:</strong> cs.AI, cs.CL</p>
                    <p><strong>Summary:</strong> The rapid advancement of General Purpose AI (GPAI) models necessitates robust evaluation frameworks, especially with emerging regulations like the EU AI Act and its associated Code of Practice (CoP). Current AI evaluation practices depend heavily on established benchmarks, but these tools were not designed to measure the systemic risks that are the focus of the new regulatory landscape. This research addresses the urgent need to quantify this benchmark-regulation gap. We introduce Bench-2-CoP, a novel, systematic framework that uses validated LLM-as-judge analysis to map the coverage of 194,955 questions from widely-used benchmarks against the EU AI Acts taxonomy of model capabilities and propensities. Our findings reveal a profound misalignment: the evaluation ecosystem is overwhelmingly focused on a narrow set of behavioral propensities, such as Tendency to hallucinate (53.7% of the corpus) and Discriminatory bias (28.9%), while critical functional capabilities are dangerously neglected. Crucially, capabilities central to loss-of-control scenarios, including evading human oversight, self-replication, and autonomous AI development, receive zero coverage in the entire benchmark corpus. This translates to a near-total evaluation gap for systemic risks like Loss of Control (0.4% coverage) and Cyber Offence (0.8% coverage). This study provides the first comprehensive, quantitative analysis of this gap, offering critical insights for policymakers to refine the CoP and for developers to build the next generation of evaluation tools, ultimately fostering safer and more compliant AI.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.05449v1" target="_blank">Causal Mediation in Natural Experiments</a></h3>
                    <p><strong>Authors:</strong> Senan Hogan-Hennessy</p>
                    <p><strong>Published:</strong> 8/7/2025</p>
                    <p><strong>Categories:</strong> econ.EM</p>
                    <p><strong>Summary:</strong> Natural experiments are a cornerstone of applied economics, providing settings for estimating causal effects with a compelling argument for treatment randomisation, but give little indication of the mechanisms behind causal effects. Causal Mediation (CM) provides a framework to analyse mechanisms by identifying the average direct and indirect effects (CM effects), yet conventional CM methods require the relevant mediator is as-good-as-randomly assigned. When people choose the mediator based on costs and benefits (whether to visit a doctor, to attend university, etc.), this assumption fails and conventional CM analyses are at risk of bias. I propose a control function strategy that uses instrumental variation in mediator take-up costs, delivering unbiased direct and indirect effects when selection is driven by unobserved gains. The method identifies CM effects via the marginal effect of the mediator, with parametric or semi-parametric estimation that is simple to implement in two stages. Applying these methods to the Oregon Health Insurance Experiment reveals a substantial portion of the Medicaid lotterys effect on self-reported health and happiness flows through increased healthcare usage -- an effect that a conventional CM analysis would mistake. This approach gives applied researchers an alternative method to estimate CM effects when an initial treatment is quasi-randomly assigned, but the mediator is not, as is common in natural experiments.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.05441v1" target="_blank">Tail-Risk-Safe Monte Carlo Tree Search under PAC-Level Guarantees</a></h3>
                    <p><strong>Authors:</strong> Zuyuan Zhang, Arnob Ghosh, Tian Lan</p>
                    <p><strong>Published:</strong> 8/7/2025</p>
                    <p><strong>Categories:</strong> cs.LG, cs.AI</p>
                    <p><strong>Summary:</strong> Making decisions with respect to just the expected returns in Monte Carlo Tree Search (MCTS) cannot account for the potential range of high-risk, adverse outcomes associated with a decision. To this end, safety-aware MCTS often consider some constrained variants -- by introducing some form of mean risk measures or hard cost thresholds. These approaches fail to provide rigorous tail-safety guarantees with respect to extreme or high-risk outcomes (denoted as tail-risk), potentially resulting in serious consequence in high-stake scenarios. This paper addresses the problem by developing two novel solutions. We first propose CVaR-MCTS, which embeds a coherent tail risk measure, Conditional Value-at-Risk (CVaR), into MCTS. Our CVaR-MCTS with parameter $\alpha$ achieves explicit tail-risk control over the expected loss in the worst $(1-\alpha)\%$ scenarios. Second, we further address the estimation bias of tail-risk due to limited samples. We propose Wasserstein-MCTS (or W-MCTS) by introducing a first-order Wasserstein ambiguity set $\mathcal{P}_{\varepsilon_{s}}(s,a)$ with radius $\varepsilon_{s}$ to characterize the uncertainty in tail-risk estimates. We prove PAC tail-safety guarantees for both CVaR-MCTS and W-MCTS and establish their regret. Evaluations on diverse simulated environments demonstrate that our proposed methods outperform existing baselines, effectively achieving robust tail-risk guarantees with improved rewards and stability.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.05437v1" target="_blank">Online Sparsification of Bipartite-Like Clusters in Graphs</a></h3>
                    <p><strong>Authors:</strong> Joyentanuj Das, Suranjan De, He Sun</p>
                    <p><strong>Published:</strong> 8/7/2025</p>
                    <p><strong>Categories:</strong> cs.DS, cs.LG</p>
                    <p><strong>Summary:</strong> Graph clustering is an important algorithmic technique for analysing massive graphs, and has been widely applied in many research fields of data science. While the objective of most graph clustering algorithms is to find a vertex set of low conductance, a sequence of recent studies highlights the importance of the inter-connection between vertex sets when analysing real-world datasets. Following this line of research, in this work we study bipartite-like clusters and present efficient and online sparsification algorithms that find such clusters in both undirected graphs and directed ones. We conduct experimental studies on both synthetic and real-world datasets, and show that our algorithms significantly speedup the running time of existing clustering algorithms while preserving their effectiveness.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.05433v1" target="_blank">Discovering Interpretable Programmatic Policies via Multimodal LLM-assisted Evolutionary Search</a></h3>
                    <p><strong>Authors:</strong> Qinglong Hu, Xialiang Tong, Mingxuan Yuan, Fei Liu, Zhichao Lu, Qingfu Zhang</p>
                    <p><strong>Published:</strong> 8/7/2025</p>
                    <p><strong>Categories:</strong> cs.LG, cs.NE</p>
                    <p><strong>Summary:</strong> Interpretability and high performance are essential goals in designing control policies, particularly for safety-critical tasks. Deep reinforcement learning has greatly enhanced performance, yet its inherent lack of interpretability often undermines trust and hinders real-world deployment. This work addresses these dual challenges by introducing a novel approach for programmatic policy discovery, called Multimodal Large Language Model-assisted Evolutionary Search (MLES). MLES utilizes multimodal large language models as policy generators, combining them with evolutionary mechanisms for automatic policy optimization. It integrates visual feedback-driven behavior analysis within the policy generation process to identify failure patterns and facilitate targeted improvements, enhancing the efficiency of policy discovery and producing adaptable, human-aligned policies. Experimental results show that MLES achieves policy discovery capabilities and efficiency comparable to Proximal Policy Optimization (PPO) across two control tasks, while offering transparent control logic and traceable design processes. This paradigm overcomes the limitations of predefined domain-specific languages, facilitates knowledge transfer and reuse, and is scalable across various control tasks. MLES shows promise as a leading approach for the next generation of interpretable control policy discovery.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.05432v1" target="_blank">Whose Truth? Pluralistic Geo-Alignment for (Agentic) AI</a></h3>
                    <p><strong>Authors:</strong> Krzysztof Janowicz, Zilong Liu, Gengchen Mai, Zhangyu Wang, Ivan Majic, Alexandra Fortacz, Grant McKenzie, Song Gao</p>
                    <p><strong>Published:</strong> 8/7/2025</p>
                    <p><strong>Categories:</strong> cs.AI, cs.CY</p>
                    <p><strong>Summary:</strong> AI (super) alignment describes the challenge of ensuring (future) AI systems behave in accordance with societal norms and goals. While a quickly evolving literature is addressing biases and inequalities, the geographic variability of alignment remains underexplored. Simply put, what is considered appropriate, truthful, or legal can differ widely across regions due to cultural norms, political realities, and legislation. Alignment measures applied to AI/ML workflows can sometimes produce outcomes that diverge from statistical realities, such as text-to-image models depicting balanced gender ratios in company leadership despite existing imbalances. Crucially, some model outputs are globally acceptable, while others, e.g., questions about Kashmir, depend on knowing the users location and their context. This geographic sensitivity is not new. For instance, Google Maps renders Kashmirs borders differently based on user location. What is new is the unprecedented scale and automation with which AI now mediates knowledge, expresses opinions, and represents geographic reality to millions of users worldwide, often with little transparency about how context is managed. As we approach Agentic AI, the need for spatio-temporally aware alignment, rather than one-size-fits-all approaches, is increasingly urgent. This paper reviews key geographic research problems, suggests topics for future work, and outlines methods for assessing alignment sensitivity.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.05427v1" target="_blank">Large Language Models Transform Organic Synthesis From Reaction Prediction to Automation</a></h3>
                    <p><strong>Authors:</strong> Kartar Kumar Lohana Tharwani, Rajesh Kumar, Sumita, Numan Ahmed, Yong Tang</p>
                    <p><strong>Published:</strong> 8/7/2025</p>
                    <p><strong>Categories:</strong> cs.AI</p>
                    <p><strong>Summary:</strong> Large language models (LLMs) are beginning to reshape how chemists plan and run reactions in organic synthesis. Trained on millions of reported transformations, these text-based models can propose synthetic routes, forecast reaction outcomes and even instruct robots that execute experiments without human supervision. Here we survey the milestones that turned LLMs from speculative tools into practical lab partners. We show how coupling LLMs with graph neural networks, quantum calculations and real-time spectroscopy shrinks discovery cycles and supports greener, data-driven chemistry. We discuss limitations, including biased datasets, opaque reasoning and the need for safety gates that prevent unintentional hazards. Finally, we outline community initiatives open benchmarks, federated learning and explainable interfaces that aim to democratize access while keeping humans firmly in control. These advances chart a path towards rapid, reliable and inclusive molecular innovation powered by artificial intelligence and automation.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.05419v1" target="_blank">Sober topologies on a set</a></h3>
                    <p><strong>Authors:</strong> Xiangrui Li, Qingguo Li, Dongsheng Zhao</p>
                    <p><strong>Published:</strong> 8/7/2025</p>
                    <p><strong>Categories:</strong> math.GN</p>
                    <p><strong>Summary:</strong> The collection of all topologies on a set X forms a complete lattice with respect to the inclusion order, which have been investigated by many researchers. Sobriety is one of the core and extensively studied properties in non-Hausdorff topology. This property plays a crucial role in characterizing the spectral spaces of commutative rings and topological spaces determined by their lattices of open sets. In this paper, we investigate the statute of sober topologies in the complete lattice of all topologies on a given set. The main results to be proved include: (1) every T1 topology is the join of some sober topologies; (2) every topology is the meet of some sober topologies; (3) the set of all sober topologies is directed complete; (4) every Alexanderoff - discrete topology is the meet of some sober Alexanderoff - discrete topologies; (5) the minimal sober topologies are exactly the Scott topologies of sup-complete chains; (6) an example will be constructed to show that the intersection of a decreasing sequence of Hausdorff topologies need not be sober.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.05416v1" target="_blank">Echo State Networks for Bitcoin Time Series Prediction</a></h3>
                    <p><strong>Authors:</strong> Mansi Sharma, Enrico Sartor, Marc Cavazza, Helmut Prendinger</p>
                    <p><strong>Published:</strong> 8/7/2025</p>
                    <p><strong>Categories:</strong> cs.LG, cs.CE, cs.NE</p>
                    <p><strong>Summary:</strong> Forecasting stock and cryptocurrency prices is challenging due to high volatility and non-stationarity, influenced by factors like economic changes and market sentiment. Previous research shows that Echo State Networks (ESNs) can effectively model short-term stock market movements, capturing nonlinear patterns in dynamic data. To the best of our knowledge, this work is among the first to explore ESNs for cryptocurrency forecasting, especially during extreme volatility. We also conduct chaos analysis through the Lyapunov exponent in chaotic periods and show that our approach outperforms existing machine learning methods by a significant margin. Our findings are consistent with the Lyapunov exponent analysis, showing that ESNs are robust during chaotic periods and excel under high chaos compared to Boosting and Na\ive methods.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.05414v1" target="_blank">Physical Adversarial Camouflage through Gradient Calibration and Regularization</a></h3>
                    <p><strong>Authors:</strong> Jiawei Liang, Siyuan Liang, Jianjie Huang, Chenxi Si, Ming Zhang, Xiaochun Cao</p>
                    <p><strong>Published:</strong> 8/7/2025</p>
                    <p><strong>Categories:</strong> cs.CV</p>
                    <p><strong>Summary:</strong> The advancement of deep object detectors has greatly affected safety-critical fields like autonomous driving. However, physical adversarial camouflage poses a significant security risk by altering object textures to deceive detectors. Existing techniques struggle with variable physical environments, facing two main challenges: 1) inconsistent sampling point densities across distances hinder the gradient optimization from ensuring local continuity, and 2) updating texture gradients from multiple angles causes conflicts, reducing optimization stability and attack effectiveness. To address these issues, we propose a novel adversarial camouflage framework based on gradient optimization. First, we introduce a gradient calibration strategy, which ensures consistent gradient updates across distances by propagating gradients from sparsely to unsampled texture points. Additionally, we develop a gradient decorrelation method, which prioritizes and orthogonalizes gradients based on loss values, enhancing stability and effectiveness in multi-angle optimization by eliminating redundant or conflicting updates. Extensive experimental results on various detection models, angles and distances show that our method significantly exceeds the state of the art, with an average increase in attack success rate (ASR) of 13.46% across distances and 11.03% across angles. Furthermore, empirical evaluation in real-world scenarios highlights the need for more robust system design.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.05399v1" target="_blank">UNCAGE: Contrastive Attention Guidance for Masked Generative Transformers in Text-to-Image Generation</a></h3>
                    <p><strong>Authors:</strong> Wonjun Kang, Byeongkeun Ahn, Minjae Lee, Kevin Galim, Seunghyuk Oh, Hyung Il Koo, Nam Ik Cho</p>
                    <p><strong>Published:</strong> 8/7/2025</p>
                    <p><strong>Categories:</strong> cs.CV, cs.AI, cs.LG</p>
                    <p><strong>Summary:</strong> Text-to-image (T2I) generation has been actively studied using Diffusion Models and Autoregressive Models. Recently, Masked Generative Transformers have gained attention as an alternative to Autoregressive Models to overcome the inherent limitations of causal attention and autoregressive decoding through bidirectional attention and parallel decoding, enabling efficient and high-quality image generation. However, compositional T2I generation remains challenging, as even state-of-the-art Diffusion Models often fail to accurately bind attributes and achieve proper text-image alignment. While Diffusion Models have been extensively studied for this issue, Masked Generative Transformers exhibit similar limitations but have not been explored in this context. To address this, we propose Unmasking with Contrastive Attention Guidance (UNCAGE), a novel training-free method that improves compositional fidelity by leveraging attention maps to prioritize the unmasking of tokens that clearly represent individual objects. UNCAGE consistently improves performance in both quantitative and qualitative evaluations across multiple benchmarks and metrics, with negligible inference overhead. Our code is available at https://github.com/furiosa-ai/uncage.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.05394v1" target="_blank">Grouped k-threshold random grid-based visual cryptography scheme</a></h3>
                    <p><strong>Authors:</strong> Xiaoli Zhuo, Xuehu Yan, Wei Yan</p>
                    <p><strong>Published:</strong> 8/7/2025</p>
                    <p><strong>Categories:</strong> cs.CR</p>
                    <p><strong>Summary:</strong> Visual cryptography schemes (VCSs) belong to a category of secret image sharing schemes that do not require cryptographic knowledge for decryption, instead relying directly on the human visual system. Among VCSs, random grid-based VCS (RGVCS) has garnered widespread attention as it avoids pixel expansion while requiring no basic matrices design. Contrast, a core metric for RGVCS, directly determines the visual quality of recovered images, rendering its optimization a critical research objective. However, existing $(k,n)$ RGVCSs still fail to attain theoretical upper bounds on contrast, highlighting the urgent need for higher-contrast constructions. In this paper, we propose a novel sharing paradigm for RGVCS that constructs $(k,n)$-threshold schemes from arbitrary $(k,n)$-threshold schemes $(k \leq n\leq n)$, termed \emph{$n$-grouped $(k,n)$ RGVCS}. This paradigm establishes hierarchical contrast characteristics: participants within the same group achieve optimal recovery quality, while inter-group recovery shows a hierarchical contrast. We further introduce a new contrast calculation formula tailored to the new paradigm. Then, we propose a contrast-enhanced $(k,n)$ RGVCS by setting $n= k$, achieving the highest contrast value documented in the existing literature. Theoretical analysis and experimental results demonstrate the superiority of our proposed scheme in terms of contrast.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.05391v1" target="_blank">Artificial Intelligence-Based Classification of Spitz Tumors</a></h3>
                    <p><strong>Authors:</strong> Ruben T. Lucassen, Marjanna Romers, Chiel F. Ebbelaar, Aia N. Najem, Donal P. Hayes, Antien L. Mooyaart, Sara Roshani, Liliane C. D. Wynaendts, Nikolas Stathonikos, Gerben E. Breimer, Anne M. L. Jansen, Mitko Veta, Willeke A. M. Blokx</p>
                    <p><strong>Published:</strong> 8/7/2025</p>
                    <p><strong>Categories:</strong> eess.IV, cs.CV</p>
                    <p><strong>Summary:</strong> Spitz tumors are diagnostically challenging due to overlap in atypical histological features with conventional melanomas. We investigated to what extent AI models, using histological and/or clinical features, can: (1) distinguish Spitz tumors from conventional melanomas; (2) predict the underlying genetic aberration of Spitz tumors; and (3) predict the diagnostic category of Spitz tumors. The AI models were developed and validated using a dataset of 393 Spitz tumors and 379 conventional melanomas. Predictive performance was measured using the AUROC and the accuracy. The performance of the AI models was compared with that of four experienced pathologists in a reader study. Moreover, a simulation experiment was conducted to investigate the impact of implementing AI-based recommendations for ancillary diagnostic testing on the workflow of the pathology department. The best AI model based on UNI features reached an AUROC of 0.95 and an accuracy of 0.86 in differentiating Spitz tumors from conventional melanomas. The genetic aberration was predicted with an accuracy of 0.55 compared to 0.25 for randomly guessing. The diagnostic category was predicted with an accuracy of 0.51, where random chance-level accuracy equaled 0.33. On all three tasks, the AI models performed better than the four pathologists, although differences were not statistically significant for most individual comparisons. Based on the simulation experiment, implementing AI-based recommendations for ancillary diagnostic testing could reduce material costs, turnaround times, and examinations. In conclusion, the AI models achieved a strong predictive performance in distinguishing between Spitz tumors and conventional melanomas. On the more challenging tasks of predicting the genetic aberration and the diagnostic category of Spitz tumors, the AI models performed better than random chance.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.05390v1" target="_blank">Quantum State Preparation Of Multiconfigurational States For Quantum Chemistry</a></h3>
                    <p><strong>Authors:</strong> Gabriel Greene-Diniz, Georgia Prokopiou, David Zsolt Manrique, David MuÃ±oz Ramo</p>
                    <p><strong>Published:</strong> 8/7/2025</p>
                    <p><strong>Categories:</strong> quant-ph</p>
                    <p><strong>Summary:</strong> The ability to prepare states for quantum chemistry is a promising feature of quantum computers, and efficient techniques for chemical state preparation is an active area of research. In this paper, we implement and investigate two methods of quantum circuit preparation for multiconfigurational states for quantum chemical applications. It has previously been shown that controlled Givens rotations are universal for quantum chemistry. To prepare a selected linear combination of Slater determinants (represented as occupation number configurations) using Givens rotations, the gates that rotate between the reference and excited determinants need to be controlled on qubits outside the excitation (external controls), in general. We implement a method to automatically find the external controls required for utilizing Givens rotations to prepare multiconfigurational states on a quantum circuit. We compare this approach to an alternative technique that exploits the sparsity of the chemical state vector and find that the latter can outperform the method of externally controlled Givens rotations; highly reduced circuits can be obtained by taking advantage of the sparse nature (where the number of basis states is significantly less than 2$^{n_q}$ for $n_q$ qubits) of chemical wavefunctions. We demonstrate the benefits of these techniques in a range of applications, including the ground states of a strongly correlated molecule, matrix elements of the Q-SCEOM algorithm for excited states, as well as correlated initial states for a quantum subspace method based on quantum computed moments and quantum phase estimation.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.05388v1" target="_blank">An Explainable Machine Learning Framework for Railway Predictive Maintenance using Data Streams from the Metro Operator of Portugal</a></h3>
                    <p><strong>Authors:</strong> Silvia GarcÃ­a-MÃ©ndez, Francisco de Arriba-PÃ©rez, FÃ¡tima Leal, Bruno Veloso, Benedita Malheiro, Juan Carlos Burguillo-Rial</p>
                    <p><strong>Published:</strong> 8/7/2025</p>
                    <p><strong>Categories:</strong> cs.AI</p>
                    <p><strong>Summary:</strong> This work contributes to a real-time data-driven predictive maintenance solution for Intelligent Transportation Systems. The proposed method implements a processing pipeline comprised of sample pre-processing, incremental classification with Machine Learning models, and outcome explanation. This novel online processing pipeline has two main highlights: (i) a dedicated sample pre-processing module, which builds statistical and frequency-related features on the fly, and (ii) an explainability module. This work is the first to perform online fault prediction with natural language and visual explainability. The experiments were performed with the MetroPT data set from the metro operator of Porto, Portugal. The results are above 98 % for F-measure and 99 % for accuracy. In the context of railway predictive maintenance, achieving these high values is crucial due to the practical and operational implications of accurate failure prediction. In the specific case of a high F-measure, this ensures that the system maintains an optimal balance between detecting the highest possible number of real faults and minimizing false alarms, which is crucial for maximizing service availability. Furthermore, the accuracy obtained enables reliability, directly impacting cost reduction and increased safety. The analysis demonstrates that the pipeline maintains high performance even in the presence of class imbalance and noise, and its explanations effectively reflect the decision-making process. These findings validate the methodological soundness of the approach and confirm its practical applicability for supporting proactive maintenance decisions in real-world railway operations. Therefore, by identifying the early signs of failure, this pipeline enables decision-makers to understand the underlying problems and act accordingly swiftly.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.05635v1" target="_blank">Genie Envisioner: A Unified World Foundation Platform for Robotic Manipulation</a></h3>
                    <p><strong>Authors:</strong> Yue Liao, Pengfei Zhou, Siyuan Huang, Donglin Yang, Shengcong Chen, Yuxin Jiang, Yue Hu, Jingbin Cai, Si Liu, Jianlan Luo, Liliang Chen, Shuicheng Yan, Maoqing Yao, Guanghui Ren</p>
                    <p><strong>Published:</strong> 8/7/2025</p>
                    <p><strong>Categories:</strong> cs.RO, cs.CV</p>
                    <p><strong>Summary:</strong> We introduce Genie Envisioner (GE), a unified world foundation platform for robotic manipulation that integrates policy learning, evaluation, and simulation within a single video-generative framework. At its core, GE-Base is a large-scale, instruction-conditioned video diffusion model that captures the spatial, temporal, and semantic dynamics of real-world robotic interactions in a structured latent space. Built upon this foundation, GE-Act maps latent representations to executable action trajectories through a lightweight, flow-matching decoder, enabling precise and generalizable policy inference across diverse embodiments with minimal supervision. To support scalable evaluation and training, GE-Sim serves as an action-conditioned neural simulator, producing high-fidelity rollouts for closed-loop policy development. The platform is further equipped with EWMBench, a standardized benchmark suite measuring visual fidelity, physical consistency, and instruction-action alignment. Together, these components establish Genie Envisioner as a scalable and practical foundation for instruction-driven, general-purpose embodied intelligence. All code, models, and benchmarks will be released publicly.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.05633v1" target="_blank">KuaiLive: A Real-time Interactive Dataset for Live Streaming Recommendation</a></h3>
                    <p><strong>Authors:</strong> Changle Qu, Sunhao Dai, Ke Guo, Liqin Zhao, Yanan Niu, Xiao Zhang, Jun Xu</p>
                    <p><strong>Published:</strong> 8/7/2025</p>
                    <p><strong>Categories:</strong> cs.IR, cs.AI</p>
                    <p><strong>Summary:</strong> Live streaming platforms have become a dominant form of online content consumption, offering dynamically evolving content, real-time interactions, and highly engaging user experiences. These unique characteristics introduce new challenges that differentiate live streaming recommendation from traditional recommendation settings and have garnered increasing attention from industry in recent years. However, research progress in academia has been hindered by the lack of publicly available datasets that accurately reflect the dynamic nature of live streaming environments. To address this gap, we introduce KuaiLive, the first real-time, interactive dataset collected from Kuaishou, a leading live streaming platform in China with over 400 million daily active users. The dataset records the interaction logs of 23,772 users and 452,621 streamers over a 21-day period. Compared to existing datasets, KuaiLive offers several advantages: it includes precise live room start and end timestamps, multiple types of real-time user interactions (click, comment, like, gift), and rich side information features for both users and streamers. These features enable more realistic simulation of dynamic candidate items and better modeling of user and streamer behaviors. We conduct a thorough analysis of KuaiLive from multiple perspectives and evaluate several representative recommendation methods on it, establishing a strong benchmark for future research. KuaiLive can support a wide range of tasks in the live streaming domain, such as top-K recommendation, click-through rate prediction, watch time prediction, and gift price prediction. Moreover, its fine-grained behavioral data also enables research on multi-behavior modeling, multi-task learning, and fairness-aware recommendation. The dataset and related resources are publicly available at https://imgkkk574.github.io/KuaiLive.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.05630v1" target="_blank">MOSEv2: A More Challenging Dataset for Video Object Segmentation in Complex Scenes</a></h3>
                    <p><strong>Authors:</strong> Henghui Ding, Kaining Ying, Chang Liu, Shuting He, Xudong Jiang, Yu-Gang Jiang, Philip H. S. Torr, Song Bai</p>
                    <p><strong>Published:</strong> 8/7/2025</p>
                    <p><strong>Categories:</strong> cs.CV</p>
                    <p><strong>Summary:</strong> Video object segmentation (VOS) aims to segment specified target objects throughout a video. Although state-of-the-art methods have achieved impressive performance (e.g., 90+% JF) on existing benchmarks such as DAVIS and YouTube-VOS, these datasets primarily contain salient, dominant, and isolated objects, limiting their generalization to real-world scenarios. To advance VOS toward more realistic environments, coMplex video Object SEgmentation (MOSEv1) was introduced to facilitate VOS research in complex scenes. Building on the strengths and limitations of MOSEv1, we present MOSEv2, a significantly more challenging dataset designed to further advance VOS methods under real-world conditions. MOSEv2 consists of 5,024 videos and over 701,976 high-quality masks for 10,074 objects across 200 categories. Compared to its predecessor, MOSEv2 introduces significantly greater scene complexity, including more frequent object disappearance and reappearance, severe occlusions and crowding, smaller objects, as well as a range of new challenges such as adverse weather (e.g., rain, snow, fog), low-light scenes (e.g., nighttime, underwater), multi-shot sequences, camouflaged objects, non-physical targets (e.g., shadows, reflections), scenarios requiring external knowledge, etc. We benchmark 20 representative VOS methods under 5 different settings and observe consistent performance drops. For example, SAM2 drops from 76.4% on MOSEv1 to only 50.9% on MOSEv2. We further evaluate 9 video object tracking methods and find similar declines, demonstrating that MOSEv2 presents challenges across tasks. These results highlight that despite high accuracy on existing datasets, current VOS methods still struggle under real-world complexities. MOSEv2 is publicly available at https://MOSE.video.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.05628v1" target="_blank">H-Net++: Hierarchical Dynamic Chunking for Tokenizer-Free Language Modelling in Morphologically-Rich Languages</a></h3>
                    <p><strong>Authors:</strong> Mehrdad Zakershahrak, Samira Ghodratnama</p>
                    <p><strong>Published:</strong> 8/7/2025</p>
                    <p><strong>Categories:</strong> cs.CL, cs.AI</p>
                    <p><strong>Summary:</strong> Byte-level language models eliminate fragile tokenizers but face computational challenges in morphologically-rich languages (MRLs), where words span many bytes. We propose H-NET++, a hierarchical dynamic-chunking model that learns linguistically-informed segmentation through end-to-end training. Key innovations include: (1) a lightweight Transformer context-mixer (1.9M parameters) for cross-chunk attention, (2) a two-level latent hyper-prior for document-level consistency, (3) specialized handling of orthographic artifacts (e.g. Persian ZWNJ), and (4) curriculum-based training with staged sequence lengths. On a 1.4B-token Persian corpus, H-NET++ achieves state-of-the-art results: 0.159 BPB reduction versus BPE-based GPT-2-fa (12% better compression), 5.4pp gain on ParsGLUE, 53% improved robustness to ZWNJ corruption, and 73.8% F1 on gold morphological boundaries. Our learned chunks align with Persian morphology without explicit supervision, demonstrating that hierarchical dynamic chunking provides an effective tokenizer-free solution for MRLs while maintaining computational efficiency.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.05622v1" target="_blank">Simulating Human-Like Learning Dynamics with LLM-Empowered Agents</a></h3>
                    <p><strong>Authors:</strong> Yu Yuan, Lili Zhao, Wei Chen, Guangting Zheng, Kai Zhang, Mengdi Zhang, Qi Liu</p>
                    <p><strong>Published:</strong> 8/7/2025</p>
                    <p><strong>Categories:</strong> cs.AI</p>
                    <p><strong>Summary:</strong> Capturing human learning behavior based on deep learning methods has become a major research focus in both psychology and intelligent systems. Recent approaches rely on controlled experiments or rule-based models to explore cognitive processes. However, they struggle to capture learning dynamics, track progress over time, or provide explainability. To address these challenges, we introduce LearnerAgent, a novel multi-agent framework based on Large Language Models (LLMs) to simulate a realistic teaching environment. To explore human-like learning dynamics, we construct learners with psychologically grounded profiles-such as Deep, Surface, and Lazy-as well as a persona-free General Learner to inspect the base LLMs default behavior. Through weekly knowledge acquisition, monthly strategic choices, periodic tests, and peer interaction, we can track the dynamic learning progress of individual learners over a full-year journey. Our findings are fourfold: 1) Longitudinal analysis reveals that only Deep Learner achieves sustained cognitive growth. Our specially designed trap questions effectively diagnose Surface Learners shallow knowledge. 2) The behavioral and cognitive patterns of distinct learners align closely with their psychological profiles. 3) Learners self-concept scores evolve realistically, with the General Learner developing surprisingly high self-efficacy despite its cognitive limitations. 4) Critically, the default profile of base LLM is a diligent but brittle Surface Learner-an agent that mimics the behaviors of a good student but lacks true, generalizable understanding. Extensive simulation experiments demonstrate that LearnerAgent aligns well with real scenarios, yielding more insightful findings about LLMs behavior.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.05619v1" target="_blank">The Missing Reward: Active Inference in the Era of Experience</a></h3>
                    <p><strong>Authors:</strong> Bo Wen</p>
                    <p><strong>Published:</strong> 8/7/2025</p>
                    <p><strong>Categories:</strong> cs.AI, nlin.AO, physics.bio-ph, physics.comp-ph, physics.hist-ph</p>
                    <p><strong>Summary:</strong> This paper argues that Active Inference (AIF) provides a crucial foundation for developing autonomous AI agents capable of learning from experience without continuous human reward engineering. As AI systems begin to exhaust high-quality training data and rely on increasingly large human workforces for reward design, the current paradigm faces significant scalability challenges that could impede progress toward genuinely autonomous intelligence. The proposal for an ``Era of Experience, where agents learn from self-generated data, is a promising step forward. However, this vision still depends on extensive human engineering of reward functions, effectively shifting the bottleneck from data curation to reward curation. This highlights what we identify as the \textbf{grounded-agency gap}: the inability of contemporary AI systems to autonomously formulate, adapt, and pursue objectives in response to changing circumstances. We propose that AIF can bridge this gap by replacing external reward signals with an intrinsic drive to minimize free energy, allowing agents to naturally balance exploration and exploitation through a unified Bayesian objective. By integrating Large Language Models as generative world models with AIFs principled decision-making framework, we can create agents that learn efficiently from experience while remaining aligned with human values. This synthesis offers a compelling path toward AI systems that can develop autonomously while adhering to both computational and physical constraints.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.05616v1" target="_blank">TrajEvo: Trajectory Prediction Heuristics Design via LLM-driven Evolution</a></h3>
                    <p><strong>Authors:</strong> Zhikai Zhao, Chuanbo Hua, Federico Berto, Kanghoon Lee, Zihan Ma, Jiachen Li, Jinkyoo Park</p>
                    <p><strong>Published:</strong> 8/7/2025</p>
                    <p><strong>Categories:</strong> cs.LG, cs.AI, cs.NE, cs.RO</p>
                    <p><strong>Summary:</strong> Trajectory prediction is a critical task in modeling human behavior, especially in safety-critical domains such as social robotics and autonomous vehicle navigation. Traditional heuristics based on handcrafted rules often lack accuracy and generalizability. Although deep learning approaches offer improved performance, they typically suffer from high computational cost, limited explainability, and, importantly, poor generalization to out-of-distribution (OOD) scenarios. In this paper, we introduce TrajEvo, a framework that leverages Large Language Models (LLMs) to automatically design trajectory prediction heuristics. TrajEvo employs an evolutionary algorithm to generate and refine prediction heuristics from past trajectory data. We propose two key innovations: Cross-Generation Elite Sampling to encourage population diversity, and a Statistics Feedback Loop that enables the LLM to analyze and improve alternative predictions. Our evaluations demonstrate that TrajEvo outperforms existing heuristic methods across multiple real-world datasets, and notably surpasses both heuristic and deep learning methods in generalizing to an unseen OOD real-world dataset. TrajEvo marks a promising step toward the automated design of fast, explainable, and generalizable trajectory prediction heuristics. We release our source code to facilitate future research at https://github.com/ai4co/trajevo.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.05615v1" target="_blank">Test-Time Reinforcement Learning for GUI Grounding via Region Consistency</a></h3>
                    <p><strong>Authors:</strong> Yong Du, Yuchen Yan, Fei Tang, Zhengxi Lu, Chang Zong, Weiming Lu, Shengpei Jiang, Yongliang Shen</p>
                    <p><strong>Published:</strong> 8/7/2025</p>
                    <p><strong>Categories:</strong> cs.CV, cs.AI, cs.CL</p>
                    <p><strong>Summary:</strong> Graphical User Interface (GUI) grounding, the task of mapping natural language instructions to precise screen coordinates, is fundamental to autonomous GUI agents. While existing methods achieve strong performance through extensive supervised training or reinforcement learning with labeled rewards, they remain constrained by the cost and availability of pixel-level annotations. We observe that when models generate multiple predictions for the same GUI element, the spatial overlap patterns reveal implicit confidence signals that can guide more accurate localization. Leveraging this insight, we propose GUI-RC (Region Consistency), a test-time scaling method that constructs spatial voting grids from multiple sampled predictions to identify consensus regions where models show highest agreement. Without any training, GUI-RC improves accuracy by 2-3% across various architectures on ScreenSpot benchmarks. We further introduce GUI-RCPO (Region Consistency Policy Optimization), which transforms these consistency patterns into rewards for test-time reinforcement learning. By computing how well each prediction aligns with the collective consensus, GUI-RCPO enables models to iteratively refine their outputs on unlabeled data during inference. Extensive experiments demonstrate the generality of our approach: GUI-RC boosts Qwen2.5-VL-3B-Instruct from 80.11% to 83.57% on ScreenSpot-v2, while GUI-RCPO further improves it to 85.14% through self-supervised optimization. Our approach reveals the untapped potential of test-time scaling and test-time reinforcement learning for GUI grounding, offering a promising path toward more robust and data-efficient GUI agents.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.05614v1" target="_blank">OmniEAR: Benchmarking Agent Reasoning in Embodied Tasks</a></h3>
                    <p><strong>Authors:</strong> Zixuan Wang, Dingming Li, Hongxing Li, Shuo Chen, Yuchen Yan, Wenqi Zhang, Yongliang Shen, Weiming Lu, Jun Xiao, Yueting Zhuang</p>
                    <p><strong>Published:</strong> 8/7/2025</p>
                    <p><strong>Categories:</strong> cs.CL, cs.AI</p>
                    <p><strong>Summary:</strong> Large language models excel at abstract reasoning but their capacity for embodied agent reasoning remains largely unexplored. We present OmniEAR, a comprehensive framework for evaluating how language models reason about physical interactions, tool usage, and multi-agent coordination in embodied tasks. Unlike existing benchmarks that provide predefined tool sets or explicit collaboration directives, OmniEAR requires agents to dynamically acquire capabilities and autonomously determine coordination strategies based on task demands. Through text-based environment representation, we model continuous physical properties and complex spatial relationships across 1,500 scenarios spanning household and industrial domains. Our systematic evaluation reveals severe performance degradation when models must reason from constraints: while achieving 85-96% success with explicit instructions, performance drops to 56-85% for tool reasoning and 63-85% for implicit collaboration, with compound tasks showing over 50% failure rates. Surprisingly, complete environmental information degrades coordination performance, indicating models cannot filter task-relevant constraints. Fine-tuning improves single-agent tasks dramatically (0.6% to 76.3%) but yields minimal multi-agent gains (1.5% to 5.5%), exposing fundamental architectural limitations. These findings demonstrate that embodied reasoning poses fundamentally different challenges than current models can address, establishing OmniEAR as a rigorous benchmark for evaluating and advancing embodied AI systems. Our code and data are included in the supplementary materials and will be open-sourced upon acceptance.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.05609v1" target="_blank">Hi3DEval: Advancing 3D Generation Evaluation with Hierarchical Validity</a></h3>
                    <p><strong>Authors:</strong> Yuhan Zhang, Long Zhuo, Ziyang Chu, Tong Wu, Zhibing Li, Liang Pan, Dahua Lin, Ziwei Liu</p>
                    <p><strong>Published:</strong> 8/7/2025</p>
                    <p><strong>Categories:</strong> cs.CV</p>
                    <p><strong>Summary:</strong> Despite rapid advances in 3D content generation, quality assessment for the generated 3D assets remains challenging. Existing methods mainly rely on image-based metrics and operate solely at the object level, limiting their ability to capture spatial coherence, material authenticity, and high-fidelity local details. 1) To address these challenges, we introduce Hi3DEval, a hierarchical evaluation framework tailored for 3D generative content. It combines both object-level and part-level evaluation, enabling holistic assessments across multiple dimensions as well as fine-grained quality analysis. Additionally, we extend texture evaluation beyond aesthetic appearance by explicitly assessing material realism, focusing on attributes such as albedo, saturation, and metallicness. 2) To support this framework, we construct Hi3DBench, a large-scale dataset comprising diverse 3D assets and high-quality annotations, accompanied by a reliable multi-agent annotation pipeline. We further propose a 3D-aware automated scoring system based on hybrid 3D representations. Specifically, we leverage video-based representations for object-level and material-subject evaluations to enhance modeling of spatio-temporal consistency and employ pretrained 3D features for part-level perception. Extensive experiments demonstrate that our approach outperforms existing image-based metrics in modeling 3D characteristics and achieves superior alignment with human preference, providing a scalable alternative to manual evaluations. The project page is available at https://zyh482.github.io/Hi3DEval/.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.05602v1" target="_blank">LLaVA-RE: Binary Image-Text Relevancy Evaluation with Multimodal Large Language Model</a></h3>
                    <p><strong>Authors:</strong> Tao Sun, Oliver Liu, JinJin Li, Lan Ma</p>
                    <p><strong>Published:</strong> 8/7/2025</p>
                    <p><strong>Categories:</strong> cs.CV</p>
                    <p><strong>Summary:</strong> Multimodal generative AI usually involves generating image or text responses given inputs in another modality. The evaluation of image-text relevancy is essential for measuring response quality or ranking candidate responses. In particular, binary relevancy evaluation, i.e., ``Relevant vs. ``Not Relevant, is a fundamental problem. However, this is a challenging task considering that texts have diverse formats and the definition of relevancy varies in different scenarios. We find that Multimodal Large Language Models (MLLMs) are an ideal choice to build such evaluators, as they can flexibly handle complex text formats and take in additional task information. In this paper, we present LLaVA-RE, a first attempt for binary image-text relevancy evaluation with MLLM. It follows the LLaVA architecture and adopts detailed task instructions and multimodal in-context samples. In addition, we propose a novel binary relevancy data set that covers various tasks. Experimental results validate the effectiveness of our framework.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.05598v1" target="_blank">Unveiling the Lithium-Ion Transport Mechanism in Li2ZrCl6 Solid-State Electrolyte via Deep Learning-Accelerated Molecular Dynamics Simulations</a></h3>
                    <p><strong>Authors:</strong> Hanzeng Guo, Volodymyr Koverga, Selva Chandrasekaran Selvaraj, Anh T. Ngo</p>
                    <p><strong>Published:</strong> 8/7/2025</p>
                    <p><strong>Categories:</strong> cond-mat.mtrl-sci</p>
                    <p><strong>Summary:</strong> Lithium zirconium chlorides (LZCs) present a promising class of cost-effective solid electrolyte for next-generation all-solid-state batteries. The unique crystal structure of LZCs plays a crucial role in facilitating lithium-ion mobility, which is central to their electrochemical performance. To understand the underlying mechanism governing ion transport, we employed deep learning-accelerated molecular dynamics simulation on Li2ZrCl6 (trigonal {\alpha}- and monoclinic \b{eta}-LZC), focusing specifically on the zirconium coordination environment. Our results reveal that disordered {\alpha}-LZC exhibits the highest ionic conductivity, while \b{eta}-LZC demonstrates significantly lower conductivity, closely aligning with experimental findings. Detailed analysis shows substantial differences in lithium-ion dynamics: {\alpha}-LZC phases display pronounced collective diffusion driven anisotropic interlayer transport, whereas lithium mobility in \b{eta}-LZC is largely determined by isotropic translations and individual diffusion dominated by intralayer migration. Across all phases, lithium migration proceeds via site-to-site hopping mechanism, where variations in site residence times critically impact the overall ionic conductivity. Local structure organizations analysis confirms that particular zirconium arrangements in LZC phases create varied ion channel energy barriers, influencing dynamic behaviors: In {\alpha}-LZC phases, the interlayer hopping barrier is lower than the intralayer barrier, facilitating faster ion transport. Disordered {\alpha}-LZC, with its loose zirconium arrangement, presents the lowest energy barrier, enhancing conductivity. Conversely, \b{eta}-LZC features a higher overall barrier, with intralayer hopping favored over interlayer, resulting in slower ion migration.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.05592v1" target="_blank">MathSmith: Towards Extremely Hard Mathematical Reasoning by Forging Synthetic Problems with a Reinforced Policy</a></h3>
                    <p><strong>Authors:</strong> Shaoxiong Zhan, Yanlin Lai, Ziyu Lu, Dahua Lin, Ziqing Yang, Fei Tang</p>
                    <p><strong>Published:</strong> 8/7/2025</p>
                    <p><strong>Categories:</strong> cs.CL</p>
                    <p><strong>Summary:</strong> Large language models have achieved substantial progress in mathematical reasoning, yet their advancement is limited by the scarcity of high-quality, high-difficulty training data. Existing synthesis methods largely rely on transforming human-written templates, limiting both diversity and scalability. We propose MathSmith, a novel framework for synthesizing challenging mathematical problems to enhance LLM reasoning. Rather than modifying existing problems, MathSmith constructs new ones from scratch by randomly sampling concept-explanation pairs from PlanetMath, ensuring data independence and avoiding contamination. To increase difficulty, we design nine predefined strategies as soft constraints during rationales. We further adopts reinforcement learning to jointly optimize structural validity, reasoning complexity, and answer consistency. The length of the reasoning trace generated under autoregressive prompting is used to reflect cognitive complexity, encouraging the creation of more demanding problems aligned with long-chain-of-thought reasoning. Experiments across five benchmarks, categorized as easy  medium (GSM8K, MATH-500) and hard (AIME2024, AIME2025, OlympiadBench), show that MathSmith consistently outperforms existing baselines under both short and long CoT settings. Additionally, a weakness-focused variant generation module enables targeted improvement on specific concepts. Overall, MathSmith exhibits strong scalability, generalization, and transferability, highlighting the promise of high-difficulty synthetic data in advancing LLM reasoning capabilities.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.05584v1" target="_blank">Robust adaptive fuzzy sliding mode control for trajectory tracking for of cylindrical manipulator</a></h3>
                    <p><strong>Authors:</strong> Van Cuong Pham, Minh Hai Tran, Phuc Anh Nguyen, Ngoc Son Vu, Nga Nguyen Thi</p>
                    <p><strong>Published:</strong> 8/7/2025</p>
                    <p><strong>Categories:</strong> cs.RO</p>
                    <p><strong>Summary:</strong> This research proposes a robust adaptive fuzzy sliding mode control (AFSMC) approach to enhance the trajectory tracking performance of cylindrical robotic manipulators, extensively utilized in applications such as CNC and 3D printing. The proposed approach integrates fuzzy logic with sliding mode control (SMC) to bolster adaptability and robustness, with fuzzy logic approximating the uncertain dynamics of the system, while SMC ensures strong performance. Simulation results in MATLAB/Simulink demonstrate that AFSMC significantly improves trajectory tracking accuracy, stability, and disturbance rejection compared to traditional methods. This research underscores the effectiveness of AFSMC in controlling robotic manipulators, contributing to enhanced precision in industrial robotic applications.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.05583v1" target="_blank">Research on integrated intelligent energy management system based on big data analysis and machine learning</a></h3>
                    <p><strong>Authors:</strong> Jinzhou Xu, Yadan Zhang, Paola Tapia</p>
                    <p><strong>Published:</strong> 8/7/2025</p>
                    <p><strong>Categories:</strong> eess.SY, cs.SY</p>
                    <p><strong>Summary:</strong> The application of big data is one of the significant features of integrated smart energy. Applying it to the file management of integrated smart energy projects is of great significance for improving the efficiency of project management and control. This article first discussed the benefits and challenges of implementing big data analysis in document management and control of integrated smart energy projects. In addition, an implementation framework for big data analysis in integrated smart energy project document management was developed, and a method for optimizing the efficiency of integrated smart energy project document management through machine learning was proposed. Using various types of data and information generated during the project document management process, the efficiency of the entire process project document control through three different machine learning methods was optimized. The result of fitting a penalty linear regression model shows that when there is enough data as a training set, the accuracy of the model achieved can reach over 95\%. By using big data analysis and machine learning to analyze the efficiency of comprehensive smart energy project document management, it is possible to track the entire process of comprehensive smart energy project documents and optimize business processes, thereby strengthening project construction control and improving project construction efficiency.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.05580v1" target="_blank">Follow-Your-Instruction: A Comprehensive MLLM Agent for World Data Synthesis</a></h3>
                    <p><strong>Authors:</strong> Kunyu Feng, Yue Ma, Xinhua Zhang, Boshi Liu, Yikuang Yuluo, Yinhan Zhang, Runtao Liu, Hongyu Liu, Zhiyuan Qin, Shanhui Mo, Qifeng Chen, Zeyu Wang</p>
                    <p><strong>Published:</strong> 8/7/2025</p>
                    <p><strong>Categories:</strong> cs.CV</p>
                    <p><strong>Summary:</strong> With the growing demands of AI-generated content (AIGC), the need for high-quality, diverse, and scalable data has become increasingly crucial. However, collecting large-scale real-world data remains costly and time-consuming, hindering the development of downstream applications. While some works attempt to collect task-specific data via a rendering process, most approaches still rely on manual scene construction, limiting their scalability and accuracy. To address these challenges, we propose Follow-Your-Instruction, a Multimodal Large Language Model (MLLM)-driven framework for automatically synthesizing high-quality 2D, 3D, and 4D data. Our \textbf{Follow-Your-Instruction} first collects assets and their associated descriptions through multimodal inputs using the MLLM-Collector. Then it constructs 3D layouts, and leverages Vision-Language Models (VLMs) for semantic refinement through multi-view scenes with the MLLM-Generator and MLLM-Optimizer, respectively. Finally, it uses MLLM-Planner to generate temporally coherent future frames. We evaluate the quality of the generated data through comprehensive experiments on the 2D, 3D, and 4D generative tasks. The results show that our synthetic data significantly boosts the performance of existing baseline models, demonstrating Follow-Your-Instructions potential as a scalable and effective data engine for generative intelligence.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.05572v1" target="_blank">Discrepancy-Aware Contrastive Adaptation in Medical Time Series Analysis</a></h3>
                    <p><strong>Authors:</strong> Yifan Wang, Hongfeng Ai, Ruiqi Li, Maowei Jiang, Ruiyuan Kang, Jiahua Dong, Cheng Jiang, Chenzhong Li</p>
                    <p><strong>Published:</strong> 8/7/2025</p>
                    <p><strong>Categories:</strong> cs.HC</p>
                    <p><strong>Summary:</strong> In medical time series disease diagnosis, two key challenges are identified. First, the high annotation cost of medical data leads to overfitting in models trained on label-limited, single-center datasets. To address this, we propose incorporating external data from related tasks and leveraging AE-GAN to extract prior knowledge, providing valuable references for downstream tasks. Second, many existing studies employ contrastive learning to derive more generalized medical sequence representations for diagnostic tasks, usually relying on manually designed diverse positive and negative sample pairs. However, these approaches are complex, lack generalizability, and fail to adaptively capture disease-specific features across different conditions. To overcome this, we introduce LMCF (Learnable Multi-views Contrastive Framework), a framework that integrates a multi-head attention mechanism and adaptively learns representations from different views through inter-view and intra-view contrastive learning strategies. Additionally, the pre-trained AE-GAN is used to reconstruct discrepancies in the target data as disease probabilities, which are then integrated into the contrastive learning process. Experiments on three target datasets demonstrate that our method consistently outperforms other seven baselines, highlighting its significant impact on healthcare applications such as the diagnosis of myocardial infarction, Alzheimers disease, and Parkinsons disease. We release the source code at xxxxx.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.05571v1" target="_blank">Fairy$\pm i$: the First 2-bit Complex LLM with All Parameters in $\{\pm1, \pm i\}$</a></h3>
                    <p><strong>Authors:</strong> Feiyu Wang, Guoan Wang, Yihao Zhang, Shengfan Wang, Weitao Li, Bokai Huang, Shimao Chen, Zihan Jiang, Rui Xu, Tong Yang</p>
                    <p><strong>Published:</strong> 8/7/2025</p>
                    <p><strong>Categories:</strong> cs.LG, cs.CL</p>
                    <p><strong>Summary:</strong> Quantization-Aware Training (QAT) integrates quantization into the training loop, enabling LLMs to learn robust low-bit representations, and is widely recognized as one of the most promising research directions. All current QAT research focuses on minimizing quantization error on full-precision models, where the full-precision accuracy acts as an upper bound (accuracy ceiling). No existing method has even attempted to surpass this ceiling. To break this ceiling, we propose a new paradigm: raising the ceiling (full-precision model), and then still quantizing it efficiently into 2 bits. We propose Fairy$\pm i$, the first 2-bit quantization framework for complex-valued LLMs. Specifically, our method leverages the representational advantages of the complex domain to boost full-precision accuracy. We map weights to the fourth roots of unity $\{\pm1, \pm i\}$, forming a perfectly symmetric and information-theoretically optimal 2-bit representation. Importantly, each quantized weight has either a zero real or imaginary part, enabling multiplication-free inference using only additions and element swaps. Experimental results show that Fairy$\pm i$ outperforms the ceiling of existing 2-bit quantization approaches in terms of both PPL and downstream tasks, while maintaining strict storage and compute efficiency. This work opens a new direction for building highly accurate and practical LLMs under extremely low-bit constraints.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.05570v1" target="_blank">High-Order Error Bounds for Markovian LSA with Richardson-Romberg Extrapolation</a></h3>
                    <p><strong>Authors:</strong> Ilya Levin, Alexey Naumov, Sergey Samsonov</p>
                    <p><strong>Published:</strong> 8/7/2025</p>
                    <p><strong>Categories:</strong> stat.ML, cs.LG, math.OC, math.ST, stat.TH, 62L20</p>
                    <p><strong>Summary:</strong> In this paper, we study the bias and high-order error bounds of the Linear Stochastic Approximation (LSA) algorithm with Polyak-Ruppert (PR) averaging under Markovian noise. We focus on the version of the algorithm with constant step size $\alpha$ and propose a novel decomposition of the bias via a linearization technique. We analyze the structure of the bias and show that the leading-order term is linear in $\alpha$ and cannot be eliminated by PR averaging. To address this, we apply the Richardson-Romberg (RR) extrapolation procedure, which effectively cancels the leading bias term. We derive high-order moment bounds for the RR iterates and show that the leading error term aligns with the asymptotically optimal covariance matrix of the vanilla averaged LSA iterates.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.05568v1" target="_blank">X-VFL: A New Vertical Federated Learning Framework with Cross Completion and Decision Subspace Alignment</a></h3>
                    <p><strong>Authors:</strong> Qinghua Yao, Xiangrui Xu, Zhize Li</p>
                    <p><strong>Published:</strong> 8/7/2025</p>
                    <p><strong>Categories:</strong> cs.LG, cs.CV, cs.DC, math.OC</p>
                    <p><strong>Summary:</strong> Vertical Federated Learning (VFL) enables collaborative learning by integrating disjoint feature subsets from multiple clients/parties. However, VFL typically faces two key challenges: i) the requirement for perfectly aligned data samples across all clients (missing features are not allowed); ii) the requirement for joint collaborative inference/prediction involving all clients (it does not support locally independent inference on a single client). To address these challenges, we propose X-VFL, a new VFL framework designed to deal with the non-aligned data samples with (partially) missing features and to support locally independent inference of new data samples for each client. In particular, we design two novel modules in X-VFL: Cross Completion (XCom) and Decision Subspace Alignment (DS-Align). XCom can complete/reconstruct missing features for non-aligned data samples by leveraging information from other clients. DS-Align aligns local features with completed and global features across all clients within the decision subspace, thus enabling locally independent inference at each client. Moreover, we provide convergence theorems for different algorithms used in training X-VFL, showing an $O(1/\sqrt{T})$ convergence rate for SGD-type algorithms and an $O(1/T)$ rate for PAGE-type algorithms, where $T$ denotes the number of training update steps. Extensive experiments on real-world datasets demonstrate that X-VFL significantly outperforms existing methods, e.g., achieving a 15% improvement in accuracy on the image CIFAR-10 dataset and a 43% improvement on the medical MIMIC-III dataset. These results validate the practical effectiveness and superiority of X-VFL, particularly in scenarios involving partially missing features and locally independent inference.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.05554v1" target="_blank">SPGISpeech 2.0: Transcribed multi-speaker financial audio for speaker-tagged transcription</a></h3>
                    <p><strong>Authors:</strong> Raymond Grossman, Taejin Park, Kunal Dhawan, Andrew Titus, Sophia Zhi, Yulia Shchadilova, Weiqing Wang, Jagadeesh Balam, Boris Ginsburg</p>
                    <p><strong>Published:</strong> 8/7/2025</p>
                    <p><strong>Categories:</strong> cs.SD, cs.CL, eess.AS</p>
                    <p><strong>Summary:</strong> We introduce SPGISpeech 2.0, a dataset suitable for speaker-tagged transcription in the financial domain. SPGISpeech 2.0 improves the diversity of applicable modeling tasks while maintaining the core characteristic of the original SPGISpeech dataset: audio snippets and their corresponding fully formatted text transcriptions, usable for end-to-end automatic speech recognition (ASR). SPGISpeech 2.0 consists of 3,780 additional hours of professionally transcribed earnings calls. Furthermore, the dataset contains call and speaker information for each audio snippet facilitating multi-talker ASR. We validate the utility of SPGISpeech 2.0 through improvements in speaker-tagged ASR performance of popular speech recognition models after fine-tuning on SPGISpeech 2.0. Released free for non-commercial use, we expect SPGISpeech 2.0 to foster advancements in speech recognition technologies and inspire a wide range of research applications.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.05553v1" target="_blank">Do Political Opinions Transfer Between Western Languages? An Analysis of Unaligned and Aligned Multilingual LLMs</a></h3>
                    <p><strong>Authors:</strong> Franziska Weeber, Tanise Ceron, Sebastian PadÃ³</p>
                    <p><strong>Published:</strong> 8/7/2025</p>
                    <p><strong>Categories:</strong> cs.CL, cs.CY, I.2.7; J.4</p>
                    <p><strong>Summary:</strong> Public opinion surveys show cross-cultural differences in political opinions between socio-cultural contexts. However, there is no clear evidence whether these differences translate to cross-lingual differences in multilingual large language models (MLLMs). We analyze whether opinions transfer between languages or whether there are separate opinions for each language in MLLMs of various sizes across five Western languages. We evaluate MLLMs opinions by prompting them to report their (dis)agreement with political statements from voting advice applications. To better understand the interaction between languages in the models, we evaluate them both before and after aligning them with more left or right views using direct preference optimization and English alignment data only. Our findings reveal that unaligned models show only very few significant cross-lingual differences in the political opinions they reflect. The political alignment shifts opinions almost uniformly across all five languages. We conclude that in Western language contexts, political opinions transfer between languages, demonstrating the challenges in achieving explicit socio-linguistic, cultural, and political alignment of MLLMs.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.05550v1" target="_blank">PhysiBoSS-Models: A database for multiscale models</a></h3>
                    <p><strong>Authors:</strong> Vincent Noel, Marco Ruscone, Randy Heiland, Arnau Montagud, Alfonso Valencia, Emmanuel Barillot, Paul Macklin, Laurence Calzone</p>
                    <p><strong>Published:</strong> 8/7/2025</p>
                    <p><strong>Categories:</strong> q-bio.QM</p>
                    <p><strong>Summary:</strong> PhysiBoSS is an open-source platform that integrates agent-based modeling of cell populations with intracellular stochastic Boolean networks, enabling multiscale simulations of complex biological behaviors. To promote model sharing and versioning, we present the PhysiBoSS-Models database: a curated repository for multiscale models built with PhysiBoSS. By providing a simple Python API, PhysiBoSS-Models provides an easy way to download and simulate preexisting models through tools such as PhysiCell Studio. By providing standardized access to validated models, PhysiBoSS-Models facilitates reuse, validation, and benchmarking, supporting research in biology.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.05547v1" target="_blank">Adapting Vision-Language Models Without Labels: A Comprehensive Survey</a></h3>
                    <p><strong>Authors:</strong> Hao Dong, Lijun Sheng, Jian Liang, Ran He, Eleni Chatzi, Olga Fink</p>
                    <p><strong>Published:</strong> 8/7/2025</p>
                    <p><strong>Categories:</strong> cs.LG, cs.AI, cs.CV</p>
                    <p><strong>Summary:</strong> Vision-Language Models (VLMs) have demonstrated remarkable generalization capabilities across a wide range of tasks. However, their performance often remains suboptimal when directly applied to specific downstream scenarios without task-specific adaptation. To enhance their utility while preserving data efficiency, recent research has increasingly focused on unsupervised adaptation methods that do not rely on labeled data. Despite the growing interest in this area, there remains a lack of a unified, task-oriented survey dedicated to unsupervised VLM adaptation. To bridge this gap, we present a comprehensive and structured overview of the field. We propose a taxonomy based on the availability and nature of unlabeled visual data, categorizing existing approaches into four key paradigms: Data-Free Transfer (no data), Unsupervised Domain Transfer (abundant data), Episodic Test-Time Adaptation (batch data), and Online Test-Time Adaptation (streaming data). Within this framework, we analyze core methodologies and adaptation strategies associated with each paradigm, aiming to establish a systematic understanding of the field. Additionally, we review representative benchmarks across diverse applications and highlight open challenges and promising directions for future research. An actively maintained repository of relevant literature is available at https://github.com/tim-learn/Awesome-LabelFree-VLMs.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.05543v1" target="_blank">CleanUpBench: Embodied Sweeping and Grasping Benchmark</a></h3>
                    <p><strong>Authors:</strong> Wenbo Li, Guanting Chen, Tao Zhao, Jiyao Wang, Tianxin Hu, Yuwen Liao, Weixiang Guo, Shenghai Yuan</p>
                    <p><strong>Published:</strong> 8/7/2025</p>
                    <p><strong>Categories:</strong> cs.RO</p>
                    <p><strong>Summary:</strong> Embodied AI benchmarks have advanced navigation, manipulation, and reasoning, but most target complex humanoid agents or large-scale simulations that are far from real-world deployment. In contrast, mobile cleaning robots with dual mode capabilities, such as sweeping and grasping, are rapidly emerging as realistic and commercially viable platforms. However, no benchmark currently exists that systematically evaluates these agents in structured, multi-target cleaning tasks, revealing a critical gap between academic research and real-world applications. We introduce CleanUpBench, a reproducible and extensible benchmark for evaluating embodied agents in realistic indoor cleaning scenarios. Built on NVIDIA Isaac Sim, CleanUpBench simulates a mobile service robot equipped with a sweeping mechanism and a six-degree-of-freedom robotic arm, enabling interaction with heterogeneous objects. The benchmark includes manually designed environments and one procedurally generated layout to assess generalization, along with a comprehensive evaluation suite covering task completion, spatial efficiency, motion quality, and control performance. To support comparative studies, we provide baseline agents based on heuristic strategies and map-based planning. CleanUpBench bridges the gap between low-level skill evaluation and full-scene testing, offering a scalable testbed for grounded, embodied intelligence in everyday settings.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.05532v1" target="_blank">Aircraft routing: periodicity and complexity</a></h3>
                    <p><strong>Authors:</strong> FrÃ©dÃ©ric Meunier, Axel Parmentier, Nour ElHouda Tellache</p>
                    <p><strong>Published:</strong> 8/7/2025</p>
                    <p><strong>Categories:</strong> cs.DM, math.OC</p>
                    <p><strong>Summary:</strong> The aircraft routing problem is one of the most studied problems of operations research applied to aircraft management. It involves assigning flights to aircraft while ensuring regular visits to maintenance bases. This paper examines two aspects of the problem. First, we explore the relationship between periodic instances, where flights are the same every day, and periodic solutions. The literature has implicitly assumed-without discussion-that periodic instances necessitate periodic solutions, and even periodic solutions in a stronger form, where every two airplanes perform either the exact same cyclic sequence of flights, or completely disjoint cyclic sequences. However, enforcing such periodicity may eliminate feasible solutions. We prove that, when regular maintenance is required at most every four days, there always exist periodic solutions of this form. Second, we consider the computational hardness of the problem. Even if many papers in this area refer to the NP-hardness of the aircraft routing problem, such a result is only available in the literature for periodic instances. We establish its NP-hardness for a non-periodic version. Polynomiality of a special but natural case is also proven.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.05527v1" target="_blank">AI vs. Human Moderators: A Comparative Evaluation of Multimodal LLMs in Content Moderation for Brand Safety</a></h3>
                    <p><strong>Authors:</strong> Adi Levi, Or Levi, Sardhendu Mishra, Jonathan Morra</p>
                    <p><strong>Published:</strong> 8/7/2025</p>
                    <p><strong>Categories:</strong> cs.CV, I.2.10; I.2.7; H.3.3; H.4.3; K.4.1</p>
                    <p><strong>Summary:</strong> As the volume of video content online grows exponentially, the demand for moderation of unsafe videos has surpassed human capabilities, posing both operational and mental health challenges. While recent studies demonstrated the merits of Multimodal Large Language Models (MLLMs) in various video understanding tasks, their application to multimodal content moderation, a domain that requires nuanced understanding of both visual and textual cues, remains relatively underexplored. In this work, we benchmark the capabilities of MLLMs in brand safety classification, a critical subset of content moderation for safe-guarding advertising integrity. To this end, we introduce a novel, multimodal and multilingual dataset, meticulously labeled by professional reviewers in a multitude of risk categories. Through a detailed comparative analysis, we demonstrate the effectiveness of MLLMs such as Gemini, GPT, and Llama in multimodal brand safety, and evaluate their accuracy and cost efficiency compared to professional human reviewers. Furthermore, we present an in-depth discussion shedding light on limitations of MLLMs and failure cases. We are releasing our dataset alongside this paper to facilitate future research on effective and responsible brand safety and content moderation.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.05526v1" target="_blank">When Deepfake Detection Meets Graph Neural Network:a Unified and Lightweight Learning Framework</a></h3>
                    <p><strong>Authors:</strong> Haoyu Liu, Chaoyu Gong, Mengke He, Jiate Li, Kai Han, Siqiang Luo</p>
                    <p><strong>Published:</strong> 8/7/2025</p>
                    <p><strong>Categories:</strong> cs.CV</p>
                    <p><strong>Summary:</strong> The proliferation of generative video models has made detecting AI-generated and manipulated videos an urgent challenge. Existing detection approaches often fail to generalize across diverse manipulation types due to their reliance on isolated spatial, temporal, or spectral information, and typically require large models to perform well. This paper introduces SSTGNN, a lightweight Spatial-Spectral-Temporal Graph Neural Network framework that represents videos as structured graphs, enabling joint reasoning over spatial inconsistencies, temporal artifacts, and spectral distortions. SSTGNN incorporates learnable spectral filters and temporal differential modeling into a graph-based architecture, capturing subtle manipulation traces more effectively. Extensive experiments on diverse benchmark datasets demonstrate that SSTGNN not only achieves superior performance in both in-domain and cross-domain settings, but also offers strong robustness against unseen manipulations. Remarkably, SSTGNN accomplishes these results with up to 42.4$\times$ fewer parameters than state-of-the-art models, making it highly lightweight and scalable for real-world deployment.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.05524v1" target="_blank">GASP: A Gradient-Aware Shortest Path Algorithm for Boundary-Confined Visualization of 2-Manifold Reeb Graphs</a></h3>
                    <p><strong>Authors:</strong> Sefat Rahman, Tushar M. Athawale, Paul Rosen</p>
                    <p><strong>Published:</strong> 8/7/2025</p>
                    <p><strong>Categories:</strong> cs.GR, cs.CG, cs.HC</p>
                    <p><strong>Summary:</strong> Reeb graphs are an important tool for abstracting and representing the topological structure of a function defined on a manifold. We have identified three properties for faithfully representing Reeb graphs in a visualization. Namely, they should be constrained to the boundary, compact, and aligned with the function gradient. Existing algorithms for drawing Reeb graphs are agnostic to or violate these properties. In this paper, we introduce an algorithm to generate Reeb graph visualizations, called \textit{GASP}, that is cognizant of these properties, thereby producing visualizations that are more representative of the underlying data. To demonstrate the improvements, the resulting Reeb graphs are evaluated both qualitatively and quantitatively against the geometric barycenter algorithm, using its implementation available in the Topology ToolKit (TTK), a widely adopted tool for calculating and visualizing Reeb graphs.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.05519v1" target="_blank">Leveraging AI to Accelerate Clinical Data Cleaning: A Comparative Study of AI-Assisted vs. Traditional Methods</a></h3>
                    <p><strong>Authors:</strong> Matthew Purri, Amit Patel, Erik Deurrell</p>
                    <p><strong>Published:</strong> 8/7/2025</p>
                    <p><strong>Categories:</strong> cs.CV</p>
                    <p><strong>Summary:</strong> Clinical trial data cleaning represents a critical bottleneck in drug development, with manual review processes struggling to manage exponentially increasing data volumes and complexity. This paper presents Octozi, an artificial intelligence-assisted platform that combines large language models with domain-specific heuristics to transform clinical data review. In a controlled experimental study with experienced clinical reviewers (n=10), we demonstrate that AI assistance increased data cleaning throughput by 6.03-fold while simultaneously decreasing cleaning errors from 54.67% to 8.48% (a 6.44-fold improvement). Crucially, the system reduced false positive queries by 15.48-fold, minimizing unnecessary site burden. These improvements were consistent across reviewers regardless of experience level, suggesting broad applicability. Our findings indicate that AI-assisted approaches can address fundamental inefficiencies in clinical trial operations, potentially accelerating drug development timelines and reducing costs while maintaining regulatory compliance. This work establishes a framework for integrating AI into safety-critical clinical workflows and demonstrates the transformative potential of human-AI collaboration in pharmaceutical clinical trials.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.05514v1" target="_blank">Head Anchor Enhanced Detection and Association for Crowded Pedestrian Tracking</a></h3>
                    <p><strong>Authors:</strong> Zewei Wu, CÃ©sar Teixeira, Wei Ke, Zhang Xiong</p>
                    <p><strong>Published:</strong> 8/7/2025</p>
                    <p><strong>Categories:</strong> cs.CV</p>
                    <p><strong>Summary:</strong> Visual pedestrian tracking represents a promising research field, with extensive applications in intelligent surveillance, behavior analysis, and human-computer interaction. However, real-world applications face significant occlusion challenges. When multiple pedestrians interact or overlap, the loss of target features severely compromises the trackers ability to maintain stable trajectories. Traditional tracking methods, which typically rely on full-body bounding box features extracted from {Re-ID} models and linear constant-velocity motion assumptions, often struggle in severe occlusion scenarios. To address these limitations, this work proposes an enhanced tracking framework that leverages richer feature representations and a more robust motion model. Specifically, the proposed method incorporates detection features from both the regression and classification branches of an object detector, embedding spatial and positional information directly into the feature representations. To further mitigate occlusion challenges, a head keypoint detection model is introduced, as the head is less prone to occlusion compared to the full body. In terms of motion modeling, we propose an iterative Kalman filtering approach designed to align with modern detector assumptions, integrating 3D priors to better complete motion trajectories in complex scenes. By combining these advancements in appearance and motion modeling, the proposed method offers a more robust solution for multi-object tracking in crowded environments where occlusions are prevalent.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.05513v1" target="_blank">Streamlining Admission with LOR Insights: AI-Based Leadership Assessment in Online Masters Program</a></h3>
                    <p><strong>Authors:</strong> Meryem Yilmaz Soylu, Adrian Gallard, Jeonghyun Lee, Gayane Grigoryan, Rushil Desai, Stephen Harmon</p>
                    <p><strong>Published:</strong> 8/7/2025</p>
                    <p><strong>Categories:</strong> cs.AI, cs.LG</p>
                    <p><strong>Summary:</strong> Letters of recommendation (LORs) provide valuable insights into candidates capabilities and experiences beyond standardized test scores. However, reviewing these text-heavy materials is time-consuming and labor-intensive. To address this challenge and support the admission committee in providing feedback for students professional growth, our study introduces LORI: LOR Insights, a novel AI-based detection tool for assessing leadership skills in LORs submitted by online masters program applicants. By employing natural language processing and leveraging large language models using RoBERTa and LLAMA, we seek to identify leadership attributes such as teamwork, communication, and innovation. Our latest RoBERTa model achieves a weighted F1 score of 91.6%, a precision of 92.4%, and a recall of 91.6%, showing a strong level of consistency in our test data. With the growing importance of leadership skills in the STEM sector, integrating LORI into the graduate admissions process is crucial for accurately assessing applicants leadership capabilities. This approach not only streamlines the admissions process but also automates and ensures a more comprehensive evaluation of candidates capabilities.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.05511v1" target="_blank">Adaptive Parallel Downloader for Large Genomic Datasets</a></h3>
                    <p><strong>Authors:</strong> Rasman Mubtasim Swargo, Engin Arslan, Md Arifuzzaman</p>
                    <p><strong>Published:</strong> 8/7/2025</p>
                    <p><strong>Categories:</strong> cs.DC</p>
                    <p><strong>Summary:</strong> Modern next-generation sequencing (NGS) projects routinely generate terabytes of data, which researchers commonly download from public repositories such as SRA or ENA. Existing download tools often employ static concurrency settings, leading to inefficient bandwidth utilization and prolonged download times due to their inability to adapt to dynamic network conditions. We introduce FastBioDL, a parallel file downloader designed for large biological datasets, featuring an adaptive concurrency controller. FastBioDL frames the download process as an online optimization problem, utilizing a utility function and gradient descent to adjust the number of concurrent socket streams in real-time dynamically. This approach maximizes download throughput while minimizing resource overhead. Comprehensive evaluations on public genomic datasets demonstrate that FastBioDL achieves up to $4x$ speedup over state-of-the-art tools. Moreover, in high-speed network experiments, its adaptive design was up to $2.1x$ faster than existing tools. By intelligently optimizing standard HTTP or FTP downloads on the client side, FastBioDL provides a robust and efficient solution for large-scale genomic data acquisition, democratizing high-performance data retrieval for researchers without requiring specialized commercial software or protocols.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.05509v1" target="_blank">LAG: Logic-Augmented Generation from a Cartesian Perspective</a></h3>
                    <p><strong>Authors:</strong> Yilin Xiao, Chuang Zhou, Qinggang Zhang, Su Dong, Shengyuan Chen, Xiao Huang</p>
                    <p><strong>Published:</strong> 8/7/2025</p>
                    <p><strong>Categories:</strong> cs.CL, cs.AI</p>
                    <p><strong>Summary:</strong> Large language models (LLMs) have demonstrated remarkable capabilities across a wide range of tasks, yet exhibit critical limitations in knowledge-intensive tasks, often generating hallucinations when faced with questions requiring specialized expertise. While retrieval-augmented generation (RAG) mitigates this by integrating external knowledge, it struggles with complex reasoning scenarios due to its reliance on direct semantic retrieval and lack of structured logical organization. Inspired by Cartesian principles from \textit{Discours de la m\ethode}, this paper introduces Logic-Augmented Generation (LAG), a novel paradigm that reframes knowledge augmentation through systematic question decomposition and dependency-aware reasoning. Specifically, LAG first decomposes complex questions into atomic sub-questions ordered by logical dependencies. It then resolves these sequentially, using prior answers to guide context retrieval for subsequent sub-questions, ensuring stepwise grounding in logical chain. To prevent error propagation, LAG incorporates a logical termination mechanism that halts inference upon encountering unanswerable sub-questions and reduces wasted computation on excessive reasoning. Finally, it synthesizes all sub-resolutions to generate verified responses. Experiments on four benchmark datasets demonstrate that LAG significantly enhances reasoning robustness, reduces hallucination, and aligns LLM problem-solving with human cognition, offering a principled alternative to existing RAG systems.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.05508v1" target="_blank">Auto-Eval Judge: Towards a General Agentic Framework for Task Completion Evaluation</a></h3>
                    <p><strong>Authors:</strong> Roshita Bhonsle, Rishav Dutta, Sneha Vavilapalli, Harsh Seth, Abubakarr Jaye, Yapei Chang, Mukund Rungta, Emmanuel Aboah Boateng, Sadid Hasan, Ehi Nosakhare, Soundar Srinivasan</p>
                    <p><strong>Published:</strong> 8/7/2025</p>
                    <p><strong>Categories:</strong> cs.AI</p>
                    <p><strong>Summary:</strong> The increasing adoption of foundation models as agents across diverse domains necessitates a robust evaluation framework. Current methods, such as LLM-as-a-Judge, focus only on final outputs, overlooking the step-by-step reasoning that drives agentic decision-making. Meanwhile, existing Agent-as-a-Judge systems, where one agent evaluates anothers task completion, are typically designed for narrow, domain-specific settings. To address this gap, we propose a generalizable, modular framework for evaluating agent task completion independent of the task domain. The framework emulates human-like evaluation by decomposing tasks into sub-tasks and validating each step using available information, such as the agents output and reasoning. Each module contributes to a specific aspect of the evaluation process, and their outputs are aggregated to produce a final verdict on task completion. We validate our framework by evaluating the Magentic-One Actor Agent on two benchmarks, GAIA and BigCodeBench. Our Judge Agent predicts task success with closer agreement to human evaluations, achieving 4.76% and 10.52% higher alignment accuracy, respectively, compared to the GPT-4o based LLM-as-a-Judge baseline. This demonstrates the potential of our proposed general-purpose evaluation framework.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.05506v1" target="_blank">MagicHOI: Leveraging 3D Priors for Accurate Hand-object Reconstruction from Short Monocular Video Clips</a></h3>
                    <p><strong>Authors:</strong> Shibo Wang, Haonan He, Maria Parelli, Christoph Gebhardt, Zicong Fan, Jie Song</p>
                    <p><strong>Published:</strong> 8/7/2025</p>
                    <p><strong>Categories:</strong> cs.CV</p>
                    <p><strong>Summary:</strong> Most RGB-based hand-object reconstruction methods rely on object templates, while template-free methods typically assume full object visibility. This assumption often breaks in real-world settings, where fixed camera viewpoints and static grips leave parts of the object unobserved, resulting in implausible reconstructions. To overcome this, we present MagicHOI, a method for reconstructing hands and objects from short monocular interaction videos, even under limited viewpoint variation. Our key insight is that, despite the scarcity of paired 3D hand-object data, large-scale novel view synthesis diffusion models offer rich object supervision. This supervision serves as a prior to regularize unseen object regions during hand interactions. Leveraging this insight, we integrate a novel view synthesis model into our hand-object reconstruction framework. We further align hand to object by incorporating visible contact constraints. Our results demonstrate that MagicHOI significantly outperforms existing state-of-the-art hand-object reconstruction methods. We also show that novel view synthesis diffusion priors effectively regularize unseen object regions, enhancing 3D hand-object reconstruction.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.05497v1" target="_blank">Towards Human-Centric Evaluation of Interaction-Aware Automated Vehicle Controllers: A Framework and Case Study</a></h3>
                    <p><strong>Authors:</strong> Federico ScarÃ¬, Olger Siebinga, Arkady Zgonnikov</p>
                    <p><strong>Published:</strong> 8/7/2025</p>
                    <p><strong>Categories:</strong> cs.HC, cs.RO</p>
                    <p><strong>Summary:</strong> As automated vehicles (AVs) increasingly integrate into mixed-traffic environments, evaluating their interaction with human-driven vehicles (HDVs) becomes critical. In most research focused on developing new AV control algorithms (controllers), the performance of these algorithms is assessed solely based on performance metrics such as collision avoidance or lane-keeping efficiency, while largely overlooking the human-centred dimensions of interaction with HDVs. This paper proposes a structured evaluation framework that addresses this gap by incorporating metrics grounded in the human-robot interaction literature. The framework spans four key domains: a) interaction effect, b) interaction perception, c) interaction effort, and d) interaction ability. These domains capture both the performance of the AV and its impact on human drivers around it. To demonstrate the utility of the framework, we apply it to a case study evaluating how a state-of-the-art AV controller interacts with human drivers in a merging scenario in a driving simulator. Measuring HDV-HDV interactions as a baseline, this study included one representative metric per domain: a) perceived safety, b) subjective ratings, specifically how participants perceived the other vehicles driving behaviour (e.g., aggressiveness or predictability) , c) driver workload, and d) merging success. The results showed that incorporating metrics covering all four domains in the evaluation of AV controllers can illuminate critical differences in driver experience when interacting with AVs. This highlights the need for a more comprehensive evaluation approach. Our framework offers researchers, developers, and policymakers a systematic method for assessing AV behaviour beyond technical performance, fostering the development of AVs that are not only functionally capable but also understandable, acceptable, and safe from a human perspective.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.05496v1" target="_blank">InfiAlign: A Scalable and Sample-Efficient Framework for Aligning LLMs to Enhance Reasoning Capabilities</a></h3>
                    <p><strong>Authors:</strong> Shuo Cai, Su Lu, Qi Zhou, Kejing Yang, Zhijie Sang, Congkai Xie, Hongxia Yang</p>
                    <p><strong>Published:</strong> 8/7/2025</p>
                    <p><strong>Categories:</strong> cs.AI</p>
                    <p><strong>Summary:</strong> Large language models (LLMs) have exhibited impressive reasoning abilities on a wide range of complex tasks. However, enhancing these capabilities through post-training remains resource intensive, particularly in terms of data and computational cost. Although recent efforts have sought to improve sample efficiency through selective data curation, existing methods often rely on heuristic or task-specific strategies that hinder scalability. In this work, we introduce InfiAlign, a scalable and sample-efficient post-training framework that integrates supervised fine-tuning (SFT) with Direct Preference Optimization (DPO) to align LLMs for enhanced reasoning. At the core of InfiAlign is a robust data selection pipeline that automatically curates high-quality alignment data from open-source reasoning datasets using multidimensional quality metrics. This pipeline enables significant performance gains while drastically reducing data requirements and remains extensible to new data sources. When applied to the Qwen2.5-Math-7B-Base model, our SFT model achieves performance on par with DeepSeek-R1-Distill-Qwen-7B, while using only approximately 12% of the training data, and demonstrates strong generalization across diverse reasoning tasks. Additional improvements are obtained through the application of DPO, with particularly notable gains in mathematical reasoning tasks. The model achieves an average improvement of 3.89% on AIME 24/25 benchmarks. Our results highlight the effectiveness of combining principled data selection with full-stage post-training, offering a practical solution for aligning large reasoning models in a scalable and data-efficient manner. The model checkpoints are available at https://huggingface.co/InfiX-ai/InfiAlign-Qwen-7B-SFT.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.05495v1" target="_blank">A 20-Year Retrospective on Power and Thermal Modeling and Management</a></h3>
                    <p><strong>Authors:</strong> David Atienza, Kai Zhu, Darong Huang, Luis Costero</p>
                    <p><strong>Published:</strong> 8/7/2025</p>
                    <p><strong>Categories:</strong> eess.SY, cs.SY</p>
                    <p><strong>Summary:</strong> As processor performance advances, increasing power densities and complex thermal behaviors threaten both energy efficiency and system reliability. This survey covers more than two decades of research on power and thermal modeling and management in modern processors. We start by comparing analytical, regression-based, and neural network-based techniques for power estimation, then review thermal modeling methods, including finite element, finite difference, and data-driven approaches. Next, we categorize dynamic runtime management strategies that balance performance, power consumption, and reliability. Finally, we conclude with a discussion of emerging challenges and promising research directions.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.05493v1" target="_blank">Exact and Heuristic Algorithms for Constrained Biclustering</a></h3>
                    <p><strong>Authors:</strong> Antonio M. Sudoso</p>
                    <p><strong>Published:</strong> 8/7/2025</p>
                    <p><strong>Categories:</strong> math.OC, cs.LG</p>
                    <p><strong>Summary:</strong> Biclustering, also known as co-clustering or two-way clustering, simultaneously partitions the rows and columns of a data matrix to reveal submatrices with coherent patterns. Incorporating background knowledge into clustering to enhance solution quality and interpretability has attracted growing interest in mathematical optimization and machine learning research. Extending this paradigm to biclustering enables prior information to guide the joint grouping of rows and columns. We study constrained biclustering with pairwise constraints, namely must-link and cannot-link constraints, which specify whether objects should belong to the same or different biclusters. As a model problem, we address the constrained version of the k-densest disjoint biclique problem, which aims to identify k disjoint complete bipartite subgraphs (called bicliques) in a weighted complete bipartite graph, maximizing the total density while satisfying pairwise constraints. We propose both exact and heuristic algorithms. The exact approach is a tailored branch-and-cut algorithm based on a low-dimensional semidefinite programming (SDP) relaxation, strengthened with valid inequalities and solved in a cutting-plane fashion. Exploiting integer programming tools, a rounding scheme converts SDP solutions into feasible biclusterings at each node. For large-scale instances, we introduce an efficient heuristic based on the low-rank factorization of the SDP. The resulting nonlinear optimization problem is tackled with an augmented Lagrangian method, where the subproblem is solved by decomposition through a block-coordinate projected gradient algorithm. Extensive experiments on synthetic and real-world datasets show that the exact method significantly outperforms general-purpose solvers, while the heuristic achieves high-quality solutions efficiently on large instances.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.05491v1" target="_blank">Deconstructing the Crystal Ball: From Ad-Hoc Prediction to Principled Startup Evaluation with the SAISE Framework</a></h3>
                    <p><strong>Authors:</strong> Seyed Mohammad Ali Jafari, Ali Mobini Dehkordi, Ehsan Chitsaz, Yadollah Yaghoobzadeh</p>
                    <p><strong>Published:</strong> 8/7/2025</p>
                    <p><strong>Categories:</strong> cs.CE, econ.GN, q-fin.EC</p>
                    <p><strong>Summary:</strong> The integration of Artificial Intelligence (AI) into startup evaluation represents a significant technological shift, yet the academic research underpinning this transition remains methodologically fragmented. Existing studies often employ ad-hoc approaches, leading to a body of work with inconsistent definitions of success, atheoretical features, and a lack of rigorous validation. This fragmentation severely limits the comparability, reliability, and practical utility of current predictive models. To address this critical gap, this paper presents a comprehensive systematic literature review of 57 empirical studies. We deconstruct the current state-of-the-art by systematically mapping the features, algorithms, data sources, and evaluation practices that define the AI-driven startup prediction landscape. Our synthesis reveals a field defined by a central paradox: a strong convergence on a common toolkit -- venture databases and tree-based ensembles -- but a stark divergence in methodological rigor. We identify four foundational weaknesses: a fragmented definition of success, a divide between theory-informed and data-driven feature engineering, a chasm between common and best-practice model validation, and a nascent approach to data ethics and explainability. In response to these findings, our primary contribution is the proposal of the Systematic AI-driven Startup Evaluation (SAISE) Framework. This novel, five-stage prescriptive roadmap is designed to guide researchers from ad-hoc prediction toward principled evaluation. By mandating a coherent, end-to-end methodology that emphasizes stage-aware problem definition, theory-informed data synthesis, principled feature engineering, rigorous validation, and risk-aware interpretation, the SAISE framework provides a new standard for conducting more comparable, robust, and practically relevant research in this rapidly maturing domain</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.05489v1" target="_blank">Keep It Real: Challenges in Attacking Compression-Based Adversarial Purification</a></h3>
                    <p><strong>Authors:</strong> Samuel RÃ¤ber, Till Aczel, Andreas Plesner, Roger Wattenhofer</p>
                    <p><strong>Published:</strong> 8/7/2025</p>
                    <p><strong>Categories:</strong> cs.CV, cs.LG, eess.IV</p>
                    <p><strong>Summary:</strong> Previous work has suggested that preprocessing images through lossy compression can defend against adversarial perturbations, but comprehensive attack evaluations have been lacking. In this paper, we construct strong white-box and adaptive attacks against various compression models and identify a critical challenge for attackers: high realism in reconstructed images significantly increases attack difficulty. Through rigorous evaluation across multiple attack scenarios, we demonstrate that compression models capable of producing realistic, high-fidelity reconstructions are substantially more resistant to our attacks. In contrast, low-realism compression models can be broken. Our analysis reveals that this is not due to gradient masking. Rather, realistic reconstructions maintaining distributional alignment with natural images seem to offer inherent robustness. This work highlights a significant obstacle for future adversarial attacks and suggests that developing more effective techniques to overcome realism represents an essential challenge for comprehensive security evaluation.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.05478v1" target="_blank">Modulation of the Monokinetic Limit for Models of Collective Dynamics</a></h3>
                    <p><strong>Authors:</strong> Alina Chertock, Roman Shvydkoy, Trevor Teolis</p>
                    <p><strong>Published:</strong> 8/7/2025</p>
                    <p><strong>Categories:</strong> math.AP, cs.NA, math.NA, 37A60, 92D50</p>
                    <p><strong>Summary:</strong> In this work, we perform modulation analysis of monokinetic limits from the kinetic Cucker- Smale model to the pressureless Euler alignment system. Two regimes are considered -- a strong Fokker- Planck force with vanishing noise and Knudsen number, and a pure noiseless Vlasov scheme. In the former case, we demonstrate convergence of the modulated profile to the standard Gaussian distribution, while in the latter case, the distribution converges to a profile satisfying an explicit transport equation along limiting characteristics.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.05476v1" target="_blank">MM2CT: MR-to-CT translation for multi-modal image fusion with mamba</a></h3>
                    <p><strong>Authors:</strong> Chaohui Gong, Zhiying Wu, Zisheng Huang, Gaofeng Meng, Zhen Lei, Hongbin Liu</p>
                    <p><strong>Published:</strong> 8/7/2025</p>
                    <p><strong>Categories:</strong> eess.IV</p>
                    <p><strong>Summary:</strong> Magnetic resonance (MR)-to-computed tomography (CT) translation offers significant advantages, including the elimination of radiation exposure associated with CT scans and the mitigation of imaging artifacts caused by patient motion. The existing approaches are based on single-modality MR-to-CT translation, with limited research exploring multimodal fusion. To address this limitation, we introduce Multi-modal MR to CT (MM2CT) translation method by leveraging multimodal T1- and T2-weighted MRI data, an innovative Mamba-based framework for multi-modal medical image synthesis. Mamba effectively overcomes the limited local receptive field in CNNs and the high computational complexity issues in Transformers. MM2CT leverages this advantage to maintain long-range dependencies modeling capabilities while achieving multi-modal MR feature integration. Additionally, we incorporate a dynamic local convolution module and a dynamic enhancement module to improve MRI-to-CT synthesis. The experiments on a public pelvis dataset demonstrate that MM2CT achieves state-of-the-art performance in terms of Structural Similarity Index Measure (SSIM) and Peak Signal-to-Noise Ratio (PSNR). Our code is publicly available at https://github.com/Gots-ch/MM2CT.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.05473v1" target="_blank">Embedding Alignment in Code Generation for Audio</a></h3>
                    <p><strong>Authors:</strong> Sam Kouteili, Hiren Madhu, George Typaldos, Mark Santolucito</p>
                    <p><strong>Published:</strong> 8/7/2025</p>
                    <p><strong>Categories:</strong> cs.MM, cs.AI, cs.SD, eess.AS</p>
                    <p><strong>Summary:</strong> LLM-powered code generation has the potential to revolutionize creative coding endeavors, such as live-coding, by enabling users to focus on structural motifs over syntactic details. In such domains, when prompting an LLM, users may benefit from considering multiple varied code candidates to better realize their musical intentions. Code generation models, however, struggle to present unique and diverse code candidates, with no direct insight into the codes audio output. To better establish a relationship between code candidates and produced audio, we investigate the topology of the mapping between code and audio embedding spaces. We find that code and audio embeddings do not exhibit a simple linear relationship, but supplement this with a constructed predictive model that shows an embedding alignment map could be learned. Supplementing the aim for musically diverse output, we present a model that given code predicts output audio embedding, constructing a code-audio embedding alignment map.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.05471v1" target="_blank">An Improved Approximation Algorithm for the Capacitated Arc Routing Problem</a></h3>
                    <p><strong>Authors:</strong> Jingyang Zhao, Mingyu Xiao</p>
                    <p><strong>Published:</strong> 8/7/2025</p>
                    <p><strong>Categories:</strong> cs.DS</p>
                    <p><strong>Summary:</strong> The Capacitated Arc Routing Problem (CARP), introduced by Golden and Wong in 1981, is an important arc routing problem in Operations Research, which generalizes the famous Capacitated Vehicle Routing Problem (CVRP). When every customer has a unit demand, the best known approximation ratio for CARP, given by Jansen in 1993, remains $\frac{5}{2}-\frac{1.5}{k}$, where $k$ denotes the vehicle capacity. Based on recent progress in approximating CVRP, we improve this result by proposing a $(\frac{5}{2}-\Theta(\frac{1}{\sqrt{k}}))$-approximation algorithm, which to the best of our knowledge constitutes the first improvement over Jansens bound.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.05470v1" target="_blank">Rethinking Creativity Evaluation: A Critical Analysis of Existing Creativity Evaluations</a></h3>
                    <p><strong>Authors:</strong> Li-Chun Lu, Miri Liu, Pin-Chun Lu, Yufei Tian, Shao-Hua Sun, Nanyun Peng</p>
                    <p><strong>Published:</strong> 8/7/2025</p>
                    <p><strong>Categories:</strong> cs.CL</p>
                    <p><strong>Summary:</strong> We systematically examine, analyze, and compare representative creativity measures--creativity index, perplexity, syntactic templates, and LLM-as-a-Judge--across diverse creative domains, including creative writing, unconventional problem-solving, and research ideation. Our analyses reveal that these metrics exhibit limited consistency, capturing different dimensions of creativity. We highlight key limitations, including the creativity indexs focus on lexical diversity, perplexitys sensitivity to model confidence, and syntactic templates inability to capture conceptual creativity. Additionally, LLM-as-a-Judge shows instability and bias. Our findings underscore the need for more robust, generalizable evaluation frameworks that better align with human judgments of creativity.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.05469v1" target="_blank">Lets Measure Information Step-by-Step: LLM-Based Evaluation Beyond Vibes</a></h3>
                    <p><strong>Authors:</strong> Zachary Robertson, Sanmi Koyejo</p>
                    <p><strong>Published:</strong> 8/7/2025</p>
                    <p><strong>Categories:</strong> cs.LG, cs.IT, math.IT</p>
                    <p><strong>Summary:</strong> We develop mechanisms for evaluating AI systems without ground truth by exploiting a connection between gaming resistance and output quality. The data processing inequality ensures post-hoc attempts to game a metric degrades both information content and task performance. We prove that f-mutual information measures are the unique gaming resistant mechanisms under natural conditions, with the overseer acting as an agent. While Shannon mutual information faces exponential sample complexity, bounded measures like total variation distance remain tractable. Empirically, across ten domains from translation to peer review, all information-theoretic mechanisms achieve perfect discrimination (d  0.5) between faithful and strategic agents. In contrast, LLM judges exhibit systematic evaluation inversion, preferring fabricated content over accurate summaries. Our mechanisms show 10-100x better robustness to adversarial manipulation than current practices. We also find performance follows an inverted-U curve with compression ratio, peaking at 10:1 where agent responses exhibit optimal information diversity (3 effective dimensions), giving a bias-variance perspective on when our approach is expected to be most effective.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.05468v1" target="_blank">TASE: Token Awareness and Structured Evaluation for Multilingual Language Models</a></h3>
                    <p><strong>Authors:</strong> Chenzhuo Zhao, Xinda Wang, Yue Huang, Junting Lu, Ziqian Liu</p>
                    <p><strong>Published:</strong> 8/7/2025</p>
                    <p><strong>Categories:</strong> cs.CL</p>
                    <p><strong>Summary:</strong> While large language models (LLMs) have demonstrated remarkable performance on high-level semantic tasks, they often struggle with fine-grained, token-level understanding and structural reasoning--capabilities that are essential for applications requiring precision and control. We introduce TASE, a comprehensive benchmark designed to evaluate LLMs ability to perceive and reason about token-level information across languages. TASE covers 10 tasks under two core categories: token awareness and structural understanding, spanning Chinese, English, and Korean, with a 35,927-instance evaluation set and a scalable synthetic data generation pipeline for training. Tasks include character counting, token alignment, syntactic structure parsing, and length constraint satisfaction. We evaluate over 30 leading commercial and open-source LLMs, including O3, Claude 4, Gemini 2.5 Pro, and DeepSeek-R1, and train a custom Qwen2.5-14B model using the GRPO training method. Results show that human performance significantly outpaces current LLMs, revealing persistent weaknesses in token-level reasoning. TASE sheds light on these limitations and provides a new diagnostic lens for future improvements in low-level language understanding and cross-lingual generalization. Our code and dataset are publicly available at https://github.com/cyzcz/Tase .</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.05464v1" target="_blank">Bench-2-CoP: Can We Trust Benchmarking for EU AI Compliance?</a></h3>
                    <p><strong>Authors:</strong> Matteo Prandi, Vincenzo Suriani, Federico Pierucci, Marcello Galisai, Daniele Nardi, Piercosma Bisconti</p>
                    <p><strong>Published:</strong> 8/7/2025</p>
                    <p><strong>Categories:</strong> cs.AI, cs.CL</p>
                    <p><strong>Summary:</strong> The rapid advancement of General Purpose AI (GPAI) models necessitates robust evaluation frameworks, especially with emerging regulations like the EU AI Act and its associated Code of Practice (CoP). Current AI evaluation practices depend heavily on established benchmarks, but these tools were not designed to measure the systemic risks that are the focus of the new regulatory landscape. This research addresses the urgent need to quantify this benchmark-regulation gap. We introduce Bench-2-CoP, a novel, systematic framework that uses validated LLM-as-judge analysis to map the coverage of 194,955 questions from widely-used benchmarks against the EU AI Acts taxonomy of model capabilities and propensities. Our findings reveal a profound misalignment: the evaluation ecosystem is overwhelmingly focused on a narrow set of behavioral propensities, such as Tendency to hallucinate (53.7% of the corpus) and Discriminatory bias (28.9%), while critical functional capabilities are dangerously neglected. Crucially, capabilities central to loss-of-control scenarios, including evading human oversight, self-replication, and autonomous AI development, receive zero coverage in the entire benchmark corpus. This translates to a near-total evaluation gap for systemic risks like Loss of Control (0.4% coverage) and Cyber Offence (0.8% coverage). This study provides the first comprehensive, quantitative analysis of this gap, offering critical insights for policymakers to refine the CoP and for developers to build the next generation of evaluation tools, ultimately fostering safer and more compliant AI.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.05636v1" target="_blank">FaceAnonyMixer: Cancelable Faces via Identity Consistent Latent Space Mixing</a></h3>
                    <p><strong>Authors:</strong> Mohammed Talha Alam, Fahad Shamshad, Fakhri Karray, Karthik Nandakumar</p>
                    <p><strong>Published:</strong> 8/7/2025</p>
                    <p><strong>Categories:</strong> cs.CV</p>
                    <p><strong>Summary:</strong> Advancements in face recognition (FR) technologies have amplified privacy concerns, necessitating methods that protect identity while maintaining recognition utility. Existing face anonymization methods typically focus on obscuring identity but fail to meet the requirements of biometric template protection, including revocability, unlinkability, and irreversibility. We propose FaceAnonyMixer, a cancelable face generation framework that leverages the latent space of a pre-trained generative model to synthesize privacy-preserving face images. The core idea of FaceAnonyMixer is to irreversibly mix the latent code of a real face image with a synthetic code derived from a revocable key. The mixed latent code is further refined through a carefully designed multi-objective loss to satisfy all cancelable biometric requirements. FaceAnonyMixer is capable of generating high-quality cancelable faces that can be directly matched using existing FR systems without requiring any modifications. Extensive experiments on benchmark datasets demonstrate that FaceAnonyMixer delivers superior recognition accuracy while providing significantly stronger privacy protection, achieving over an 11% gain on commercial API compared to recent cancelable biometric methods. Code is available at: https://github.com/talha-alam/faceanonymixer.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.05635v1" target="_blank">Genie Envisioner: A Unified World Foundation Platform for Robotic Manipulation</a></h3>
                    <p><strong>Authors:</strong> Yue Liao, Pengfei Zhou, Siyuan Huang, Donglin Yang, Shengcong Chen, Yuxin Jiang, Yue Hu, Jingbin Cai, Si Liu, Jianlan Luo, Liliang Chen, Shuicheng Yan, Maoqing Yao, Guanghui Ren</p>
                    <p><strong>Published:</strong> 8/7/2025</p>
                    <p><strong>Categories:</strong> cs.RO, cs.CV</p>
                    <p><strong>Summary:</strong> We introduce Genie Envisioner (GE), a unified world foundation platform for robotic manipulation that integrates policy learning, evaluation, and simulation within a single video-generative framework. At its core, GE-Base is a large-scale, instruction-conditioned video diffusion model that captures the spatial, temporal, and semantic dynamics of real-world robotic interactions in a structured latent space. Built upon this foundation, GE-Act maps latent representations to executable action trajectories through a lightweight, flow-matching decoder, enabling precise and generalizable policy inference across diverse embodiments with minimal supervision. To support scalable evaluation and training, GE-Sim serves as an action-conditioned neural simulator, producing high-fidelity rollouts for closed-loop policy development. The platform is further equipped with EWMBench, a standardized benchmark suite measuring visual fidelity, physical consistency, and instruction-action alignment. Together, these components establish Genie Envisioner as a scalable and practical foundation for instruction-driven, general-purpose embodied intelligence. All code, models, and benchmarks will be released publicly.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.05634v1" target="_blank">Towards Generalizable Safety in Crowd Navigation via Conformal Uncertainty Handling</a></h3>
                    <p><strong>Authors:</strong> Jianpeng Yao, Xiaopan Zhang, Yu Xia, Zejin Wang, Amit K. Roy-Chowdhury, Jiachen Li</p>
                    <p><strong>Published:</strong> 8/7/2025</p>
                    <p><strong>Categories:</strong> cs.RO, cs.AI, cs.CV, cs.LG, cs.SY, eess.SY</p>
                    <p><strong>Summary:</strong> Mobile robots navigating in crowds trained using reinforcement learning are known to suffer performance degradation when faced with out-of-distribution scenarios. We propose that by properly accounting for the uncertainties of pedestrians, a robot can learn safe navigation policies that are robust to distribution shifts. Our method augments agent observations with prediction uncertainty estimates generated by adaptive conformal inference, and it uses these estimates to guide the agents behavior through constrained reinforcement learning. The system helps regulate the agents actions and enables it to adapt to distribution shifts. In the in-distribution setting, our approach achieves a 96.93% success rate, which is over 8.80% higher than the previous state-of-the-art baselines with over 3.72 times fewer collisions and 2.43 times fewer intrusions into ground-truth human future trajectories. In three out-of-distribution scenarios, our method shows much stronger robustness when facing distribution shifts in velocity variations, policy changes, and transitions from individual to group dynamics. We deploy our method on a real robot, and experiments show that the robot makes safe and robust decisions when interacting with both sparse and dense crowds. Our code and videos are available on https://gen-safe-nav.github.io/.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.05632v1" target="_blank">Partial projected ensembles and spatiotemporal structure of information scrambling</a></h3>
                    <p><strong>Authors:</strong> Saptarshi Mandal, Pieter W. Claeys, Sthitadhi Roy</p>
                    <p><strong>Published:</strong> 8/7/2025</p>
                    <p><strong>Categories:</strong> quant-ph, cond-mat.dis-nn, cond-mat.stat-mech</p>
                    <p><strong>Summary:</strong> Thermalisation and information scrambling in out-of-equilibrium quantum many-body systems are deeply intertwined: local subsystems dynamically approach thermal density matrices while their entropies track information spreading. Projected ensembles--ensembles of pure states conditioned on measurement outcomes of complementary subsystems--provide higher-order probes of thermalisation, converging at late times to universal maximum-entropy ensembles. In this work, we introduce the partial projected ensemble (PPE) as a framework to study how the spatiotemporal structure of scrambling is imprinted on projected ensembles. The PPE consists of an ensemble of mixed states induced on a subsystem by measurements on a spatially separated part of its complement, tracing out the remainder, naturally capturing scenarios involving discarded outcomes or noise-induced losses. We show that statistical fluctuations of the PPE faithfully track the causal lightcone of information spreading, revealing how scrambling dynamics are encoded in ensemble structure. In addition, we demonstrate that the probabilities of bit-string probabilities (PoPs) associated with the PPE exhibit distinct dynamical regimes and provide an experimentally accessible probe of scrambling. Both PPE fluctuations and PoPs display exponential sensitivity to the size of the discarded region, reflecting exponential degradation of quantum correlations under erasure. We substantiate these findings using the non-integrable kicked Ising chain, combining numerics in the ergodic regime with exact results at its self-dual point. We extend our analysis to a many-body localised (MBL) regime numerically, along with analytic results for the $\ell$-bit model. The linear and logarithmic lightcones characteristic of ergodic and MBL regimes emerge naturally from PPE dynamics, establishing it as a powerful tool for probing scrambling and deep thermalisation.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.05630v1" target="_blank">MOSEv2: A More Challenging Dataset for Video Object Segmentation in Complex Scenes</a></h3>
                    <p><strong>Authors:</strong> Henghui Ding, Kaining Ying, Chang Liu, Shuting He, Xudong Jiang, Yu-Gang Jiang, Philip H. S. Torr, Song Bai</p>
                    <p><strong>Published:</strong> 8/7/2025</p>
                    <p><strong>Categories:</strong> cs.CV</p>
                    <p><strong>Summary:</strong> Video object segmentation (VOS) aims to segment specified target objects throughout a video. Although state-of-the-art methods have achieved impressive performance (e.g., 90+% JF) on existing benchmarks such as DAVIS and YouTube-VOS, these datasets primarily contain salient, dominant, and isolated objects, limiting their generalization to real-world scenarios. To advance VOS toward more realistic environments, coMplex video Object SEgmentation (MOSEv1) was introduced to facilitate VOS research in complex scenes. Building on the strengths and limitations of MOSEv1, we present MOSEv2, a significantly more challenging dataset designed to further advance VOS methods under real-world conditions. MOSEv2 consists of 5,024 videos and over 701,976 high-quality masks for 10,074 objects across 200 categories. Compared to its predecessor, MOSEv2 introduces significantly greater scene complexity, including more frequent object disappearance and reappearance, severe occlusions and crowding, smaller objects, as well as a range of new challenges such as adverse weather (e.g., rain, snow, fog), low-light scenes (e.g., nighttime, underwater), multi-shot sequences, camouflaged objects, non-physical targets (e.g., shadows, reflections), scenarios requiring external knowledge, etc. We benchmark 20 representative VOS methods under 5 different settings and observe consistent performance drops. For example, SAM2 drops from 76.4% on MOSEv1 to only 50.9% on MOSEv2. We further evaluate 9 video object tracking methods and find similar declines, demonstrating that MOSEv2 presents challenges across tasks. These results highlight that despite high accuracy on existing datasets, current VOS methods still struggle under real-world complexities. MOSEv2 is publicly available at https://MOSE.video.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.05631v1" target="_blank">GAP: Gaussianize Any Point Clouds with Text Guidance</a></h3>
                    <p><strong>Authors:</strong> Weiqi Zhang, Junsheng Zhou, Haotian Geng, Wenyuan Zhang, Yu-Shen Liu</p>
                    <p><strong>Published:</strong> 8/7/2025</p>
                    <p><strong>Categories:</strong> cs.CV</p>
                    <p><strong>Summary:</strong> 3D Gaussian Splatting (3DGS) has demonstrated its advantages in achieving fast and high-quality rendering. As point clouds serve as a widely-used and easily accessible form of 3D representation, bridging the gap between point clouds and Gaussians becomes increasingly important. Recent studies have explored how to convert the colored points into Gaussians, but directly generating Gaussians from colorless 3D point clouds remains an unsolved challenge. In this paper, we propose GAP, a novel approach that gaussianizes raw point clouds into high-fidelity 3D Gaussians with text guidance. Our key idea is to design a multi-view optimization framework that leverages a depth-aware image diffusion model to synthesize consistent appearances across different viewpoints. To ensure geometric accuracy, we introduce a surface-anchoring mechanism that effectively constrains Gaussians to lie on the surfaces of 3D shapes during optimization. Furthermore, GAP incorporates a diffuse-based inpainting strategy that specifically targets at completing hard-to-observe regions. We evaluate GAP on the Point-to-Gaussian generation task across varying complexity levels, from synthetic point clouds to challenging real-world scans, and even large-scale scenes. Project Page: https://weiqi-zhang.github.io/GAP.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.05628v1" target="_blank">H-Net++: Hierarchical Dynamic Chunking for Tokenizer-Free Language Modelling in Morphologically-Rich Languages</a></h3>
                    <p><strong>Authors:</strong> Mehrdad Zakershahrak, Samira Ghodratnama</p>
                    <p><strong>Published:</strong> 8/7/2025</p>
                    <p><strong>Categories:</strong> cs.CL, cs.AI</p>
                    <p><strong>Summary:</strong> Byte-level language models eliminate fragile tokenizers but face computational challenges in morphologically-rich languages (MRLs), where words span many bytes. We propose H-NET++, a hierarchical dynamic-chunking model that learns linguistically-informed segmentation through end-to-end training. Key innovations include: (1) a lightweight Transformer context-mixer (1.9M parameters) for cross-chunk attention, (2) a two-level latent hyper-prior for document-level consistency, (3) specialized handling of orthographic artifacts (e.g. Persian ZWNJ), and (4) curriculum-based training with staged sequence lengths. On a 1.4B-token Persian corpus, H-NET++ achieves state-of-the-art results: 0.159 BPB reduction versus BPE-based GPT-2-fa (12% better compression), 5.4pp gain on ParsGLUE, 53% improved robustness to ZWNJ corruption, and 73.8% F1 on gold morphological boundaries. Our learned chunks align with Persian morphology without explicit supervision, demonstrating that hierarchical dynamic chunking provides an effective tokenizer-free solution for MRLs while maintaining computational efficiency.</p>
                
            
                
                    <h3><a href="http://dx.doi.org/10.1145/3721238.3730666" target="_blank">Physically Controllable Relighting of Photographs</a></h3>
                    <p><strong>Authors:</strong> Chris Careaga, YaÄŸÄ±z Aksoy</p>
                    <p><strong>Published:</strong> 8/7/2025</p>
                    <p><strong>Categories:</strong> cs.GR, cs.CV, I.4</p>
                    <p><strong>Summary:</strong> We present a self-supervised approach to in-the-wild image relighting that enables fully controllable, physically based illumination editing. We achieve this by combining the physical accuracy of traditional rendering with the photorealistic appearance made possible by neural rendering. Our pipeline works by inferring a colored mesh representation of a given scene using monocular estimates of geometry and intrinsic components. This representation allows users to define their desired illumination configuration in 3D. The scene under the new lighting can then be rendered using a path-tracing engine. We send this approximate rendering of the scene through a feed-forward neural renderer to predict the final photorealistic relighting result. We develop a differentiable rendering process to reconstruct in-the-wild scene illumination, enabling self-supervised training of our neural renderer on raw image collections. Our method represents a significant step in bringing the explicit physical control over lights available in typical 3D computer graphics tools, such as Blender, to in-the-wild relighting.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.05625v1" target="_blank">How Do LLMs Persuade? Linear Probes Can Uncover Persuasion Dynamics in Multi-Turn Conversations</a></h3>
                    <p><strong>Authors:</strong> Brandon Jaipersaud, David Krueger, Ekdeep Singh Lubana</p>
                    <p><strong>Published:</strong> 8/7/2025</p>
                    <p><strong>Categories:</strong> cs.CL, cs.AI, cs.LG</p>
                    <p><strong>Summary:</strong> Large Language Models (LLMs) have started to demonstrate the ability to persuade humans, yet our understanding of how this dynamic transpires is limited. Recent work has used linear probes, lightweight tools for analyzing model representations, to study various LLM skills such as the ability to model user sentiment and political perspective. Motivated by this, we apply probes to study persuasion dynamics in natural, multi-turn conversations. We leverage insights from cognitive science to train probes on distinct aspects of persuasion: persuasion success, persuadee personality, and persuasion strategy. Despite their simplicity, we show that they capture various aspects of persuasion at both the sample and dataset levels. For instance, probes can identify the point in a conversation where the persuadee was persuaded or where persuasive success generally occurs across the entire dataset. We also show that in addition to being faster than expensive prompting-based approaches, probes can do just as well and even outperform prompting in some settings, such as when uncovering persuasion strategy. This suggests probes as a plausible avenue for studying other complex behaviours such as deception and manipulation, especially in multi-turn settings and large-scale dataset analysis where prompting-based methods would be computationally inefficient.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.05624v1" target="_blank">Latent Space Diffusion for Topology Optimization</a></h3>
                    <p><strong>Authors:</strong> Aaron Lutheran, Srijan Das, Alireza Tabarraei</p>
                    <p><strong>Published:</strong> 8/7/2025</p>
                    <p><strong>Categories:</strong> cs.CE</p>
                    <p><strong>Summary:</strong> Topology optimization enables the automated design of efficient structures by optimally distributing material within a defined domain. However, traditional gradient-based methods often scale poorly with increasing resolution and dimensionality due to the need for repeated finite element analyses and sensitivity evaluations. In this work, we propose a novel framework that combines latent diffusion models (LDMs) with variational autoencoders (VAEs) to enable fast, conditional generation of optimized topologies. Unlike prior approaches, our method conditions the generative process on physically meaningful fields, specifically von Mises stress, strain energy density, volume fraction, and loading information, embedded as dense input channels. To further guide the generation process, we introduce auxiliary loss functions that penalize floating material, load imbalance, and volume fraction deviation, thereby encouraging physically realistic and manufacturable designs. Numerical experiments on a large synthetic dataset demonstrate that our VAE-LDM framework outperforms existing diffusion-based methods in compliance accuracy, volume control, and structural connectivity, providing a robust and scalable alternative to conventional</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.05621v1" target="_blank">Back to Bits: Extending Shannons communication performance framework to computing</a></h3>
                    <p><strong>Authors:</strong> Max Hawkins, Richard Vuduc</p>
                    <p><strong>Published:</strong> 8/7/2025</p>
                    <p><strong>Categories:</strong> cs.PF, D.4.8; K.6.2</p>
                    <p><strong>Summary:</strong> This work proposes a novel computing performance unit grounded in information theory. Modern computing systems are increasingly diverse, supporting low-precision formats, hardware specialization, and emerging paradigms such as analog, quantum, and reversible logic. Traditional metrics like floating-point operations (flops) no longer accurately capture this complexity. We frame computing as the transformation of information through a channel and define performance in terms of the mutual information between a systems inputs and outputs. This approach measures not just the quantity of data processed, but the amount of meaningful information encoded, manipulated, and retained through computation. Our framework provides a principled, implementation-agnostic foundation for evaluating performance.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.05619v1" target="_blank">The Missing Reward: Active Inference in the Era of Experience</a></h3>
                    <p><strong>Authors:</strong> Bo Wen</p>
                    <p><strong>Published:</strong> 8/7/2025</p>
                    <p><strong>Categories:</strong> cs.AI, nlin.AO, physics.bio-ph, physics.comp-ph, physics.hist-ph</p>
                    <p><strong>Summary:</strong> This paper argues that Active Inference (AIF) provides a crucial foundation for developing autonomous AI agents capable of learning from experience without continuous human reward engineering. As AI systems begin to exhaust high-quality training data and rely on increasingly large human workforces for reward design, the current paradigm faces significant scalability challenges that could impede progress toward genuinely autonomous intelligence. The proposal for an ``Era of Experience, where agents learn from self-generated data, is a promising step forward. However, this vision still depends on extensive human engineering of reward functions, effectively shifting the bottleneck from data curation to reward curation. This highlights what we identify as the \textbf{grounded-agency gap}: the inability of contemporary AI systems to autonomously formulate, adapt, and pursue objectives in response to changing circumstances. We propose that AIF can bridge this gap by replacing external reward signals with an intrinsic drive to minimize free energy, allowing agents to naturally balance exploration and exploitation through a unified Bayesian objective. By integrating Large Language Models as generative world models with AIFs principled decision-making framework, we can create agents that learn efficiently from experience while remaining aligned with human values. This synthesis offers a compelling path toward AI systems that can develop autonomously while adhering to both computational and physical constraints.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.05618v1" target="_blank">Learning to Reason for Factuality</a></h3>
                    <p><strong>Authors:</strong> Xilun Chen, Ilia Kulikov, Vincent-Pierre Berges, Barlas OÄŸuz, Rulin Shao, Gargi Ghosh, Jason Weston, Wen-tau Yih</p>
                    <p><strong>Published:</strong> 8/7/2025</p>
                    <p><strong>Categories:</strong> cs.CL</p>
                    <p><strong>Summary:</strong> Reasoning Large Language Models (R-LLMs) have significantly advanced complex reasoning tasks but often struggle with factuality, generating substantially more hallucinations than their non-reasoning counterparts on long-form factuality benchmarks. However, extending online Reinforcement Learning (RL), a key component in recent R-LLM advancements, to the long-form factuality setting poses several unique challenges due to the lack of reliable verification methods. Previous work has utilized automatic factuality evaluation frameworks such as FActScore to curate preference data in the offline RL setting, yet we find that directly leveraging such methods as the reward in online RL leads to reward hacking in multiple ways, such as producing less detailed or relevant responses. We propose a novel reward function that simultaneously considers the factual precision, response detail level, and answer relevance, and applies online RL to learn high quality factual reasoning. Evaluated on six long-form factuality benchmarks, our factual reasoning model achieves an average reduction of 23.1 percentage points in hallucination rate, a 23% increase in answer detail level, and no degradation in the overall response helpfulness.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.05616v1" target="_blank">TrajEvo: Trajectory Prediction Heuristics Design via LLM-driven Evolution</a></h3>
                    <p><strong>Authors:</strong> Zhikai Zhao, Chuanbo Hua, Federico Berto, Kanghoon Lee, Zihan Ma, Jiachen Li, Jinkyoo Park</p>
                    <p><strong>Published:</strong> 8/7/2025</p>
                    <p><strong>Categories:</strong> cs.LG, cs.AI, cs.NE, cs.RO</p>
                    <p><strong>Summary:</strong> Trajectory prediction is a critical task in modeling human behavior, especially in safety-critical domains such as social robotics and autonomous vehicle navigation. Traditional heuristics based on handcrafted rules often lack accuracy and generalizability. Although deep learning approaches offer improved performance, they typically suffer from high computational cost, limited explainability, and, importantly, poor generalization to out-of-distribution (OOD) scenarios. In this paper, we introduce TrajEvo, a framework that leverages Large Language Models (LLMs) to automatically design trajectory prediction heuristics. TrajEvo employs an evolutionary algorithm to generate and refine prediction heuristics from past trajectory data. We propose two key innovations: Cross-Generation Elite Sampling to encourage population diversity, and a Statistics Feedback Loop that enables the LLM to analyze and improve alternative predictions. Our evaluations demonstrate that TrajEvo outperforms existing heuristic methods across multiple real-world datasets, and notably surpasses both heuristic and deep learning methods in generalizing to an unseen OOD real-world dataset. TrajEvo marks a promising step toward the automated design of fast, explainable, and generalizable trajectory prediction heuristics. We release our source code to facilitate future research at https://github.com/ai4co/trajevo.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.05615v1" target="_blank">Test-Time Reinforcement Learning for GUI Grounding via Region Consistency</a></h3>
                    <p><strong>Authors:</strong> Yong Du, Yuchen Yan, Fei Tang, Zhengxi Lu, Chang Zong, Weiming Lu, Shengpei Jiang, Yongliang Shen</p>
                    <p><strong>Published:</strong> 8/7/2025</p>
                    <p><strong>Categories:</strong> cs.CV, cs.AI, cs.CL</p>
                    <p><strong>Summary:</strong> Graphical User Interface (GUI) grounding, the task of mapping natural language instructions to precise screen coordinates, is fundamental to autonomous GUI agents. While existing methods achieve strong performance through extensive supervised training or reinforcement learning with labeled rewards, they remain constrained by the cost and availability of pixel-level annotations. We observe that when models generate multiple predictions for the same GUI element, the spatial overlap patterns reveal implicit confidence signals that can guide more accurate localization. Leveraging this insight, we propose GUI-RC (Region Consistency), a test-time scaling method that constructs spatial voting grids from multiple sampled predictions to identify consensus regions where models show highest agreement. Without any training, GUI-RC improves accuracy by 2-3% across various architectures on ScreenSpot benchmarks. We further introduce GUI-RCPO (Region Consistency Policy Optimization), which transforms these consistency patterns into rewards for test-time reinforcement learning. By computing how well each prediction aligns with the collective consensus, GUI-RCPO enables models to iteratively refine their outputs on unlabeled data during inference. Extensive experiments demonstrate the generality of our approach: GUI-RC boosts Qwen2.5-VL-3B-Instruct from 80.11% to 83.57% on ScreenSpot-v2, while GUI-RCPO further improves it to 85.14% through self-supervised optimization. Our approach reveals the untapped potential of test-time scaling and test-time reinforcement learning for GUI grounding, offering a promising path toward more robust and data-efficient GUI agents.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.05614v1" target="_blank">OmniEAR: Benchmarking Agent Reasoning in Embodied Tasks</a></h3>
                    <p><strong>Authors:</strong> Zixuan Wang, Dingming Li, Hongxing Li, Shuo Chen, Yuchen Yan, Wenqi Zhang, Yongliang Shen, Weiming Lu, Jun Xiao, Yueting Zhuang</p>
                    <p><strong>Published:</strong> 8/7/2025</p>
                    <p><strong>Categories:</strong> cs.CL, cs.AI</p>
                    <p><strong>Summary:</strong> Large language models excel at abstract reasoning but their capacity for embodied agent reasoning remains largely unexplored. We present OmniEAR, a comprehensive framework for evaluating how language models reason about physical interactions, tool usage, and multi-agent coordination in embodied tasks. Unlike existing benchmarks that provide predefined tool sets or explicit collaboration directives, OmniEAR requires agents to dynamically acquire capabilities and autonomously determine coordination strategies based on task demands. Through text-based environment representation, we model continuous physical properties and complex spatial relationships across 1,500 scenarios spanning household and industrial domains. Our systematic evaluation reveals severe performance degradation when models must reason from constraints: while achieving 85-96% success with explicit instructions, performance drops to 56-85% for tool reasoning and 63-85% for implicit collaboration, with compound tasks showing over 50% failure rates. Surprisingly, complete environmental information degrades coordination performance, indicating models cannot filter task-relevant constraints. Fine-tuning improves single-agent tasks dramatically (0.6% to 76.3%) but yields minimal multi-agent gains (1.5% to 5.5%), exposing fundamental architectural limitations. These findings demonstrate that embodied reasoning poses fundamentally different challenges than current models can address, establishing OmniEAR as a rigorous benchmark for evaluating and advancing embodied AI systems. Our code and data are included in the supplementary materials and will be open-sourced upon acceptance.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.05613v1" target="_blank">Cooper: Co-Optimizing Policy and Reward Models in Reinforcement Learning for Large Language Models</a></h3>
                    <p><strong>Authors:</strong> Haitao Hong, Yuchen Yan, Xingyu Wu, Guiyang Hou, Wenqi Zhang, Weiming Lu, Yongliang Shen, Jun Xiao</p>
                    <p><strong>Published:</strong> 8/7/2025</p>
                    <p><strong>Categories:</strong> cs.CL, cs.AI</p>
                    <p><strong>Summary:</strong> Large language models (LLMs) have demonstrated remarkable performance in reasoning tasks, where reinforcement learning (RL) serves as a key algorithm for enhancing their reasoning capabilities. Currently, there are two mainstream reward paradigms: model-based rewards and rule-based rewards. However, both approaches suffer from limitations: rule-based rewards lack robustness, while model-based rewards are vulnerable to reward hacking. To address these issues, we propose Cooper(Co-optimizing Policy Model and Reward Model), a RL framework that jointly optimizes both the policy model and the reward model. Cooper leverages the high precision of rule-based rewards when identifying correct responses, and dynamically constructs and selects positive-negative sample pairs for continued training the reward model. This design enhances robustness and mitigates the risk of reward hacking. To further support Cooper, we introduce a hybrid annotation strategy that efficiently and accurately generates training data for the reward model. We also propose a reference-based reward modeling paradigm, where the reward model takes a reference answer as input. Based on this design, we train a reward model named VerifyRM, which achieves higher accuracy on VerifyBench compared to other models of the same size. We conduct reinforcement learning using both VerifyRM and Cooper. Our experiments show that Cooper not only alleviates reward hacking but also improves end-to-end RL performance, for instance, achieving a 0.54% gain in average accuracy on Qwen2.5-1.5B-Instruct. Our findings demonstrate that dynamically updating reward model is an effective way to combat reward hacking, providing a reference for better integrating reward models into RL.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.05611v1" target="_blank">Mind the Gap: From Resolving Theoretical Foundations of Chiral(ity)-Induced Spin Selectivity to Pioneering Implementations in Quantum Sensing</a></h3>
                    <p><strong>Authors:</strong> Yan Xi Foo, Aisha Kermiche, Farhan T. Chowdhury, Clarice D. Aiello, Luke D. Smith</p>
                    <p><strong>Published:</strong> 8/7/2025</p>
                    <p><strong>Categories:</strong> physics.chem-ph, cond-mat.mtrl-sci, quant-ph</p>
                    <p><strong>Summary:</strong> The chiral(ity)-induced spin selectivity (CISS) effect, where electrons passing through a chiral medium acquire significant spin-polarization at ambient temperatures, has been widely observed experimentally, yet its theoretical foundations remain actively debated. Open questions persist regarding whether CISS originates from helical geometry or more general chirality, and whether a unified mechanism can account for phenomena across solid-state and soft-matter systems, mesoscopic films, and single molecules. Clarifying the interrelations between existing models is essential to determine if a universal picture of CISS can be found or whether system-specific models are required, and if so, where their common starting point should lie for a workable classification of CISS manifestations. Despite this theoretical fragmentation, recent studies of CISS effects in electron transfer systems, magnetic field sensitivity and coherence of radical pair reactions, polarized electroluminescence in chiral hybrid perovskites, DNA-based biosensors, and enantioselective detection, highlight its broad conceptual relevance and potential applications in spintronics, molecular sensors, and quantum information processing. In this review, we help bridge the gap between theory, experiment, and implementation, with a particular focus on prospects for quantum sensing and metrology. We outline fundamental frameworks of CISS, clarifying what constitutes the `chiral, the `induced, and the `spin-selectivity that makes up CISS, before going on to survey key model realizations and their assumptions. We examine some of the emerging quantum sensing applications and assess the model-specific implications, in particular exemplifying these in the context of spin-correlated radical pairs, which offer a promising, tunable, and biomimetic platform for emerging molecular quantum technologies.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.05610v1" target="_blank">On a 5D UV completion of Argyres-Douglas theories</a></h3>
                    <p><strong>Authors:</strong> Giulio Bonelli, Pavlo Gavrylenko, Ideal Majtara, Alessandro Tanzini</p>
                    <p><strong>Published:</strong> 8/7/2025</p>
                    <p><strong>Categories:</strong> hep-th, math-ph, math.MP</p>
                    <p><strong>Summary:</strong> We discuss a novel UV completion of a class of Argyres-Douglas (AD) theories by its embedding into the renormalisation group flow from five dimensional $\mathcal{N}=1$ superconformal field theories (SCFT) on $S^1$. This is obtained via analysing these theories in the light of ($q$-)Painlev\e/gauge theory correspondence, which allows to compute the five dimensional BPS partition functions as an expansion in the Wilson loop vev with integer $q$-polynomials coefficients. These are derived formulating the gauge theory on a blown-up geometry and using a five-dimensional lift of (topological) operator/state correspondence. We discuss in detail the phase diagram of the four dimensional limits, pinpointing the special AD loci. Explicit computations are reported for $\tilde E_1$ SCFT and its limit to H$_0=(A_1,A_2)$ AD theory.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.05609v1" target="_blank">Hi3DEval: Advancing 3D Generation Evaluation with Hierarchical Validity</a></h3>
                    <p><strong>Authors:</strong> Yuhan Zhang, Long Zhuo, Ziyang Chu, Tong Wu, Zhibing Li, Liang Pan, Dahua Lin, Ziwei Liu</p>
                    <p><strong>Published:</strong> 8/7/2025</p>
                    <p><strong>Categories:</strong> cs.CV</p>
                    <p><strong>Summary:</strong> Despite rapid advances in 3D content generation, quality assessment for the generated 3D assets remains challenging. Existing methods mainly rely on image-based metrics and operate solely at the object level, limiting their ability to capture spatial coherence, material authenticity, and high-fidelity local details. 1) To address these challenges, we introduce Hi3DEval, a hierarchical evaluation framework tailored for 3D generative content. It combines both object-level and part-level evaluation, enabling holistic assessments across multiple dimensions as well as fine-grained quality analysis. Additionally, we extend texture evaluation beyond aesthetic appearance by explicitly assessing material realism, focusing on attributes such as albedo, saturation, and metallicness. 2) To support this framework, we construct Hi3DBench, a large-scale dataset comprising diverse 3D assets and high-quality annotations, accompanied by a reliable multi-agent annotation pipeline. We further propose a 3D-aware automated scoring system based on hybrid 3D representations. Specifically, we leverage video-based representations for object-level and material-subject evaluations to enhance modeling of spatio-temporal consistency and employ pretrained 3D features for part-level perception. Extensive experiments demonstrate that our approach outperforms existing image-based metrics in modeling 3D characteristics and achieves superior alignment with human preference, providing a scalable alternative to manual evaluations. The project page is available at https://zyh482.github.io/Hi3DEval/.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.05608v1" target="_blank">Ultra-Large-Scale Compilation and Manipulation of Quantum Circuits with Pandora</a></h3>
                    <p><strong>Authors:</strong> Ioana Moflic, Alexandru Paler</p>
                    <p><strong>Published:</strong> 8/7/2025</p>
                    <p><strong>Categories:</strong> quant-ph</p>
                    <p><strong>Summary:</strong> There is an enormous gap between what quantum circuit sizes can be compiled and manipulated with the current generation of quantum software and the sizes required by practical applications such as quantum chemistry or Shors algorithm. We present Pandora, an efficient, open-source, multithreaded, high-performance-computing-enabled tool based on circuit rewrites. Pandora can be used for quantum circuit equivalence checking, full compilations of large circuits, and scalable, streaming quantum resource estimation frameworks. Pandora can easily handle billions of gates and can stream circuit partitions in resource estimation pipelines at very high rates. We utilized Pandora for full compilations of Fermi-Hubbard 100x100 and 1024-bit Shors algorithm circuits. Compared to TKET and Qiskit, we determine a performance advantage for manipulating circuits of more than 10000 gates. For equivalence checking tasks, Pandora outperforms MQT.QCEC on specific circuits that have more than 32 qubits. The performance and versatility of Pandora open novel paths in quantum software.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.05607v1" target="_blank">The Mpemba Effect in Pure Water Has a Stochastic Origin. Experimental and Theoretical Resolution of the Paradox</a></h3>
                    <p><strong>Authors:</strong> Andrei A. Klimov, Alexei V. Finkelstein</p>
                    <p><strong>Published:</strong> 8/7/2025</p>
                    <p><strong>Categories:</strong> physics.chem-ph</p>
                    <p><strong>Summary:</strong> The Mpemba effect is the name given to the assertion that hot water freezes quicker than cold water1 or, in a modern and more general form, that the system that is initially more distant from its equilibrium state comes to this state earlier2. This counterintuitive statement seems to breach fundamental thermodynamic and kinetic laws; however, numerous experiments3-10 with classical and quantum systems demonstrate this paradoxical Mpemba effect, leading to extensive discssions in prominent scientific jornals2,5,9,12-14. However, the fundamental physical mechanisms behind this effect have remained elusive14. Here we performed the water freezing experiments under carefully controlled conditions, and found that the Mpemba effect only occurred when the freezer temperature was very close to the temperature of ice nucleation. In this case, the range of freezing times for both hot and cold water was so great that it exceeded the delayed cooling of the initially hotter liquid, and therefore sometimes the hot water froze before the cold water. Our theoretical analysis of this fact shows that the Mpemba paradox associated with water freezing is rooted in the stochastic nature of ice nucleation, typical of first-order phase transitions. We anticipate our assay to be a starting point for reconsidering the famous Mpemba paradox in water and other systems undergoing similar phase transitions.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.05606v1" target="_blank">Uni-cot: Towards Unified Chain-of-Thought Reasoning Across Text and Vision</a></h3>
                    <p><strong>Authors:</strong> Luozheng Qin, Jia Gong, Yuqing Sun, Tianjiao Li, Mengping Yang, Xiaomeng Yang, Chao Qu, Zhiyu Tan, Hao Li</p>
                    <p><strong>Published:</strong> 8/7/2025</p>
                    <p><strong>Categories:</strong> cs.CV, cs.CL</p>
                    <p><strong>Summary:</strong> Chain-of-Thought (CoT) reasoning has been widely adopted to enhance Large Language Models (LLMs) by decomposing complex tasks into simpler, sequential subtasks. However, extending CoT to vision-language reasoning tasks remains challenging, as it often requires interpreting transitions of visual states to support reasoning. Existing methods often struggle with this due to limited capacity of modeling visual state transitions or incoherent visual trajectories caused by fragmented architectures. To overcome these limitations, we propose Uni-CoT, a Unified Chain-of-Thought framework that enables coherent and grounded multimodal reasoning within a single unified model. The key idea is to leverage a model capable of both image understanding and generation to reason over visual content and model evolving visual states. However, empowering a unified model to achieve that is non-trivial, given the high computational cost and the burden of training. To address this, Uni-CoT introduces a novel two-level reasoning paradigm: A Macro-Level CoT for high-level task planning and A Micro-Level CoT for subtask execution. This design significantly reduces the computational overhead. Furthermore, we introduce a structured training paradigm that combines interleaved image-text supervision for macro-level CoT with multi-task objectives for micro-level CoT. Together, these innovations allow Uni-CoT to perform scalable and coherent multi-modal reasoning. Furthermore, thanks to our design, all experiments can be efficiently completed using only 8 A100 GPUs with 80GB VRAM each. Experimental results on reasoning-driven image generation benchmark (WISE) and editing benchmarks (RISE and KRIS) indicates that Uni-CoT demonstrates SOTA performance and strong generalization, establishing Uni-CoT as a promising solution for multi-modal reasoning. Project Page and Code: https://sais-fuxi.github.io/projects/uni-cot/</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.05605v1" target="_blank">Annular SL(2) and SL(3) web algebras</a></h3>
                    <p><strong>Authors:</strong> Rostislav Akhmechet, Mikhail Khovanov, Melissa Zhang</p>
                    <p><strong>Published:</strong> 8/7/2025</p>
                    <p><strong>Categories:</strong> math.GT, math.QA, 57K18, 57K16, 18N25</p>
                    <p><strong>Summary:</strong> We use annular foam TQFTs introduced by the first two authors to define equivariant $SL(2)$ and $SL(3)$ web algebras in the annulus. To a diagram of a tangle in the thickened annulus we assign a complex of bimodules over these algebras whose chain homotopy type is an invariant of the tangle. Several properties of algebras and bimodules are established. An essential technical part of the paper provides a bijective correspondence between non-elliptic annular $SL(3)$ webs and closed paths in the $SL(3)$ weight lattice. This generalizes an analogous bijection in the planar setting.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.05602v1" target="_blank">LLaVA-RE: Binary Image-Text Relevancy Evaluation with Multimodal Large Language Model</a></h3>
                    <p><strong>Authors:</strong> Tao Sun, Oliver Liu, JinJin Li, Lan Ma</p>
                    <p><strong>Published:</strong> 8/7/2025</p>
                    <p><strong>Categories:</strong> cs.CV</p>
                    <p><strong>Summary:</strong> Multimodal generative AI usually involves generating image or text responses given inputs in another modality. The evaluation of image-text relevancy is essential for measuring response quality or ranking candidate responses. In particular, binary relevancy evaluation, i.e., ``Relevant vs. ``Not Relevant, is a fundamental problem. However, this is a challenging task considering that texts have diverse formats and the definition of relevancy varies in different scenarios. We find that Multimodal Large Language Models (MLLMs) are an ideal choice to build such evaluators, as they can flexibly handle complex text formats and take in additional task information. In this paper, we present LLaVA-RE, a first attempt for binary image-text relevancy evaluation with MLLM. It follows the LLaVA architecture and adopts detailed task instructions and multimodal in-context samples. In addition, we propose a novel binary relevancy data set that covers various tasks. Experimental results validate the effectiveness of our framework.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.05599v1" target="_blank">WeTok: Powerful Discrete Tokenization for High-Fidelity Visual Reconstruction</a></h3>
                    <p><strong>Authors:</strong> Shaobin Zhuang, Yiwei Guo, Canmiao Fu, Zhipeng Huang, Zeyue Tian, Ying Zhang, Chen Li, Yali Wang</p>
                    <p><strong>Published:</strong> 8/7/2025</p>
                    <p><strong>Categories:</strong> cs.CV</p>
                    <p><strong>Summary:</strong> Visual tokenizer is a critical component for vision generation. However, the existing tokenizers often face unsatisfactory trade-off between compression ratios and reconstruction fidelity. To fill this gap, we introduce a powerful and concise WeTok tokenizer, which surpasses the previous leading tokenizers via two core innovations. (1) Group-wise lookup-free Quantization (GQ). We partition the latent features into groups, and perform lookup-free quantization for each group. As a result, GQ can efficiently overcome memory and computation limitations of prior tokenizers, while achieving a reconstruction breakthrough with more scalable codebooks. (2) Generative Decoding (GD). Different from prior tokenizers, we introduce a generative decoder with a prior of extra noise variable. In this case, GD can probabilistically model the distribution of visual data conditioned on discrete tokens, allowing WeTok to reconstruct visual details, especially at high compression ratios. Extensive experiments on mainstream benchmarks show superior performance of our WeTok. On the ImageNet 50k validation set, WeTok achieves a record-low zero-shot rFID (WeTok: 0.12 vs. FLUX-VAE: 0.18 vs. SD-VAE 3.5: 0.19). Furthermore, our highest compression model achieves a zero-shot rFID of 3.49 with a compression ratio of 768, outperforming Cosmos (384) 4.57 which has only 50% compression rate of ours. Code and models are available: https://github.com/zhuangshaobin/WeTok.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.05597v1" target="_blank">NP-Hardness and ETH-Based Inapproximability of Communication Complexity via Relaxed Interlacing</a></h3>
                    <p><strong>Authors:</strong> Serge Gaspers, Zixu He, Simon Mackenzie</p>
                    <p><strong>Published:</strong> 8/7/2025</p>
                    <p><strong>Categories:</strong> cs.CC, cs.DS, math.CO</p>
                    <p><strong>Summary:</strong> We prove that computing the deterministic communication complexity D(f) of a Boolean function is NP-hard, even when protocols are limited to a constant number of alternations, resolving a question first posed by Yao (1979). Our reduction builds and expands on a suite of structural interlacing lemmas introduced by Mackenzie and Saffidine (arXiv:2411.19003); these lemmas can be reused as black boxes in future lower-bound constructions. The instances produced by our reduction admit optimal protocols that use only constant alternations, so NP-hardness holds under stronger restrictions than those considered in concurrent and independent work by Hirahara, Ilango, and Loff (arXiv:2507.10426), whose proof requires unbounded alternations. Because the gadgets in our construction are self-similar, they can be recursively embedded. We sketch how this yields, under the Exponential-Time Hypothesis, an additive inapproximability gap that grows without bound, and we outline a route toward NP-hardness of approximating D(f) within a fixed constant additive error. Full details of the ETH-based inapproximability results will appear in a future version. Beyond settling the complexity of deterministic communication complexity itself, the modular framework we develop opens the door to a wider class of reductions and, we believe, will prove useful in tackling other long-standing questions in communication complexity.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.05593v1" target="_blank">Secure Quantum Key Distribution via Entangled Quantum Walkers</a></h3>
                    <p><strong>Authors:</strong> Chia-Tso Lai</p>
                    <p><strong>Published:</strong> 8/7/2025</p>
                    <p><strong>Categories:</strong> quant-ph</p>
                    <p><strong>Summary:</strong> Quantum Key Distribution (QKD) is an emerging cryptographic method designed for secure key sharing. Its security is theoretically guaranteed by fundamental principles of quantum mechanics, making it a leading candidate for future communication protocols. Quantum Random Walks (QRWs), on the other hand, are quantum processes that exhibit intriguing phenomena such as interference and superposition, enabling the generation of decentralized and asymmetric probability distributions. Inspired by both fields of study, we propose a novel QKD protocol based on two entangled quantum walkers. Our protocol exploits the unique correlations between the walkers at extremal positions of the walk to establish secret keys shared exclusively by the two parties. The security of the protocol is augmented by analyzing the joint probability distributions of the walkers measured positions and their associated coin states.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.05592v1" target="_blank">MathSmith: Towards Extremely Hard Mathematical Reasoning by Forging Synthetic Problems with a Reinforced Policy</a></h3>
                    <p><strong>Authors:</strong> Shaoxiong Zhan, Yanlin Lai, Ziyu Lu, Dahua Lin, Ziqing Yang, Fei Tang</p>
                    <p><strong>Published:</strong> 8/7/2025</p>
                    <p><strong>Categories:</strong> cs.CL</p>
                    <p><strong>Summary:</strong> Large language models have achieved substantial progress in mathematical reasoning, yet their advancement is limited by the scarcity of high-quality, high-difficulty training data. Existing synthesis methods largely rely on transforming human-written templates, limiting both diversity and scalability. We propose MathSmith, a novel framework for synthesizing challenging mathematical problems to enhance LLM reasoning. Rather than modifying existing problems, MathSmith constructs new ones from scratch by randomly sampling concept-explanation pairs from PlanetMath, ensuring data independence and avoiding contamination. To increase difficulty, we design nine predefined strategies as soft constraints during rationales. We further adopts reinforcement learning to jointly optimize structural validity, reasoning complexity, and answer consistency. The length of the reasoning trace generated under autoregressive prompting is used to reflect cognitive complexity, encouraging the creation of more demanding problems aligned with long-chain-of-thought reasoning. Experiments across five benchmarks, categorized as easy  medium (GSM8K, MATH-500) and hard (AIME2024, AIME2025, OlympiadBench), show that MathSmith consistently outperforms existing baselines under both short and long CoT settings. Additionally, a weakness-focused variant generation module enables targeted improvement on specific concepts. Overall, MathSmith exhibits strong scalability, generalization, and transferability, highlighting the promise of high-difficulty synthetic data in advancing LLM reasoning capabilities.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.05589v1" target="_blank">Thin-Film Solar Photovoltaics: Trends and Future Directions</a></h3>
                    <p><strong>Authors:</strong> Donald Intal, Abasifreke U. Ebong</p>
                    <p><strong>Published:</strong> 8/7/2025</p>
                    <p><strong>Categories:</strong> physics.app-ph</p>
                    <p><strong>Summary:</strong> Thin-film photovoltaic (PV) technologies address crucial challenges in solar energy applications, including scalability, cost-effectiveness, and environmental sustainability. This paper reviews critically, thin-film technologies such as amorphous silicon (a-Si), cadmium telluride (CdTe), and copper indium gallium selenide (CIGS). It also discusses emerging technologies, including perovskites, copper zinc tin sulfide (CZTS), quantum dots (QDs), organic photovoltaics (OPV), and dye-sensitized solar cells (DSSC). Among these, CdTe and CIGS currently dominate commercial viability, achieving laboratory-scale efficiencies of 23.1% and 23.6%, respectively. Perovskites have notably advanced, reaching a laboratory efficiency of 26.7%. Thin-film PV technologies significantly reduce material use and manufacturing costs, offering distinct advantages such as flexibility and lightweight structures, thereby enabling diverse applications from building-integrated systems to portable electronic devices. Despite these benefits, broader adoption remains limited by challenges including long-term stability, toxicity concerns, and material scarcity. Addressing these challenges through advancements in tandem architectures, improved encapsulation strategies, and sustainable material sourcing is essential for thin-film PV technologies to substantially contribute to the global renewable energy transition.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.05588v1" target="_blank">Quench dynamics of entanglement entropy under projective charge measurements: the free fermion case</a></h3>
                    <p><strong>Authors:</strong> Riccardo Travaglino, Colin Rylands, Pasquale Calabrese</p>
                    <p><strong>Published:</strong> 8/7/2025</p>
                    <p><strong>Categories:</strong> quant-ph, cond-mat.stat-mech</p>
                    <p><strong>Summary:</strong> We consider the effect of projective measurements on the quench dynamics of the bipartite entanglement entropy in one dimensional free fermionic systems. In our protocol, we consider projective measurements of a $U(1)$ conserved charge, the particle number, on some large subsystem, and study the entanglement entropies between the same subsystem and its complement. We compare the dynamics emanating from two classes of initial states, one which is an eigenstate of the charge and another which is not. Moreover, we consider the effects of a single measurement as well as multiple which are periodically performed. Using the quasiparticle picture, we obtain analytic expressions for the behaviour of the entanglement which admit a transparent physical interpretation. In general, we find that measurements introduce two distinct types of corrections to the entanglement, which can be interpreted separately as classical and quantum contributions. The classical contribution is independent of the measurement outcome and scales logarithmically with variance of the charge distribution. In contrast, the quantum contribution depends on the specific measurement outcome and can be significant for individual realizations; however, it becomes negligible when averaged over all possible outcomes. Our expressions reduce to previously known results for symmetry resolved entanglement and full counting statistics in some relevant limits, and are confirmed by an exact calculation performed on the N\eel initial state.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.05585v1" target="_blank">DART: Dual Adaptive Refinement Transfer for Open-Vocabulary Multi-Label Recognition</a></h3>
                    <p><strong>Authors:</strong> Haijing Liu, Tao Pu, Hefeng Wu, Keze Wang, Liang Lin</p>
                    <p><strong>Published:</strong> 8/7/2025</p>
                    <p><strong>Categories:</strong> cs.CV</p>
                    <p><strong>Summary:</strong> Open-Vocabulary Multi-Label Recognition (OV-MLR) aims to identify multiple seen and unseen object categories within an image, requiring both precise intra-class localization to pinpoint objects and effective inter-class reasoning to model complex category dependencies. While Vision-Language Pre-training (VLP) models offer a strong open-vocabulary foundation, they often struggle with fine-grained localization under weak supervision and typically fail to explicitly leverage structured relational knowledge beyond basic semantics, limiting performance especially for unseen classes. To overcome these limitations, we propose the Dual Adaptive Refinement Transfer (DART) framework. DART enhances a frozen VLP backbone via two synergistic adaptive modules. For intra-class refinement, an Adaptive Refinement Module (ARM) refines patch features adaptively, coupled with a novel Weakly Supervised Patch Selecting (WPS) loss that enables discriminative localization using only image-level labels. Concurrently, for inter-class transfer, an Adaptive Transfer Module (ATM) leverages a Class Relationship Graph (CRG), constructed using structured knowledge mined from a Large Language Model (LLM), and employs graph attention network to adaptively transfer relational information between class representations. DART is the first framework, to our knowledge, to explicitly integrate external LLM-derived relational knowledge for adaptive inter-class transfer while simultaneously performing adaptive intra-class refinement under weak supervision for OV-MLR. Extensive experiments on challenging benchmarks demonstrate that our DART achieves new state-of-the-art performance, validating its effectiveness.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.05582v1" target="_blank">A New Three-Players Auction Bridge with Dynamic Opponents and Team Members</a></h3>
                    <p><strong>Authors:</strong> Sourish Sarkar, Aritrabha Majumdar, Moutushi Chatterjee</p>
                    <p><strong>Published:</strong> 8/7/2025</p>
                    <p><strong>Categories:</strong> cs.GT</p>
                    <p><strong>Summary:</strong> This article presents a new three-player version of the bridge playing card game for the purpose of ending fixed partnerships so that the play can be more dynamic and flexible. By dynamically redefining team makeup in real time, this game design increases unpredictability and forces players to repeatedly update strategy. A novel scoring system is introduced to reduce biases present in conventional rule-based games by favoring fairness via reward systems that enforce tactical decision making and risk assessment. Being subject to regular bridge rules, this version tests players to collaborate without fixed friendships, requiring fluid adjustment and adaptive bidding behavior in real time. Strategic issues involve aggressive and defensive bidding, adaptable playing styles, and loss-seeking strategies specific to the three-player structure. The article discusses probabilistic issues of bidding, trump and no-trump declarative effects, and algorithmic methods to trick-taking. Simulation outcomes illustrate the efficiency of diverse strategies. The games architecture is ideal for competitions and possibly influential in broadening entry pools for tournament card games.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.05581v1" target="_blank">Iterative Learning of Computable Phenotypes for Treatment Resistant Hypertension using Large Language Models</a></h3>
                    <p><strong>Authors:</strong> Guilherme Seidyo Imai Aldeia, Daniel S. Herman, William G. La Cava</p>
                    <p><strong>Published:</strong> 8/7/2025</p>
                    <p><strong>Categories:</strong> cs.LG, cs.AI, cs.CL</p>
                    <p><strong>Summary:</strong> Large language models (LLMs) have demonstrated remarkable capabilities for medical question answering and programming, but their potential for generating interpretable computable phenotypes (CPs) is under-explored. In this work, we investigate whether LLMs can generate accurate and concise CPs for six clinical phenotypes of varying complexity, which could be leveraged to enable scalable clinical decision support to improve care for patients with hypertension. In addition to evaluating zero-short performance, we propose and test a synthesize, execute, debug, instruct strategy that uses LLMs to generate and iteratively refine CPs using data-driven feedback. Our results show that LLMs, coupled with iterative learning, can generate interpretable and reasonably accurate programs that approach the performance of state-of-the-art ML methods while requiring significantly fewer training examples.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.05580v1" target="_blank">Follow-Your-Instruction: A Comprehensive MLLM Agent for World Data Synthesis</a></h3>
                    <p><strong>Authors:</strong> Kunyu Feng, Yue Ma, Xinhua Zhang, Boshi Liu, Yikuang Yuluo, Yinhan Zhang, Runtao Liu, Hongyu Liu, Zhiyuan Qin, Shanhui Mo, Qifeng Chen, Zeyu Wang</p>
                    <p><strong>Published:</strong> 8/7/2025</p>
                    <p><strong>Categories:</strong> cs.CV</p>
                    <p><strong>Summary:</strong> With the growing demands of AI-generated content (AIGC), the need for high-quality, diverse, and scalable data has become increasingly crucial. However, collecting large-scale real-world data remains costly and time-consuming, hindering the development of downstream applications. While some works attempt to collect task-specific data via a rendering process, most approaches still rely on manual scene construction, limiting their scalability and accuracy. To address these challenges, we propose Follow-Your-Instruction, a Multimodal Large Language Model (MLLM)-driven framework for automatically synthesizing high-quality 2D, 3D, and 4D data. Our \textbf{Follow-Your-Instruction} first collects assets and their associated descriptions through multimodal inputs using the MLLM-Collector. Then it constructs 3D layouts, and leverages Vision-Language Models (VLMs) for semantic refinement through multi-view scenes with the MLLM-Generator and MLLM-Optimizer, respectively. Finally, it uses MLLM-Planner to generate temporally coherent future frames. We evaluate the quality of the generated data through comprehensive experiments on the 2D, 3D, and 4D generative tasks. The results show that our synthetic data significantly boosts the performance of existing baseline models, demonstrating Follow-Your-Instructions potential as a scalable and effective data engine for generative intelligence.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.05578v1" target="_blank">Adiabatic lapse rate estimation using a van der Waals-type equation of state</a></h3>
                    <p><strong>Authors:</strong> Julio HernÃ¡ndez, Miguel Machucho, Jhony RamÃ­rez</p>
                    <p><strong>Published:</strong> 8/7/2025</p>
                    <p><strong>Categories:</strong> astro-ph.EP, physics.ao-ph</p>
                    <p><strong>Summary:</strong> We revisit a family of temperature-dependent van der Waals-type equations of state (EOS) to improve the estimation of the adiabatic lapse rate in planetary atmospheres. These EOS generalize the classical van der Waals and Berthelot models by introducing a single parameter that modulates the temperature dependence of intermolecular interactions. We analyze their thermodynamic properties, including critical behavior, spinodal and coexistence curves, and entropy. The adiabatic curves are computed by incorporating explicitly the contribution of molecular vibrational and rotational degrees of freedom. Using a generalized expression for the adiabatic lapse rate, we estimate the adiabatic lapse rate in the troposphere of Titan and Venus. Our results show that the van der Waals-type EOS reproduce observed lapse rates more accurately than the van der Waals EOS.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.05577v1" target="_blank">The Anisotropic Interface Continuum Solvation Model and the Finite-Element Anisotropic Poisson Solver</a></h3>
                    <p><strong>Authors:</strong> Ziwei Chai, Sandra Luber</p>
                    <p><strong>Published:</strong> 8/7/2025</p>
                    <p><strong>Categories:</strong> physics.chem-ph, cond-mat.mtrl-sci</p>
                    <p><strong>Summary:</strong> We propose an anisotropic interfacial continuum solvation (AICS) model to simulate the distinct in-plane and out-of-plane dielectric constants of liquids near solid-liquid interfaces and their spatial variations along the surface normal direction. In low-electron-density regions, each dielectric function in the diagonal components of a dielectric tensor varies monotonically with distance from the solid surface along the surface normal; in high-electron-density regions near the surface, each dielectric function adopts the electron-density-based formulation proposed by Andreussi et al. (J. Chem. Phys. 136, 064102 (2012)) The resulting dielectric tensor is continuously differentiable with respect to both electron density and spatial coordinates. We derived analytical expressions for electrostatic contributions to the KS potential and forces, and implemented AICS, including these analytical derivatives, into CP2K. To solve the anisotropic Poisson equations, we developed a parallel finite-element anisotropic Poisson solver (FEAPS) based on the FEniCSx platform and its interface with CP2K. Analytical forces were validated against finite-difference calculations, while electrostatic potentials computed under vacuum and isotropic solvent conditions using AICS and FEAPS were benchmarked against standard vacuum DFT and SCCS results, respectively. In the anisotropic solvent environment characterized by the enhanced in-plane and reduced out-of-plane dielectric functions near the Ag(111) surface, we calculated the resulting work functions and electrostatic potentials, and optimized the adsorption geometry for OH. Compared to the isotropic case, we observed more pronounced work function shifts and spatially modulated electrostatic profiles across different charge states. Our results also showed that OH tilted more towards the plane parallel to the surface under the anisotropic dielectric conditions.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.05574v1" target="_blank">Latency Minimization for Multi-AAV-Enabled ISCC Systems with Movable Antenna</a></h3>
                    <p><strong>Authors:</strong> Yiyang Chen, Wenchao Liu, Chunjie Wang, Yinyu Wu, Xuhui Zhang, Yanyan Shen</p>
                    <p><strong>Published:</strong> 8/7/2025</p>
                    <p><strong>Categories:</strong> cs.IT, eess.SP, math.IT</p>
                    <p><strong>Summary:</strong> This paper investigates an autonomous aerial vehicle (AAV)-enabled integrated sensing, communication, and computation system, with a particular focus on integrating movable antennas (MAs) into the system for enhancing overall system performance. Specifically, multiple MA-enabled AVVs perform sensing tasks and simultaneously transmit the generated computational tasks to the base station for processing. To minimize the maximum latency under the sensing and resource constraints, we formulate an optimization problem that jointly coordinates the position of the MAs, the computation resource allocation, and the transmit beamforming. Due to the non-convexity of the objective function and strong coupling among variables, we propose a two-layer iterative algorithm leveraging particle swarm optimization and convex optimization to address it. The simulation results demonstrate that the proposed scheme achieves significant latency improvements compared to the baseline schemes.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.05571v1" target="_blank">Fairy$\pm i$: the First 2-bit Complex LLM with All Parameters in $\{\pm1, \pm i\}$</a></h3>
                    <p><strong>Authors:</strong> Feiyu Wang, Guoan Wang, Yihao Zhang, Shengfan Wang, Weitao Li, Bokai Huang, Shimao Chen, Zihan Jiang, Rui Xu, Tong Yang</p>
                    <p><strong>Published:</strong> 8/7/2025</p>
                    <p><strong>Categories:</strong> cs.LG, cs.CL</p>
                    <p><strong>Summary:</strong> Quantization-Aware Training (QAT) integrates quantization into the training loop, enabling LLMs to learn robust low-bit representations, and is widely recognized as one of the most promising research directions. All current QAT research focuses on minimizing quantization error on full-precision models, where the full-precision accuracy acts as an upper bound (accuracy ceiling). No existing method has even attempted to surpass this ceiling. To break this ceiling, we propose a new paradigm: raising the ceiling (full-precision model), and then still quantizing it efficiently into 2 bits. We propose Fairy$\pm i$, the first 2-bit quantization framework for complex-valued LLMs. Specifically, our method leverages the representational advantages of the complex domain to boost full-precision accuracy. We map weights to the fourth roots of unity $\{\pm1, \pm i\}$, forming a perfectly symmetric and information-theoretically optimal 2-bit representation. Importantly, each quantized weight has either a zero real or imaginary part, enabling multiplication-free inference using only additions and element swaps. Experimental results show that Fairy$\pm i$ outperforms the ceiling of existing 2-bit quantization approaches in terms of both PPL and downstream tasks, while maintaining strict storage and compute efficiency. This work opens a new direction for building highly accurate and practical LLMs under extremely low-bit constraints.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.05568v1" target="_blank">X-VFL: A New Vertical Federated Learning Framework with Cross Completion and Decision Subspace Alignment</a></h3>
                    <p><strong>Authors:</strong> Qinghua Yao, Xiangrui Xu, Zhize Li</p>
                    <p><strong>Published:</strong> 8/7/2025</p>
                    <p><strong>Categories:</strong> cs.LG, cs.CV, cs.DC, math.OC</p>
                    <p><strong>Summary:</strong> Vertical Federated Learning (VFL) enables collaborative learning by integrating disjoint feature subsets from multiple clients/parties. However, VFL typically faces two key challenges: i) the requirement for perfectly aligned data samples across all clients (missing features are not allowed); ii) the requirement for joint collaborative inference/prediction involving all clients (it does not support locally independent inference on a single client). To address these challenges, we propose X-VFL, a new VFL framework designed to deal with the non-aligned data samples with (partially) missing features and to support locally independent inference of new data samples for each client. In particular, we design two novel modules in X-VFL: Cross Completion (XCom) and Decision Subspace Alignment (DS-Align). XCom can complete/reconstruct missing features for non-aligned data samples by leveraging information from other clients. DS-Align aligns local features with completed and global features across all clients within the decision subspace, thus enabling locally independent inference at each client. Moreover, we provide convergence theorems for different algorithms used in training X-VFL, showing an $O(1/\sqrt{T})$ convergence rate for SGD-type algorithms and an $O(1/T)$ rate for PAGE-type algorithms, where $T$ denotes the number of training update steps. Extensive experiments on real-world datasets demonstrate that X-VFL significantly outperforms existing methods, e.g., achieving a 15% improvement in accuracy on the image CIFAR-10 dataset and a 43% improvement on the medical MIMIC-III dataset. These results validate the practical effectiveness and superiority of X-VFL, particularly in scenarios involving partially missing features and locally independent inference.</p>
                
            
                
                    <h3><a href="http://dx.doi.org/10.4310/22-SII773" target="_blank">L1-Regularized Functional Support Vector Machine</a></h3>
                    <p><strong>Authors:</strong> Bingfan Liu, Peijun Sang</p>
                    <p><strong>Published:</strong> 8/7/2025</p>
                    <p><strong>Categories:</strong> stat.ML, cs.LG, stat.CO</p>
                    <p><strong>Summary:</strong> In functional data analysis, binary classification with one functional covariate has been extensively studied. We aim to fill in the gap of considering multivariate functional covariates in classification. In particular, we propose an $L_1$-regularized functional support vector machine for binary classification. An accompanying algorithm is developed to fit the classifier. By imposing an $L_1$ penalty, the algorithm enables us to identify relevant functional covariates of the binary response. Numerical results from simulations and one real-world application demonstrate that the proposed classifier enjoys good performance in both prediction and feature selection.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.05563v1" target="_blank">Carleson operators on doubling metric measure spaces</a></h3>
                    <p><strong>Authors:</strong> Lars Becker, Floris van Doorn, Asgar Jamneshan, Rajula Srivastava, Christoph Thiele</p>
                    <p><strong>Published:</strong> 8/7/2025</p>
                    <p><strong>Categories:</strong> math.CA, 42B20</p>
                    <p><strong>Summary:</strong> Doubling metric measure spaces provide a natural framework for singular integral operators. In contrast, the study of maximally modulated singular integral operators, the so-called Carleson operators, has largely been limited to Euclidean space with modulation functions such as polynomials defined by algebraic means. We present a general axiomatic approach to modulation functions on doubling metric measure spaces and prove $L^p$ bounds for the corresponding Carleson operators in Theorem 1.1 and Theorem 1.2. This generalizes classical and modern results on Carleson operators. In addition to the proofs presented here, our main results have been computer verified using the language Lean and the library mathlib, as documented in the sibling communication arXiv:2405.06423.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.05561v1" target="_blank">Flavour interferometry in Reissner-Nordstrom background</a></h3>
                    <p><strong>Authors:</strong> Jean Alexandre, Emilio Meryn</p>
                    <p><strong>Published:</strong> 8/7/2025</p>
                    <p><strong>Categories:</strong> gr-qc, astro-ph.HE, hep-ph</p>
                    <p><strong>Summary:</strong> We derive the phase acquired by a neutral scalar particle propagating along Reissner-Nordstrom geodesics. Considering two flavours propagating on different trajectories which intersect, we plot the interference pattern induced by gravitational lensing from the charged compact object. Although the effect of the charge is subdominant in the metric, it proves to be significant in the phase, and shifts the interference pattern, compared to the Schwarzschild case. This pattern is characterised by two oscillation lengths which, if known, would allow the determination of both eigen masses independently.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.05559v1" target="_blank">On the Design of Expressive and Trainable Pulse-based Quantum Machine Learning Models</a></h3>
                    <p><strong>Authors:</strong> Han-Xiao Tao, Xin Wang, Re-Bing Wu</p>
                    <p><strong>Published:</strong> 8/7/2025</p>
                    <p><strong>Categories:</strong> quant-ph, cs.LG</p>
                    <p><strong>Summary:</strong> Pulse-based Quantum Machine Learning (QML) has emerged as a novel paradigm in quantum artificial intelligence due to its exceptional hardware efficiency. For practical applications, pulse-based models must be both expressive and trainable. Previous studies suggest that pulse-based models under dynamic symmetry can be effectively trained, thanks to a favorable loss landscape that has no barren plateaus. However, the resulting uncontrollability may compromise expressivity when the model is inadequately designed. This paper investigates the requirements for pulse-based QML models to be expressive while preserving trainability. We present a necessary condition pertaining to the systems initial state, the measurement observable, and the underlying dynamical symmetry Lie algebra, supported by numerical simulations. Our findings establish a framework for designing practical pulse-based QML models that balance expressivity and trainability.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.05558v1" target="_blank">Joint parameter estimation and multidimensional reconciliation for CV-QKD</a></h3>
                    <p><strong>Authors:</strong> Jisheng Dai, Xue-Qin Jiang, Peng Huang, Tao Wang, Guihua Zeng</p>
                    <p><strong>Published:</strong> 8/7/2025</p>
                    <p><strong>Categories:</strong> quant-ph, eess.SP</p>
                    <p><strong>Summary:</strong> Accurate quantum channel parameter estimation is essential for effective information reconciliation in continuous-variable quantum key distribution (CV-QKD). However, conventional maximum likelihood (ML) estimators rely on a large amount of discarded data (or pilot symbols), leading to a significant loss in symbol efficiency. Moreover, the separation between the estimation and reconciliation phases can introduce error propagation. In this paper, we propose a novel joint message-passing scheme that unifies channel parameter estimation and information reconciliation within a Bayesian framework. By leveraging the expectation-maximization (EM) algorithm, the proposed method simultaneously estimates unknown parameters during decoding, eliminating the need for separate ML estimation. Furthermore, we introduce a hybrid multidimensional rotation scheme that removes the requirement for norm feedback, significantly reducing classical channel overhead. To the best of our knowledge, this is the first work to unify multidimensional reconciliation and channel parameter estimation in CV-QKD, providing a practical solution for high-efficiency reconciliation with minimal pilots.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.05554v1" target="_blank">SPGISpeech 2.0: Transcribed multi-speaker financial audio for speaker-tagged transcription</a></h3>
                    <p><strong>Authors:</strong> Raymond Grossman, Taejin Park, Kunal Dhawan, Andrew Titus, Sophia Zhi, Yulia Shchadilova, Weiqing Wang, Jagadeesh Balam, Boris Ginsburg</p>
                    <p><strong>Published:</strong> 8/7/2025</p>
                    <p><strong>Categories:</strong> cs.SD, cs.CL, eess.AS</p>
                    <p><strong>Summary:</strong> We introduce SPGISpeech 2.0, a dataset suitable for speaker-tagged transcription in the financial domain. SPGISpeech 2.0 improves the diversity of applicable modeling tasks while maintaining the core characteristic of the original SPGISpeech dataset: audio snippets and their corresponding fully formatted text transcriptions, usable for end-to-end automatic speech recognition (ASR). SPGISpeech 2.0 consists of 3,780 additional hours of professionally transcribed earnings calls. Furthermore, the dataset contains call and speaker information for each audio snippet facilitating multi-talker ASR. We validate the utility of SPGISpeech 2.0 through improvements in speaker-tagged ASR performance of popular speech recognition models after fine-tuning on SPGISpeech 2.0. Released free for non-commercial use, we expect SPGISpeech 2.0 to foster advancements in speech recognition technologies and inspire a wide range of research applications.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.05553v1" target="_blank">Do Political Opinions Transfer Between Western Languages? An Analysis of Unaligned and Aligned Multilingual LLMs</a></h3>
                    <p><strong>Authors:</strong> Franziska Weeber, Tanise Ceron, Sebastian PadÃ³</p>
                    <p><strong>Published:</strong> 8/7/2025</p>
                    <p><strong>Categories:</strong> cs.CL, cs.CY, I.2.7; J.4</p>
                    <p><strong>Summary:</strong> Public opinion surveys show cross-cultural differences in political opinions between socio-cultural contexts. However, there is no clear evidence whether these differences translate to cross-lingual differences in multilingual large language models (MLLMs). We analyze whether opinions transfer between languages or whether there are separate opinions for each language in MLLMs of various sizes across five Western languages. We evaluate MLLMs opinions by prompting them to report their (dis)agreement with political statements from voting advice applications. To better understand the interaction between languages in the models, we evaluate them both before and after aligning them with more left or right views using direct preference optimization and English alignment data only. Our findings reveal that unaligned models show only very few significant cross-lingual differences in the political opinions they reflect. The political alignment shifts opinions almost uniformly across all five languages. We conclude that in Western language contexts, political opinions transfer between languages, demonstrating the challenges in achieving explicit socio-linguistic, cultural, and political alignment of MLLMs.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.05547v1" target="_blank">Adapting Vision-Language Models Without Labels: A Comprehensive Survey</a></h3>
                    <p><strong>Authors:</strong> Hao Dong, Lijun Sheng, Jian Liang, Ran He, Eleni Chatzi, Olga Fink</p>
                    <p><strong>Published:</strong> 8/7/2025</p>
                    <p><strong>Categories:</strong> cs.LG, cs.AI, cs.CV</p>
                    <p><strong>Summary:</strong> Vision-Language Models (VLMs) have demonstrated remarkable generalization capabilities across a wide range of tasks. However, their performance often remains suboptimal when directly applied to specific downstream scenarios without task-specific adaptation. To enhance their utility while preserving data efficiency, recent research has increasingly focused on unsupervised adaptation methods that do not rely on labeled data. Despite the growing interest in this area, there remains a lack of a unified, task-oriented survey dedicated to unsupervised VLM adaptation. To bridge this gap, we present a comprehensive and structured overview of the field. We propose a taxonomy based on the availability and nature of unlabeled visual data, categorizing existing approaches into four key paradigms: Data-Free Transfer (no data), Unsupervised Domain Transfer (abundant data), Episodic Test-Time Adaptation (batch data), and Online Test-Time Adaptation (streaming data). Within this framework, we analyze core methodologies and adaptation strategies associated with each paradigm, aiming to establish a systematic understanding of the field. Additionally, we review representative benchmarks across diverse applications and highlight open challenges and promising directions for future research. An actively maintained repository of relevant literature is available at https://github.com/tim-learn/Awesome-LabelFree-VLMs.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.05546v1" target="_blank">Modular Architecture for High-Performance and Low Overhead Data Transfers</a></h3>
                    <p><strong>Authors:</strong> Rasman Mubtasim Swargo, Engin Arslan, Md Arifuzzaman</p>
                    <p><strong>Published:</strong> 8/7/2025</p>
                    <p><strong>Categories:</strong> cs.DC</p>
                    <p><strong>Summary:</strong> High-performance applications necessitate rapid and dependable transfer of massive datasets across geographically dispersed locations. Traditional file transfer tools often suffer from resource underutilization and instability because of fixed configurations or monolithic optimization methods. We propose AutoMDT, a novel modular data transfer architecture that employs a deep reinforcement learning based agent to simultaneously optimize concurrency levels for read, network, and write operations. Our solution incorporates a lightweight network-system simulator, enabling offline training of a Proximal Policy Optimization (PPO) agent in approximately 45 minutes on average, thereby overcoming the impracticality of lengthy online training in production networks. AutoMDTs modular design decouples I/O and network tasks, allowing the agent to capture complex buffer dynamics precisely and to adapt quickly to changing system and network conditions. Evaluations on production-grade testbeds show that AutoMDT achieves up to 8x faster convergence and a 68% reduction in transfer completion times compared with state-of-the-art solutions.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.05544v1" target="_blank">Conformal Sets in Multiple-Choice Question Answering under Black-Box Settings with Provable Coverage Guarantees</a></h3>
                    <p><strong>Authors:</strong> Guang Yang, Xinyang Liu</p>
                    <p><strong>Published:</strong> 8/7/2025</p>
                    <p><strong>Categories:</strong> cs.CL, cs.AI</p>
                    <p><strong>Summary:</strong> Large Language Models (LLMs) have shown remarkable progress in multiple-choice question answering (MCQA), but their inherent unreliability, such as hallucination and overconfidence, limits their application in high-risk domains. To address this, we propose a frequency-based uncertainty quantification method under black-box settings, leveraging conformal prediction (CP) to ensure provable coverage guarantees. Our approach involves multiple independent samplings of the models output distribution for each input, with the most frequent sample serving as a reference to calculate predictive entropy (PE). Experimental evaluations across six LLMs and four datasets (MedMCQA, MedQA, MMLU, MMLU-Pro) demonstrate that frequency-based PE outperforms logit-based PE in distinguishing between correct and incorrect predictions, as measured by AUROC. Furthermore, the method effectively controls the empirical miscoverage rate under user-specified risk levels, validating that sampling frequency can serve as a viable substitute for logit-based probabilities in black-box scenarios. This work provides a distribution-free model-agnostic framework for reliable uncertainty quantification in MCQA with guaranteed coverage, enhancing the trustworthiness of LLMs in practical applications.</p>
                
            
        
        
            <p><em>Generated by AI News Agent</em></p>
        
    

