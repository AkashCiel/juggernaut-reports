
    
        <h1>ðŸ¤– AI Research Report</h1>
        
            <strong>Date:</strong> 2025-08-23<br>
            <strong>Topics:</strong> ai safety research, ai alignment research, quantum computing<br>
            <strong>Papers Found:</strong> 120
        
        
        
            
                <h2>ðŸ¤– AI Summary</h2>
                <p>## ai safety research

The field of AI safety research is experiencing significant advancements across various dimensions, including model robustness, ethical considerations, and safe deployment strategies. One key area of focus is enhancing the robustness of AI systems to ensure they can reliably operate in diverse environments. For instance, the study on **Probability Density from Latent Diffusion Models for Out-of-Distribution Detection** highlights the importance of detecting inputs that deviate from training data distributions to prevent unsafe predictions. Similarly, **LiveMCP-101** and **SafetyFlow** aim to benchmark AI agents performance in realistic scenarios, revealing significant gaps in current modelsâ€™ ability to handle complex, multi-step tasks and suggesting the need for improved orchestration and safety evaluation methods.

Moreover, ethical concerns, such as preventing parasocial relationships with AI and ensuring privacy in voice anonymization, are being actively addressed. The paper **Response and Prompt Evaluation to Prevent Parasocial Relationships with Chatbots** demonstrates a framework for real-time detection of parasocial cues, aiming to mitigate the social risks associated with AI interactions. **Any-to-any Speaker Attribute Perturbation for Asynchronous Voice Anonymization** advances privacy protection by anonymizing voice data, which is crucial for maintaining user confidentiality. Additionally, the integration of AI in critical applications, as discussed in **Measuring the Environmental Impact of Delivering AI at Google Scale**, underscores the need for sustainable practices in AI deployment, emphasizing the reduction of energy usage and carbon footprint. Collectively, these works reflect a trend towards developing more robust, ethical, and environmentally sustainable AI systems, which are crucial for ensuring the safe integration of AI into society.

*Based on 50 research papers*

---

## quantum computing

In the context of quantum computing, the research paper titled **A framework for robust quantum speedups in practical correlated electronic structure and dynamics** by Jielun Chen and Garnet Kin-Lic Chan addresses the challenge of achieving quantum advantage in electronic structure calculations, a critical area where quantum computing is expected to outperform classical methods. This paper highlights the difficulty of finding problems where classical heuristics fail and demonstrates how robust quantum speedups can be achieved in the electronic structure and dynamics domain. The authors propose methods that can bypass the limitations of classical heuristics, providing pathways to harness quantum computings potential in practical applications, such as chemical simulations and materials science.

Another relevant paper, **Evaluating classical simulations with a quantum processor** by Alberto Nocera et al., explores the use of quantum processors to validate classical simulation methods in quantum dynamics. This study emphasizes the importance of quantum processors in providing ground truth data against which classical methods, particularly tensor-network methods, can be evaluated. By demonstrating discrepancies between classical predictions and quantum results, the paper underscores the necessity of integrating quantum computing into the validation loop of classical simulation techniques, which could lead to more accurate and reliable simulations of quantum systems. These studies collectively indicate a trend towards utilizing quantum computing not only for direct computational tasks but also as a benchmark to enhance classical simulation techniques, potentially accelerating the development of quantum technologies and their application in complex scientific domains.

*Based on 20 research papers*</p>
            
        
        
        <h2>ðŸ“š Research Papers</h2>
        
                
                    <h3><a href="http://arxiv.org/abs/2508.15772v1" target="_blank">Visual Autoregressive Modeling for Instruction-Guided Image Editing</a></h3>
                    <p><strong>Authors:</strong> Qingyang Mao, Qi Cai, Yehao Li, Yingwei Pan, Mingyue Cheng, Ting Yao, Qi Liu, Tao Mei</p>
                    <p><strong>Published:</strong> 8/21/2025</p>
                    <p><strong>Categories:</strong> cs.CV, cs.MM</p>
                    <p><strong>Summary:</strong> Recent advances in diffusion models have brought remarkable visual fidelity to instruction-guided image editing. However, their global denoising process inherently entangles the edited region with the entire image context, leading to unintended spurious modifications and compromised adherence to editing instructions. In contrast, autoregressive models offer a distinct paradigm by formulating image synthesis as a sequential process over discrete visual tokens. Their causal and compositional mechanism naturally circumvents the adherence challenges of diffusion-based methods. In this paper, we present VAREdit, a visual autoregressive (VAR) framework that reframes image editing as a next-scale prediction problem. Conditioned on source image features and text instructions, VAREdit generates multi-scale target features to achieve precise edits. A core challenge in this paradigm is how to effectively condition the source image tokens. We observe that finest-scale source features cannot effectively guide the prediction of coarser target features. To bridge this gap, we introduce a Scale-Aligned Reference (SAR) module, which injects scale-matched conditioning information into the first self-attention layer. VAREdit demonstrates significant advancements in both editing adherence and efficiency. On standard benchmarks, it outperforms leading diffusion-based methods by 30\%+ higher GPT-Balance score. Moreover, it completes a $512\times512$ editing in 1.2 seconds, making it 2.2$\times$ faster than the similarly sized UltraEdit. The models are available at https://github.com/HiDream-ai/VAREdit.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.15771v1" target="_blank">Overview of complex organic molecule observations in protostellar systems</a></h3>
                    <p><strong>Authors:</strong> P. Nazari</p>
                    <p><strong>Published:</strong> 8/21/2025</p>
                    <p><strong>Categories:</strong> astro-ph.GA</p>
                    <p><strong>Summary:</strong> Complex organic molecules (COMs) have been detected abundantly at various stages of star formation, particularly in the warm protostellar phase. The progress in gas-phase measurements has been accelerated by the advent of the Atacama Large Millimeter/submillimeter Array and in ice measurements by the James Webb Space Telescope. Particularly, the community has moved from single-source studies of COMs to statistical analyses because of these powerful instruments. In this article, I review surveys that consider COMs in the gas and ice. The two takeaways from this review include; 1. Gas-phase abundance ratios for some COMs show a small difference across many objects and the ice abundance ratios show similar or higher values to the gas, both pointing to the importance of ice chemistry in COM formation, 2. Some COM ratios show larger differences across many objects which could be due to either chemical or physical effects, thus both factors need to be considered when interpreting the data.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.15769v1" target="_blank">SceneGen: Single-Image 3D Scene Generation in One Feedforward Pass</a></h3>
                    <p><strong>Authors:</strong> Yanxu Meng, Haoning Wu, Ya Zhang, Weidi Xie</p>
                    <p><strong>Published:</strong> 8/21/2025</p>
                    <p><strong>Categories:</strong> cs.CV, cs.AI</p>
                    <p><strong>Summary:</strong> 3D content generation has recently attracted significant research interest due to its applications in VR/AR and embodied AI. In this work, we address the challenging task of synthesizing multiple 3D assets within a single scene image. Concretely, our contributions are fourfold: (i) we present SceneGen, a novel framework that takes a scene image and corresponding object masks as input, simultaneously producing multiple 3D assets with geometry and texture. Notably, SceneGen operates with no need for optimization or asset retrieval; (ii) we introduce a novel feature aggregation module that integrates local and global scene information from visual and geometric encoders within the feature extraction module. Coupled with a position head, this enables the generation of 3D assets and their relative spatial positions in a single feedforward pass; (iii) we demonstrate SceneGens direct extensibility to multi-image input scenarios. Despite being trained solely on single-image inputs, our architectural design enables improved generation performance with multi-image inputs; and (iv) extensive quantitative and qualitative evaluations confirm the efficiency and robust generation abilities of our approach. We believe this paradigm offers a novel solution for high-quality 3D content generation, potentially advancing its practical applications in downstream tasks. The code and model will be publicly available at: https://mengmouxu.github.io/SceneGen.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.15763v1" target="_blank">Intern-S1: A Scientific Multimodal Foundation Model</a></h3>
                    <p><strong>Authors:</strong> Lei Bai, Zhongrui Cai, Maosong Cao, Weihan Cao, Chiyu Chen, Haojiong Chen, Kai Chen, Pengcheng Chen, Ying Chen, Yongkang Chen, Yu Cheng, Yu Cheng, Pei Chu, Tao Chu, Erfei Cui, Ganqu Cui, Long Cui, Ziyun Cui, Nianchen Deng, Ning Ding, Nanqin Dong, Peijie Dong, Shihan Dou, Sinan Du, Haodong Duan, Caihua Fan, Ben Gao, Changjiang Gao, Jianfei Gao, Songyang Gao, Yang Gao, Zhangwei Gao, Jiaye Ge, Qiming Ge, Lixin Gu, Yuzhe Gu, Aijia Guo, Qipeng Guo, Xu Guo, Conghui He, Junjun He, Yili Hong, Siyuan Hou, Caiyu Hu, Hanglei Hu, Jucheng Hu, Ming Hu, Zhouqi Hua, Haian Huang, Junhao Huang, Xu Huang, Zixian Huang, Zhe Jiang, Lingkai Kong, Linyang Li, Peiji Li, Pengze Li, Shuaibin Li, Tianbin Li, Wei Li, Yuqiang Li, Dahua Lin, Junyao Lin, Tianyi Lin, Zhishan Lin, Hongwei Liu, Jiangning Liu, Jiyao Liu, Junnan Liu, Kai Liu, Kaiwen Liu, Kuikun Liu, Shichun Liu, Shudong Liu, Wei Liu, Xinyao Liu, Yuhong Liu, Zhan Liu, Yinquan Lu, Haijun Lv, Hongxia Lv, Huijie Lv, Qidang Lv, Ying Lv, Chengqi Lyu, Chenglong Ma, Jianpeng Ma, Ren Ma, Runmin Ma, Runyuan Ma, Xinzhu Ma, Yichuan Ma, Zihan Ma, Sixuan Mi, Junzhi Ning, Wenchang Ning, Xinle Pang, Jiahui Peng, Runyu Peng, Yu Qiao, Jiantao Qiu, Xiaoye Qu, Yuan Qu, Yuchen Ren, Fukai Shang, Wenqi Shao, Junhao Shen, Shuaike Shen, Chunfeng Song, Demin Song, Diping Song, Chenlin Su, Weijie Su, Weigao Sun, Yu Sun, Qian Tan, Cheng Tang, Huanze Tang, Kexian Tang, Shixiang Tang, Jian Tong, Aoran Wang, Bin Wang, Dong Wang, Lintao Wang, Rui Wang, Weiyun Wang, Wenhai Wang, Yi Wang, Ziyi Wang, Ling-I Wu, Wen Wu, Yue Wu, Zijian Wu, Linchen Xiao, Shuhao Xing, Chao Xu, Huihui Xu, Jun Xu, Ruiliang Xu, Wanghan Xu, GanLin Yang, Yuming Yang, Haochen Ye, Jin Ye, Shenglong Ye, Jia Yu, Jiashuo Yu, Jing Yu, Fei Yuan, Bo Zhang, Chao Zhang, Chen Zhang, Hongjie Zhang, Jin Zhang, Qiaosheng Zhang, Qiuyinzhe Zhang, Songyang Zhang, Taolin Zhang, Wenlong Zhang, Wenwei Zhang, Yechen Zhang, Ziyang Zhang, Haiteng Zhao, Qian Zhao, Xiangyu Zhao, Xiangyu Zhao, Bowen Zhou, Dongzhan Zhou, Peiheng Zhou, Yuhao Zhou, Yunhua Zhou, Dongsheng Zhu, Lin Zhu, Yicheng Zou</p>
                    <p><strong>Published:</strong> 8/21/2025</p>
                    <p><strong>Categories:</strong> cs.LG, cs.CL, cs.CV</p>
                    <p><strong>Summary:</strong> In recent years, a plethora of open-source foundation models have emerged, achieving remarkable progress in some widely attended fields, with performance being quite close to that of closed-source models. However, in high-value but more challenging scientific professional fields, either the fields still rely on expert models, or the progress of general foundation models lags significantly compared to those in popular areas, far from sufficient for transforming scientific research and leaving substantial gap between open-source models and closed-source models in these scientific domains. To mitigate this gap and explore a step further toward Artificial General Intelligence (AGI), we introduce Intern-S1, a specialized generalist equipped with general understanding and reasoning capabilities with expertise to analyze multiple science modal data. Intern-S1 is a multimodal Mixture-of-Experts (MoE) model with 28 billion activated parameters and 241 billion total parameters, continually pre-trained on 5T tokens, including over 2.5T tokens from scientific domains. In the post-training stage, Intern-S1 undergoes offline and then online reinforcement learning (RL) in InternBootCamp, where we propose Mixture-of-Rewards (MoR) to synergize the RL training on more than 1000 tasks simultaneously. Through integrated innovations in algorithms, data, and training systems, Intern-S1 achieved top-tier performance in online RL training.On comprehensive evaluation benchmarks, Intern-S1 demonstrates competitive performance on general reasoning tasks among open-source models and significantly outperforms open-source models in scientific domains, surpassing closed-source state-of-the-art models in professional tasks, such as molecular synthesis planning, reaction condition prediction, predicting thermodynamic stabilities for crystals. Our models are available at https://huggingface.co/internlm/Intern-S1.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.15760v1" target="_blank">LiveMCP-101: Stress Testing and Diagnosing MCP-enabled Agents on Challenging Queries</a></h3>
                    <p><strong>Authors:</strong> Ming Yin, Dinghan Shen, Silei Xu, Jianbing Han, Sixun Dong, Mian Zhang, Yebowen Hu, Shujian Liu, Simin Ma, Song Wang, Sathish Reddy Indurthi, Xun Wang, Yiran Chen, Kaiqiang Song</p>
                    <p><strong>Published:</strong> 8/21/2025</p>
                    <p><strong>Categories:</strong> cs.CL, cs.AI</p>
                    <p><strong>Summary:</strong> Tool calling has emerged as a critical capability for AI agents to interact with the real world and solve complex tasks. While the Model Context Protocol (MCP) provides a powerful standardized framework for tool integration, there is a significant gap in benchmarking how well AI agents can effectively solve multi-step tasks using diverse MCP tools in realistic, dynamic scenarios. In this work, we present LiveMCP-101, a benchmark of 101 carefully curated real-world queries, refined through iterative LLM rewriting and manual review, that require coordinated use of multiple MCP tools including web search, file operations, mathematical reasoning, and data analysis. Moreover, we introduce a novel evaluation approach that leverages ground-truth execution plans rather than raw API outputs, better reflecting the evolving nature of real-world environments. Experiments show that even frontier LLMs achieve a success rate below 60\%, highlighting major challenges in tool orchestration. Detailed ablations and error analysis further reveal distinct failure modes and inefficiencies in token usage, pointing to concrete directions for advancing current models. LiveMCP-101 sets a rigorous standard for evaluating real-world agent capabilities, advancing toward autonomous AI systems that reliably execute complex tasks through tool use.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.15752v1" target="_blank">Does the cafe entrance look accessible? Where is the door? Towards Geospatial AI Agents for Visual Inquiries</a></h3>
                    <p><strong>Authors:</strong> Jon E. Froehlich, Jared Hwang, Zeyu Wang, John S. OMeara, Xia Su, William Huang, Yang Zhang, Alex Fiannaca, Philip Nelson, Shaun Kane</p>
                    <p><strong>Published:</strong> 8/21/2025</p>
                    <p><strong>Categories:</strong> cs.HC, cs.AI, cs.CV, H.5; I.2</p>
                    <p><strong>Summary:</strong> Interactive digital maps have revolutionized how people travel and learn about the world; however, they rely on pre-existing structured data in GIS databases (e.g., road networks, POI indices), limiting their ability to address geo-visual questions related to what the world looks like. We introduce our vision for Geo-Visual Agents--multimodal AI agents capable of understanding and responding to nuanced visual-spatial inquiries about the world by analyzing large-scale repositories of geospatial images, including streetscapes (e.g., Google Street View), place-based photos (e.g., TripAdvisor, Yelp), and aerial imagery (e.g., satellite photos) combined with traditional GIS data sources. We define our vision, describe sensing and interaction approaches, provide three exemplars, and enumerate key challenges and opportunities for future work.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.15748v1" target="_blank">Response and Prompt Evaluation to Prevent Parasocial Relationships with Chatbots</a></h3>
                    <p><strong>Authors:</strong> Emma Rath, Stuart Armstrong, Rebecca Gorman</p>
                    <p><strong>Published:</strong> 8/21/2025</p>
                    <p><strong>Categories:</strong> cs.AI</p>
                    <p><strong>Summary:</strong> The development of parasocial relationships with AI agents has severe, and in some cases, tragic effects for human well-being. Yet preventing such dynamics is challenging: parasocial cues often emerge gradually in private conversations, and not all forms of emotional engagement are inherently harmful. We address this challenge by introducing a simple response evaluation framework, created by repurposing a state-of-the-art language model, that evaluates ongoing conversations for parasocial cues in real time. To test the feasibility of this approach, we constructed a small synthetic dataset of thirty dialogues spanning parasocial, sycophantic, and neutral conversations. Iterative evaluation with five stage testing successfully identified all parasocial conversations while avoiding false positives under a tolerant unanimity rule, with detection typically occurring within the first few exchanges. These findings provide preliminary evidence that evaluation agents can provide a viable solution for the prevention of parasocial relations.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.15737v1" target="_blank">Probability Density from Latent Diffusion Models for Out-of-Distribution Detection</a></h3>
                    <p><strong>Authors:</strong> Joonas JÃ¤rve, Karl Kaspar Haavel, Meelis Kull</p>
                    <p><strong>Published:</strong> 8/21/2025</p>
                    <p><strong>Categories:</strong> cs.LG, cs.CV</p>
                    <p><strong>Summary:</strong> Despite rapid advances in AI, safety remains the main bottleneck to deploying machine-learning systems. A critical safety component is out-of-distribution detection: given an input, decide whether it comes from the same distribution as the training data. In generative models, the most natural OOD score is the data likelihood. Actually, under the assumption of uniformly distributed OOD data, the likelihood is even the optimal OOD detector, as we show in this work. However, earlier work reported that likelihood often fails in practice, raising doubts about its usefulness. We explore whether, in practice, the representation space also suffers from the inability to learn good density estimation for OOD detection, or if it is merely a problem of the pixel space typically used in generative models. To test this, we trained a Variational Diffusion Model not on images, but on the representation space of a pre-trained ResNet-18 to assess the performance of our likelihood-based detector in comparison to state-of-the-art methods from the OpenOOD suite.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.15734v1" target="_blank">Measuring the environmental impact of delivering AI at Google Scale</a></h3>
                    <p><strong>Authors:</strong> Cooper Elsworth, Keguo Huang, David Patterson, Ian Schneider, Robert Sedivy, Savannah Goodman, Ben Townsend, Parthasarathy Ranganathan, Jeff Dean, Amin Vahdat, Ben Gomes, James Manyika</p>
                    <p><strong>Published:</strong> 8/21/2025</p>
                    <p><strong>Categories:</strong> cs.AI</p>
                    <p><strong>Summary:</strong> The transformative power of AI is undeniable - but as user adoption accelerates, so does the need to understand and mitigate the environmental impact of AI serving. However, no studies have measured AI serving environmental metrics in a production environment. This paper addresses this gap by proposing and executing a comprehensive methodology for measuring the energy usage, carbon emissions, and water consumption of AI inference workloads in a large-scale, AI production environment. Our approach accounts for the full stack of AI serving infrastructure - including active AI accelerator power, host system energy, idle machine capacity, and data center energy overhead. Through detailed instrumentation of Googles AI infrastructure for serving the Gemini AI assistant, we find the median Gemini Apps text prompt consumes 0.24 Wh of energy - a figure substantially lower than many public estimates. We also show that Googles software efficiency efforts and clean energy procurement have driven a 33x reduction in energy consumption and a 44x reduction in carbon footprint for the median Gemini Apps text prompt over one year. We identify that the median Gemini Apps text prompt uses less energy than watching nine seconds of television (0.24 Wh) and consumes the equivalent of five drops of water (0.26 mL). While these impacts are low compared to other daily activities, reducing the environmental impact of AI serving continues to warrant important attention. Towards this objective, we propose that a comprehensive measurement of AI serving environmental metrics is critical for accurately comparing models, and to properly incentivize efficiency gains across the full AI serving stack.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.15732v1" target="_blank">Understanding and Utilizing Dynamic Coupling in Free-Floating Space Manipulators for On-Orbit Servicing</a></h3>
                    <p><strong>Authors:</strong> Gargi Das, Daegyun Choi, Donghoon Kim</p>
                    <p><strong>Published:</strong> 8/21/2025</p>
                    <p><strong>Categories:</strong> cs.RO, cs.SY, eess.SY</p>
                    <p><strong>Summary:</strong> This study proposes a dynamic coupling-informed trajectory optimization algorithm for free-floating space manipulator systems (SMSs). Dynamic coupling between the base and the manipulator arms plays a critical role in influencing the systems behavior. While prior research has predominantly focused on minimizing this coupling, often overlooking its potential advantages, this work investigates how dynamic coupling can instead be leveraged to improve trajectory planning. Singular value decomposition (SVD) of the dynamic coupling matrix is employed to identify the dominant components governing coupling behavior. A quantitative metric is then formulated to characterize the strength and directionality of the coupling and is incorporated into a trajectory optimization framework. To assess the feasibility of the optimized trajectory, a sliding mode control-based tracking controller is designed to generate the required joint torque inputs. Simulation results demonstrate that explicitly accounting for dynamic coupling in trajectory planning enables more informed and potentially more efficient operation, offering new directions for the control of free-floating SMSs.</p>
                
            
                
                    <h3><a href="http://dx.doi.org/10.1145/3746059.3747779" target="_blank">Demystifying Reward Design in Reinforcement Learning for Upper Extremity Interaction: Practical Guidelines for Biomechanical Simulations in HCI</a></h3>
                    <p><strong>Authors:</strong> Hannah Selder, Florian Fischer, Per Ola Kristensson, Arthur Fleig</p>
                    <p><strong>Published:</strong> 8/21/2025</p>
                    <p><strong>Categories:</strong> cs.HC, H.5.2; F.m</p>
                    <p><strong>Summary:</strong> Designing effective reward functions is critical for reinforcement learning-based biomechanical simulations, yet HCI researchers and practitioners often waste (computation) time with unintuitive trial-and-error tuning. This paper demystifies reward function design by systematically analyzing the impact of effort minimization, task completion bonuses, and target proximity incentives on typical HCI tasks such as pointing, tracking, and choice reaction. We show that proximity incentives are essential for guiding movement, while completion bonuses ensure task success. Effort terms, though optional, help refine motion regularity when appropriately scaled. We perform an extensive analysis of how sensitive task success and completion time depend on the weights of these three reward components. From these results we derive practical guidelines to create plausible biomechanical simulations without the need for reinforcement learning expertise, which we then validate on remote control and keyboard typing tasks. This paper advances simulation-based interaction design and evaluation in HCI by improving the efficiency and applicability of biomechanical user modeling for real-world interface development.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.15724v1" target="_blank">Numerical models outperform AI weather forecasts of record-breaking extremes</a></h3>
                    <p><strong>Authors:</strong> Zhongwei Zhang, Erich Fischer, Jakob Zscheischler, Sebastian Engelke</p>
                    <p><strong>Published:</strong> 8/21/2025</p>
                    <p><strong>Categories:</strong> physics.ao-ph, cs.AI, stat.AP, J.2; I.6.4</p>
                    <p><strong>Summary:</strong> Artificial intelligence (AI)-based models are revolutionizing weather forecasting and have surpassed leading numerical weather prediction systems on various benchmark tasks. However, their ability to extrapolate and reliably forecast unprecedented extreme events remains unclear. Here, we show that for record-breaking weather extremes, the numerical model High RESolution forecast (HRES) from the European Centre for Medium-Range Weather Forecasts still consistently outperforms state-of-the-art AI models GraphCast, GraphCast operational, Pangu-Weather, Pangu-Weather operational, and Fuxi. We demonstrate that forecast errors in AI models are consistently larger for record-breaking heat, cold, and wind than in HRES across nearly all lead times. We further find that the examined AI models tend to underestimate both the frequency and intensity of record-breaking events, and they underpredict hot records and overestimate cold records with growing errors for larger record exceedance. Our findings underscore the current limitations of AI weather models in extrapolating beyond their training domain and in forecasting the potentially most impactful record-breaking weather events that are particularly frequent in a rapidly warming climate. Further rigorous verification and model development is needed before these models can be solely relied upon for high-stakes applications such as early warning systems and disaster management.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.15722v1" target="_blank">The Status of the Astrophysical Parameters of Upper Main Sequence Stars</a></h3>
                    <p><strong>Authors:</strong> Lukas KueÃŸ, Ernst Paunzen</p>
                    <p><strong>Published:</strong> 8/21/2025</p>
                    <p><strong>Categories:</strong> astro-ph.SR, astro-ph.GA</p>
                    <p><strong>Summary:</strong> Calibrating the ages, masses, and radii of stars on the upper main sequence depends heavily on accurate measurements of the effective temperature ($T_\mathrm{eff}$) and surface gravity ($\log g$). These parameters are difficult to obtain meticulously due to the nature of hot stars, which exhibit features such as rapid rotation, atomic diffusion, pulsation, and stellar winds. We compare the $T_\mathrm{eff}$, and $\log g$, values of apparent normal B-F stars in four recent catalogues that employ different methods and pipelines to obtain these parameters. We derived various statistical parameters to compare the differences between the catalogues and discussed the astrophysical implications of these differences. Our results show that the huge differences in $T_\mathrm{eff}$, (up to $10^4$\,K) and $\log g$, (up to 2 dex) between the catalogues have serious implications on the determination of ages, masses, and radii of the stars in question. We conclude that there appears to be no homogeneous set of stellar parameters on the upper main sequence, and one must be cautious when interpreting results obtained from using only one of the catalogues. The homogenisation of said parameters is an essential task for the future and will have a significant impact on astrophysical research dealing with stars on the upper main sequence.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.15719v1" target="_blank">Tutorial on the Probabilistic Unification of Estimation Theory, Machine Learning, and Generative AI</a></h3>
                    <p><strong>Authors:</strong> Mohammed Elmusrati</p>
                    <p><strong>Published:</strong> 8/21/2025</p>
                    <p><strong>Categories:</strong> cs.LG, cs.AI</p>
                    <p><strong>Summary:</strong> Extracting meaning from uncertain, noisy data is a fundamental problem across time series analysis, pattern recognition, and language modeling. This survey presents a unified mathematical framework that connects classical estimation theory, statistical inference, and modern machine learning, including deep learning and large language models. By analyzing how techniques such as maximum likelihood estimation, Bayesian inference, and attention mechanisms address uncertainty, the paper illustrates that many AI methods are rooted in shared probabilistic principles. Through illustrative scenarios including system identification, image classification, and language generation, we show how increasingly complex models build upon these foundations to tackle practical challenges like overfitting, data sparsity, and interpretability. In other words, the work demonstrates that maximum likelihood, MAP estimation, Bayesian classification, and deep learning all represent different facets of a shared goal: inferring hidden causes from noisy and/or biased observations. It serves as both a theoretical synthesis and a practical guide for students and researchers navigating the evolving landscape of machine learning.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.15716v1" target="_blank">Foundation Models for Cross-Domain EEG Analysis Application: A Survey</a></h3>
                    <p><strong>Authors:</strong> Hongqi Li, Yitong Chen, Yujuan Wang, Weihang Ni, Haodong Zhang</p>
                    <p><strong>Published:</strong> 8/21/2025</p>
                    <p><strong>Categories:</strong> cs.HC, cs.AI</p>
                    <p><strong>Summary:</strong> Electroencephalography (EEG) analysis stands at the forefront of neuroscience and artificial intelligence research, where foundation models are reshaping the traditional EEG analysis paradigm by leveraging their powerful representational capacity and cross-modal generalization. However, the rapid proliferation of these techniques has led to a fragmented research landscape, characterized by diverse model roles, inconsistent architectures, and a lack of systematic categorization. To bridge this gap, this study presents the first comprehensive modality-oriented taxonomy for foundation models in EEG analysis, systematically organizing research advances based on output modalities of the native EEG decoding, EEG-text, EEG-vision, EEG-audio, and broader multimodal frameworks. We rigorously analyze each categorys research ideas, theoretical foundations, and architectural innovations, while highlighting open challenges such as model interpretability, cross-domain generalization, and real-world applicability in EEG-based systems. By unifying this dispersed field, our work not only provides a reference framework for future methodology development but accelerates the translation of EEG foundation models into scalable, interpretable, and online actionable solutions.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.15711v1" target="_blank">Stemming -- The Evolution and Current State with a Focus on Bangla</a></h3>
                    <p><strong>Authors:</strong> Abhijit Paul, Mashiat Amin Farin, Sharif Md. Abdullah, Ahmedul Kabir, Zarif Masud, Shebuti Rayana</p>
                    <p><strong>Published:</strong> 8/21/2025</p>
                    <p><strong>Categories:</strong> cs.CL, cs.IR</p>
                    <p><strong>Summary:</strong> Bangla, the seventh most widely spoken language worldwide with 300 million native speakers, faces digital under-representation due to limited resources and lack of annotated datasets. Stemming, a critical preprocessing step in language analysis, is essential for low-resource, highly-inflectional languages like Bangla, because it can reduce the complexity of algorithms and models by significantly reducing the number of words the algorithm needs to consider. This paper conducts a comprehensive survey of stemming approaches, emphasizing the importance of handling morphological variants effectively. While exploring the landscape of Bangla stemming, it becomes evident that there is a significant gap in the existing literature. The paper highlights the discontinuity from previous research and the scarcity of accessible implementations for replication. Furthermore, it critiques the evaluation methodologies, stressing the need for more relevant metrics. In the context of Banglas rich morphology and diverse dialects, the paper acknowledges the challenges it poses. To address these challenges, the paper suggests directions for Bangla stemmer development. It concludes by advocating for robust Bangla stemmers and continued research in the field to enhance language analysis and processing.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.15700v1" target="_blank">Detection of non-absolute separability in quantum states and channels through moments</a></h3>
                    <p><strong>Authors:</strong> Bivas Mallick, Saheli Mukherjee, Nirman Ganguly, A. S. Majumdar</p>
                    <p><strong>Published:</strong> 8/21/2025</p>
                    <p><strong>Categories:</strong> quant-ph</p>
                    <p><strong>Summary:</strong> In quantum information and computation, generation of entanglement through unitary gates remains a significant and active area of research. However, there are states termed as absolutely separable, from which entanglement cannot be created through any non-local unitary action. Thus, from a resource-theoretic perspective, non-absolutely separable states are useful as they can be turned into entangled states using some appropriate unitary gates. In this work, we propose an efficient method to detect non-absolutely separable states. Our approach relies on evaluating moments that can bypass the need for full state tomography, thereby enhancing its practical applicability. We then present several examples in support of our detection scheme. We also address a closely related problem concerning states whose partial transpose remains positive under any arbitrary non-local unitary action. Furthermore, we examine the effectiveness of our moment-based approach in the detection of quantum channels that are not absolutely separating, which entails the detection of resource preserving channels. Finally, we demonstrate the operational significance of non-absolutely separable states by proving that every such state can provide an advantage in a quantum-channel discrimination task.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.15693v1" target="_blank">NiceWebRL: a Python library for human subject experiments with reinforcement learning environments</a></h3>
                    <p><strong>Authors:</strong> Wilka Carvalho, Vikram Goddla, Ishaan Sinha, Hoon Shin, Kunal Jha</p>
                    <p><strong>Published:</strong> 8/21/2025</p>
                    <p><strong>Categories:</strong> cs.AI</p>
                    <p><strong>Summary:</strong> We present NiceWebRL, a research tool that enables researchers to use machine reinforcement learning (RL) environments for online human subject experiments. NiceWebRL is a Python library that allows any Jax-based environment to be transformed into an online interface, supporting both single-agent and multi-agent environments. As such, NiceWebRL enables AI researchers to compare their algorithms to human performance, cognitive scientists to test ML algorithms as theories for human cognition, and multi-agent researchers to develop algorithms for human-AI collaboration. We showcase NiceWebRL with 3 case studies that demonstrate its potential to help develop Human-like AI, Human-compatible AI, and Human-assistive AI. In the first case study (Human-like AI), NiceWebRL enables the development of a novel RL model of cognition. Here, NiceWebRL facilitates testing this model against human participants in both a grid world and Craftax, a 2D Minecraft domain. In our second case study (Human-compatible AI), NiceWebRL enables the development of a novel multi-agent RL algorithm that can generalize to human partners in the Overcooked domain. Finally, in our third case study (Human-assistive AI), we show how NiceWebRL can allow researchers to study how an LLM can assist humans on complex tasks in XLand-Minigrid, an environment with millions of hierarchical tasks. The library is available at https://github.com/KempnerInstitute/nicewebrl.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.15680v1" target="_blank">Futurity as Infrastructure: A Techno-Philosophical Interpretation of the AI Lifecycle</a></h3>
                    <p><strong>Authors:</strong> Mark Cote, Susana Aires</p>
                    <p><strong>Published:</strong> 8/21/2025</p>
                    <p><strong>Categories:</strong> cs.AI, cs.HC, I.2.6; I.2.11; K.4.1; K.6.0</p>
                    <p><strong>Summary:</strong> This paper argues that a techno-philosophical reading of the EU AI Act provides insight into the long-term dynamics of data in AI systems, specifically, how the lifecycle from ingestion to deployment generates recursive value chains that challenge existing frameworks for Responsible AI. We introduce a conceptual tool to frame the AI pipeline, spanning data, training regimes, architectures, feature stores, and transfer learning. Using cross-disciplinary methods, we develop a technically grounded and philosophically coherent analysis of regulatory blind spots. Our central claim is that what remains absent from policymaking is an account of the dynamic of becoming that underpins both the technical operation and economic logic of AI. To address this, we advance a formal reading of AI inspired by Simondonian philosophy of technology, reworking his concept of individuation to model the AI lifecycle, including the pre-individual milieu, individuation, and individuated AI. To translate these ideas, we introduce futurity: the self-reinforcing lifecycle of AI, where more data enhances performance, deepens personalisation, and expands application domains. Futurity highlights the recursively generative, non-rivalrous nature of data, underpinned by infrastructures like feature stores that enable feedback, adaptation, and temporal recursion. Our intervention foregrounds escalating power asymmetries, particularly the tech oligarchy whose infrastructures of capture, training, and deployment concentrate value and decision-making. We argue that effective regulation must address these infrastructural and temporal dynamics, and propose measures including lifecycle audits, temporal traceability, feedback accountability, recursion transparency, and a right to contest recursive reuse.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.15679v1" target="_blank">An Efficient Open World Environment for Multi-Agent Social Learning</a></h3>
                    <p><strong>Authors:</strong> Eric Ye, Ren Tao, Natasha Jaques</p>
                    <p><strong>Published:</strong> 8/21/2025</p>
                    <p><strong>Categories:</strong> cs.LG</p>
                    <p><strong>Summary:</strong> Many challenges remain before AI agents can be deployed in real-world environments. However, one virtue of such environments is that they are inherently multi-agent and contain human experts. Using advanced social intelligence in such an environment can help an AI agent learn adaptive skills and behaviors that a known expert exhibits. While social intelligence could accelerate training, it is currently difficult to study due to the lack of open-ended multi-agent environments. In this work, we present an environment in which multiple self-interested agents can pursue complex and independent goals, reflective of real world challenges. This environment will enable research into the development of socially intelligent AI agents in open-ended multi-agent settings, where agents may be implicitly incentivized to cooperate to defeat common enemies, build and share tools, and achieve long horizon goals. In this work, we investigate the impact on agent performance due to social learning in the presence of experts and implicit cooperation such as emergent collaborative tool use, and whether agents can benefit from either cooperation or competition in this environment.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.15672v1" target="_blank">CM2LoD3: Reconstructing LoD3 Building Models Using Semantic Conflict Maps</a></h3>
                    <p><strong>Authors:</strong> Franz Hanke, Antonia Bieringer, Olaf Wysocki, Boris Jutzi</p>
                    <p><strong>Published:</strong> 8/21/2025</p>
                    <p><strong>Categories:</strong> cs.CV, eess.IV</p>
                    <p><strong>Summary:</strong> Detailed 3D building models are crucial for urban planning, digital twins, and disaster management applications. While Level of Detail 1 (LoD)1 and LoD2 building models are widely available, they lack detailed facade elements essential for advanced urban analysis. In contrast, LoD3 models address this limitation by incorporating facade elements such as windows, doors, and underpasses. However, their generation has traditionally required manual modeling, making large-scale adoption challenging. In this contribution, CM2LoD3, we present a novel method for reconstructing LoD3 building models leveraging Conflict Maps (CMs) obtained from ray-to-model-prior analysis. Unlike previous works, we concentrate on semantically segmenting real-world CMs with synthetically generated CMs from our developed Semantic Conflict Map Generator (SCMG). We also observe that additional segmentation of textured models can be fused with CMs using confidence scores to further increase segmentation performance and thus increase 3D reconstruction accuracy. Experimental results demonstrate the effectiveness of our CM2LoD3 method in segmenting and reconstructing building openings, with the 61% performance with uncertainty-aware fusion of segmented building textures. This research contributes to the advancement of automated LoD3 model reconstruction, paving the way for scalable and efficient 3D city modeling. Our project is available: https://github.com/InFraHank/CM2LoD3</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.15663v1" target="_blank">Mind and Motion Aligned: A Joint Evaluation IsaacSim Benchmark for Task Planning and Low-Level Policies in Mobile Manipulation</a></h3>
                    <p><strong>Authors:</strong> Nikita Kachaev, Andrei Spiridonov, Andrey Gorodetsky, Kirill Muravyev, Nikita Oskolkov, Aditya Narendra, Vlad Shakhuro, Dmitry Makarov, Aleksandr I. Panov, Polina Fedotova, Alexey K. Kovalev</p>
                    <p><strong>Published:</strong> 8/21/2025</p>
                    <p><strong>Categories:</strong> cs.RO, cs.AI</p>
                    <p><strong>Summary:</strong> Benchmarks are crucial for evaluating progress in robotics and embodied AI. However, a significant gap exists between benchmarks designed for high-level language instruction following, which often assume perfect low-level execution, and those for low-level robot control, which rely on simple, one-step commands. This disconnect prevents a comprehensive evaluation of integrated systems where both task planning and physical execution are critical. To address this, we propose Kitchen-R, a novel benchmark that unifies the evaluation of task planning and low-level control within a simulated kitchen environment. Built as a digital twin using the Isaac Sim simulator and featuring more than 500 complex language instructions, Kitchen-R supports a mobile manipulator robot. We provide baseline methods for our benchmark, including a task-planning strategy based on a vision-language model and a low-level control policy based on diffusion policy. We also provide a trajectory collection system. Our benchmark offers a flexible framework for three evaluation modes: independent assessment of the planning module, independent assessment of the control policy, and, crucially, an integrated evaluation of the whole system. Kitchen-R bridges a key gap in embodied AI research, enabling more holistic and realistic benchmarking of language-guided robotic agents.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.15658v1" target="_blank">Benchmarking Computer Science Survey Generation</a></h3>
                    <p><strong>Authors:</strong> Weihang Su, Anzhe Xie, Qingyao Ai, Jianming Long, Jiaxin Mao, Ziyi Ye, Yiqun Liu</p>
                    <p><strong>Published:</strong> 8/21/2025</p>
                    <p><strong>Categories:</strong> cs.CL, cs.AI, cs.IR</p>
                    <p><strong>Summary:</strong> Scientific survey articles play a vital role in summarizing research progress, yet their manual creation is becoming increasingly infeasible due to the rapid growth of academic literature. While large language models (LLMs) offer promising capabilities for automating this process, progress in this area is hindered by the absence of standardized benchmarks and evaluation protocols. To address this gap, we introduce SurGE (Survey Generation Evaluation), a new benchmark for evaluating scientific survey generation in the computer science domain. SurGE consists of (1) a collection of test instances, each including a topic description, an expert-written survey, and its full set of cited references, and (2) a large-scale academic corpus of over one million papers that serves as the retrieval pool. In addition, we propose an automated evaluation framework that measures generated surveys across four dimensions: information coverage, referencing accuracy, structural organization, and content quality. Our evaluation of diverse LLM-based approaches shows that survey generation remains highly challenging, even for advanced self-reflection frameworks. These findings highlight the complexity of the task and the necessity for continued research. We have open-sourced all the code, data, and models at: https://github.com/oneal2000/SurGE</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.15648v1" target="_blank">SDGO: Self-Discrimination-Guided Optimization for Consistent Safety in Large Language Models</a></h3>
                    <p><strong>Authors:</strong> Peng Ding, Wen Sun, Dailin Li, Wei Zou, Jiaming Wang, Jiajun Chen, Shujian Huang</p>
                    <p><strong>Published:</strong> 8/21/2025</p>
                    <p><strong>Categories:</strong> cs.CL</p>
                    <p><strong>Summary:</strong> Large Language Models (LLMs) excel at various natural language processing tasks but remain vulnerable to jailbreaking attacks that induce harmful content generation. In this paper, we reveal a critical safety inconsistency: LLMs can more effectively identify harmful requests as discriminators than defend against them as generators. This insight inspires us to explore aligning the models inherent discrimination and generation capabilities. To this end, we propose SDGO (Self-Discrimination-Guided Optimization), a reinforcement learning framework that leverages the models own discrimination capabilities as a reward signal to enhance generation safety through iterative self-improvement. Our method does not require any additional annotated data or external models during the training phase. Extensive experiments demonstrate that SDGO significantly improves model safety compared to both prompt-based and training-based baselines while maintaining helpfulness on general benchmarks. By aligning LLMs discrimination and generation capabilities, SDGO brings robust performance against out-of-distribution (OOD) jailbreaking attacks. This alignment achieves tighter coupling between these two capabilities, enabling the models generation capability to be further enhanced with only a small amount of discriminative samples. Our code and datasets are available at https://github.com/NJUNLP/SDGO.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.15643v1" target="_blank">Reading Between the Lines: A Study of Thematic Bias in Book Recommender Systems</a></h3>
                    <p><strong>Authors:</strong> Nityaa Kalra, Savvina Daniil</p>
                    <p><strong>Published:</strong> 8/21/2025</p>
                    <p><strong>Categories:</strong> cs.IR</p>
                    <p><strong>Summary:</strong> Recommender systems help users discover new content, but can also reinforce existing biases, leading to unfair exposure and reduced diversity. This paper introduces and investigates thematic bias in book recommendations, defined as a disproportionate favouring or neglect of certain book themes. We adopt a multi-stage bias evaluation framework using the Book-Crossing dataset to evaluate thematic bias in recommendations and its impact on different user groups. Our findings show that thematic bias originates from content imbalances and is amplified by user engagement patterns. By segmenting users based on their thematic preferences, we find that users with niche and long-tail interests receive less personalised recommendations, whereas users with diverse interests receive more consistent recommendations. These findings suggest that recommender systems should be carefully designed to accommodate a broader range of user interests. By contributing to the broader goal of responsible AI, this work also lays the groundwork for extending thematic bias analysis to other domains.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.15637v1" target="_blank">Classification errors distort findings in automated speech processing: examples and solutions from child-development research</a></h3>
                    <p><strong>Authors:</strong> Lucas Gautheron, Evan Kidd, Anton Malko, Marvin Lavechin, Alejandrina Cristia</p>
                    <p><strong>Published:</strong> 8/21/2025</p>
                    <p><strong>Categories:</strong> cs.LG, cs.CL, stat.AP</p>
                    <p><strong>Summary:</strong> With the advent of wearable recorders, scientists are increasingly turning to automated methods of analysis of audio and video data in order to measure childrens experience, behavior, and outcomes, with a sizable literature employing long-form audio-recordings to study language acquisition. While numerous articles report on the accuracy and reliability of the most popular automated classifiers, less has been written on the downstream effects of classification errors on measurements and statistical inferences (e.g., the estimate of correlations and effect sizes in regressions). This paper proposes a Bayesian approach to study the effects of algorithmic errors on key scientific questions, including the effect of siblings on childrens language experience and the association between childrens production and their input. In both the most commonly used \gls{lena}, and an open-source alternative (the Voice Type Classifier from the ACLEW system), we find that classification errors can significantly distort estimates. For instance, automated annotations underestimated the negative effect of siblings on adult input by 20--80\%, potentially placing it below statistical significance thresholds. We further show that a Bayesian calibration approach for recovering unbiased estimates of effect sizes can be effective and insightful, but does not provide a fool-proof solution. Both the issue reported and our solution may apply to any classifier involving event detection and classification with non-zero error rates.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.15635v1" target="_blank">Label Uncertainty for Ultrasound Segmentation</a></h3>
                    <p><strong>Authors:</strong> Malini Shivaram, Gautam Rajendrakumar Gare, Laura Hutchins, Jacob Duplantis, Thomas Deiss, Thales Nogueira Gomes, Thong Tran, Keyur H. Patel, Thomas H Fox, Amita Krishnan, Deva Ramanan, Bennett DeBoisblanc, Ricardo Rodriguez, John Galeotti</p>
                    <p><strong>Published:</strong> 8/21/2025</p>
                    <p><strong>Categories:</strong> eess.IV, cs.AI, cs.CV, cs.LG, stat.ML</p>
                    <p><strong>Summary:</strong> In medical imaging, inter-observer variability among radiologists often introduces label uncertainty, particularly in modalities where visual interpretation is subjective. Lung ultrasound (LUS) is a prime example-it frequently presents a mixture of highly ambiguous regions and clearly discernible structures, making consistent annotation challenging even for experienced clinicians. In this work, we introduce a novel approach to both labeling and training AI models using expert-supplied, per-pixel confidence values. Rather than treating annotations as absolute ground truth, we design a data annotation protocol that captures the confidence that radiologists have in each labeled region, modeling the inherent aleatoric uncertainty present in real-world clinical data. We demonstrate that incorporating these confidence values during training leads to improved segmentation performance. More importantly, we show that this enhanced segmentation quality translates into better performance on downstream clinically-critical tasks-specifically, estimating S/F oxygenation ratio values, classifying S/F ratio change, and predicting 30-day patient readmission. While we empirically evaluate many methods for exposing the uncertainty to the learning model, we find that a simple approach that trains a model on binarized labels obtained with a (60%) confidence threshold works well. Importantly, high thresholds work far better than a naive approach of a 50% threshold, indicating that training on very confident pixels is far more effective. Our study systematically investigates the impact of training with varying confidence thresholds, comparing not only segmentation metrics but also downstream clinical outcomes. These results suggest that label confidence is a valuable signal that, when properly leveraged, can significantly enhance the reliability and clinical utility of AI in medical imaging.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.15620v1" target="_blank">Low-Power Control of Resistance Switching Transitions in First-Order Memristors</a></h3>
                    <p><strong>Authors:</strong> Valeriy A. Slipko, Alon Ascoli, Fernando Corinto, Yuriy V. Pershin</p>
                    <p><strong>Published:</strong> 8/21/2025</p>
                    <p><strong>Categories:</strong> cs.ET, cond-mat.mes-hall</p>
                    <p><strong>Summary:</strong> In many cases, the behavior of physical memristive devices can be relatively well captured by using a single internal state variable. This study investigates the low-power control of first-order memristive devices to derive the most energy-efficient protocols for programming their resistances. A unique yet general approach to optimizing the switching transitions in devices of this kind is introduced. For pedagogical purposes, without loss of generality, the proposed control paradigm is applied to a couple of differential algebraic equation sets for voltage-controlled devices, specifically Kvatinskys Voltage ThrEshold Adaptive Memristor mathematical description and Mirandas and Sunes dynamic balance model. It is demonstrated that, depending upon intrinsic physical properties of the device, captured in the model formulas and parameter setting, and upon constraints on programming time and voltages, the optimal protocol for either of the two switching scenarios may require the application of a single square voltage pulse of height set to a certain level within the admissible range across a fraction or entire given programming time interval, or of some more involved voltage stimulus of unique polarity, including analogue continuous waveforms that can be approximated by trains of square voltage pulses of different heights, over the entire programming time interval. The practical implications of these research findings are significant, as the development of energy-efficient protocols to program memristive devices, resolving the so-called voltage-time dilemma in the device physics community, is a subject under intensive and extensive studies across the academic community and industry.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.15610v1" target="_blank">Transduction is All You Need for Structured Data Workflows</a></h3>
                    <p><strong>Authors:</strong> Alfio Gliozzo, Naweed Khan, Christodoulos Constantinides, Nandana Mihindukulasooriya, Nahuel Defosse, Junkyu Lee</p>
                    <p><strong>Published:</strong> 8/21/2025</p>
                    <p><strong>Categories:</strong> cs.AI, cs.LG</p>
                    <p><strong>Summary:</strong> This paper introduces Agentics, a modular framework for building agent-based systems capable of structured reasoning and compositional generalization over complex data. Designed with research and practical applications in mind, Agentics offers a novel perspective on working with data and AI workflows. In this framework, agents are abstracted from the logical flow and they are used internally to the data type to enable logical transduction among data. Agentics encourages AI developers to focus on modeling data rather than crafting prompts, enabling a declarative language in which data types are provided by LLMs and composed through logical transduction, which is executed by LLMs when types are connected. We provide empirical evidence demonstrating the applicability of this framework across domain-specific multiple-choice question answering, semantic parsing for text-to-SQL, and automated prompt optimization tasks, achieving state-of-the-art accuracy or improved scalability without sacrificing performance. The open-source implementation is available at \texttt{https://github.com/IBM/agentics}.</p>
                
            
                
                    <h3><a href="http://dx.doi.org/10.1016/j.jmst.2024.11.026" target="_blank">Ultrastrong and ductile CoNiMoAl medium-entropy alloys enabled by L12 nanoprecipitate-induced multiple deformation mechanisms</a></h3>
                    <p><strong>Authors:</strong> Min Young Sung, Tae Jin Jang, Sang Yoon Song, Gunjick Lee, KenHee Ryou, Sang-Ho Oh, Byeong-Joo Lee, Pyuck-Pa Choi, JÃ¶rg Neugebauer, Blazej Grabowski, Fritz KÃ¶rmann, Yuji Ikeda, Alireza Zargaran, Seok Su Sohn</p>
                    <p><strong>Published:</strong> 8/21/2025</p>
                    <p><strong>Categories:</strong> cond-mat.mtrl-sci</p>
                    <p><strong>Summary:</strong> L12 precipitates are known to significantly enhance the strength and ductility of single-phase face-centered cubic (FCC) medium- or high-entropy alloys (M/HEAs). However, further improvements in mechanical properties remain untapped, as alloy design has historically focused on systems with specific CrCoNi- or FeCoCrNi-based FCC matrix and Ni3Al L12 phase compositions. This study introduces novel Co-Ni-Mo-Al alloys with L12 precipitates by systematically altering Al content, aiming to bridge this research gap by revealing the strengthening mechanisms. The (CoNi)81Mo12Al7 alloy achieves yield strength of 1086 MPa, tensile strength of 1520 MPa, and ductility of 35 %, demonstrating an impressive synergy of strength, ductility, and strain-hardening capacity. Dislocation analysis via transmission electron microscopy, supported by generalized stacking fault energy (GSFE) calculations using density functional theory (DFT), demonstrates that Mo substitution for Al in the L12 phase alters dislocation behavior, promoting the formation of multiple deformation modes, including stacking faults, super-dislocation pairs, Lomer-Cottrell locks, and unusual nano-twin formation even at low strains. These behaviors are facilitated by the low stacking fault energy (SFE) of the FCC matrix, overlapping of SFs, and dislocation dissociation across anti-phase boundaries (APBs). The increased energy barrier for superlattice intrinsic stacking fault (SISF) formation compared to APBs, due to Mo substitution, further influences dislocation activity. This work demonstrates a novel strategy for designing high-performance M/HEAs by expanding the range of FCC matrix and L12 compositions through precipitation hardening.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.15595v1" target="_blank">Interface on demand: Towards AI native Control interfaces for 6G</a></h3>
                    <p><strong>Authors:</strong> Abhishek Dandekar, Prashiddha D. Thapa, Ashrafur Rahman, Julius Schulz-Zander</p>
                    <p><strong>Published:</strong> 8/21/2025</p>
                    <p><strong>Categories:</strong> cs.NI</p>
                    <p><strong>Summary:</strong> Traditional standardized network interfaces face significant limitations, including vendor-specific incompatibilities, rigid design assumptions, and lack of adaptability for new functionalities. We propose a multi-agent framework leveraging large language models (LLMs) to generate control interfaces on demand between network functions (NFs). This includes a matching agent, which aligns required control functionalities with NF capabilities, and a code-generation agent, which generates the necessary API server for interface realization. We validate our approach using simulated multi-vendor gNB and WLAN AP environments. The performance evaluations highlight the trade-offs between cost and latency across LLMs for interface generation tasks. Our work sets the foundation for AI-native dynamic control interface generation, paving the way for enhanced interoperability and adaptability in future mobile networks.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.15588v1" target="_blank">A Dynamical Systems Framework for Reinforcement Learning Safety and Robustness Verification</a></h3>
                    <p><strong>Authors:</strong> Ahmed Nasir, Abdelhafid Zenati</p>
                    <p><strong>Published:</strong> 8/21/2025</p>
                    <p><strong>Categories:</strong> cs.AI</p>
                    <p><strong>Summary:</strong> The application of reinforcement learning to safety-critical systems is limited by the lack of formal methods for verifying the robustness and safety of learned policies. This paper introduces a novel framework that addresses this gap by analyzing the combination of an RL agent and its environment as a discrete-time autonomous dynamical system. By leveraging tools from dynamical systems theory, specifically the Finite-Time Lyapunov Exponent (FTLE), we identify and visualize Lagrangian Coherent Structures (LCS) that act as the hidden skeleton governing the systems behavior. We demonstrate that repelling LCS function as safety barriers around unsafe regions, while attracting LCS reveal the systems convergence properties and potential failure modes, such as unintended trap states. To move beyond qualitative visualization, we introduce a suite of quantitative metrics, Mean Boundary Repulsion (MBR), Aggregated Spurious Attractor Strength (ASAS), and Temporally-Aware Spurious Attractor Strength (TASAS), to formally measure a policys safety margin and robustness. We further provide a method for deriving local stability guarantees and extend the analysis to handle model uncertainty. Through experiments in both discrete and continuous control environments, we show that this framework provides a comprehensive and interpretable assessment of policy behavior, successfully identifying critical flaws in policies that appear successful based on reward alone.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.15570v1" target="_blank">Establishing Technical Debt Management -- A Five-Step Workshop Approach and an Action Research Study</a></h3>
                    <p><strong>Authors:</strong> Marion Wiese, Kamila Serwa, Anastasia Besier, Ariane S. Marion-Jetten, Eva Bittner</p>
                    <p><strong>Published:</strong> 8/21/2025</p>
                    <p><strong>Categories:</strong> cs.SE</p>
                    <p><strong>Summary:</strong> Context. Technical debt (TD) items are constructs in a software system providing short-term benefits but hindering future changes. TD management (TDM) is frequently researched but rarely adopted in practice. Goal. This study aimed to establish a TDM process in an IT company based on a predefined workshop concept. We analyzed which research approaches practitioners adopted for each TD activity and the TDMs long-term effect on TD awareness. Method. We used action research (five action cycles in 16 months) with an IT team that creates IT solutions for signal processing. To examine TD awareness, we (1) analyzed questionnaires completed during each workshop, (2) observed team meetings, (3) adopted a method from psychology for measuring awareness in decision-making situations called TD-SAGAT, and (4) evaluated the backlog data. Results. Practitioners preferred TD repayment and prioritization based on the systems evolution and cost calculations, i.e., repayment of so-called low-hanging fruits. Reminders in the backlog items, such as checkboxes or text templates, led to a sustainable rise in TD awareness. Conclusions. We showed that a workshop-based approach is feasible and leads to sustainable process changes. New ideas for TDM applicable to other IT teams emerged, e.g., using a re-submission date, using a Talked about TD checkbox, and using visualizations for TD prioritization.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.15569v1" target="_blank">Conformalized Exceptional Model Mining: Telling Where Your Model Performs (Not) Well</a></h3>
                    <p><strong>Authors:</strong> Xin Du, Sikun Yang, Wouter Duivesteijn, Mykola Pechenizkiy</p>
                    <p><strong>Published:</strong> 8/21/2025</p>
                    <p><strong>Categories:</strong> cs.LG</p>
                    <p><strong>Summary:</strong> Understanding the nuanced performance of machine learning models is essential for responsible deployment, especially in high-stakes domains like healthcare and finance. This paper introduces a novel framework, Conformalized Exceptional Model Mining, which combines the rigor of Conformal Prediction with the explanatory power of Exceptional Model Mining (EMM). The proposed framework identifies cohesive subgroups within data where model performance deviates exceptionally, highlighting regions of both high confidence and high uncertainty. We develop a new model class, mSMoPE (multiplex Soft Model Performance Evaluation), which quantifies uncertainty through conformal predictions rigorous coverage guarantees. By defining a new quality measure, Relative Average Uncertainty Loss (RAUL), our framework isolates subgroups with exceptional performance patterns in multi-class classification and regression tasks. Experimental results across diverse datasets demonstrate the frameworks effectiveness in uncovering interpretable subgroups that provide critical insights into model behavior. This work lays the groundwork for enhancing model interpretability and reliability, advancing the state-of-the-art in explainable AI and uncertainty quantification.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.15565v1" target="_blank">Any-to-any Speaker Attribute Perturbation for Asynchronous Voice Anonymization</a></h3>
                    <p><strong>Authors:</strong> Liping Chen, Chenyang Guo, Rui Wang, Kong Aik Lee, Zhenhua Ling</p>
                    <p><strong>Published:</strong> 8/21/2025</p>
                    <p><strong>Categories:</strong> cs.SD</p>
                    <p><strong>Summary:</strong> Speaker attribute perturbation offers a feasible approach to asynchronous voice anonymization by employing adversarially perturbed speech as anonymized output. In order to enhance the identity unlinkability among anonymized utterances from the same original speaker, the targeted attack training strategy is usually applied to anonymize the utterances to a common designated speaker. However, this strategy may violate the privacy of the designated speaker who is an actual speaker. To mitigate this risk, this paper proposes an any-to-any training strategy. It is accomplished by defining a batch mean loss to anonymize the utterances from various speakers within a training mini-batch to a common pseudo-speaker, which is approximated as the average speaker in the mini-batch. Based on this, a speaker-adversarial speech generation model is proposed, incorporating the supervision from both the untargeted attack and the any-to-any strategies. The speaker attribute perturbations are generated and incorporated into the original speech to produce its anonymized version. The effectiveness of the proposed model was justified in asynchronous voice anonymization through experiments conducted on the VoxCeleb datasets. Additional experiments were carried out to explore the potential limitations of speaker-adversarial speech in voice privacy protection. With them, we aim to provide insights for future research on its protective efficacy against black-box speaker extractors \textcolor{black}{and adaptive attacks, as well as} generalization to out-of-domain datasets \textcolor{black}{and stability}. Audio samples and open-source code are published in https://github.com/VoicePrivacy/any-to-any-speaker-attribute-perturbation.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.15554v1" target="_blank">Uncertainty Relation for the Wigner-Yanase Skew Information and Quantum Sobolev Inequalities</a></h3>
                    <p><strong>Authors:</strong> Laurent Lafleche</p>
                    <p><strong>Published:</strong> 8/21/2025</p>
                    <p><strong>Categories:</strong> math-ph, math.FA, math.MP, quant-ph, 81S07, 46E35, 81S30 (Primary) 47A30 (Secondary)</p>
                    <p><strong>Summary:</strong> This note explores uncertainty inequalities for quantum analogues of the Fisher information including the Wigner-Yanase skew information, and their connection to the quantum Sobolev inequalities proved by the author in [Journal of Functional Analysis, 286 (10) 2024]. Some additional inequalities concerning commutators are derived and others are left as open problems.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.15550v1" target="_blank">AI-Powered Machine Learning Approaches for Fault Diagnosis in Industrial Pumps</a></h3>
                    <p><strong>Authors:</strong> Khaled M. A. Alghtus, Ayad Gannan, Khalid M. Alhajri, Ali L. A. Al Jubouri, Hassan A. I. Al-Janahi</p>
                    <p><strong>Published:</strong> 8/21/2025</p>
                    <p><strong>Categories:</strong> cs.LG</p>
                    <p><strong>Summary:</strong> This study presents a practical approach for early fault detection in industrial pump systems using real-world sensor data from a large-scale vertical centrifugal pump operating in a demanding marine environment. Five key operational parameters were monitored: vibration, temperature, flow rate, pressure, and electrical current. A dual-threshold labeling method was applied, combining fixed engineering limits with adaptive thresholds calculated as the 95th percentile of historical sensor values. To address the rarity of documented failures, synthetic fault signals were injected into the data using domain-specific rules, simulating critical alerts within plausible operating ranges. Three machine learning classifiers - Random Forest, Extreme Gradient Boosting (XGBoost), and Support Vector Machine (SVM) - were trained to distinguish between normal operation, early warnings, and critical alerts. Results showed that Random Forest and XGBoost models achieved high accuracy across all classes, including minority cases representing rare or emerging faults, while the SVM model exhibited lower sensitivity to anomalies. Visual analyses, including grouped confusion matrices and time-series plots, indicated that the proposed hybrid method provides robust detection capabilities. The framework is scalable, interpretable, and suitable for real-time industrial deployment, supporting proactive maintenance decisions before failures occur. Furthermore, it can be adapted to other machinery with similar sensor architectures, highlighting its potential as a scalable solution for predictive maintenance in complex systems.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.15526v1" target="_blank">SafetyFlow: An Agent-Flow System for Automated LLM Safety Benchmarking</a></h3>
                    <p><strong>Authors:</strong> Xiangyang Zhu, Yuan Tian, Chunyi Li, Kaiwei Zhang, Wei Sun, Guangtao Zhai</p>
                    <p><strong>Published:</strong> 8/21/2025</p>
                    <p><strong>Categories:</strong> cs.CL</p>
                    <p><strong>Summary:</strong> The rapid proliferation of large language models (LLMs) has intensified the requirement for reliable safety evaluation to uncover model vulnerabilities. To this end, numerous LLM safety evaluation benchmarks are proposed. However, existing benchmarks generally rely on labor-intensive manual curation, which causes excessive time and resource consumption. They also exhibit significant redundancy and limited difficulty. To alleviate these problems, we introduce SafetyFlow, the first agent-flow system designed to automate the construction of LLM safety benchmarks. SafetyFlow can automatically build a comprehensive safety benchmark in only four days without any human intervention by orchestrating seven specialized agents, significantly reducing time and resource cost. Equipped with versatile tools, the agents of SafetyFlow ensure process and cost controllability while integrating human expertise into the automatic pipeline. The final constructed dataset, SafetyFlowBench, contains 23,446 queries with low redundancy and strong discriminative power. Our contribution includes the first fully automated benchmarking pipeline and a comprehensive safety benchmark. We evaluate the safety of 49 advanced LLMs on our dataset and conduct extensive experiments to validate our efficacy and efficiency.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.15516v1" target="_blank">The Digital Life of Parisian Parks: Multifunctionality and Urban Context Uncovered by Mobile Application Traffic</a></h3>
                    <p><strong>Authors:</strong> AndrÃ© Felipe Zanella, Linus W. Dietz, Sanja Å Ä‡epanoviÄ‡, Ke Zhou, Zbigniew Smoreda, Daniele Quercia</p>
                    <p><strong>Published:</strong> 8/21/2025</p>
                    <p><strong>Categories:</strong> cs.CY</p>
                    <p><strong>Summary:</strong> Landscape architecture typically considers urban parks through the lens of form and function. While past research on equitable access has focused mainly on form, studies of functions have been constrained by limited scale and coarse measurement. Existing efforts have partially quantified functions through small-scale surveys and movement data (e.g., GPS) or general usage records (e.g., CDR), but have not captured the activities and motivations underlying park visits. As a result, our understanding of the functional roles urban parks play remains incomplete. To address this gap, we introduce a method that refines mobile base station coverage using antenna azimuths, enabling clearer distinction of mobile traffic within parks versus surrounding areas. Using Paris as a case study, we analyze a large-scale set of passively collected per-app mobile network traffic - 492 million hourly records for 45 parks. We test two hypotheses: the central-city hypothesis, which posits multifunctional parks emerge in dense, high-rent areas due to land scarcity; and the socio-spatial hypothesis, which views parks as reflections of neighborhood routines and preferences. Our analysis shows that parks have distinctive mobile traffic signatures, differing from both their surroundings and from each other. By clustering parks on temporal and app usage patterns, we identify three functional types - lunchbreak, cultural, and recreational - with different visitation motivations. Centrally located parks (cultural and lunchbreak) display more diverse app use and temporal variation, while suburban (recreational) parks reflect digital behaviors of nearby communities, with app preferences aligned to neighborhood income. These findings demonstrate the value of mobile traffic as a proxy for studying urban green space functions, with implications for park planning, public health, and well-being.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.15510v1" target="_blank">Super-additive Cooperation in Language Model Agents</a></h3>
                    <p><strong>Authors:</strong> Filippo Tonini, Lukas Galke</p>
                    <p><strong>Published:</strong> 8/21/2025</p>
                    <p><strong>Categories:</strong> cs.AI, I.2.11; I.2.0; J.4; K.4.0; I.2.6</p>
                    <p><strong>Summary:</strong> With the prospect of autonomous artificial intelligence (AI) agents, studying their tendency for cooperative behavior becomes an increasingly relevant topic. This study is inspired by the super-additive cooperation theory, where the combined effects of repeated interactions and inter-group rivalry have been argued to be the cause for cooperative tendencies found in humans. We devised a virtual tournament where language model agents, grouped into teams, face each other in a Prisoners Dilemma game. By simulating both internal team dynamics and external competition, we discovered that this blend substantially boosts both overall and initial, one-shot cooperation levels (the tendency to cooperate in one-off interactions). This research provides a novel framework for large language models to strategize and act in complex social scenarios and offers evidence for how intergroup competition can, counter-intuitively, result in more cooperative behavior. These insights are crucial for designing future multi-agent AI systems that can effectively work together and better align with human values. Source code is available at https://github.com/pippot/Superadditive-cooperation-LLMs.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.15503v1" target="_blank">Evaluation Guidelines for Empirical Studies in Software Engineering involving LLMs</a></h3>
                    <p><strong>Authors:</strong> Sebastian Baltes, Florian Angermeir, Chetan Arora, Marvin MuÃ±oz BarÃ³n, Chunyang Chen, Lukas BÃ¶hme, Fabio Calefato, Neil Ernst, Davide Falessi, Brian Fitzgerald, Davide Fucci, Marcos Kalinowski, Stefano Lambiase, Daniel Russo, Mircea Lungu, Lutz Prechelt, Paul Ralph, Christoph Treude, Stefan Wagner</p>
                    <p><strong>Published:</strong> 8/21/2025</p>
                    <p><strong>Categories:</strong> cs.SE</p>
                    <p><strong>Summary:</strong> Large language models (LLMs) are increasingly being integrated into software engineering (SE) research and practice, yet their non-determinism, opaque training data, and evolving architectures complicate the reproduction and replication of empirical studies. We present a community effort to scope this space, introducing a taxonomy of LLM-based study types together with eight guidelines for designing and reporting empirical studies involving LLMs. The guidelines present essential (must) criteria as well as desired (should) criteria and target transparency throughout the research process. Our recommendations, contextualized by our study types, are: (1) to declare LLM usage and role; (2) to report model versions, configurations, and fine-tuning; (3) to document tool architectures; (4) to disclose prompts and interaction logs; (5) to use human validation; (6) to employ an open LLM as a baseline; (7) to report suitable baselines, benchmarks, and metrics; and (8) to openly articulate limitations and mitigations. Our goal is to enable reproducibility and replicability despite LLM-specific barriers to open science. We maintain the study types and guidelines online as a living resource for the community to use and shape (llm-guidelines.org).</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.15496v1" target="_blank">Towards the Assessment of Task-based Chatbots: From the TOFU-R Snapshot to the BRASATO Curated Dataset</a></h3>
                    <p><strong>Authors:</strong> Elena Masserini, Diego Clerissi, Daniela Micucci, JoÃ£o R. Campos, Leonardo Mariani</p>
                    <p><strong>Published:</strong> 8/21/2025</p>
                    <p><strong>Categories:</strong> cs.SE</p>
                    <p><strong>Summary:</strong> Task-based chatbots are increasingly being used to deliver real services, yet assessing their reliability, security, and robustness remains underexplored, also due to the lack of large-scale, high-quality datasets. The emerging automated quality assessment techniques targeting chatbots often rely on limited pools of subjects, such as custom-made toy examples, or outdated, no longer available, or scarcely popular agents, complicating the evaluation of such techniques. In this paper, we present two datasets and the tool support necessary to create and maintain these datasets. The first dataset is RASA TASK-BASED CHATBOTS FROM GITHUB (TOFU-R), which is a snapshot of the Rasa chatbots available on GitHub, representing the state of the practice in open-source chatbot development with Rasa. The second dataset is BOT RASA COLLECTION (BRASATO), a curated selection of the most relevant chatbots for dialogue complexity, functional complexity, and utility, whose goal is to ease reproducibility and facilitate research on chatbot reliability.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.15487v1" target="_blank">Dream 7B: Diffusion Large Language Models</a></h3>
                    <p><strong>Authors:</strong> Jiacheng Ye, Zhihui Xie, Lin Zheng, Jiahui Gao, Zirui Wu, Xin Jiang, Zhenguo Li, Lingpeng Kong</p>
                    <p><strong>Published:</strong> 8/21/2025</p>
                    <p><strong>Categories:</strong> cs.CL</p>
                    <p><strong>Summary:</strong> We introduce Dream 7B, the most powerful open diffusion large language model to date. Unlike autoregressive (AR) models that generate tokens sequentially, Dream 7B employs discrete diffusion modeling to refine sequences in parallel through iterative denoising. Our model consistently outperforms existing diffusion language models on general, mathematical, and coding tasks. Dream 7B demonstrates superior planning abilities and inference flexibility, including arbitrary-order generation, infilling capabilities, and tunable quality-speed trade-offs. These results are achieved through simple yet effective training techniques, including AR-based LLM initialization and context-adaptive token-level noise rescheduling. We release both Dream-Base and Dream-Instruct to facilitate further research in diffusion-based language modeling.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.15486v1" target="_blank">LongRetriever: Towards Ultra-Long Sequence based Candidate Retrieval for Recommendation</a></h3>
                    <p><strong>Authors:</strong> Ren Qin, Chai Zheng, Xiao Xijun, Zheng Yuchao, Wu Di</p>
                    <p><strong>Published:</strong> 8/21/2025</p>
                    <p><strong>Categories:</strong> cs.IR</p>
                    <p><strong>Summary:</strong> Precisely modeling user ultra-long sequences is critical for industrial recommender systems. Current approaches predominantly focus on leveraging ultra-long sequences in the ranking stage, whereas research for the candidate retrieval stage remains under-explored. This paper presents LongRetriever, a practical framework for incorporating ultra-long sequences into the retrieval stage of recommenders. Specifically, we propose in-context training and multi-context retrieval, which enable candidate-specific interaction between user sequence and candidate item, and ensure training-serving consistency under the search-based paradigm. Extensive online A/B testing conducted on a large-scale e-commerce platform demonstrates statistically significant improvements, confirming the frameworks effectiveness. Currently, LongRetriever has been fully deployed in the platform, impacting billions of users.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.15483v1" target="_blank">HebID: Detecting Social Identities in Hebrew-language Political Text</a></h3>
                    <p><strong>Authors:</strong> Guy Mor-Lan, Naama Rivlin-Angert, Yael R. Kaplan, Tamir Sheafer, Shaul R. Shenhav</p>
                    <p><strong>Published:</strong> 8/21/2025</p>
                    <p><strong>Categories:</strong> cs.CL</p>
                    <p><strong>Summary:</strong> Political language is deeply intertwined with social identities. While social identities are often shaped by specific cultural contexts and expressed through particular uses of language, existing datasets for group and identity detection are predominantly English-centric, single-label and focus on coarse identity categories. We introduce HebID, the first multilabel Hebrew corpus for social identity detection: 5,536 sentences from Israeli politicians Facebook posts (Dec 2018-Apr 2021), manually annotated for twelve nuanced social identities (e.g. Rightist, Ultra-Orthodox, Socially-oriented) grounded by survey data. We benchmark multilabel and single-label encoders alongside 2B-9B-parameter generative LLMs, finding that Hebrew-tuned LLMs provide the best results (macro-$F_1$ = 0.74). We apply our classifier to politicians Facebook posts and parliamentary speeches, evaluating differences in popularity, temporal trends, clustering patterns, and gender-related variations in identity expression. We utilize identity choices from a national public survey, enabling a comparison between identities portrayed in elite discourse and the publics identity priorities. HebID provides a comprehensive foundation for studying social identities in Hebrew and can serve as a model for similar research in other non-English political contexts.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.15481v1" target="_blank">On Evaluating the Adversarial Robustness of Foundation Models for Multimodal Entity Linking</a></h3>
                    <p><strong>Authors:</strong> Fang Wang, Yongjie Wang, Zonghao Yang, Minghao Hu, Xiaoying Bai</p>
                    <p><strong>Published:</strong> 8/21/2025</p>
                    <p><strong>Categories:</strong> cs.IR</p>
                    <p><strong>Summary:</strong> The explosive growth of multimodal data has driven the rapid development of multimodal entity linking (MEL) models. However, existing studies have not systematically investigated the impact of visual adversarial attacks on MEL models. We conduct the first comprehensive evaluation of the robustness of mainstream MEL models under different adversarial attack scenarios, covering two core tasks: Image-to-Text (I2T) and Image+Text-to-Text (IT2T). Experimental results show that current MEL models generally lack sufficient robustness against visual perturbations. Interestingly, contextual semantic information in input can partially mitigate the impact of adversarial perturbations. Based on this insight, we propose an LLM and Retrieval-Augmented Entity Linking (LLM-RetLink), which significantly improves the models anti-interference ability through a two-stage process: first, extracting initial entity descriptions using large vision models (LVMs), and then dynamically generating candidate descriptive sentences via web-based retrieval. Experiments on five datasets demonstrate that LLM-RetLink improves the accuracy of MEL by 0.4%-35.7%, especially showing significant advantages under adversarial conditions. This research highlights a previously unexplored facet of MEL robustness, constructs and releases the first MEL adversarial example dataset, and sets the stage for future work aimed at strengthening the resilience of multimodal systems in adversarial environments.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.15478v1" target="_blank">SLM-Bench: A Comprehensive Benchmark of Small Language Models on Environmental Impacts -- Extended Version</a></h3>
                    <p><strong>Authors:</strong> Nghiem Thanh Pham, Tung Kieu, Duc-Manh Nguyen, Son Ha Xuan, Nghia Duong-Trung, Danh Le-Phuoc</p>
                    <p><strong>Published:</strong> 8/21/2025</p>
                    <p><strong>Categories:</strong> cs.CL, cs.CY, cs.PF</p>
                    <p><strong>Summary:</strong> Small Language Models (SLMs) offer computational efficiency and accessibility, yet a systematic evaluation of their performance and environmental impact remains lacking. We introduce SLM-Bench, the first benchmark specifically designed to assess SLMs across multiple dimensions, including accuracy, computational efficiency, and sustainability metrics. SLM-Bench evaluates 15 SLMs on 9 NLP tasks using 23 datasets spanning 14 domains. The evaluation is conducted on 4 hardware configurations, providing a rigorous comparison of their effectiveness. Unlike prior benchmarks, SLM-Bench quantifies 11 metrics across correctness, computation, and consumption, enabling a holistic assessment of efficiency trade-offs. Our evaluation considers controlled hardware conditions, ensuring fair comparisons across models. We develop an open-source benchmarking pipeline with standardized evaluation protocols to facilitate reproducibility and further research. Our findings highlight the diverse trade-offs among SLMs, where some models excel in accuracy while others achieve superior energy efficiency. SLM-Bench sets a new standard for SLM evaluation, bridging the gap between resource efficiency and real-world applicability.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.15477v1" target="_blank">Radial Oscillations of Scalar Hair in Black Hole Bombs</a></h3>
                    <p><strong>Authors:</strong> Lang Zhao, Lin Chen, Cheng-Yong Zhang</p>
                    <p><strong>Published:</strong> 8/21/2025</p>
                    <p><strong>Categories:</strong> gr-qc</p>
                    <p><strong>Summary:</strong> Recent research has revealed a novel nonlinear mechanism, distinct from the linear superradiant instability, which triggers the black hole bomb phenomenon. Introducing a massive complex scalar field with nonlinear self-interactions drives the Reissner-Nordstr\om black hole to shed substantial energy, thereby triggering a black hole bomb. Radial oscillations in the scalar hair profile are observed during this process. In this paper, we further reveal that physical quantities associated with scalar hair exhibit identical oscillation patterns during the evolution of the black hole-scalar field system. Moreover, the oscillation frequency exhibits a linear dependence on the gauge coupling constant of the scalar field with other parameters fixed. Meanwhile, the horizon radius of hairy black holes and the mass within the horizon increase monotonically with the gauge coupling constant. We have also identified a critical initial charge value that distinguishes hairy solutions that trigger black hole bombs from those that do not.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.15473v1" target="_blank">EffortNet: A Deep Learning Framework for Objective Assessment of Speech Enhancement Technologies Using EEG-Based Alpha Oscillations</a></h3>
                    <p><strong>Authors:</strong> Ching-Chih Sung, Cheng-Hung Hsin, Yu-Anne Shiah, Bo-Jyun Lin, Yi-Xuan Lai, Chia-Ying Lee, Yu-Te Wang, Borchin Su, Yu Tsao</p>
                    <p><strong>Published:</strong> 8/21/2025</p>
                    <p><strong>Categories:</strong> eess.AS</p>
                    <p><strong>Summary:</strong> This paper presents EffortNet, a novel deep learning framework for decoding individual listening effort from electroencephalography (EEG) during speech comprehension. Listening effort represents a significant challenge in speech-hearing research, particularly for aging populations and those with hearing impairment. We collected 64-channel EEG data from 122 participants during speech comprehension under four conditions: clean, noisy, MMSE-enhanced, and Transformer-enhanced speech. Statistical analyses confirmed that alpha oscillations (8-13 Hz) exhibited significantly higher power during noisy speech processing compared to clean or enhanced conditions, confirming their validity as objective biomarkers of listening effort. To address the substantial inter-individual variability in EEG signals, EffortNet integrates three complementary learning paradigms: self-supervised learning to leverage unlabeled data, incremental learning for progressive adaptation to individual characteristics, and transfer learning for efficient knowledge transfer to new subjects. Our experimental results demonstrate that Effort- Net achieves 80.9% classification accuracy with only 40% training data from new subjects, significantly outperforming conventional CNN (62.3%) and STAnet (61.1%) models. The probability-based metric derived from our model revealed that Transformer-enhanced speech elicited neural responses more similar to clean speech than MMSEenhanced speech. This finding contrasted with subjective intelligibility ratings but aligned with objective metrics. The proposed framework provides a practical solution for personalized assessment of hearing technologies, with implications for designing cognitive-aware speech enhancement systems.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.15471v1" target="_blank">SLM4Offer: Personalized Marketing Offer Generation Using Contrastive Learning Based Fine-Tuning</a></h3>
                    <p><strong>Authors:</strong> Vedasamhitha Challapalli, Konduru Venkat Sai, Piyush Pratap Singh, Rupesh Prasad, Arvind Maurya, Atul Singh</p>
                    <p><strong>Published:</strong> 8/21/2025</p>
                    <p><strong>Categories:</strong> cs.CL</p>
                    <p><strong>Summary:</strong> Personalized marketing has emerged as a pivotal strategy for enhancing customer engagement and driving business growth. Academic and industry efforts have predominantly focused on recommendation systems and personalized advertisements. Nonetheless, this facet of personalization holds significant potential for increasing conversion rates and improving customer satisfaction. Prior studies suggest that well-executed personalization strategies can boost revenue by up to 40 percent, underscoring the strategic importance of developing intelligent, data-driven approaches for offer generation. This work introduces SLM4Offer, a generative AI model for personalized offer generation, developed by fine-tuning a pre-trained encoder-decoder language model, specifically Googles Text-to-Text Transfer Transformer (T5-Small 60M) using a contrastive learning approach. SLM4Offer employs InfoNCE (Information Noise-Contrastive Estimation) loss to align customer personas with relevant offers in a shared embedding space. A key innovation in SLM4Offer lies in the adaptive learning behaviour introduced by contrastive loss, which reshapes the latent space during training and enhances the models generalizability. The model is fine-tuned and evaluated on a synthetic dataset designed to simulate customer behaviour and offer acceptance patterns. Experimental results demonstrate a 17 percent improvement in offer acceptance rate over a supervised fine-tuning baseline, highlighting the effectiveness of contrastive objectives in advancing personalized marketing.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.15772v1" target="_blank">Visual Autoregressive Modeling for Instruction-Guided Image Editing</a></h3>
                    <p><strong>Authors:</strong> Qingyang Mao, Qi Cai, Yehao Li, Yingwei Pan, Mingyue Cheng, Ting Yao, Qi Liu, Tao Mei</p>
                    <p><strong>Published:</strong> 8/21/2025</p>
                    <p><strong>Categories:</strong> cs.CV, cs.MM</p>
                    <p><strong>Summary:</strong> Recent advances in diffusion models have brought remarkable visual fidelity to instruction-guided image editing. However, their global denoising process inherently entangles the edited region with the entire image context, leading to unintended spurious modifications and compromised adherence to editing instructions. In contrast, autoregressive models offer a distinct paradigm by formulating image synthesis as a sequential process over discrete visual tokens. Their causal and compositional mechanism naturally circumvents the adherence challenges of diffusion-based methods. In this paper, we present VAREdit, a visual autoregressive (VAR) framework that reframes image editing as a next-scale prediction problem. Conditioned on source image features and text instructions, VAREdit generates multi-scale target features to achieve precise edits. A core challenge in this paradigm is how to effectively condition the source image tokens. We observe that finest-scale source features cannot effectively guide the prediction of coarser target features. To bridge this gap, we introduce a Scale-Aligned Reference (SAR) module, which injects scale-matched conditioning information into the first self-attention layer. VAREdit demonstrates significant advancements in both editing adherence and efficiency. On standard benchmarks, it outperforms leading diffusion-based methods by 30\%+ higher GPT-Balance score. Moreover, it completes a $512\times512$ editing in 1.2 seconds, making it 2.2$\times$ faster than the similarly sized UltraEdit. The models are available at https://github.com/HiDream-ai/VAREdit.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.15771v1" target="_blank">Overview of complex organic molecule observations in protostellar systems</a></h3>
                    <p><strong>Authors:</strong> P. Nazari</p>
                    <p><strong>Published:</strong> 8/21/2025</p>
                    <p><strong>Categories:</strong> astro-ph.GA</p>
                    <p><strong>Summary:</strong> Complex organic molecules (COMs) have been detected abundantly at various stages of star formation, particularly in the warm protostellar phase. The progress in gas-phase measurements has been accelerated by the advent of the Atacama Large Millimeter/submillimeter Array and in ice measurements by the James Webb Space Telescope. Particularly, the community has moved from single-source studies of COMs to statistical analyses because of these powerful instruments. In this article, I review surveys that consider COMs in the gas and ice. The two takeaways from this review include; 1. Gas-phase abundance ratios for some COMs show a small difference across many objects and the ice abundance ratios show similar or higher values to the gas, both pointing to the importance of ice chemistry in COM formation, 2. Some COM ratios show larger differences across many objects which could be due to either chemical or physical effects, thus both factors need to be considered when interpreting the data.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.15769v1" target="_blank">SceneGen: Single-Image 3D Scene Generation in One Feedforward Pass</a></h3>
                    <p><strong>Authors:</strong> Yanxu Meng, Haoning Wu, Ya Zhang, Weidi Xie</p>
                    <p><strong>Published:</strong> 8/21/2025</p>
                    <p><strong>Categories:</strong> cs.CV, cs.AI</p>
                    <p><strong>Summary:</strong> 3D content generation has recently attracted significant research interest due to its applications in VR/AR and embodied AI. In this work, we address the challenging task of synthesizing multiple 3D assets within a single scene image. Concretely, our contributions are fourfold: (i) we present SceneGen, a novel framework that takes a scene image and corresponding object masks as input, simultaneously producing multiple 3D assets with geometry and texture. Notably, SceneGen operates with no need for optimization or asset retrieval; (ii) we introduce a novel feature aggregation module that integrates local and global scene information from visual and geometric encoders within the feature extraction module. Coupled with a position head, this enables the generation of 3D assets and their relative spatial positions in a single feedforward pass; (iii) we demonstrate SceneGens direct extensibility to multi-image input scenarios. Despite being trained solely on single-image inputs, our architectural design enables improved generation performance with multi-image inputs; and (iv) extensive quantitative and qualitative evaluations confirm the efficiency and robust generation abilities of our approach. We believe this paradigm offers a novel solution for high-quality 3D content generation, potentially advancing its practical applications in downstream tasks. The code and model will be publicly available at: https://mengmouxu.github.io/SceneGen.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.15763v1" target="_blank">Intern-S1: A Scientific Multimodal Foundation Model</a></h3>
                    <p><strong>Authors:</strong> Lei Bai, Zhongrui Cai, Maosong Cao, Weihan Cao, Chiyu Chen, Haojiong Chen, Kai Chen, Pengcheng Chen, Ying Chen, Yongkang Chen, Yu Cheng, Yu Cheng, Pei Chu, Tao Chu, Erfei Cui, Ganqu Cui, Long Cui, Ziyun Cui, Nianchen Deng, Ning Ding, Nanqin Dong, Peijie Dong, Shihan Dou, Sinan Du, Haodong Duan, Caihua Fan, Ben Gao, Changjiang Gao, Jianfei Gao, Songyang Gao, Yang Gao, Zhangwei Gao, Jiaye Ge, Qiming Ge, Lixin Gu, Yuzhe Gu, Aijia Guo, Qipeng Guo, Xu Guo, Conghui He, Junjun He, Yili Hong, Siyuan Hou, Caiyu Hu, Hanglei Hu, Jucheng Hu, Ming Hu, Zhouqi Hua, Haian Huang, Junhao Huang, Xu Huang, Zixian Huang, Zhe Jiang, Lingkai Kong, Linyang Li, Peiji Li, Pengze Li, Shuaibin Li, Tianbin Li, Wei Li, Yuqiang Li, Dahua Lin, Junyao Lin, Tianyi Lin, Zhishan Lin, Hongwei Liu, Jiangning Liu, Jiyao Liu, Junnan Liu, Kai Liu, Kaiwen Liu, Kuikun Liu, Shichun Liu, Shudong Liu, Wei Liu, Xinyao Liu, Yuhong Liu, Zhan Liu, Yinquan Lu, Haijun Lv, Hongxia Lv, Huijie Lv, Qidang Lv, Ying Lv, Chengqi Lyu, Chenglong Ma, Jianpeng Ma, Ren Ma, Runmin Ma, Runyuan Ma, Xinzhu Ma, Yichuan Ma, Zihan Ma, Sixuan Mi, Junzhi Ning, Wenchang Ning, Xinle Pang, Jiahui Peng, Runyu Peng, Yu Qiao, Jiantao Qiu, Xiaoye Qu, Yuan Qu, Yuchen Ren, Fukai Shang, Wenqi Shao, Junhao Shen, Shuaike Shen, Chunfeng Song, Demin Song, Diping Song, Chenlin Su, Weijie Su, Weigao Sun, Yu Sun, Qian Tan, Cheng Tang, Huanze Tang, Kexian Tang, Shixiang Tang, Jian Tong, Aoran Wang, Bin Wang, Dong Wang, Lintao Wang, Rui Wang, Weiyun Wang, Wenhai Wang, Yi Wang, Ziyi Wang, Ling-I Wu, Wen Wu, Yue Wu, Zijian Wu, Linchen Xiao, Shuhao Xing, Chao Xu, Huihui Xu, Jun Xu, Ruiliang Xu, Wanghan Xu, GanLin Yang, Yuming Yang, Haochen Ye, Jin Ye, Shenglong Ye, Jia Yu, Jiashuo Yu, Jing Yu, Fei Yuan, Bo Zhang, Chao Zhang, Chen Zhang, Hongjie Zhang, Jin Zhang, Qiaosheng Zhang, Qiuyinzhe Zhang, Songyang Zhang, Taolin Zhang, Wenlong Zhang, Wenwei Zhang, Yechen Zhang, Ziyang Zhang, Haiteng Zhao, Qian Zhao, Xiangyu Zhao, Xiangyu Zhao, Bowen Zhou, Dongzhan Zhou, Peiheng Zhou, Yuhao Zhou, Yunhua Zhou, Dongsheng Zhu, Lin Zhu, Yicheng Zou</p>
                    <p><strong>Published:</strong> 8/21/2025</p>
                    <p><strong>Categories:</strong> cs.LG, cs.CL, cs.CV</p>
                    <p><strong>Summary:</strong> In recent years, a plethora of open-source foundation models have emerged, achieving remarkable progress in some widely attended fields, with performance being quite close to that of closed-source models. However, in high-value but more challenging scientific professional fields, either the fields still rely on expert models, or the progress of general foundation models lags significantly compared to those in popular areas, far from sufficient for transforming scientific research and leaving substantial gap between open-source models and closed-source models in these scientific domains. To mitigate this gap and explore a step further toward Artificial General Intelligence (AGI), we introduce Intern-S1, a specialized generalist equipped with general understanding and reasoning capabilities with expertise to analyze multiple science modal data. Intern-S1 is a multimodal Mixture-of-Experts (MoE) model with 28 billion activated parameters and 241 billion total parameters, continually pre-trained on 5T tokens, including over 2.5T tokens from scientific domains. In the post-training stage, Intern-S1 undergoes offline and then online reinforcement learning (RL) in InternBootCamp, where we propose Mixture-of-Rewards (MoR) to synergize the RL training on more than 1000 tasks simultaneously. Through integrated innovations in algorithms, data, and training systems, Intern-S1 achieved top-tier performance in online RL training.On comprehensive evaluation benchmarks, Intern-S1 demonstrates competitive performance on general reasoning tasks among open-source models and significantly outperforms open-source models in scientific domains, surpassing closed-source state-of-the-art models in professional tasks, such as molecular synthesis planning, reaction condition prediction, predicting thermodynamic stabilities for crystals. Our models are available at https://huggingface.co/internlm/Intern-S1.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.15761v1" target="_blank">Waver: Wave Your Way to Lifelike Video Generation</a></h3>
                    <p><strong>Authors:</strong> Yifu Zhang, Hao Yang, Yuqi Zhang, Yifei Hu, Fengda Zhu, Chuang Lin, Xiaofeng Mei, Yi Jiang, Zehuan Yuan, Bingyue Peng</p>
                    <p><strong>Published:</strong> 8/21/2025</p>
                    <p><strong>Categories:</strong> cs.CV</p>
                    <p><strong>Summary:</strong> We present Waver, a high-performance foundation model for unified image and video generation. Waver can directly generate videos with durations ranging from 5 to 10 seconds at a native resolution of 720p, which are subsequently upscaled to 1080p. The model simultaneously supports text-to-video (T2V), image-to-video (I2V), and text-to-image (T2I) generation within a single, integrated framework. We introduce a Hybrid Stream DiT architecture to enhance modality alignment and accelerate training convergence. To ensure training data quality, we establish a comprehensive data curation pipeline and manually annotate and train an MLLM-based video quality model to filter for the highest-quality samples. Furthermore, we provide detailed training and inference recipes to facilitate the generation of high-quality videos. Building on these contributions, Waver excels at capturing complex motion, achieving superior motion amplitude and temporal consistency in video synthesis. Notably, it ranks among the Top 3 on both the T2V and I2V leaderboards at Artificial Analysis (data as of 2025-07-30 10:00 GMT+8), consistently outperforming existing open-source models and matching or surpassing state-of-the-art commercial solutions. We hope this technical report will help the community more efficiently train high-quality video generation models and accelerate progress in video generation technologies. Official page: https://github.com/FoundationVision/Waver.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.15760v1" target="_blank">LiveMCP-101: Stress Testing and Diagnosing MCP-enabled Agents on Challenging Queries</a></h3>
                    <p><strong>Authors:</strong> Ming Yin, Dinghan Shen, Silei Xu, Jianbing Han, Sixun Dong, Mian Zhang, Yebowen Hu, Shujian Liu, Simin Ma, Song Wang, Sathish Reddy Indurthi, Xun Wang, Yiran Chen, Kaiqiang Song</p>
                    <p><strong>Published:</strong> 8/21/2025</p>
                    <p><strong>Categories:</strong> cs.CL, cs.AI</p>
                    <p><strong>Summary:</strong> Tool calling has emerged as a critical capability for AI agents to interact with the real world and solve complex tasks. While the Model Context Protocol (MCP) provides a powerful standardized framework for tool integration, there is a significant gap in benchmarking how well AI agents can effectively solve multi-step tasks using diverse MCP tools in realistic, dynamic scenarios. In this work, we present LiveMCP-101, a benchmark of 101 carefully curated real-world queries, refined through iterative LLM rewriting and manual review, that require coordinated use of multiple MCP tools including web search, file operations, mathematical reasoning, and data analysis. Moreover, we introduce a novel evaluation approach that leverages ground-truth execution plans rather than raw API outputs, better reflecting the evolving nature of real-world environments. Experiments show that even frontier LLMs achieve a success rate below 60\%, highlighting major challenges in tool orchestration. Detailed ablations and error analysis further reveal distinct failure modes and inefficiencies in token usage, pointing to concrete directions for advancing current models. LiveMCP-101 sets a rigorous standard for evaluating real-world agent capabilities, advancing toward autonomous AI systems that reliably execute complex tasks through tool use.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.15752v1" target="_blank">Does the cafe entrance look accessible? Where is the door? Towards Geospatial AI Agents for Visual Inquiries</a></h3>
                    <p><strong>Authors:</strong> Jon E. Froehlich, Jared Hwang, Zeyu Wang, John S. OMeara, Xia Su, William Huang, Yang Zhang, Alex Fiannaca, Philip Nelson, Shaun Kane</p>
                    <p><strong>Published:</strong> 8/21/2025</p>
                    <p><strong>Categories:</strong> cs.HC, cs.AI, cs.CV, H.5; I.2</p>
                    <p><strong>Summary:</strong> Interactive digital maps have revolutionized how people travel and learn about the world; however, they rely on pre-existing structured data in GIS databases (e.g., road networks, POI indices), limiting their ability to address geo-visual questions related to what the world looks like. We introduce our vision for Geo-Visual Agents--multimodal AI agents capable of understanding and responding to nuanced visual-spatial inquiries about the world by analyzing large-scale repositories of geospatial images, including streetscapes (e.g., Google Street View), place-based photos (e.g., TripAdvisor, Yelp), and aerial imagery (e.g., satellite photos) combined with traditional GIS data sources. We define our vision, describe sensing and interaction approaches, provide three exemplars, and enumerate key challenges and opportunities for future work.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.15748v1" target="_blank">Response and Prompt Evaluation to Prevent Parasocial Relationships with Chatbots</a></h3>
                    <p><strong>Authors:</strong> Emma Rath, Stuart Armstrong, Rebecca Gorman</p>
                    <p><strong>Published:</strong> 8/21/2025</p>
                    <p><strong>Categories:</strong> cs.AI</p>
                    <p><strong>Summary:</strong> The development of parasocial relationships with AI agents has severe, and in some cases, tragic effects for human well-being. Yet preventing such dynamics is challenging: parasocial cues often emerge gradually in private conversations, and not all forms of emotional engagement are inherently harmful. We address this challenge by introducing a simple response evaluation framework, created by repurposing a state-of-the-art language model, that evaluates ongoing conversations for parasocial cues in real time. To test the feasibility of this approach, we constructed a small synthetic dataset of thirty dialogues spanning parasocial, sycophantic, and neutral conversations. Iterative evaluation with five stage testing successfully identified all parasocial conversations while avoiding false positives under a tolerant unanimity rule, with detection typically occurring within the first few exchanges. These findings provide preliminary evidence that evaluation agents can provide a viable solution for the prevention of parasocial relations.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.15737v1" target="_blank">Probability Density from Latent Diffusion Models for Out-of-Distribution Detection</a></h3>
                    <p><strong>Authors:</strong> Joonas JÃ¤rve, Karl Kaspar Haavel, Meelis Kull</p>
                    <p><strong>Published:</strong> 8/21/2025</p>
                    <p><strong>Categories:</strong> cs.LG, cs.CV</p>
                    <p><strong>Summary:</strong> Despite rapid advances in AI, safety remains the main bottleneck to deploying machine-learning systems. A critical safety component is out-of-distribution detection: given an input, decide whether it comes from the same distribution as the training data. In generative models, the most natural OOD score is the data likelihood. Actually, under the assumption of uniformly distributed OOD data, the likelihood is even the optimal OOD detector, as we show in this work. However, earlier work reported that likelihood often fails in practice, raising doubts about its usefulness. We explore whether, in practice, the representation space also suffers from the inability to learn good density estimation for OOD detection, or if it is merely a problem of the pixel space typically used in generative models. To test this, we trained a Variational Diffusion Model not on images, but on the representation space of a pre-trained ResNet-18 to assess the performance of our likelihood-based detector in comparison to state-of-the-art methods from the OpenOOD suite.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.15734v1" target="_blank">Measuring the environmental impact of delivering AI at Google Scale</a></h3>
                    <p><strong>Authors:</strong> Cooper Elsworth, Keguo Huang, David Patterson, Ian Schneider, Robert Sedivy, Savannah Goodman, Ben Townsend, Parthasarathy Ranganathan, Jeff Dean, Amin Vahdat, Ben Gomes, James Manyika</p>
                    <p><strong>Published:</strong> 8/21/2025</p>
                    <p><strong>Categories:</strong> cs.AI</p>
                    <p><strong>Summary:</strong> The transformative power of AI is undeniable - but as user adoption accelerates, so does the need to understand and mitigate the environmental impact of AI serving. However, no studies have measured AI serving environmental metrics in a production environment. This paper addresses this gap by proposing and executing a comprehensive methodology for measuring the energy usage, carbon emissions, and water consumption of AI inference workloads in a large-scale, AI production environment. Our approach accounts for the full stack of AI serving infrastructure - including active AI accelerator power, host system energy, idle machine capacity, and data center energy overhead. Through detailed instrumentation of Googles AI infrastructure for serving the Gemini AI assistant, we find the median Gemini Apps text prompt consumes 0.24 Wh of energy - a figure substantially lower than many public estimates. We also show that Googles software efficiency efforts and clean energy procurement have driven a 33x reduction in energy consumption and a 44x reduction in carbon footprint for the median Gemini Apps text prompt over one year. We identify that the median Gemini Apps text prompt uses less energy than watching nine seconds of television (0.24 Wh) and consumes the equivalent of five drops of water (0.26 mL). While these impacts are low compared to other daily activities, reducing the environmental impact of AI serving continues to warrant important attention. Towards this objective, we propose that a comprehensive measurement of AI serving environmental metrics is critical for accurately comparing models, and to properly incentivize efficiency gains across the full AI serving stack.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.15732v1" target="_blank">Understanding and Utilizing Dynamic Coupling in Free-Floating Space Manipulators for On-Orbit Servicing</a></h3>
                    <p><strong>Authors:</strong> Gargi Das, Daegyun Choi, Donghoon Kim</p>
                    <p><strong>Published:</strong> 8/21/2025</p>
                    <p><strong>Categories:</strong> cs.RO, cs.SY, eess.SY</p>
                    <p><strong>Summary:</strong> This study proposes a dynamic coupling-informed trajectory optimization algorithm for free-floating space manipulator systems (SMSs). Dynamic coupling between the base and the manipulator arms plays a critical role in influencing the systems behavior. While prior research has predominantly focused on minimizing this coupling, often overlooking its potential advantages, this work investigates how dynamic coupling can instead be leveraged to improve trajectory planning. Singular value decomposition (SVD) of the dynamic coupling matrix is employed to identify the dominant components governing coupling behavior. A quantitative metric is then formulated to characterize the strength and directionality of the coupling and is incorporated into a trajectory optimization framework. To assess the feasibility of the optimized trajectory, a sliding mode control-based tracking controller is designed to generate the required joint torque inputs. Simulation results demonstrate that explicitly accounting for dynamic coupling in trajectory planning enables more informed and potentially more efficient operation, offering new directions for the control of free-floating SMSs.</p>
                
            
                
                    <h3><a href="http://dx.doi.org/10.1145/3746059.3747779" target="_blank">Demystifying Reward Design in Reinforcement Learning for Upper Extremity Interaction: Practical Guidelines for Biomechanical Simulations in HCI</a></h3>
                    <p><strong>Authors:</strong> Hannah Selder, Florian Fischer, Per Ola Kristensson, Arthur Fleig</p>
                    <p><strong>Published:</strong> 8/21/2025</p>
                    <p><strong>Categories:</strong> cs.HC, H.5.2; F.m</p>
                    <p><strong>Summary:</strong> Designing effective reward functions is critical for reinforcement learning-based biomechanical simulations, yet HCI researchers and practitioners often waste (computation) time with unintuitive trial-and-error tuning. This paper demystifies reward function design by systematically analyzing the impact of effort minimization, task completion bonuses, and target proximity incentives on typical HCI tasks such as pointing, tracking, and choice reaction. We show that proximity incentives are essential for guiding movement, while completion bonuses ensure task success. Effort terms, though optional, help refine motion regularity when appropriately scaled. We perform an extensive analysis of how sensitive task success and completion time depend on the weights of these three reward components. From these results we derive practical guidelines to create plausible biomechanical simulations without the need for reinforcement learning expertise, which we then validate on remote control and keyboard typing tasks. This paper advances simulation-based interaction design and evaluation in HCI by improving the efficiency and applicability of biomechanical user modeling for real-world interface development.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.15724v1" target="_blank">Numerical models outperform AI weather forecasts of record-breaking extremes</a></h3>
                    <p><strong>Authors:</strong> Zhongwei Zhang, Erich Fischer, Jakob Zscheischler, Sebastian Engelke</p>
                    <p><strong>Published:</strong> 8/21/2025</p>
                    <p><strong>Categories:</strong> physics.ao-ph, cs.AI, stat.AP, J.2; I.6.4</p>
                    <p><strong>Summary:</strong> Artificial intelligence (AI)-based models are revolutionizing weather forecasting and have surpassed leading numerical weather prediction systems on various benchmark tasks. However, their ability to extrapolate and reliably forecast unprecedented extreme events remains unclear. Here, we show that for record-breaking weather extremes, the numerical model High RESolution forecast (HRES) from the European Centre for Medium-Range Weather Forecasts still consistently outperforms state-of-the-art AI models GraphCast, GraphCast operational, Pangu-Weather, Pangu-Weather operational, and Fuxi. We demonstrate that forecast errors in AI models are consistently larger for record-breaking heat, cold, and wind than in HRES across nearly all lead times. We further find that the examined AI models tend to underestimate both the frequency and intensity of record-breaking events, and they underpredict hot records and overestimate cold records with growing errors for larger record exceedance. Our findings underscore the current limitations of AI weather models in extrapolating beyond their training domain and in forecasting the potentially most impactful record-breaking weather events that are particularly frequent in a rapidly warming climate. Further rigorous verification and model development is needed before these models can be solely relied upon for high-stakes applications such as early warning systems and disaster management.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.15722v1" target="_blank">The Status of the Astrophysical Parameters of Upper Main Sequence Stars</a></h3>
                    <p><strong>Authors:</strong> Lukas KueÃŸ, Ernst Paunzen</p>
                    <p><strong>Published:</strong> 8/21/2025</p>
                    <p><strong>Categories:</strong> astro-ph.SR, astro-ph.GA</p>
                    <p><strong>Summary:</strong> Calibrating the ages, masses, and radii of stars on the upper main sequence depends heavily on accurate measurements of the effective temperature ($T_\mathrm{eff}$) and surface gravity ($\log g$). These parameters are difficult to obtain meticulously due to the nature of hot stars, which exhibit features such as rapid rotation, atomic diffusion, pulsation, and stellar winds. We compare the $T_\mathrm{eff}$, and $\log g$, values of apparent normal B-F stars in four recent catalogues that employ different methods and pipelines to obtain these parameters. We derived various statistical parameters to compare the differences between the catalogues and discussed the astrophysical implications of these differences. Our results show that the huge differences in $T_\mathrm{eff}$, (up to $10^4$\,K) and $\log g$, (up to 2 dex) between the catalogues have serious implications on the determination of ages, masses, and radii of the stars in question. We conclude that there appears to be no homogeneous set of stellar parameters on the upper main sequence, and one must be cautious when interpreting results obtained from using only one of the catalogues. The homogenisation of said parameters is an essential task for the future and will have a significant impact on astrophysical research dealing with stars on the upper main sequence.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.15719v1" target="_blank">Tutorial on the Probabilistic Unification of Estimation Theory, Machine Learning, and Generative AI</a></h3>
                    <p><strong>Authors:</strong> Mohammed Elmusrati</p>
                    <p><strong>Published:</strong> 8/21/2025</p>
                    <p><strong>Categories:</strong> cs.LG, cs.AI</p>
                    <p><strong>Summary:</strong> Extracting meaning from uncertain, noisy data is a fundamental problem across time series analysis, pattern recognition, and language modeling. This survey presents a unified mathematical framework that connects classical estimation theory, statistical inference, and modern machine learning, including deep learning and large language models. By analyzing how techniques such as maximum likelihood estimation, Bayesian inference, and attention mechanisms address uncertainty, the paper illustrates that many AI methods are rooted in shared probabilistic principles. Through illustrative scenarios including system identification, image classification, and language generation, we show how increasingly complex models build upon these foundations to tackle practical challenges like overfitting, data sparsity, and interpretability. In other words, the work demonstrates that maximum likelihood, MAP estimation, Bayesian classification, and deep learning all represent different facets of a shared goal: inferring hidden causes from noisy and/or biased observations. It serves as both a theoretical synthesis and a practical guide for students and researchers navigating the evolving landscape of machine learning.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.15716v1" target="_blank">Foundation Models for Cross-Domain EEG Analysis Application: A Survey</a></h3>
                    <p><strong>Authors:</strong> Hongqi Li, Yitong Chen, Yujuan Wang, Weihang Ni, Haodong Zhang</p>
                    <p><strong>Published:</strong> 8/21/2025</p>
                    <p><strong>Categories:</strong> cs.HC, cs.AI</p>
                    <p><strong>Summary:</strong> Electroencephalography (EEG) analysis stands at the forefront of neuroscience and artificial intelligence research, where foundation models are reshaping the traditional EEG analysis paradigm by leveraging their powerful representational capacity and cross-modal generalization. However, the rapid proliferation of these techniques has led to a fragmented research landscape, characterized by diverse model roles, inconsistent architectures, and a lack of systematic categorization. To bridge this gap, this study presents the first comprehensive modality-oriented taxonomy for foundation models in EEG analysis, systematically organizing research advances based on output modalities of the native EEG decoding, EEG-text, EEG-vision, EEG-audio, and broader multimodal frameworks. We rigorously analyze each categorys research ideas, theoretical foundations, and architectural innovations, while highlighting open challenges such as model interpretability, cross-domain generalization, and real-world applicability in EEG-based systems. By unifying this dispersed field, our work not only provides a reference framework for future methodology development but accelerates the translation of EEG foundation models into scalable, interpretable, and online actionable solutions.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.15711v1" target="_blank">Stemming -- The Evolution and Current State with a Focus on Bangla</a></h3>
                    <p><strong>Authors:</strong> Abhijit Paul, Mashiat Amin Farin, Sharif Md. Abdullah, Ahmedul Kabir, Zarif Masud, Shebuti Rayana</p>
                    <p><strong>Published:</strong> 8/21/2025</p>
                    <p><strong>Categories:</strong> cs.CL, cs.IR</p>
                    <p><strong>Summary:</strong> Bangla, the seventh most widely spoken language worldwide with 300 million native speakers, faces digital under-representation due to limited resources and lack of annotated datasets. Stemming, a critical preprocessing step in language analysis, is essential for low-resource, highly-inflectional languages like Bangla, because it can reduce the complexity of algorithms and models by significantly reducing the number of words the algorithm needs to consider. This paper conducts a comprehensive survey of stemming approaches, emphasizing the importance of handling morphological variants effectively. While exploring the landscape of Bangla stemming, it becomes evident that there is a significant gap in the existing literature. The paper highlights the discontinuity from previous research and the scarcity of accessible implementations for replication. Furthermore, it critiques the evaluation methodologies, stressing the need for more relevant metrics. In the context of Banglas rich morphology and diverse dialects, the paper acknowledges the challenges it poses. To address these challenges, the paper suggests directions for Bangla stemmer development. It concludes by advocating for robust Bangla stemmers and continued research in the field to enhance language analysis and processing.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.15700v1" target="_blank">Detection of non-absolute separability in quantum states and channels through moments</a></h3>
                    <p><strong>Authors:</strong> Bivas Mallick, Saheli Mukherjee, Nirman Ganguly, A. S. Majumdar</p>
                    <p><strong>Published:</strong> 8/21/2025</p>
                    <p><strong>Categories:</strong> quant-ph</p>
                    <p><strong>Summary:</strong> In quantum information and computation, generation of entanglement through unitary gates remains a significant and active area of research. However, there are states termed as absolutely separable, from which entanglement cannot be created through any non-local unitary action. Thus, from a resource-theoretic perspective, non-absolutely separable states are useful as they can be turned into entangled states using some appropriate unitary gates. In this work, we propose an efficient method to detect non-absolutely separable states. Our approach relies on evaluating moments that can bypass the need for full state tomography, thereby enhancing its practical applicability. We then present several examples in support of our detection scheme. We also address a closely related problem concerning states whose partial transpose remains positive under any arbitrary non-local unitary action. Furthermore, we examine the effectiveness of our moment-based approach in the detection of quantum channels that are not absolutely separating, which entails the detection of resource preserving channels. Finally, we demonstrate the operational significance of non-absolutely separable states by proving that every such state can provide an advantage in a quantum-channel discrimination task.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.15694v1" target="_blank">GoVector: An I/O-Efficient Caching Strategy for High-Dimensional Vector Nearest Neighbor Search</a></h3>
                    <p><strong>Authors:</strong> Yijie Zhou, Shengyuan Lin, Shufeng Gong, Song Yu, Shuhao Fan, Yanfeng Zhang, Ge Yu</p>
                    <p><strong>Published:</strong> 8/21/2025</p>
                    <p><strong>Categories:</strong> cs.DB</p>
                    <p><strong>Summary:</strong> Graph-based high-dimensional vector indices have become a mainstream solution for large-scale approximate nearest neighbor search (ANNS). However, their substantial memory footprint often requires storage on secondary devices, where frequent on-demand loading of graph and vector data leads to I/O becoming the dominant bottleneck, accounting for over 90\% of query latency. Existing static caching strategies mitigate this issue only in the initial navigation phase by preloading entry points and multi-hop neighbors, but they fail in the second phase where query-dependent nodes must be dynamically accessed to achieve high recall. We propose GoVector, an I/O-efficient caching strategy tailored for disk-based graph indices. GoVector combines (1) a static cache that stores entry points and frequently accessed neighbors, and (2) a dynamic cache that adaptively captures nodes with high spatial locality during the second search phase. To further align storage layout with similarity-driven search patterns, GoVector reorders nodes on disk so that similar vectors are colocated on the same or adjacent pages, thereby improving locality and reducing I/O overhead. Extensive experiments on multiple public datasets show that GoVector achieves substantial performance improvements. At 90% recall, it reduces I/O operations by 46% on average, increases query throughput by 1.73x, and lowers query latency by 42% compared to state-of-the-art disk-based graph indexing systems.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.15693v1" target="_blank">NiceWebRL: a Python library for human subject experiments with reinforcement learning environments</a></h3>
                    <p><strong>Authors:</strong> Wilka Carvalho, Vikram Goddla, Ishaan Sinha, Hoon Shin, Kunal Jha</p>
                    <p><strong>Published:</strong> 8/21/2025</p>
                    <p><strong>Categories:</strong> cs.AI</p>
                    <p><strong>Summary:</strong> We present NiceWebRL, a research tool that enables researchers to use machine reinforcement learning (RL) environments for online human subject experiments. NiceWebRL is a Python library that allows any Jax-based environment to be transformed into an online interface, supporting both single-agent and multi-agent environments. As such, NiceWebRL enables AI researchers to compare their algorithms to human performance, cognitive scientists to test ML algorithms as theories for human cognition, and multi-agent researchers to develop algorithms for human-AI collaboration. We showcase NiceWebRL with 3 case studies that demonstrate its potential to help develop Human-like AI, Human-compatible AI, and Human-assistive AI. In the first case study (Human-like AI), NiceWebRL enables the development of a novel RL model of cognition. Here, NiceWebRL facilitates testing this model against human participants in both a grid world and Craftax, a 2D Minecraft domain. In our second case study (Human-compatible AI), NiceWebRL enables the development of a novel multi-agent RL algorithm that can generalize to human partners in the Overcooked domain. Finally, in our third case study (Human-assistive AI), we show how NiceWebRL can allow researchers to study how an LLM can assist humans on complex tasks in XLand-Minigrid, an environment with millions of hierarchical tasks. The library is available at https://github.com/KempnerInstitute/nicewebrl.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.15690v1" target="_blank">GRAFT: GRaPH and Table Reasoning for Textual Alignment -- A Benchmark for Structured Instruction Following and Visual Reasoning</a></h3>
                    <p><strong>Authors:</strong> Abhigya Verma, Sriram Puttagunta, Seganrasan Subramanian, Sravan Ramachandran</p>
                    <p><strong>Published:</strong> 8/21/2025</p>
                    <p><strong>Categories:</strong> cs.AI, cs.LG, cs.MM</p>
                    <p><strong>Summary:</strong> GRAFT is a structured multimodal benchmark for evaluating models on instruction-following, visual reasoning, and visual-textual alignment tasks. It features programmatically generated charts and synthetically rendered tables, created with Python visualization libraries to ensure control over data semantics, structure, and clarity. Each GRAFT instance pairs a chart or table image with a systematically generated, multi-step analytical question based solely on visual content. Answers are provided in structured formats such as JSON or YAML, supporting consistent evaluation of both reasoning and output format. The benchmark introduces a taxonomy of reasoning types including comparison, trend identification, ranking, aggregation, proportion estimation, and anomaly detection to enable comprehensive assessment. Reference answers follow strict factual and formatting guidelines for precise, aspect-based evaluation. GRAFT offers a unified, scalable framework for fine-grained benchmarking of multimodal models on visually grounded, structured reasoning tasks, setting a new evaluation standard in this field.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.15688v1" target="_blank">LLM-empowered Dynamic Prompt Routing for Vision-Language Models Tuning under Long-Tailed Distributions</a></h3>
                    <p><strong>Authors:</strong> Yongju Jia, Jiarui Ma, Xiangxian Li, Baiqiao Zhang, Xianhui Cao, Juan Liu, Yulong Bian</p>
                    <p><strong>Published:</strong> 8/21/2025</p>
                    <p><strong>Categories:</strong> cs.CV, I.4.10</p>
                    <p><strong>Summary:</strong> Pre-trained vision-language models (VLMs), such as CLIP, have demonstrated impressive capability in visual tasks, but their fine-tuning often suffers from bias in class-imbalanced scene. Recent works have introduced large language models (LLMs) to enhance VLM fine-tuning with supplementing semantic information. However, they often overlook inherent class imbalance in VLMs pre-training, which may lead to bias accumulation in downstream tasks. To address this problem, this paper proposes a Multi-dimensional Dynamic Prompt Routing (MDPR) framework. MDPR constructs a comprehensive knowledge base for classes, spanning five visual-semantic dimensions. During fine-tuning, the dynamic routing mechanism aligns global visual classes, retrieves optimal prompts, and balances fine-grained semantics, yielding stable predictions through logits fusion. Extensive experiments on long-tailed benchmarks, including CIFAR-LT, ImageNet-LT, and Places-LT, demonstrate that MDPR achieves comparable results with current SOTA methods. Ablation studies further confirm the effectiveness of our semantic library for tail classes, and show that our dynamic routing incurs minimal computational overhead, making MDPR a flexible and efficient enhancement for VLM fine-tuning under data imbalance.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.15680v1" target="_blank">Futurity as Infrastructure: A Techno-Philosophical Interpretation of the AI Lifecycle</a></h3>
                    <p><strong>Authors:</strong> Mark Cote, Susana Aires</p>
                    <p><strong>Published:</strong> 8/21/2025</p>
                    <p><strong>Categories:</strong> cs.AI, cs.HC, I.2.6; I.2.11; K.4.1; K.6.0</p>
                    <p><strong>Summary:</strong> This paper argues that a techno-philosophical reading of the EU AI Act provides insight into the long-term dynamics of data in AI systems, specifically, how the lifecycle from ingestion to deployment generates recursive value chains that challenge existing frameworks for Responsible AI. We introduce a conceptual tool to frame the AI pipeline, spanning data, training regimes, architectures, feature stores, and transfer learning. Using cross-disciplinary methods, we develop a technically grounded and philosophically coherent analysis of regulatory blind spots. Our central claim is that what remains absent from policymaking is an account of the dynamic of becoming that underpins both the technical operation and economic logic of AI. To address this, we advance a formal reading of AI inspired by Simondonian philosophy of technology, reworking his concept of individuation to model the AI lifecycle, including the pre-individual milieu, individuation, and individuated AI. To translate these ideas, we introduce futurity: the self-reinforcing lifecycle of AI, where more data enhances performance, deepens personalisation, and expands application domains. Futurity highlights the recursively generative, non-rivalrous nature of data, underpinned by infrastructures like feature stores that enable feedback, adaptation, and temporal recursion. Our intervention foregrounds escalating power asymmetries, particularly the tech oligarchy whose infrastructures of capture, training, and deployment concentrate value and decision-making. We argue that effective regulation must address these infrastructural and temporal dynamics, and propose measures including lifecycle audits, temporal traceability, feedback accountability, recursion transparency, and a right to contest recursive reuse.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.15679v1" target="_blank">An Efficient Open World Environment for Multi-Agent Social Learning</a></h3>
                    <p><strong>Authors:</strong> Eric Ye, Ren Tao, Natasha Jaques</p>
                    <p><strong>Published:</strong> 8/21/2025</p>
                    <p><strong>Categories:</strong> cs.LG</p>
                    <p><strong>Summary:</strong> Many challenges remain before AI agents can be deployed in real-world environments. However, one virtue of such environments is that they are inherently multi-agent and contain human experts. Using advanced social intelligence in such an environment can help an AI agent learn adaptive skills and behaviors that a known expert exhibits. While social intelligence could accelerate training, it is currently difficult to study due to the lack of open-ended multi-agent environments. In this work, we present an environment in which multiple self-interested agents can pursue complex and independent goals, reflective of real world challenges. This environment will enable research into the development of socially intelligent AI agents in open-ended multi-agent settings, where agents may be implicitly incentivized to cooperate to defeat common enemies, build and share tools, and achieve long horizon goals. In this work, we investigate the impact on agent performance due to social learning in the presence of experts and implicit cooperation such as emergent collaborative tool use, and whether agents can benefit from either cooperation or competition in this environment.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.15672v1" target="_blank">CM2LoD3: Reconstructing LoD3 Building Models Using Semantic Conflict Maps</a></h3>
                    <p><strong>Authors:</strong> Franz Hanke, Antonia Bieringer, Olaf Wysocki, Boris Jutzi</p>
                    <p><strong>Published:</strong> 8/21/2025</p>
                    <p><strong>Categories:</strong> cs.CV, eess.IV</p>
                    <p><strong>Summary:</strong> Detailed 3D building models are crucial for urban planning, digital twins, and disaster management applications. While Level of Detail 1 (LoD)1 and LoD2 building models are widely available, they lack detailed facade elements essential for advanced urban analysis. In contrast, LoD3 models address this limitation by incorporating facade elements such as windows, doors, and underpasses. However, their generation has traditionally required manual modeling, making large-scale adoption challenging. In this contribution, CM2LoD3, we present a novel method for reconstructing LoD3 building models leveraging Conflict Maps (CMs) obtained from ray-to-model-prior analysis. Unlike previous works, we concentrate on semantically segmenting real-world CMs with synthetically generated CMs from our developed Semantic Conflict Map Generator (SCMG). We also observe that additional segmentation of textured models can be fused with CMs using confidence scores to further increase segmentation performance and thus increase 3D reconstruction accuracy. Experimental results demonstrate the effectiveness of our CM2LoD3 method in segmenting and reconstructing building openings, with the 61% performance with uncertainty-aware fusion of segmented building textures. This research contributes to the advancement of automated LoD3 model reconstruction, paving the way for scalable and efficient 3D city modeling. Our project is available: https://github.com/InFraHank/CM2LoD3</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.15663v1" target="_blank">Mind and Motion Aligned: A Joint Evaluation IsaacSim Benchmark for Task Planning and Low-Level Policies in Mobile Manipulation</a></h3>
                    <p><strong>Authors:</strong> Nikita Kachaev, Andrei Spiridonov, Andrey Gorodetsky, Kirill Muravyev, Nikita Oskolkov, Aditya Narendra, Vlad Shakhuro, Dmitry Makarov, Aleksandr I. Panov, Polina Fedotova, Alexey K. Kovalev</p>
                    <p><strong>Published:</strong> 8/21/2025</p>
                    <p><strong>Categories:</strong> cs.RO, cs.AI</p>
                    <p><strong>Summary:</strong> Benchmarks are crucial for evaluating progress in robotics and embodied AI. However, a significant gap exists between benchmarks designed for high-level language instruction following, which often assume perfect low-level execution, and those for low-level robot control, which rely on simple, one-step commands. This disconnect prevents a comprehensive evaluation of integrated systems where both task planning and physical execution are critical. To address this, we propose Kitchen-R, a novel benchmark that unifies the evaluation of task planning and low-level control within a simulated kitchen environment. Built as a digital twin using the Isaac Sim simulator and featuring more than 500 complex language instructions, Kitchen-R supports a mobile manipulator robot. We provide baseline methods for our benchmark, including a task-planning strategy based on a vision-language model and a low-level control policy based on diffusion policy. We also provide a trajectory collection system. Our benchmark offers a flexible framework for three evaluation modes: independent assessment of the planning module, independent assessment of the control policy, and, crucially, an integrated evaluation of the whole system. Kitchen-R bridges a key gap in embodied AI research, enabling more holistic and realistic benchmarking of language-guided robotic agents.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.15658v1" target="_blank">Benchmarking Computer Science Survey Generation</a></h3>
                    <p><strong>Authors:</strong> Weihang Su, Anzhe Xie, Qingyao Ai, Jianming Long, Jiaxin Mao, Ziyi Ye, Yiqun Liu</p>
                    <p><strong>Published:</strong> 8/21/2025</p>
                    <p><strong>Categories:</strong> cs.CL, cs.AI, cs.IR</p>
                    <p><strong>Summary:</strong> Scientific survey articles play a vital role in summarizing research progress, yet their manual creation is becoming increasingly infeasible due to the rapid growth of academic literature. While large language models (LLMs) offer promising capabilities for automating this process, progress in this area is hindered by the absence of standardized benchmarks and evaluation protocols. To address this gap, we introduce SurGE (Survey Generation Evaluation), a new benchmark for evaluating scientific survey generation in the computer science domain. SurGE consists of (1) a collection of test instances, each including a topic description, an expert-written survey, and its full set of cited references, and (2) a large-scale academic corpus of over one million papers that serves as the retrieval pool. In addition, we propose an automated evaluation framework that measures generated surveys across four dimensions: information coverage, referencing accuracy, structural organization, and content quality. Our evaluation of diverse LLM-based approaches shows that survey generation remains highly challenging, even for advanced self-reflection frameworks. These findings highlight the complexity of the task and the necessity for continued research. We have open-sourced all the code, data, and models at: https://github.com/oneal2000/SurGE</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.15653v1" target="_blank">MapKD: Unlocking Prior Knowledge with Cross-Modal Distillation for Efficient Online HD Map Construction</a></h3>
                    <p><strong>Authors:</strong> Ziyang Yan, Ruikai Li, Zhiyong Cui, Bohan Li, Han Jiang, Yilong Ren, Aoyong Li, Zhenning Li, Sijia Wen, Haiyang Yu</p>
                    <p><strong>Published:</strong> 8/21/2025</p>
                    <p><strong>Categories:</strong> cs.CV</p>
                    <p><strong>Summary:</strong> Online HD map construction is a fundamental task in autonomous driving systems, aiming to acquire semantic information of map elements around the ego vehicle based on real-time sensor inputs. Recently, several approaches have achieved promising results by incorporating offline priors such as SD maps and HD maps or by fusing multi-modal data. However, these methods depend on stale offline maps and multi-modal sensor suites, resulting in avoidable computational overhead at inference. To address these limitations, we employ a knowledge distillation strategy to transfer knowledge from multimodal models with prior knowledge to an efficient, low-cost, and vision-centric student model. Specifically, we propose MapKD, a novel multi-level cross-modal knowledge distillation framework with an innovative Teacher-Coach-Student (TCS) paradigm. This framework consists of: (1) a camera-LiDAR fusion model with SD/HD map priors serving as the teacher; (2) a vision-centric coach model with prior knowledge and simulated LiDAR to bridge the cross-modal knowledge transfer gap; and (3) a lightweight vision-based student model. Additionally, we introduce two targeted knowledge distillation strategies: Token-Guided 2D Patch Distillation (TGPD) for birds eye view feature alignment and Masked Semantic Response Distillation (MSRD) for semantic learning guidance. Extensive experiments on the challenging nuScenes dataset demonstrate that MapKD improves the student model by +6.68 mIoU and +10.94 mAP while simultaneously accelerating inference speed. The code is available at:https://github.com/2004yan/MapKD2026.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.15652v1" target="_blank">Understanding Action Effects through Instrumental Empowerment in Multi-Agent Reinforcement Learning</a></h3>
                    <p><strong>Authors:</strong> Ardian Selmonaj, Miroslav Strupl, Oleg Szehr, Alessandro Antonucci</p>
                    <p><strong>Published:</strong> 8/21/2025</p>
                    <p><strong>Categories:</strong> cs.AI, cs.IT, cs.LG, cs.MA, math.IT</p>
                    <p><strong>Summary:</strong> To reliably deploy Multi-Agent Reinforcement Learning (MARL) systems, it is crucial to understand individual agent behaviors within a team. While prior work typically evaluates overall team performance based on explicit reward signals or learned value functions, it is unclear how to infer agent contributions in the absence of any value feedback. In this work, we investigate whether meaningful insights into agent behaviors can be extracted that are consistent with the underlying value functions, solely by analyzing the policy distribution. Inspired by the phenomenon that intelligent agents tend to pursue convergent instrumental values, which generally increase the likelihood of task success, we introduce Intended Cooperation Values (ICVs), a method based on information-theoretic Shapley values for quantifying each agents causal influence on their co-players instrumental empowerment. Specifically, ICVs measure an agents action effect on its teammates policies by assessing their decision uncertainty and preference alignment. The analysis across cooperative and competitive MARL environments reveals the extent to which agents adopt similar or diverse strategies. By comparing action effects between policies and value functions, our method identifies which agent behaviors are beneficial to team success, either by fostering deterministic decisions or by preserving flexibility for future action choices. Our proposed method offers novel insights into cooperation dynamics and enhances explainability in MARL systems.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.15648v1" target="_blank">SDGO: Self-Discrimination-Guided Optimization for Consistent Safety in Large Language Models</a></h3>
                    <p><strong>Authors:</strong> Peng Ding, Wen Sun, Dailin Li, Wei Zou, Jiaming Wang, Jiajun Chen, Shujian Huang</p>
                    <p><strong>Published:</strong> 8/21/2025</p>
                    <p><strong>Categories:</strong> cs.CL</p>
                    <p><strong>Summary:</strong> Large Language Models (LLMs) excel at various natural language processing tasks but remain vulnerable to jailbreaking attacks that induce harmful content generation. In this paper, we reveal a critical safety inconsistency: LLMs can more effectively identify harmful requests as discriminators than defend against them as generators. This insight inspires us to explore aligning the models inherent discrimination and generation capabilities. To this end, we propose SDGO (Self-Discrimination-Guided Optimization), a reinforcement learning framework that leverages the models own discrimination capabilities as a reward signal to enhance generation safety through iterative self-improvement. Our method does not require any additional annotated data or external models during the training phase. Extensive experiments demonstrate that SDGO significantly improves model safety compared to both prompt-based and training-based baselines while maintaining helpfulness on general benchmarks. By aligning LLMs discrimination and generation capabilities, SDGO brings robust performance against out-of-distribution (OOD) jailbreaking attacks. This alignment achieves tighter coupling between these two capabilities, enabling the models generation capability to be further enhanced with only a small amount of discriminative samples. Our code and datasets are available at https://github.com/NJUNLP/SDGO.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.15643v1" target="_blank">Reading Between the Lines: A Study of Thematic Bias in Book Recommender Systems</a></h3>
                    <p><strong>Authors:</strong> Nityaa Kalra, Savvina Daniil</p>
                    <p><strong>Published:</strong> 8/21/2025</p>
                    <p><strong>Categories:</strong> cs.IR</p>
                    <p><strong>Summary:</strong> Recommender systems help users discover new content, but can also reinforce existing biases, leading to unfair exposure and reduced diversity. This paper introduces and investigates thematic bias in book recommendations, defined as a disproportionate favouring or neglect of certain book themes. We adopt a multi-stage bias evaluation framework using the Book-Crossing dataset to evaluate thematic bias in recommendations and its impact on different user groups. Our findings show that thematic bias originates from content imbalances and is amplified by user engagement patterns. By segmenting users based on their thematic preferences, we find that users with niche and long-tail interests receive less personalised recommendations, whereas users with diverse interests receive more consistent recommendations. These findings suggest that recommender systems should be carefully designed to accommodate a broader range of user interests. By contributing to the broader goal of responsible AI, this work also lays the groundwork for extending thematic bias analysis to other domains.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.15641v1" target="_blank">When and What: Diffusion-Grounded VideoLLM with Entity Aware Segmentation for Long Video Understanding</a></h3>
                    <p><strong>Authors:</strong> Pengcheng Fang, Yuxia Chen, Rui Guo</p>
                    <p><strong>Published:</strong> 8/21/2025</p>
                    <p><strong>Categories:</strong> cs.CV</p>
                    <p><strong>Summary:</strong> Understanding videos requires more than answering open ended questions, it demands the ability to pinpoint when events occur and how entities interact across time. While recent Video LLMs have achieved remarkable progress in holistic reasoning, they remain coarse in temporal perception: timestamps are encoded only implicitly, frame level features are weak in capturing continuity, and language vision alignment often drifts from the entities of interest. In this paper, we present Grounded VideoDiT, a Video LLM designed to overcome these limitations by introducing three key innovations. First, a Diffusion Temporal Latent (DTL) encoder enhances boundary sensitivity and maintains temporal consistency. Second, object grounded representations explicitly bind query entities to localized visual evidence, strengthening alignment. Third, a mixed token scheme with discrete temporal tokens provides explicit timestamp modeling, enabling fine grained temporal reasoning. Together, these designs equip Grounded VideoDiT with robust grounding capabilities, as validated by state of the art results on Charades STA, NExT GQA, and multiple VideoQA benchmarks.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.15637v1" target="_blank">Classification errors distort findings in automated speech processing: examples and solutions from child-development research</a></h3>
                    <p><strong>Authors:</strong> Lucas Gautheron, Evan Kidd, Anton Malko, Marvin Lavechin, Alejandrina Cristia</p>
                    <p><strong>Published:</strong> 8/21/2025</p>
                    <p><strong>Categories:</strong> cs.LG, cs.CL, stat.AP</p>
                    <p><strong>Summary:</strong> With the advent of wearable recorders, scientists are increasingly turning to automated methods of analysis of audio and video data in order to measure childrens experience, behavior, and outcomes, with a sizable literature employing long-form audio-recordings to study language acquisition. While numerous articles report on the accuracy and reliability of the most popular automated classifiers, less has been written on the downstream effects of classification errors on measurements and statistical inferences (e.g., the estimate of correlations and effect sizes in regressions). This paper proposes a Bayesian approach to study the effects of algorithmic errors on key scientific questions, including the effect of siblings on childrens language experience and the association between childrens production and their input. In both the most commonly used \gls{lena}, and an open-source alternative (the Voice Type Classifier from the ACLEW system), we find that classification errors can significantly distort estimates. For instance, automated annotations underestimated the negative effect of siblings on adult input by 20--80\%, potentially placing it below statistical significance thresholds. We further show that a Bayesian calibration approach for recovering unbiased estimates of effect sizes can be effective and insightful, but does not provide a fool-proof solution. Both the issue reported and our solution may apply to any classifier involving event detection and classification with non-zero error rates.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.15635v1" target="_blank">Label Uncertainty for Ultrasound Segmentation</a></h3>
                    <p><strong>Authors:</strong> Malini Shivaram, Gautam Rajendrakumar Gare, Laura Hutchins, Jacob Duplantis, Thomas Deiss, Thales Nogueira Gomes, Thong Tran, Keyur H. Patel, Thomas H Fox, Amita Krishnan, Deva Ramanan, Bennett DeBoisblanc, Ricardo Rodriguez, John Galeotti</p>
                    <p><strong>Published:</strong> 8/21/2025</p>
                    <p><strong>Categories:</strong> eess.IV, cs.AI, cs.CV, cs.LG, stat.ML</p>
                    <p><strong>Summary:</strong> In medical imaging, inter-observer variability among radiologists often introduces label uncertainty, particularly in modalities where visual interpretation is subjective. Lung ultrasound (LUS) is a prime example-it frequently presents a mixture of highly ambiguous regions and clearly discernible structures, making consistent annotation challenging even for experienced clinicians. In this work, we introduce a novel approach to both labeling and training AI models using expert-supplied, per-pixel confidence values. Rather than treating annotations as absolute ground truth, we design a data annotation protocol that captures the confidence that radiologists have in each labeled region, modeling the inherent aleatoric uncertainty present in real-world clinical data. We demonstrate that incorporating these confidence values during training leads to improved segmentation performance. More importantly, we show that this enhanced segmentation quality translates into better performance on downstream clinically-critical tasks-specifically, estimating S/F oxygenation ratio values, classifying S/F ratio change, and predicting 30-day patient readmission. While we empirically evaluate many methods for exposing the uncertainty to the learning model, we find that a simple approach that trains a model on binarized labels obtained with a (60%) confidence threshold works well. Importantly, high thresholds work far better than a naive approach of a 50% threshold, indicating that training on very confident pixels is far more effective. Our study systematically investigates the impact of training with varying confidence thresholds, comparing not only segmentation metrics but also downstream clinical outcomes. These results suggest that label confidence is a valuable signal that, when properly leveraged, can significantly enhance the reliability and clinical utility of AI in medical imaging.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.15620v1" target="_blank">Low-Power Control of Resistance Switching Transitions in First-Order Memristors</a></h3>
                    <p><strong>Authors:</strong> Valeriy A. Slipko, Alon Ascoli, Fernando Corinto, Yuriy V. Pershin</p>
                    <p><strong>Published:</strong> 8/21/2025</p>
                    <p><strong>Categories:</strong> cs.ET, cond-mat.mes-hall</p>
                    <p><strong>Summary:</strong> In many cases, the behavior of physical memristive devices can be relatively well captured by using a single internal state variable. This study investigates the low-power control of first-order memristive devices to derive the most energy-efficient protocols for programming their resistances. A unique yet general approach to optimizing the switching transitions in devices of this kind is introduced. For pedagogical purposes, without loss of generality, the proposed control paradigm is applied to a couple of differential algebraic equation sets for voltage-controlled devices, specifically Kvatinskys Voltage ThrEshold Adaptive Memristor mathematical description and Mirandas and Sunes dynamic balance model. It is demonstrated that, depending upon intrinsic physical properties of the device, captured in the model formulas and parameter setting, and upon constraints on programming time and voltages, the optimal protocol for either of the two switching scenarios may require the application of a single square voltage pulse of height set to a certain level within the admissible range across a fraction or entire given programming time interval, or of some more involved voltage stimulus of unique polarity, including analogue continuous waveforms that can be approximated by trains of square voltage pulses of different heights, over the entire programming time interval. The practical implications of these research findings are significant, as the development of energy-efficient protocols to program memristive devices, resolving the so-called voltage-time dilemma in the device physics community, is a subject under intensive and extensive studies across the academic community and industry.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.15610v1" target="_blank">Transduction is All You Need for Structured Data Workflows</a></h3>
                    <p><strong>Authors:</strong> Alfio Gliozzo, Naweed Khan, Christodoulos Constantinides, Nandana Mihindukulasooriya, Nahuel Defosse, Junkyu Lee</p>
                    <p><strong>Published:</strong> 8/21/2025</p>
                    <p><strong>Categories:</strong> cs.AI, cs.LG</p>
                    <p><strong>Summary:</strong> This paper introduces Agentics, a modular framework for building agent-based systems capable of structured reasoning and compositional generalization over complex data. Designed with research and practical applications in mind, Agentics offers a novel perspective on working with data and AI workflows. In this framework, agents are abstracted from the logical flow and they are used internally to the data type to enable logical transduction among data. Agentics encourages AI developers to focus on modeling data rather than crafting prompts, enabling a declarative language in which data types are provided by LLMs and composed through logical transduction, which is executed by LLMs when types are connected. We provide empirical evidence demonstrating the applicability of this framework across domain-specific multiple-choice question answering, semantic parsing for text-to-SQL, and automated prompt optimization tasks, achieving state-of-the-art accuracy or improved scalability without sacrificing performance. The open-source implementation is available at \texttt{https://github.com/IBM/agentics}.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.15601v1" target="_blank">Efficient Mixed-Precision Large Language Model Inference with TurboMind</a></h3>
                    <p><strong>Authors:</strong> Li Zhang, Youhe Jiang, Guoliang He, Xin Chen, Han Lv, Qian Yao, Fangcheng Fu, Kai Chen</p>
                    <p><strong>Published:</strong> 8/21/2025</p>
                    <p><strong>Categories:</strong> cs.DC, cs.PF</p>
                    <p><strong>Summary:</strong> Mixed-precision inference techniques reduce the memory and computational demands of Large Language Models (LLMs) by applying hybrid precision formats to model weights, activations, and KV caches. This work introduces mixed-precision LLM inference techniques that encompass (i) systematic memory and compute optimization across hierarchical storage and tensor core architectures, and (ii) comprehensive end-to-end mixed-precision optimization across diverse precision formats and hardware configurations. Our approach features two novel mixed-precision pipelines designed for optimal hardware utilization: a General Matrix Multiply (GEMM) pipeline that optimizes matrix operations through offline weight packing and online acceleration, and an attention pipeline that enables efficient attention computation with arbitrary Query, Key, and Value precision combinations. The key implementation of the pipelines includes (i) hardware-aware weight packing for automatic format optimization, (ii) adaptive head alignment for efficient attention computation, (iii) instruction-level parallelism for memory hierarchy exploitation, and (iv) KV memory loading pipeline for enhanced inference efficiency. We conduct comprehensive evaluations across 16 popular LLMs and 4 representative GPU architectures. Results demonstrate that our approach achieves up to 61% lower serving latency (30% on average) and up to 156% higher throughput (58% on average) in mixed-precision workloads compared to existing mixed-precision frameworks, establishing consistent performance improvements across all tested configurations and hardware types. This work is integrated into TurboMind, a high-performance inference engine of the LMDeploy project, which is open-sourced and publicly available at https://github.com/InternLM/lmdeploy.</p>
                
            
                
                    <h3><a href="http://dx.doi.org/10.1016/j.jmst.2024.11.026" target="_blank">Ultrastrong and ductile CoNiMoAl medium-entropy alloys enabled by L12 nanoprecipitate-induced multiple deformation mechanisms</a></h3>
                    <p><strong>Authors:</strong> Min Young Sung, Tae Jin Jang, Sang Yoon Song, Gunjick Lee, KenHee Ryou, Sang-Ho Oh, Byeong-Joo Lee, Pyuck-Pa Choi, JÃ¶rg Neugebauer, Blazej Grabowski, Fritz KÃ¶rmann, Yuji Ikeda, Alireza Zargaran, Seok Su Sohn</p>
                    <p><strong>Published:</strong> 8/21/2025</p>
                    <p><strong>Categories:</strong> cond-mat.mtrl-sci</p>
                    <p><strong>Summary:</strong> L12 precipitates are known to significantly enhance the strength and ductility of single-phase face-centered cubic (FCC) medium- or high-entropy alloys (M/HEAs). However, further improvements in mechanical properties remain untapped, as alloy design has historically focused on systems with specific CrCoNi- or FeCoCrNi-based FCC matrix and Ni3Al L12 phase compositions. This study introduces novel Co-Ni-Mo-Al alloys with L12 precipitates by systematically altering Al content, aiming to bridge this research gap by revealing the strengthening mechanisms. The (CoNi)81Mo12Al7 alloy achieves yield strength of 1086 MPa, tensile strength of 1520 MPa, and ductility of 35 %, demonstrating an impressive synergy of strength, ductility, and strain-hardening capacity. Dislocation analysis via transmission electron microscopy, supported by generalized stacking fault energy (GSFE) calculations using density functional theory (DFT), demonstrates that Mo substitution for Al in the L12 phase alters dislocation behavior, promoting the formation of multiple deformation modes, including stacking faults, super-dislocation pairs, Lomer-Cottrell locks, and unusual nano-twin formation even at low strains. These behaviors are facilitated by the low stacking fault energy (SFE) of the FCC matrix, overlapping of SFs, and dislocation dissociation across anti-phase boundaries (APBs). The increased energy barrier for superlattice intrinsic stacking fault (SISF) formation compared to APBs, due to Mo substitution, further influences dislocation activity. This work demonstrates a novel strategy for designing high-performance M/HEAs by expanding the range of FCC matrix and L12 compositions through precipitation hardening.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.15595v1" target="_blank">Interface on demand: Towards AI native Control interfaces for 6G</a></h3>
                    <p><strong>Authors:</strong> Abhishek Dandekar, Prashiddha D. Thapa, Ashrafur Rahman, Julius Schulz-Zander</p>
                    <p><strong>Published:</strong> 8/21/2025</p>
                    <p><strong>Categories:</strong> cs.NI</p>
                    <p><strong>Summary:</strong> Traditional standardized network interfaces face significant limitations, including vendor-specific incompatibilities, rigid design assumptions, and lack of adaptability for new functionalities. We propose a multi-agent framework leveraging large language models (LLMs) to generate control interfaces on demand between network functions (NFs). This includes a matching agent, which aligns required control functionalities with NF capabilities, and a code-generation agent, which generates the necessary API server for interface realization. We validate our approach using simulated multi-vendor gNB and WLAN AP environments. The performance evaluations highlight the trade-offs between cost and latency across LLMs for interface generation tasks. Our work sets the foundation for AI-native dynamic control interface generation, paving the way for enhanced interoperability and adaptability in future mobile networks.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.15593v1" target="_blank">Inductive Domain Transfer In Misspecified Simulation-Based Inference</a></h3>
                    <p><strong>Authors:</strong> Ortal Senouf, Antoine Wehenkel, CÃ©dric Vincent-Cuaz, Emmanuel AbbÃ©, Pascal Frossard</p>
                    <p><strong>Published:</strong> 8/21/2025</p>
                    <p><strong>Categories:</strong> cs.LG</p>
                    <p><strong>Summary:</strong> Simulation-based inference (SBI) is a statistical inference approach for estimating latent parameters of a physical system when the likelihood is intractable but simulations are available. In practice, SBI is often hindered by model misspecification--the mismatch between simulated and real-world observations caused by inherent modeling simplifications. RoPE, a recent SBI approach, addresses this challenge through a two-stage domain transfer process that combines semi-supervised calibration with optimal transport (OT)-based distribution alignment. However, RoPE operates in a fully transductive setting, requiring access to a batch of test samples at inference time, which limits scalability and generalization. We propose here a fully inductive and amortized SBI framework that integrates calibration and distributional alignment into a single, end-to-end trainable model. Our method leverages mini-batch OT with a closed-form coupling to align real and simulated observations that correspond to the same latent parameters, using both paired calibration data and unpaired samples. A conditional normalizing flow is then trained to approximate the OT-induced posterior, enabling efficient inference without simulation access at test time. Across a range of synthetic and real-world benchmarks--including complex medical biomarker estimation--our approach matches or surpasses the performance of RoPE, as well as other standard SBI and non-SBI estimators, while offering improved scalability and applicability in challenging, misspecified environments.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.15587v1" target="_blank">Investigating the sliding behavior of graphene nanoribbons</a></h3>
                    <p><strong>Authors:</strong> Gourav Yadav, Aningi Mokhalingam, Roger A. Sauer, Shakti S. Gupta</p>
                    <p><strong>Published:</strong> 8/21/2025</p>
                    <p><strong>Categories:</strong> physics.comp-ph</p>
                    <p><strong>Summary:</strong> This work presents a Euler-Bernoulli beam finite element (FE) model to study the interlayer interaction mechanics of graphene nanoribbon (GNR) over a graphene substrate. The FE model is calibrated using molecular dynamics (MD) simulations employing the potential of Kolmogorov and Crespi. This study focuses mainly on the effect of boundary conditions on sliding behavior and strain transfer between layers when the substrate is subjected to uniform biaxial deformations. The interlayer shearing or sliding behavior is found to depend on the presence of critical parameters, namely, the applied strain to the substrate and the length of the GNR. The FE results indicate that the applied strain transferred from the substrate to the GNR varies linearly up to a critical value ec beyond which it decreases suddenly. Further, ec is found to appear beyond a critical GNR length, Le is approximately 10 nm. Furthermore, a length parameter Ld is approximately 10 nm is computed, beyond which the sliding of GNR is dissipative. Through FE simulations, it is also found that for a GNR length is greater than or equal to 17 nm, the edge pulling force saturates. Our results also highlight the importance of the inertia of GNR on its sliding for different boundary conditions. It is also concluded that the maximum strain that can be transferred to GNR lies between 0.57% and 1.15%. The results of the FE approach align with MD simulations within an error of approximately 10% that can be attributed to the choice of material parameters and the simulation setup.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.15570v1" target="_blank">Establishing Technical Debt Management -- A Five-Step Workshop Approach and an Action Research Study</a></h3>
                    <p><strong>Authors:</strong> Marion Wiese, Kamila Serwa, Anastasia Besier, Ariane S. Marion-Jetten, Eva Bittner</p>
                    <p><strong>Published:</strong> 8/21/2025</p>
                    <p><strong>Categories:</strong> cs.SE</p>
                    <p><strong>Summary:</strong> Context. Technical debt (TD) items are constructs in a software system providing short-term benefits but hindering future changes. TD management (TDM) is frequently researched but rarely adopted in practice. Goal. This study aimed to establish a TDM process in an IT company based on a predefined workshop concept. We analyzed which research approaches practitioners adopted for each TD activity and the TDMs long-term effect on TD awareness. Method. We used action research (five action cycles in 16 months) with an IT team that creates IT solutions for signal processing. To examine TD awareness, we (1) analyzed questionnaires completed during each workshop, (2) observed team meetings, (3) adopted a method from psychology for measuring awareness in decision-making situations called TD-SAGAT, and (4) evaluated the backlog data. Results. Practitioners preferred TD repayment and prioritization based on the systems evolution and cost calculations, i.e., repayment of so-called low-hanging fruits. Reminders in the backlog items, such as checkboxes or text templates, led to a sustainable rise in TD awareness. Conclusions. We showed that a workshop-based approach is feasible and leads to sustainable process changes. New ideas for TDM applicable to other IT teams emerged, e.g., using a re-submission date, using a Talked about TD checkbox, and using visualizations for TD prioritization.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.15569v1" target="_blank">Conformalized Exceptional Model Mining: Telling Where Your Model Performs (Not) Well</a></h3>
                    <p><strong>Authors:</strong> Xin Du, Sikun Yang, Wouter Duivesteijn, Mykola Pechenizkiy</p>
                    <p><strong>Published:</strong> 8/21/2025</p>
                    <p><strong>Categories:</strong> cs.LG</p>
                    <p><strong>Summary:</strong> Understanding the nuanced performance of machine learning models is essential for responsible deployment, especially in high-stakes domains like healthcare and finance. This paper introduces a novel framework, Conformalized Exceptional Model Mining, which combines the rigor of Conformal Prediction with the explanatory power of Exceptional Model Mining (EMM). The proposed framework identifies cohesive subgroups within data where model performance deviates exceptionally, highlighting regions of both high confidence and high uncertainty. We develop a new model class, mSMoPE (multiplex Soft Model Performance Evaluation), which quantifies uncertainty through conformal predictions rigorous coverage guarantees. By defining a new quality measure, Relative Average Uncertainty Loss (RAUL), our framework isolates subgroups with exceptional performance patterns in multi-class classification and regression tasks. Experimental results across diverse datasets demonstrate the frameworks effectiveness in uncovering interpretable subgroups that provide critical insights into model behavior. This work lays the groundwork for enhancing model interpretability and reliability, advancing the state-of-the-art in explainable AI and uncertainty quantification.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.15568v1" target="_blank">Backpropagation-Free Test-Time Adaptation via Probabilistic Gaussian Alignment</a></h3>
                    <p><strong>Authors:</strong> Youjia Zhang, Youngeun Kim, Young-Geun Choi, Hongyeob Kim, Huiling Liu, Sungeun Hong</p>
                    <p><strong>Published:</strong> 8/21/2025</p>
                    <p><strong>Categories:</strong> cs.CV, cs.LG</p>
                    <p><strong>Summary:</strong> Test-time adaptation (TTA) enhances the zero-shot robustness under distribution shifts by leveraging unlabeled test data during inference. Despite notable advances, several challenges still limit its broader applicability. First, most methods rely on backpropagation or iterative optimization, which limits scalability and hinders real-time deployment. Second, they lack explicit modeling of class-conditional feature distributions. This modeling is crucial for producing reliable decision boundaries and calibrated predictions, but it remains underexplored due to the lack of both source data and supervision at test time. In this paper, we propose ADAPT, an Advanced Distribution-Aware and backPropagation-free Test-time adaptation method. We reframe TTA as a Gaussian probabilistic inference task by modeling class-conditional likelihoods using gradually updated class means and a shared covariance matrix. This enables closed-form, training-free inference. To correct potential likelihood bias, we introduce lightweight regularization guided by CLIP priors and a historical knowledge bank. ADAPT requires no source data, no gradient updates, and no full access to target data, supporting both online and transductive settings. Extensive experiments across diverse benchmarks demonstrate that our method achieves state-of-the-art performance under a wide range of distribution shifts with superior scalability and robustness.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.15565v1" target="_blank">Any-to-any Speaker Attribute Perturbation for Asynchronous Voice Anonymization</a></h3>
                    <p><strong>Authors:</strong> Liping Chen, Chenyang Guo, Rui Wang, Kong Aik Lee, Zhenhua Ling</p>
                    <p><strong>Published:</strong> 8/21/2025</p>
                    <p><strong>Categories:</strong> cs.SD</p>
                    <p><strong>Summary:</strong> Speaker attribute perturbation offers a feasible approach to asynchronous voice anonymization by employing adversarially perturbed speech as anonymized output. In order to enhance the identity unlinkability among anonymized utterances from the same original speaker, the targeted attack training strategy is usually applied to anonymize the utterances to a common designated speaker. However, this strategy may violate the privacy of the designated speaker who is an actual speaker. To mitigate this risk, this paper proposes an any-to-any training strategy. It is accomplished by defining a batch mean loss to anonymize the utterances from various speakers within a training mini-batch to a common pseudo-speaker, which is approximated as the average speaker in the mini-batch. Based on this, a speaker-adversarial speech generation model is proposed, incorporating the supervision from both the untargeted attack and the any-to-any strategies. The speaker attribute perturbations are generated and incorporated into the original speech to produce its anonymized version. The effectiveness of the proposed model was justified in asynchronous voice anonymization through experiments conducted on the VoxCeleb datasets. Additional experiments were carried out to explore the potential limitations of speaker-adversarial speech in voice privacy protection. With them, we aim to provide insights for future research on its protective efficacy against black-box speaker extractors \textcolor{black}{and adaptive attacks, as well as} generalization to out-of-domain datasets \textcolor{black}{and stability}. Audio samples and open-source code are published in https://github.com/VoicePrivacy/any-to-any-speaker-attribute-perturbation.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.15554v1" target="_blank">Uncertainty Relation for the Wigner-Yanase Skew Information and Quantum Sobolev Inequalities</a></h3>
                    <p><strong>Authors:</strong> Laurent Lafleche</p>
                    <p><strong>Published:</strong> 8/21/2025</p>
                    <p><strong>Categories:</strong> math-ph, math.FA, math.MP, quant-ph, 81S07, 46E35, 81S30 (Primary) 47A30 (Secondary)</p>
                    <p><strong>Summary:</strong> This note explores uncertainty inequalities for quantum analogues of the Fisher information including the Wigner-Yanase skew information, and their connection to the quantum Sobolev inequalities proved by the author in [Journal of Functional Analysis, 286 (10) 2024]. Some additional inequalities concerning commutators are derived and others are left as open problems.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.15550v1" target="_blank">AI-Powered Machine Learning Approaches for Fault Diagnosis in Industrial Pumps</a></h3>
                    <p><strong>Authors:</strong> Khaled M. A. Alghtus, Ayad Gannan, Khalid M. Alhajri, Ali L. A. Al Jubouri, Hassan A. I. Al-Janahi</p>
                    <p><strong>Published:</strong> 8/21/2025</p>
                    <p><strong>Categories:</strong> cs.LG</p>
                    <p><strong>Summary:</strong> This study presents a practical approach for early fault detection in industrial pump systems using real-world sensor data from a large-scale vertical centrifugal pump operating in a demanding marine environment. Five key operational parameters were monitored: vibration, temperature, flow rate, pressure, and electrical current. A dual-threshold labeling method was applied, combining fixed engineering limits with adaptive thresholds calculated as the 95th percentile of historical sensor values. To address the rarity of documented failures, synthetic fault signals were injected into the data using domain-specific rules, simulating critical alerts within plausible operating ranges. Three machine learning classifiers - Random Forest, Extreme Gradient Boosting (XGBoost), and Support Vector Machine (SVM) - were trained to distinguish between normal operation, early warnings, and critical alerts. Results showed that Random Forest and XGBoost models achieved high accuracy across all classes, including minority cases representing rare or emerging faults, while the SVM model exhibited lower sensitivity to anomalies. Visual analyses, including grouped confusion matrices and time-series plots, indicated that the proposed hybrid method provides robust detection capabilities. The framework is scalable, interpretable, and suitable for real-time industrial deployment, supporting proactive maintenance decisions before failures occur. Furthermore, it can be adapted to other machinery with similar sensor architectures, highlighting its potential as a scalable solution for predictive maintenance in complex systems.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.15516v1" target="_blank">The Digital Life of Parisian Parks: Multifunctionality and Urban Context Uncovered by Mobile Application Traffic</a></h3>
                    <p><strong>Authors:</strong> AndrÃ© Felipe Zanella, Linus W. Dietz, Sanja Å Ä‡epanoviÄ‡, Ke Zhou, Zbigniew Smoreda, Daniele Quercia</p>
                    <p><strong>Published:</strong> 8/21/2025</p>
                    <p><strong>Categories:</strong> cs.CY</p>
                    <p><strong>Summary:</strong> Landscape architecture typically considers urban parks through the lens of form and function. While past research on equitable access has focused mainly on form, studies of functions have been constrained by limited scale and coarse measurement. Existing efforts have partially quantified functions through small-scale surveys and movement data (e.g., GPS) or general usage records (e.g., CDR), but have not captured the activities and motivations underlying park visits. As a result, our understanding of the functional roles urban parks play remains incomplete. To address this gap, we introduce a method that refines mobile base station coverage using antenna azimuths, enabling clearer distinction of mobile traffic within parks versus surrounding areas. Using Paris as a case study, we analyze a large-scale set of passively collected per-app mobile network traffic - 492 million hourly records for 45 parks. We test two hypotheses: the central-city hypothesis, which posits multifunctional parks emerge in dense, high-rent areas due to land scarcity; and the socio-spatial hypothesis, which views parks as reflections of neighborhood routines and preferences. Our analysis shows that parks have distinctive mobile traffic signatures, differing from both their surroundings and from each other. By clustering parks on temporal and app usage patterns, we identify three functional types - lunchbreak, cultural, and recreational - with different visitation motivations. Centrally located parks (cultural and lunchbreak) display more diverse app use and temporal variation, while suburban (recreational) parks reflect digital behaviors of nearby communities, with app preferences aligned to neighborhood income. These findings demonstrate the value of mobile traffic as a proxy for studying urban green space functions, with implications for park planning, public health, and well-being.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.15510v1" target="_blank">Super-additive Cooperation in Language Model Agents</a></h3>
                    <p><strong>Authors:</strong> Filippo Tonini, Lukas Galke</p>
                    <p><strong>Published:</strong> 8/21/2025</p>
                    <p><strong>Categories:</strong> cs.AI, I.2.11; I.2.0; J.4; K.4.0; I.2.6</p>
                    <p><strong>Summary:</strong> With the prospect of autonomous artificial intelligence (AI) agents, studying their tendency for cooperative behavior becomes an increasingly relevant topic. This study is inspired by the super-additive cooperation theory, where the combined effects of repeated interactions and inter-group rivalry have been argued to be the cause for cooperative tendencies found in humans. We devised a virtual tournament where language model agents, grouped into teams, face each other in a Prisoners Dilemma game. By simulating both internal team dynamics and external competition, we discovered that this blend substantially boosts both overall and initial, one-shot cooperation levels (the tendency to cooperate in one-off interactions). This research provides a novel framework for large language models to strategize and act in complex social scenarios and offers evidence for how intergroup competition can, counter-intuitively, result in more cooperative behavior. These insights are crucial for designing future multi-agent AI systems that can effectively work together and better align with human values. Source code is available at https://github.com/pippot/Superadditive-cooperation-LLMs.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.15505v1" target="_blank">Task-Generalized Adaptive Cross-Domain Learning for Multimodal Image Fusion</a></h3>
                    <p><strong>Authors:</strong> Mengyu Wang, Zhenyu Liu, Kun Li, Yu Wang, Yuwei Wang, Yanyan Wei, Fei Wang</p>
                    <p><strong>Published:</strong> 8/21/2025</p>
                    <p><strong>Categories:</strong> cs.CV</p>
                    <p><strong>Summary:</strong> Multimodal Image Fusion (MMIF) aims to integrate complementary information from different imaging modalities to overcome the limitations of individual sensors. It enhances image quality and facilitates downstream applications such as remote sensing, medical diagnostics, and robotics. Despite significant advancements, current MMIF methods still face challenges such as modality misalignment, high-frequency detail destruction, and task-specific limitations. To address these challenges, we propose AdaSFFuse, a novel framework for task-generalized MMIF through adaptive cross-domain co-fusion learning. AdaSFFuse introduces two key innovations: the Adaptive Approximate Wavelet Transform (AdaWAT) for frequency decoupling, and the Spatial-Frequency Mamba Blocks for efficient multimodal fusion. AdaWAT adaptively separates the high- and low-frequency components of multimodal images from different scenes, enabling fine-grained extraction and alignment of distinct frequency characteristics for each modality. The Spatial-Frequency Mamba Blocks facilitate cross-domain fusion in both spatial and frequency domains, enhancing this process. These blocks dynamically adjust through learnable mappings to ensure robust fusion across diverse modalities. By combining these components, AdaSFFuse improves the alignment and integration of multimodal features, reduces frequency loss, and preserves critical details. Extensive experiments on four MMIF tasks -- Infrared-Visible Image Fusion (IVF), Multi-Focus Image Fusion (MFF), Multi-Exposure Image Fusion (MEF), and Medical Image Fusion (MIF) -- demonstrate AdaSFFuses superior fusion performance, ensuring both low computational cost and a compact network, offering a strong balance between performance and efficiency. The code will be publicly available at https://github.com/Zhen-yu-Liu/AdaSFFuse.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.15773v1" target="_blank">Scaling Group Inference for Diverse and High-Quality Generation</a></h3>
                    <p><strong>Authors:</strong> Gaurav Parmar, Or Patashnik, Daniil Ostashev, Kuan-Chieh Wang, Kfir Aberman, Srinivasa Narasimhan, Jun-Yan Zhu</p>
                    <p><strong>Published:</strong> 8/21/2025</p>
                    <p><strong>Categories:</strong> cs.CV, cs.GR, cs.LG</p>
                    <p><strong>Summary:</strong> Generative models typically sample outputs independently, and recent inference-time guidance and scaling algorithms focus on improving the quality of individual samples. However, in real-world applications, users are often presented with a set of multiple images (e.g., 4-8) for each prompt, where independent sampling tends to lead to redundant results, limiting user choices and hindering idea exploration. In this work, we introduce a scalable group inference method that improves both the diversity and quality of a group of samples. We formulate group inference as a quadratic integer assignment problem: candidate outputs are modeled as graph nodes, and a subset is selected to optimize sample quality (unary term) while maximizing group diversity (binary term). To substantially improve runtime efficiency, we progressively prune the candidate set using intermediate predictions, allowing our method to scale up to large candidate sets. Extensive experiments show that our method significantly improves group diversity and quality compared to independent sampling baselines and recent inference algorithms. Our framework generalizes across a wide range of tasks, including text-to-image, image-to-image, image prompting, and video generation, enabling generative models to treat multiple outputs as cohesive groups rather than independent samples.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.15774v1" target="_blank">CineScale: Free Lunch in High-Resolution Cinematic Visual Generation</a></h3>
                    <p><strong>Authors:</strong> Haonan Qiu, Ning Yu, Ziqi Huang, Paul Debevec, Ziwei Liu</p>
                    <p><strong>Published:</strong> 8/21/2025</p>
                    <p><strong>Categories:</strong> cs.CV</p>
                    <p><strong>Summary:</strong> Visual diffusion models achieve remarkable progress, yet they are typically trained at limited resolutions due to the lack of high-resolution data and constrained computation resources, hampering their ability to generate high-fidelity images or videos at higher resolutions. Recent efforts have explored tuning-free strategies to exhibit the untapped potential higher-resolution visual generation of pre-trained models. However, these methods are still prone to producing low-quality visual content with repetitive patterns. The key obstacle lies in the inevitable increase in high-frequency information when the model generates visual content exceeding its training resolution, leading to undesirable repetitive patterns deriving from the accumulated errors. In this work, we propose CineScale, a novel inference paradigm to enable higher-resolution visual generation. To tackle the various issues introduced by the two types of video generation architectures, we propose dedicated variants tailored to each. Unlike existing baseline methods that are confined to high-resolution T2I and T2V generation, CineScale broadens the scope by enabling high-resolution I2V and V2V synthesis, built atop state-of-the-art open-source video generation frameworks. Extensive experiments validate the superiority of our paradigm in extending the capabilities of higher-resolution visual generation for both image and video models. Remarkably, our approach enables 8k image generation without any fine-tuning, and achieves 4k video generation with only minimal LoRA fine-tuning. Generated video samples are available at our website: https://eyeline-labs.github.io/CineScale/.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.15772v1" target="_blank">Visual Autoregressive Modeling for Instruction-Guided Image Editing</a></h3>
                    <p><strong>Authors:</strong> Qingyang Mao, Qi Cai, Yehao Li, Yingwei Pan, Mingyue Cheng, Ting Yao, Qi Liu, Tao Mei</p>
                    <p><strong>Published:</strong> 8/21/2025</p>
                    <p><strong>Categories:</strong> cs.CV, cs.MM</p>
                    <p><strong>Summary:</strong> Recent advances in diffusion models have brought remarkable visual fidelity to instruction-guided image editing. However, their global denoising process inherently entangles the edited region with the entire image context, leading to unintended spurious modifications and compromised adherence to editing instructions. In contrast, autoregressive models offer a distinct paradigm by formulating image synthesis as a sequential process over discrete visual tokens. Their causal and compositional mechanism naturally circumvents the adherence challenges of diffusion-based methods. In this paper, we present VAREdit, a visual autoregressive (VAR) framework that reframes image editing as a next-scale prediction problem. Conditioned on source image features and text instructions, VAREdit generates multi-scale target features to achieve precise edits. A core challenge in this paradigm is how to effectively condition the source image tokens. We observe that finest-scale source features cannot effectively guide the prediction of coarser target features. To bridge this gap, we introduce a Scale-Aligned Reference (SAR) module, which injects scale-matched conditioning information into the first self-attention layer. VAREdit demonstrates significant advancements in both editing adherence and efficiency. On standard benchmarks, it outperforms leading diffusion-based methods by 30\%+ higher GPT-Balance score. Moreover, it completes a $512\times512$ editing in 1.2 seconds, making it 2.2$\times$ faster than the similarly sized UltraEdit. The models are available at https://github.com/HiDream-ai/VAREdit.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.15770v1" target="_blank">Quantum cohomology of variations of GIT quotients and flips</a></h3>
                    <p><strong>Authors:</strong> Zhaoxing Gu, Song Yu, Tony Yue YU</p>
                    <p><strong>Published:</strong> 8/21/2025</p>
                    <p><strong>Categories:</strong> math.AG, math.SG, Primary 14N35, Secondary 14E30</p>
                    <p><strong>Summary:</strong> We prove a decomposition theorem for the quantum cohomology of variations of GIT quotients. More precisely, for any reductive group $G$ and a simple $G$-VGIT wall-crossing $X_- \dashrightarrow X_+$ with a wall $S$, we show that the quantum $D$-module of $X_-$ can be decomposed into a direct sum of that of $X_+$ and copies of that of $S$. As an application, we obtain a decomposition theorem for the quantum cohomology of local models of standard flips in birational geometry.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.15769v1" target="_blank">SceneGen: Single-Image 3D Scene Generation in One Feedforward Pass</a></h3>
                    <p><strong>Authors:</strong> Yanxu Meng, Haoning Wu, Ya Zhang, Weidi Xie</p>
                    <p><strong>Published:</strong> 8/21/2025</p>
                    <p><strong>Categories:</strong> cs.CV, cs.AI</p>
                    <p><strong>Summary:</strong> 3D content generation has recently attracted significant research interest due to its applications in VR/AR and embodied AI. In this work, we address the challenging task of synthesizing multiple 3D assets within a single scene image. Concretely, our contributions are fourfold: (i) we present SceneGen, a novel framework that takes a scene image and corresponding object masks as input, simultaneously producing multiple 3D assets with geometry and texture. Notably, SceneGen operates with no need for optimization or asset retrieval; (ii) we introduce a novel feature aggregation module that integrates local and global scene information from visual and geometric encoders within the feature extraction module. Coupled with a position head, this enables the generation of 3D assets and their relative spatial positions in a single feedforward pass; (iii) we demonstrate SceneGens direct extensibility to multi-image input scenarios. Despite being trained solely on single-image inputs, our architectural design enables improved generation performance with multi-image inputs; and (iv) extensive quantitative and qualitative evaluations confirm the efficiency and robust generation abilities of our approach. We believe this paradigm offers a novel solution for high-quality 3D content generation, potentially advancing its practical applications in downstream tasks. The code and model will be publicly available at: https://mengmouxu.github.io/SceneGen.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.15767v1" target="_blank">ATLAS: Decoupling Skeletal and Shape Parameters for Expressive Parametric Human Modeling</a></h3>
                    <p><strong>Authors:</strong> Jinhyung Park, Javier Romero, Shunsuke Saito, Fabian Prada, Takaaki Shiratori, Yichen Xu, Federica Bogo, Shoou-I Yu, Kris Kitani, Rawal Khirodkar</p>
                    <p><strong>Published:</strong> 8/21/2025</p>
                    <p><strong>Categories:</strong> cs.CV</p>
                    <p><strong>Summary:</strong> Parametric body models offer expressive 3D representation of humans across a wide range of poses, shapes, and facial expressions, typically derived by learning a basis over registered 3D meshes. However, existing human mesh modeling approaches struggle to capture detailed variations across diverse body poses and shapes, largely due to limited training data diversity and restrictive modeling assumptions. Moreover, the common paradigm first optimizes the external body surface using a linear basis, then regresses internal skeletal joints from surface vertices. This approach introduces problematic dependencies between internal skeleton and outer soft tissue, limiting direct control over body height and bone lengths. To address these issues, we present ATLAS, a high-fidelity body model learned from 600k high-resolution scans captured using 240 synchronized cameras. Unlike previous methods, we explicitly decouple the shape and skeleton bases by grounding our mesh representation in the human skeleton. This decoupling enables enhanced shape expressivity, fine-grained customization of body attributes, and keypoint fitting independent of external soft-tissue characteristics. ATLAS outperforms existing methods by fitting unseen subjects in diverse poses more accurately, and quantitative evaluations show that our non-linear pose correctives more effectively capture complex poses compared to linear models.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.15766v1" target="_blank">Discovering Hidden Algebraic Structures via Transformers with Rank-Aware Beam GRPO</a></h3>
                    <p><strong>Authors:</strong> Jaeha Lee, Gio Huh, Ning Su, Tony Yue YU</p>
                    <p><strong>Published:</strong> 8/21/2025</p>
                    <p><strong>Categories:</strong> cs.LG, cs.AI</p>
                    <p><strong>Summary:</strong> Recent efforts have extended the capabilities of transformers in logical reasoning and symbolic computations. In this work, we investigate their capacity for non-linear latent pattern discovery in the context of functional decomposition, focusing on the challenging algebraic task of multivariate polynomial decomposition. This problem, with widespread applications in science and engineering, is proved to be NP-hard, and demands both precision and insight. Our contributions are threefold: First, we develop a synthetic data generation pipeline providing fine-grained control over problem complexity. Second, we train transformer models via supervised learning and evaluate them across four key dimensions involving scaling behavior and generalizability. Third, we propose Beam Grouped Relative Policy Optimization (BGRPO), a rank-aware reinforcement learning method suitable for hard algebraic problems. Finetuning with BGRPO improves accuracy while reducing beam width by up to half, resulting in approximately 75% lower inference compute. Additionally, our model demonstrates competitive performance in polynomial simplification, outperforming Mathematica in various cases.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.15765v1" target="_blank">A framework for robust quantum speedups in practical correlated electronic structure and dynamics</a></h3>
                    <p><strong>Authors:</strong> Jielun Chen, Garnet Kin-Lic Chan</p>
                    <p><strong>Published:</strong> 8/21/2025</p>
                    <p><strong>Categories:</strong> quant-ph</p>
                    <p><strong>Summary:</strong> Proposed quantum advantage in electronic structure has so far required significant fine-tuning to find problems where classical heuristics fail. We describe how to obtain robust quantum speedups for correlated electronic structure and dynamics precisely in the regime where widely used classical heuristics are most successful.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.15763v1" target="_blank">Intern-S1: A Scientific Multimodal Foundation Model</a></h3>
                    <p><strong>Authors:</strong> Lei Bai, Zhongrui Cai, Maosong Cao, Weihan Cao, Chiyu Chen, Haojiong Chen, Kai Chen, Pengcheng Chen, Ying Chen, Yongkang Chen, Yu Cheng, Yu Cheng, Pei Chu, Tao Chu, Erfei Cui, Ganqu Cui, Long Cui, Ziyun Cui, Nianchen Deng, Ning Ding, Nanqin Dong, Peijie Dong, Shihan Dou, Sinan Du, Haodong Duan, Caihua Fan, Ben Gao, Changjiang Gao, Jianfei Gao, Songyang Gao, Yang Gao, Zhangwei Gao, Jiaye Ge, Qiming Ge, Lixin Gu, Yuzhe Gu, Aijia Guo, Qipeng Guo, Xu Guo, Conghui He, Junjun He, Yili Hong, Siyuan Hou, Caiyu Hu, Hanglei Hu, Jucheng Hu, Ming Hu, Zhouqi Hua, Haian Huang, Junhao Huang, Xu Huang, Zixian Huang, Zhe Jiang, Lingkai Kong, Linyang Li, Peiji Li, Pengze Li, Shuaibin Li, Tianbin Li, Wei Li, Yuqiang Li, Dahua Lin, Junyao Lin, Tianyi Lin, Zhishan Lin, Hongwei Liu, Jiangning Liu, Jiyao Liu, Junnan Liu, Kai Liu, Kaiwen Liu, Kuikun Liu, Shichun Liu, Shudong Liu, Wei Liu, Xinyao Liu, Yuhong Liu, Zhan Liu, Yinquan Lu, Haijun Lv, Hongxia Lv, Huijie Lv, Qidang Lv, Ying Lv, Chengqi Lyu, Chenglong Ma, Jianpeng Ma, Ren Ma, Runmin Ma, Runyuan Ma, Xinzhu Ma, Yichuan Ma, Zihan Ma, Sixuan Mi, Junzhi Ning, Wenchang Ning, Xinle Pang, Jiahui Peng, Runyu Peng, Yu Qiao, Jiantao Qiu, Xiaoye Qu, Yuan Qu, Yuchen Ren, Fukai Shang, Wenqi Shao, Junhao Shen, Shuaike Shen, Chunfeng Song, Demin Song, Diping Song, Chenlin Su, Weijie Su, Weigao Sun, Yu Sun, Qian Tan, Cheng Tang, Huanze Tang, Kexian Tang, Shixiang Tang, Jian Tong, Aoran Wang, Bin Wang, Dong Wang, Lintao Wang, Rui Wang, Weiyun Wang, Wenhai Wang, Yi Wang, Ziyi Wang, Ling-I Wu, Wen Wu, Yue Wu, Zijian Wu, Linchen Xiao, Shuhao Xing, Chao Xu, Huihui Xu, Jun Xu, Ruiliang Xu, Wanghan Xu, GanLin Yang, Yuming Yang, Haochen Ye, Jin Ye, Shenglong Ye, Jia Yu, Jiashuo Yu, Jing Yu, Fei Yuan, Bo Zhang, Chao Zhang, Chen Zhang, Hongjie Zhang, Jin Zhang, Qiaosheng Zhang, Qiuyinzhe Zhang, Songyang Zhang, Taolin Zhang, Wenlong Zhang, Wenwei Zhang, Yechen Zhang, Ziyang Zhang, Haiteng Zhao, Qian Zhao, Xiangyu Zhao, Xiangyu Zhao, Bowen Zhou, Dongzhan Zhou, Peiheng Zhou, Yuhao Zhou, Yunhua Zhou, Dongsheng Zhu, Lin Zhu, Yicheng Zou</p>
                    <p><strong>Published:</strong> 8/21/2025</p>
                    <p><strong>Categories:</strong> cs.LG, cs.CL, cs.CV</p>
                    <p><strong>Summary:</strong> In recent years, a plethora of open-source foundation models have emerged, achieving remarkable progress in some widely attended fields, with performance being quite close to that of closed-source models. However, in high-value but more challenging scientific professional fields, either the fields still rely on expert models, or the progress of general foundation models lags significantly compared to those in popular areas, far from sufficient for transforming scientific research and leaving substantial gap between open-source models and closed-source models in these scientific domains. To mitigate this gap and explore a step further toward Artificial General Intelligence (AGI), we introduce Intern-S1, a specialized generalist equipped with general understanding and reasoning capabilities with expertise to analyze multiple science modal data. Intern-S1 is a multimodal Mixture-of-Experts (MoE) model with 28 billion activated parameters and 241 billion total parameters, continually pre-trained on 5T tokens, including over 2.5T tokens from scientific domains. In the post-training stage, Intern-S1 undergoes offline and then online reinforcement learning (RL) in InternBootCamp, where we propose Mixture-of-Rewards (MoR) to synergize the RL training on more than 1000 tasks simultaneously. Through integrated innovations in algorithms, data, and training systems, Intern-S1 achieved top-tier performance in online RL training.On comprehensive evaluation benchmarks, Intern-S1 demonstrates competitive performance on general reasoning tasks among open-source models and significantly outperforms open-source models in scientific domains, surpassing closed-source state-of-the-art models in professional tasks, such as molecular synthesis planning, reaction condition prediction, predicting thermodynamic stabilities for crystals. Our models are available at https://huggingface.co/internlm/Intern-S1.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.15761v1" target="_blank">Waver: Wave Your Way to Lifelike Video Generation</a></h3>
                    <p><strong>Authors:</strong> Yifu Zhang, Hao Yang, Yuqi Zhang, Yifei Hu, Fengda Zhu, Chuang Lin, Xiaofeng Mei, Yi Jiang, Zehuan Yuan, Bingyue Peng</p>
                    <p><strong>Published:</strong> 8/21/2025</p>
                    <p><strong>Categories:</strong> cs.CV</p>
                    <p><strong>Summary:</strong> We present Waver, a high-performance foundation model for unified image and video generation. Waver can directly generate videos with durations ranging from 5 to 10 seconds at a native resolution of 720p, which are subsequently upscaled to 1080p. The model simultaneously supports text-to-video (T2V), image-to-video (I2V), and text-to-image (T2I) generation within a single, integrated framework. We introduce a Hybrid Stream DiT architecture to enhance modality alignment and accelerate training convergence. To ensure training data quality, we establish a comprehensive data curation pipeline and manually annotate and train an MLLM-based video quality model to filter for the highest-quality samples. Furthermore, we provide detailed training and inference recipes to facilitate the generation of high-quality videos. Building on these contributions, Waver excels at capturing complex motion, achieving superior motion amplitude and temporal consistency in video synthesis. Notably, it ranks among the Top 3 on both the T2V and I2V leaderboards at Artificial Analysis (data as of 2025-07-30 10:00 GMT+8), consistently outperforming existing open-source models and matching or surpassing state-of-the-art commercial solutions. We hope this technical report will help the community more efficiently train high-quality video generation models and accelerate progress in video generation technologies. Official page: https://github.com/FoundationVision/Waver.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.15760v1" target="_blank">LiveMCP-101: Stress Testing and Diagnosing MCP-enabled Agents on Challenging Queries</a></h3>
                    <p><strong>Authors:</strong> Ming Yin, Dinghan Shen, Silei Xu, Jianbing Han, Sixun Dong, Mian Zhang, Yebowen Hu, Shujian Liu, Simin Ma, Song Wang, Sathish Reddy Indurthi, Xun Wang, Yiran Chen, Kaiqiang Song</p>
                    <p><strong>Published:</strong> 8/21/2025</p>
                    <p><strong>Categories:</strong> cs.CL, cs.AI</p>
                    <p><strong>Summary:</strong> Tool calling has emerged as a critical capability for AI agents to interact with the real world and solve complex tasks. While the Model Context Protocol (MCP) provides a powerful standardized framework for tool integration, there is a significant gap in benchmarking how well AI agents can effectively solve multi-step tasks using diverse MCP tools in realistic, dynamic scenarios. In this work, we present LiveMCP-101, a benchmark of 101 carefully curated real-world queries, refined through iterative LLM rewriting and manual review, that require coordinated use of multiple MCP tools including web search, file operations, mathematical reasoning, and data analysis. Moreover, we introduce a novel evaluation approach that leverages ground-truth execution plans rather than raw API outputs, better reflecting the evolving nature of real-world environments. Experiments show that even frontier LLMs achieve a success rate below 60\%, highlighting major challenges in tool orchestration. Detailed ablations and error analysis further reveal distinct failure modes and inefficiencies in token usage, pointing to concrete directions for advancing current models. LiveMCP-101 sets a rigorous standard for evaluating real-world agent capabilities, advancing toward autonomous AI systems that reliably execute complex tasks through tool use.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.15759v1" target="_blank">Evaluating classical simulations with a quantum processor</a></h3>
                    <p><strong>Authors:</strong> Alberto Nocera, Jack Raymond, William Bernoudy, Mohammad H. Amin, Andrew D. King</p>
                    <p><strong>Published:</strong> 8/21/2025</p>
                    <p><strong>Categories:</strong> quant-ph</p>
                    <p><strong>Summary:</strong> As simulations of quantum systems cross the limits of classical computability, both quantum and classical approaches become hard to verify. Scaling predictions are therefore based on local structure and asymptotic assumptions, typically with classical methods being used to evaluate quantum simulators where possible. Here, in contrast, we use a quantum annealing processor to produce a ground truth for evaluating classical tensor-network methods whose scaling has not yet been firmly established. Our observations run contrary to previous scaling predictions, demonstrating the need for caution when extrapolating the accuracy of classical simulations of quantum dynamics. Our results demonstrate that the virtuous cycle of competition between classical and quantum simulations can lend insight in both directions.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.15758v1" target="_blank">Skyrmion Lattice Order Controlled by Confinement Geometry</a></h3>
                    <p><strong>Authors:</strong> Raphael Gruber, Jan RothÃ¶rl, Simon M. FrÃ¶hlich, Maarten A. Brems, Fabian Kammerbauer, Maria-Andromachi Syskaki, Elizabeth M. Jefremovas, Sachin Krishnia, Asle SudbÃ¸, Peter Virnau, Mathias KlÃ¤ui</p>
                    <p><strong>Published:</strong> 8/21/2025</p>
                    <p><strong>Categories:</strong> cond-mat.mes-hall, cond-mat.mtrl-sci, cond-mat.stat-mech</p>
                    <p><strong>Summary:</strong> Magnetic skyrmions forming two-dimensional (2D) lattices provide a versatile platform for investigating phase transitions predicted by Kosterlitz-Thouless-Halperin-Nelson-Young (KTHNY) theory. While 2D melting in skyrmion systems has been demonstrated, achieving controlled ordering in skyrmion lattices remains challenging due to pinning effects from a non-uniform energy landscape, which often results in polycrystalline structures. Skyrmions in thin films, however, offer thermal diffusion with high tunability and can be directly imaged via Kerr microscopy, enabling real-time observation of their dynamics. To regulate lattice order in such flexible systems, we introduce geometric confinements of varying shapes. Combining Kerr microscopy experiments with Thiele model simulations, we demonstrate that confinement geometry critically influences lattice order. Specifically, hexagonal confinements commensurate with the skyrmion lattice stabilize monodomain hexagonal ordering, while incommensurate geometries induce domain formation and reduce overall order. Understanding these boundary-driven effects is essential for advancing the study of 2D phase behavior and for the design of skyrmion-based spintronic applications, ranging from memory devices to unconventional computing architectures.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.15757v1" target="_blank">Language-Guided Tuning: Enhancing Numeric Optimization with Textual Feedback</a></h3>
                    <p><strong>Authors:</strong> Yuxing Lu, Yucheng Hu, Nan Sun, Xukai Zhao</p>
                    <p><strong>Published:</strong> 8/21/2025</p>
                    <p><strong>Categories:</strong> cs.AI, cs.CL, cs.LG, cs.MA</p>
                    <p><strong>Summary:</strong> Configuration optimization remains a critical bottleneck in machine learning, requiring coordinated tuning across model architecture, training strategy, feature engineering, and hyperparameters. Traditional approaches treat these dimensions independently and lack interpretability, while recent automated methods struggle with dynamic adaptability and semantic reasoning about optimization decisions. We introduce Language-Guided Tuning (LGT), a novel framework that employs multi-agent Large Language Models to intelligently optimize configurations through natural language reasoning. We apply textual gradients - qualitative feedback signals that complement numerical optimization by providing semantic understanding of training dynamics and configuration interdependencies. LGT coordinates three specialized agents: an Advisor that proposes configuration changes, an Evaluator that assesses progress, and an Optimizer that refines the decision-making process, creating a self-improving feedback loop. Through comprehensive evaluation on six diverse datasets, LGT demonstrates substantial improvements over traditional optimization methods, achieving performance gains while maintaining high interpretability.</p>
                
            
                
                    <h3><a href="http://dx.doi.org/10.3847/1538-3881/adfb6b" target="_blank">Robust Data Interpretation for Perturbed Nulling Interferometers via Proper Handling of Correlated Errors</a></h3>
                    <p><strong>Authors:</strong> Philipp A. Huber, Felix A. Dannert, Romain Laugier, Taro Matsuo, Loes W. Rutten, Adrian M. Glauser, Sascha P. Quanz</p>
                    <p><strong>Published:</strong> 8/21/2025</p>
                    <p><strong>Categories:</strong> astro-ph.IM</p>
                    <p><strong>Summary:</strong> The detection and atmospheric characterization of potentially habitable, temperate terrestrial exoplanets using a space-based mid-infrared nulling interferometer is a major goal of contemporary astrophysics. A central part of the analysis of such an instrument are correlated errors arising from perturbations in the system. While previous studies have often treated their effects in a limited manner, we aim to treat them comprehensively here and argue that data whitening based on the covariance of these errors is a suitable method to mitigate their impact. We present a framework that quantitatively connects instrumental perturbations to performance metrics and develop two computational tools to support our analysis: PHRINGE, for the generation of synthetic nulling data, and LIFEsimMC, a new Monte Carlo-based end-to-end simulator for the Large Interferometer For Exoplanets (LIFE). Applying our framework to a reference observation of an Earth twin orbiting a Sun twin at 10 pc, we find that whitening is not only essential for a correct interpretation of the detection metric used in hypothesis testing, but also improves the estimates of the planetary properties. Moreover, our approach enables an estimation of the spectral covariance of the extracted planetary spectra, providing valuable additional input for future atmospheric retrievals. We therefore recommend incorporating the framework into performance assessments and requirement derivations for future nulling interferometers.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.15754v1" target="_blank">Dissecting Tool-Integrated Reasoning: An Empirical Study and Analysis</a></h3>
                    <p><strong>Authors:</strong> Yufeng Zhao, Junnan Liu, Hongwei Liu, Dongsheng Zhu, Yuan Shen, Songyang Zhang, Kai Chen</p>
                    <p><strong>Published:</strong> 8/21/2025</p>
                    <p><strong>Categories:</strong> cs.CL, cs.AI</p>
                    <p><strong>Summary:</strong> Large Language Models (LLMs) have made significant strides in reasoning tasks through methods like chain-of-thought (CoT) reasoning. However, they often fall short in tasks requiring precise computations. Tool-Integrated Reasoning (TIR) has emerged as a solution by incorporating external tools into the reasoning process. Nevertheless, the generalization of TIR in improving the reasoning ability of LLM is still unclear. Additionally, whether TIR has improved the models reasoning behavior and helped the model think remains to be studied. We introduce ReasonZoo, a comprehensive benchmark encompassing nine diverse reasoning categories, to evaluate the effectiveness of TIR across various domains. Additionally, we propose two novel metrics, Performance-Aware Cost (PAC) and Area Under the Performance-Cost Curve (AUC-PCC), to assess reasoning efficiency. Our empirical evaluation demonstrates that TIR-enabled models consistently outperform their non-TIR counterparts in both mathematical and non-mathematical tasks. Furthermore, TIR enhances reasoning efficiency, as evidenced by improved PAC and AUC-PCC, indicating reduced overthinking and more streamlined reasoning. These findings underscore the domain-general benefits of TIR and its potential to advance LLM capabilities in complex reasoning tasks.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.15753v1" target="_blank">PyKirigami: An interactive Python simulator for kirigami metamaterials</a></h3>
                    <p><strong>Authors:</strong> Qinghai Jiang, Gary P. T. Choi</p>
                    <p><strong>Published:</strong> 8/21/2025</p>
                    <p><strong>Categories:</strong> cond-mat.soft, cond-mat.mtrl-sci, cs.CG</p>
                    <p><strong>Summary:</strong> In recent years, the concept of kirigami has been used in creating deployable structures for various scientific and technological applications. While the design of kirigami metamaterials has been widely studied, the simulation of the deployment and shape transformation process is less explored. In this work, we develop PyKirigami, an efficient Python-based open-source computational tool for the deployment simulation of kirigami metamaterials. In particular, our tool is capable of simulating both two- and three-dimensional deployments of a large variety of kirigami metamaterials with different geometric, topological, and physical properties. Altogether, our work paves a new way for the modelling and design of shape-morphing mechanical metamaterials.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.15752v1" target="_blank">Does the cafe entrance look accessible? Where is the door? Towards Geospatial AI Agents for Visual Inquiries</a></h3>
                    <p><strong>Authors:</strong> Jon E. Froehlich, Jared Hwang, Zeyu Wang, John S. OMeara, Xia Su, William Huang, Yang Zhang, Alex Fiannaca, Philip Nelson, Shaun Kane</p>
                    <p><strong>Published:</strong> 8/21/2025</p>
                    <p><strong>Categories:</strong> cs.HC, cs.AI, cs.CV, H.5; I.2</p>
                    <p><strong>Summary:</strong> Interactive digital maps have revolutionized how people travel and learn about the world; however, they rely on pre-existing structured data in GIS databases (e.g., road networks, POI indices), limiting their ability to address geo-visual questions related to what the world looks like. We introduce our vision for Geo-Visual Agents--multimodal AI agents capable of understanding and responding to nuanced visual-spatial inquiries about the world by analyzing large-scale repositories of geospatial images, including streetscapes (e.g., Google Street View), place-based photos (e.g., TripAdvisor, Yelp), and aerial imagery (e.g., satellite photos) combined with traditional GIS data sources. We define our vision, describe sensing and interaction approaches, provide three exemplars, and enumerate key challenges and opportunities for future work.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.15751v1" target="_blank">Fine-grained Multi-class Nuclei Segmentation with Molecular-empowered All-in-SAM Model</a></h3>
                    <p><strong>Authors:</strong> Xueyuan Li, Can Cui, Ruining Deng, Yucheng Tang, Quan Liu, Tianyuan Yao, Shunxing Bao, Naweed Chowdhury, Haichun Yang, Yuankai Huo</p>
                    <p><strong>Published:</strong> 8/21/2025</p>
                    <p><strong>Categories:</strong> cs.CV</p>
                    <p><strong>Summary:</strong> Purpose: Recent developments in computational pathology have been driven by advances in Vision Foundation Models, particularly the Segment Anything Model (SAM). This model facilitates nuclei segmentation through two primary methods: prompt-based zero-shot segmentation and the use of cell-specific SAM models for direct segmentation. These approaches enable effective segmentation across a range of nuclei and cells. However, general vision foundation models often face challenges with fine-grained semantic segmentation, such as identifying specific nuclei subtypes or particular cells. Approach: In this paper, we propose the molecular-empowered All-in-SAM Model to advance computational pathology by leveraging the capabilities of vision foundation models. This model incorporates a full-stack approach, focusing on: (1) annotation-engaging lay annotators through molecular-empowered learning to reduce the need for detailed pixel-level annotations, (2) learning-adapting the SAM model to emphasize specific semantics, which utilizes its strong generalizability with SAM adapter, and (3) refinement-enhancing segmentation accuracy by integrating Molecular-Oriented Corrective Learning (MOCL). Results: Experimental results from both in-house and public datasets show that the All-in-SAM model significantly improves cell classification performance, even when faced with varying annotation quality. Conclusions: Our approach not only reduces the workload for annotators but also extends the accessibility of precise biomedical image analysis to resource-limited settings, thereby advancing medical diagnostics and automating pathology image analysis.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.15747v1" target="_blank">Robustness of Solutions of the Quantum Kinetic Equations in the Presence of Matter Density Fluctuations</a></h3>
                    <p><strong>Authors:</strong> Shashank Shalgar, Mariam Gogilashvili</p>
                    <p><strong>Published:</strong> 8/21/2025</p>
                    <p><strong>Categories:</strong> astro-ph.HE, hep-ph</p>
                    <p><strong>Summary:</strong> We investigate the role of fluctuations in the matter density on neutrino flavor evolution by studying their effects on the collision terms in the spherically symmetric quantum kinetics equations (QKEs). We solve the QKEs with varying radial resolution ($r_{\mathrm{bins}} = 150 \, , 1500 \, , 15000$) to assess numerical convergence in angular distributions, number densities, and energy spectra for four neutrino flavors ($\nu_e$, $\bar{\nu}_e$, $\nu_x$, $\bar{\nu}_x$). Our results demonstrate that the solutions are numerically converged already at the coarsest resolution, with higher resolutions yielding almost identical outcomes. We introduce random perturbations to each radial bin, thus adding perturbations with a length scale that is related to the radial resolution. We study both time-independent and time-dependent perturbations to the matter density that affect the collision term and analyze their effects on neutrino flavor evolution. We find that such fluctuations do not induce any significant instabilities or qualitative changes in flavor evolution. Angular structure remains robust, and flavor-dependent number densities and energy spectra show only minor deviations compared to the unperturbed case. These findings suggest that matter perturbations have a negligible effect on neutrino flavor evolution in spherically symmetric settings.</p>
                
            
        
        
            <p><em>Generated by AI News Agent</em></p>
        
    

