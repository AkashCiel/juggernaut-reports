
    
        <h1>ðŸ¤– AI Research Report</h1>
        
            <strong>Date:</strong> 2025-07-23<br>
            <strong>Topics:</strong> ai safety research, ai alignment research, quantum computing<br>
            <strong>Papers Found:</strong> 150
        
        
        
            
                <h2>ðŸ¤– AI Summary</h2>
                <p>## ai safety research

The landscape of AI safety research is marked by several significant trends and breakthroughs, as demonstrated by the papers provided. A key theme is enhancing the robustness and adaptability of AI models in dynamic environments. For instance, the ThinkAct framework introduces a dual-system approach to vision-language-action reasoning, enabling agents to perform long-horizon planning and self-correction. This addresses safety concerns in AI by improving adaptability and decision-making processes in complex tasks. Similarly, ChatChecker focuses on testing dialogue systems using non-cooperative user simulations, which helps identify weaknesses and improve system robustness, thereby preventing potential failures in real-world deployments.

In terms of technical infrastructure and resource management, the study on liquid-cooled versus air-cooled GPU systems highlights the importance of energy efficiency and thermal stability in data centers that support AI workloads. This has implications for AI safety by ensuring that hardware systems can sustain heavy computational demands without overheating, reducing the risk of hardware failures. Moreover, the exploration of Copy-Guided Attacks (CGA) in LLMs uncovers vulnerabilities in AI systems that can be exploited to manipulate reasoning processes. This emphasizes the need for robust defense mechanisms to safeguard AI systems against such attacks, maintaining the integrity and reliability of AI outputs.

Overall, these research directions underscore the importance of developing AI systems that are not only efficient and effective but also resilient and secure against both internal and external threats. The integration of advanced testing frameworks, enhanced resource management, and proactive defense strategies is crucial for advancing AI safety and ensuring reliable AI deployment across various domains.

*Based on 50 research papers*

---

## quantum computing

The provided research papers predominantly explore various advancements and challenges across multiple domains, with limited direct focus on quantum computing. However, a few papers do touch upon aspects related to quantum technologies and their implications.

The most relevant paper in the context of quantum computing is Quantum teleportation of an elemental silicon nanophotonic CNOT gate, which reports on the successful demonstration of a quantum gate teleportation using a silicon photonic chip. This breakthrough is significant as it highlights advancements in scalable quantum computing architectures by enabling remote quantum operations through non-local quantum gates. The use of a photonic chip for quantum gate teleportation can potentially lead to more efficient and scalable quantum computing systems, paving the way for practical implementations of large-scale quantum computers.

Another paper, No-go theorems for logical gates on product quantum codes, addresses the limitations of implementing fault-tolerant logical gates in quantum error-correcting codes, particularly those related to hypergraph product codes. This research extends the understanding of constraints imposed by algebraic structures on logical gates, which is crucial for the development of robust quantum computing systems. These findings contribute to the ongoing efforts to build reliable quantum computers capable of overcoming the challenges posed by quantum noise and errors.

Together, these papers underscore the ongoing efforts and breakthroughs in the field of quantum computing, particularly in enhancing the scalability, efficiency, and fault tolerance of quantum systems. These advancements are critical for realizing the full potential of quantum technologies in solving complex computational problems that are beyond the reach of classical computers.

*Based on 50 research papers*</p>
            
        
        
        <h2>ðŸ“š Research Papers</h2>
        
                
                    <h3><a href="http://arxiv.org/abs/2507.16815v1" target="_blank">ThinkAct: Vision-Language-Action Reasoning via Reinforced Visual Latent Planning</a></h3>
                    <p><strong>Authors:</strong> Chi-Pin Huang, Yueh-Hua Wu, Min-Hung Chen, Yu-Chiang Frank Wang, Fu-En Yang</p>
                    <p><strong>Published:</strong> 7/22/2025</p>
                    <p><strong>Categories:</strong> cs.CV, cs.AI, cs.LG, cs.RO</p>
                    <p><strong>Summary:</strong> Vision-language-action (VLA) reasoning tasks require agents to interpret multimodal instructions, perform long-horizon planning, and act adaptively in dynamic environments. Existing approaches typically train VLA models in an end-to-end fashion, directly mapping inputs to actions without explicit reasoning, which hinders their ability to plan over multiple steps or adapt to complex task variations. In this paper, we propose ThinkAct, a dual-system framework that bridges high-level reasoning with low-level action execution via reinforced visual latent planning. ThinkAct trains a multimodal LLM to generate embodied reasoning plans guided by reinforcing action-aligned visual rewards based on goal completion and trajectory consistency. These reasoning plans are compressed into a visual plan latent that conditions a downstream action model for robust action execution on target environments. Extensive experiments on embodied reasoning and robot manipulation benchmarks demonstrate that ThinkAct enables few-shot adaptation, long-horizon planning, and self-correction behaviors in complex embodied AI tasks.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2507.16812v1" target="_blank">MegaScience: Pushing the Frontiers of Post-Training Datasets for Science Reasoning</a></h3>
                    <p><strong>Authors:</strong> Run-Ze Fan, Zengzhi Wang, Pengfei Liu</p>
                    <p><strong>Published:</strong> 7/22/2025</p>
                    <p><strong>Categories:</strong> cs.CL, cs.AI, cs.LG</p>
                    <p><strong>Summary:</strong> Scientific reasoning is critical for developing AI scientists and supporting human researchers in advancing the frontiers of natural science discovery. However, the open-source community has primarily focused on mathematics and coding while neglecting the scientific domain, largely due to the absence of open, large-scale, high-quality, verifiable scientific reasoning datasets. To bridge this gap, we first present TextbookReasoning, an open dataset featuring truthful reference answers extracted from 12k university-level scientific textbooks, comprising 650k reasoning questions spanning 7 scientific disciplines. We further introduce MegaScience, a large-scale mixture of high-quality open-source datasets totaling 1.25 million instances, developed through systematic ablation studies that evaluate various data selection methodologies to identify the optimal subset for each publicly available scientific dataset. Meanwhile, we build a comprehensive evaluation system covering diverse subjects and question types across 15 benchmarks, incorporating comprehensive answer extraction strategies to ensure accurate evaluation metrics. Our experiments demonstrate that our datasets achieve superior performance and training efficiency with more concise response lengths compared to existing open-source scientific datasets. Furthermore, we train Llama3.1, Qwen2.5, and Qwen3 series base models on MegaScience, which significantly outperform the corresponding official instruct models in average performance. In addition, MegaScience exhibits greater effectiveness for larger and stronger models, suggesting a scaling benefit for scientific tuning. We release our data curation pipeline, evaluation system, datasets, and seven trained models to the community to advance scientific reasoning research.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2507.16808v1" target="_blank">Rethinking LLM-Based RTL Code Optimization Via Timing Logic Metamorphosis</a></h3>
                    <p><strong>Authors:</strong> Zhihao Xu, Bixin Li, Lulu Wang</p>
                    <p><strong>Published:</strong> 7/22/2025</p>
                    <p><strong>Categories:</strong> cs.SE, cs.AI, 68N19, 68T05, B.6.3; D.3.4; I.2.2; I.2.6</p>
                    <p><strong>Summary:</strong> Register Transfer Level(RTL) code optimization is crucial for achieving high performance and low power consumption in digital circuit design. However, traditional optimization methods often rely on manual tuning and heuristics, which can be time-consuming and error-prone. Recent studies proposed to leverage Large Language Models(LLMs) to assist in RTL code optimization. LLMs can generate optimized code snippets based on natural language descriptions, potentially speeding up the optimization process. However, existing approaches have not thoroughly evaluated the effectiveness of LLM-Based code optimization methods for RTL code with complex timing logic. To address this gap, we conducted a comprehensive empirical investigation to assess the capability of LLM-Based RTL code optimization methods in handling RTL code with complex timing logic. In this study, we first propose a new benchmark for RTL optimization evaluation. It comprises four subsets, each corresponding to a specific area of RTL code optimization. Then we introduce a method based on metamorphosis to systematically evaluate the effectiveness of LLM-Based RTL code optimization methods.Our key insight is that the optimization effectiveness should remain consistent for semantically equivalent but more complex code. After intensive experiments, we revealed several key findings. (1) LLM-Based RTL optimization methods can effectively optimize logic operations and outperform existing compiler-based methods. (2) LLM-Based RTL optimization methods do not perform better than existing compiler-based methods on RTL code with complex timing logic, particularly in timing control flow optimization and clock domain optimization. This is primarily attributed to the challenges LLMs face in understanding timing logic in RTL code. Based on these findings, we provide insights for further research in leveraging LLMs for RTL code optimization.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2507.16792v1" target="_blank">ChatChecker: A Framework for Dialogue System Testing and Evaluation Through Non-cooperative User Simulation</a></h3>
                    <p><strong>Authors:</strong> Roman Mayr, Michel Schimpf, Thomas BohnÃ©</p>
                    <p><strong>Published:</strong> 7/22/2025</p>
                    <p><strong>Categories:</strong> cs.AI</p>
                    <p><strong>Summary:</strong> While modern dialogue systems heavily rely on large language models (LLMs), their implementation often goes beyond pure LLM interaction. Developers integrate multiple LLMs, external tools, and databases. Therefore, assessment of the underlying LLM alone does not suffice, and the dialogue systems must be tested and evaluated as a whole. However, this remains a major challenge. With most previous work focusing on turn-level analysis, less attention has been paid to integrated dialogue-level quality assurance. To address this, we present ChatChecker, a framework for automated evaluation and testing of complex dialogue systems. ChatChecker uses LLMs to simulate diverse user interactions, identify dialogue breakdowns, and evaluate quality. Compared to previous approaches, our design reduces setup effort and is generalizable, as it does not require reference dialogues and is decoupled from the implementation of the target dialogue system. We improve breakdown detection performance over a prior LLM-based approach by including an error taxonomy in the prompt. Additionally, we propose a novel non-cooperative user simulator based on challenging personas that uncovers weaknesses in target dialogue systems more effectively. Through this, ChatChecker contributes to thorough and scalable testing. This enables both researchers and practitioners to accelerate the development of robust dialogue systems.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2507.16784v1" target="_blank">Beyond Context Limits: Subconscious Threads for Long-Horizon Reasoning</a></h3>
                    <p><strong>Authors:</strong> Hongyin Luo, Nathaniel Morgan, Tina Li, Derek Zhao, Ai Vy Ngo, Philip Schroeder, Lijie Yang, Assaf Ben-Kish, Jack OBrien, James Glass</p>
                    <p><strong>Published:</strong> 7/22/2025</p>
                    <p><strong>Categories:</strong> cs.CL</p>
                    <p><strong>Summary:</strong> To break the context limits of large language models (LLMs) that bottleneck reasoning accuracy and efficiency, we propose the Thread Inference Model (TIM), a family of LLMs trained for recursive and decompositional problem solving, and TIMRUN, an inference runtime enabling long-horizon structured reasoning beyond context limits. Together, TIM hosted on TIMRUN supports virtually unlimited working memory and multi-hop tool calls within a single language model inference, overcoming output limits, positional-embedding constraints, and GPU-memory bottlenecks. Performance is achieved by modeling natural language as reasoning trees measured by both length and depth instead of linear sequences. The reasoning trees consist of tasks with thoughts, recursive subtasks, and conclusions based on the concept we proposed in Schroeder et al, 2025. During generation, we maintain a working memory that retains only the key-value states of the most relevant context tokens, selected by a rule-based subtask-pruning mechanism, enabling reuse of positional embeddings and GPU memory pages throughout reasoning. Experimental results show that our system sustains high inference throughput, even when manipulating up to 90% of the KV cache in GPU memory. It also delivers accurate reasoning on mathematical tasks and handles information retrieval challenges that require long-horizon reasoning and multi-hop tool use.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2507.16781v1" target="_blank">Cooling Matters: Benchmarking Large Language Models and Vision-Language Models on Liquid-Cooled Versus Air-Cooled H100 GPU Systems</a></h3>
                    <p><strong>Authors:</strong> Imran Latif, Muhammad Ali Shafique, Hayat Ullah, Alex C. Newkirk, Xi Yu, Arslan Munir</p>
                    <p><strong>Published:</strong> 7/22/2025</p>
                    <p><strong>Categories:</strong> cs.DC</p>
                    <p><strong>Summary:</strong> The unprecedented growth in artificial intelligence (AI) workloads, recently dominated by large language models (LLMs) and vision-language models (VLMs), has intensified power and cooling demands in data centers. This study benchmarks LLMs and VLMs on two HGX nodes, each with 8x NVIDIA H100 graphics processing units (GPUs), using liquid and air cooling. Leveraging GPU Burn, Weights and Biases, and IPMItool, we collect detailed thermal, power, and computation data. Results show that the liquid-cooled systems maintain GPU temperatures between 41-50 degrees Celsius, while the air-cooled counterparts fluctuate between 54-72 degrees Celsius under load. This thermal stability of liquid-cooled systems yields 17 percent higher performance (54 TFLOPs per GPU vs. 46 TFLOPs per GPU), improved performance per watt, reduced energy overhead, and greater system efficiency than the air-cooled counterparts. These findings underscore the energy and sustainability benefits of liquid cooling, offering a compelling path forward for hyperscale data centers s</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2507.16773v1" target="_blank">When LLMs Copy to Think: Uncovering Copy-Guided Attacks in Reasoning LLMs</a></h3>
                    <p><strong>Authors:</strong> Yue Li, Xiao Li, Hao Wu, Yue Zhang, Fengyuan Xu, Xiuzhen Cheng, Sheng Zhong</p>
                    <p><strong>Published:</strong> 7/22/2025</p>
                    <p><strong>Categories:</strong> cs.CR</p>
                    <p><strong>Summary:</strong> Large Language Models (LLMs) have become integral to automated code analysis, enabling tasks such as vulnerability detection and code comprehension. However, their integration introduces novel attack surfaces. In this paper, we identify and investigate a new class of prompt-based attacks, termed Copy-Guided Attacks (CGA), which exploit the inherent copying tendencies of reasoning-capable LLMs. By injecting carefully crafted triggers into external code snippets, adversaries can induce the model to replicate malicious content during inference. This behavior enables two classes of vulnerabilities: inference length manipulation, where the model generates abnormally short or excessively long reasoning traces; and inference result manipulation, where the model produces misleading or incorrect conclusions. We formalize CGA as an optimization problem and propose a gradient-based approach to synthesize effective triggers. Empirical evaluation on state-of-the-art reasoning LLMs shows that CGA reliably induces infinite loops, premature termination, false refusals, and semantic distortions in code analysis tasks. While highly effective in targeted settings, we observe challenges in generalizing CGA across diverse prompts due to computational constraints, posing an open question for future research. Our findings expose a critical yet underexplored vulnerability in LLM-powered development pipelines and call for urgent advances in prompt-level defense mechanisms.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2507.16761v1" target="_blank">Faithful, Interpretable Chest X-ray Diagnosis with Anti-Aliased B-cos Networks</a></h3>
                    <p><strong>Authors:</strong> Marcel Kleinmann, Shashank Agnihotri, Margret Keuper</p>
                    <p><strong>Published:</strong> 7/22/2025</p>
                    <p><strong>Categories:</strong> cs.CV, cs.LG</p>
                    <p><strong>Summary:</strong> Faithfulness and interpretability are essential for deploying deep neural networks (DNNs) in safety-critical domains such as medical imaging. B-cos networks offer a promising solution by replacing standard linear layers with a weight-input alignment mechanism, producing inherently interpretable, class-specific explanations without post-hoc methods. While maintaining diagnostic performance competitive with state-of-the-art DNNs, standard B-cos models suffer from severe aliasing artifacts in their explanation maps, making them unsuitable for clinical use where clarity is essential. Additionally, the original B-cos formulation is limited to multi-class settings, whereas chest X-ray analysis often requires multi-label classification due to co-occurring abnormalities. In this work, we address both limitations: (1) we introduce anti-aliasing strategies using FLCPooling (FLC) and BlurPool (BP) to significantly improve explanation quality, and (2) we extend B-cos networks to support multi-label classification. Our experiments on chest X-ray datasets demonstrate that the modified $\text{B-cos}_\text{FLC}$ and $\text{B-cos}_\text{BP}$ preserve strong predictive performance while providing faithful and artifact-free explanations suitable for clinical application in multi-label settings. Code available at: $\href{https://github.com/mkleinma/B-cos-medical-paper}{GitHub repository}$.</p>
                
            
                
                    <h3><a href="http://dx.doi.org/10.1038/s41563-025-02290-y" target="_blank">Atomic-scale Frustrated Josephson Coupling and Multi-condensate Visualization in FeSe</a></h3>
                    <p><strong>Authors:</strong> Nileema Sharma, Matthew Toole, James McKenzie, Sheng Ran, Xiaolong Liu</p>
                    <p><strong>Published:</strong> 7/22/2025</p>
                    <p><strong>Categories:</strong> cond-mat.supr-con, cond-mat.mtrl-sci</p>
                    <p><strong>Summary:</strong> In a Josephson junction involving multi-band superconductors, competition between inter-band and inter-junction Josephson coupling gives rise to frustration and spatial disjunction of superfluid densities among superconducting condensates. Such frustrated coupling manifests as quantum interference of Josephson currents from different tunneling channels and becomes tunable if channel transparency can be varied. To explore these unconventional effects in the prototypical $s^\pm$-wave superconductor FeSe, we use atomic resolution scanned Josephson tunneling microscopy SJTM for condensate resolved imaging and junction tuning -- capabilities unattainable in macroscopic Josephson devices with fixed characteristics. We quantitatively demonstrate frustrated Josephson tunneling by examining two tunneling inequalities. The relative transparency of two parallel tunneling pathways is found tunable, revealing a tendency towards a 0-pi transition with decreasing SJTM junction resistance. Simultaneous visualization of both superconducting condensates reveals anti correlated superfluid modulations, highlighting the role of inter-band scattering. Our study establishes SJTM as a powerful tool enabling new research frontiers of multi condensate superconductivity.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2507.16735v1" target="_blank">AI-enhanced conversational agents for personalized asthma support Factors for engagement, value and efficacy</a></h3>
                    <p><strong>Authors:</strong> Laura Moradbakhti, Dorian Peters, Jennifer K. Quint, BjÃ¶rn Schuller, Darren Cook, Rafael A. Calvo</p>
                    <p><strong>Published:</strong> 7/22/2025</p>
                    <p><strong>Categories:</strong> cs.HC, cs.AI, cs.CY, cs.ET, K.4.2; J.3</p>
                    <p><strong>Summary:</strong> Asthma-related deaths in the UK are the highest in Europe, and only 30% of patients access basic care. There is a need for alternative approaches to reaching people with asthma in order to provide health education, self-management support and bridges to care. Automated conversational agents (specifically, mobile chatbots) present opportunities for providing alternative and individually tailored access to health education, self-management support and risk self-assessment. But would patients engage with a chatbot, and what factors influence engagement? We present results from a patient survey (N=1257) devised by a team of asthma clinicians, patients, and technology developers, conducted to identify optimal factors for efficacy, value and engagement for a chatbot. Results indicate that most adults with asthma (53%) are interested in using a chatbot and the patients most likely to do so are those who believe their asthma is more serious and who are less confident about self-management. Results also indicate enthusiasm for 24/7 access, personalisation, and for WhatsApp as the preferred access method (compared to app, voice assistant, SMS or website). Obstacles to uptake include security/privacy concerns and skepticism of technological capabilities. We present detailed findings and consolidate these into 7 recommendations for developers for optimising efficacy of chatbot-based health support.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2507.16733v1" target="_blank">Generative Diffusion Models for Wireless Networks: Fundamental, Architecture, and State-of-the-Art</a></h3>
                    <p><strong>Authors:</strong> Dayu Fan, Rui Meng, Xiaodong Xu, Yiming Liu, Guoshun Nan, Chenyuan Feng, Shujun Han, Song Gao, Bingxuan Xu, Dusit Niyato, Tony Q. S. Quek, Ping Zhang</p>
                    <p><strong>Published:</strong> 7/22/2025</p>
                    <p><strong>Categories:</strong> eess.SP</p>
                    <p><strong>Summary:</strong> With the rapid development of Generative Artificial Intelligence (GAI) technology, Generative Diffusion Models (GDMs) have shown significant empowerment potential in the field of wireless networks due to advantages, such as noise resistance, training stability, controllability, and multimodal generation. Although there have been multiple studies focusing on GDMs for wireless networks, there is still a lack of comprehensive reviews on their technological evolution. Motivated by this, we systematically explore the application of GDMs in wireless networks. Firstly, starting from mathematical principles, we analyze technical advantages of GDMs and present six representative models. Furthermore, we propose the multi-layer wireless network architecture including sensing layer, transmission layer, application layer, and security plane. We also introduce the core mechanisms of GDM at each of the layers. Subsequently, we conduct a rigorous review on existing GDM-based schemes, with a focus on analyzing their innovative points, the role of GDMs, strengths, and weaknesses. Ultimately, we extract key challenges and provide potential solutions, with the aim of providing directional guidance for future research in this field.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2507.16729v1" target="_blank">Improving Model Classification by Optimizing the Training Dataset</a></h3>
                    <p><strong>Authors:</strong> Morad Tukan, Loay Mualem, Eitan Netzer, Liran Sigalat</p>
                    <p><strong>Published:</strong> 7/22/2025</p>
                    <p><strong>Categories:</strong> cs.LG</p>
                    <p><strong>Summary:</strong> In the era of data-centric AI, the ability to curate high-quality training data is as crucial as model design. Coresets offer a principled approach to data reduction, enabling efficient learning on large datasets through importance sampling. However, conventional sensitivity-based coreset construction often falls short in optimizing for classification performance metrics, e.g., $F1$ score, focusing instead on loss approximation. In this work, we present a systematic framework for tuning the coreset generation process to enhance downstream classification quality. Our method introduces new tunable parameters--including deterministic sampling, class-wise allocation, and refinement via active sampling, beyond traditional sensitivity scores. Through extensive experiments on diverse datasets and classifiers, we demonstrate that tuned coresets can significantly outperform both vanilla coresets and full dataset training on key classification metrics, offering an effective path towards better and more efficient model training.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2507.16717v1" target="_blank">Multi-objective Portfolio Optimization Via Gradient Descent</a></h3>
                    <p><strong>Authors:</strong> Christian Oliva, Pedro R. Ventura, Luis F. Lago-FernÃ¡ndez</p>
                    <p><strong>Published:</strong> 7/22/2025</p>
                    <p><strong>Categories:</strong> cs.CE, cs.LG</p>
                    <p><strong>Summary:</strong> Traditional approaches to portfolio optimization, often rooted in Modern Portfolio Theory and solved via quadratic programming or evolutionary algorithms, struggle with scalability or flexibility, especially in scenarios involving complex constraints, large datasets and/or multiple conflicting objectives. To address these challenges, we introduce a benchmark framework for multi-objective portfolio optimization (MPO) using gradient descent with automatic differentiation. Our method supports any optimization objective, such as minimizing risk measures (e.g., CVaR) or maximizing Sharpe ratio, along with realistic constraints, such as tracking error limits, UCITS regulations, or asset group restrictions. We have evaluated our framework across six experimental scenarios, from single-objective setups to complex multi-objective cases, and have compared its performance against standard solvers like CVXPY and SKFOLIO. Our results show that our method achieves competitive performance while offering enhanced flexibility for modeling multiple objectives and constraints. We aim to provide a practical and extensible tool for researchers and practitioners exploring advanced portfolio optimization problems in real-world conditions.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2507.16715v1" target="_blank">A Tutorial on MRI Reconstruction: From Modern Methods to Clinical Implications</a></h3>
                    <p><strong>Authors:</strong> Tolga Ã‡ukur, Salman U. H. Dar, Valiyeh Ansarian Nezhad, Yohan Jun, Tae Hyung Kim, Shohei Fujita, Berkin Bilgic</p>
                    <p><strong>Published:</strong> 7/22/2025</p>
                    <p><strong>Categories:</strong> eess.IV</p>
                    <p><strong>Summary:</strong> MRI is an indispensable clinical tool, offering a rich variety of tissue contrasts to support broad diagnostic and research applications. Clinical exams routinely acquire multiple structural sequences that provide complementary information for differential diagnosis, while research protocols often incorporate advanced functional, diffusion, spectroscopic, and relaxometry sequences to capture multidimensional insights into tissue structure and composition. However, these capabilities come at the cost of prolonged scan times, which reduce patient throughput, increase susceptibility to motion artifacts, and may require trade-offs in image quality or diagnostic scope. Over the last two decades, advances in image reconstruction algorithms--alongside improvements in hardware and pulse sequence design--have made it possible to accelerate acquisitions while preserving diagnostic quality. Central to this progress is the ability to incorporate prior information to regularize the solutions to the reconstruction problem. In this tutorial, we overview the basics of MRI reconstruction and highlight state-of-the-art approaches, beginning with classical methods that rely on explicit hand-crafted priors, and then turning to deep learning methods that leverage a combination of learned and crafted priors to further push the performance envelope. We also explore the translational aspects and eventual clinical implications of these methods. We conclude by discussing future directions to address remaining challenges in MRI reconstruction. The tutorial is accompanied by a Python toolbox (https://github.com/tutorial-MRI-recon/tutorial) to demonstrate select methods discussed in the article.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2507.16704v1" target="_blank">Screen2AX: Vision-Based Approach for Automatic macOS Accessibility Generation</a></h3>
                    <p><strong>Authors:</strong> Viktor Muryn, Marta Sumyk, Mariya Hirna, Sofiya Garkot, Maksym Shamrai</p>
                    <p><strong>Published:</strong> 7/22/2025</p>
                    <p><strong>Categories:</strong> cs.LG, cs.AI, cs.CV, cs.HC</p>
                    <p><strong>Summary:</strong> Desktop accessibility metadata enables AI agents to interpret screens and supports users who depend on tools like screen readers. Yet, many applications remain largely inaccessible due to incomplete or missing metadata provided by developers - our investigation shows that only 33% of applications on macOS offer full accessibility support. While recent work on structured screen representation has primarily addressed specific challenges, such as UI element detection or captioning, none has attempted to capture the full complexity of desktop interfaces by replicating their entire hierarchical structure. To bridge this gap, we introduce Screen2AX, the first framework to automatically create real-time, tree-structured accessibility metadata from a single screenshot. Our method uses vision-language and object detection models to detect, describe, and organize UI elements hierarchically, mirroring macOSs system-level accessibility structure. To tackle the limited availability of data for macOS desktop applications, we compiled and publicly released three datasets encompassing 112 macOS applications, each annotated for UI element detection, grouping, and hierarchical accessibility metadata alongside corresponding screenshots. Screen2AX accurately infers hierarchy trees, achieving a 77% F1 score in reconstructing a complete accessibility tree. Crucially, these hierarchy trees improve the ability of autonomous agents to interpret and interact with complex desktop interfaces. We introduce Screen2AX-Task, a benchmark specifically designed for evaluating autonomous agent task execution in macOS desktop environments. Using this benchmark, we demonstrate that Screen2AX delivers a 2.2x performance improvement over native accessibility representations and surpasses the state-of-the-art OmniParser V2 system on the ScreenSpot benchmark.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2507.16697v1" target="_blank">Pixel-Resolved Long-Context Learning for Turbulence at Exascale: Resolving Small-scale Eddies Toward the Viscous Limit</a></h3>
                    <p><strong>Authors:</strong> Junqi Yin, Mijanur Palash, M. Paul Laiu, Muralikrishnan Gopalakrishnan Meena, John Gounley, Stephen M. de Bruyn Kops, Feiyi Wang, Ramanan Sankaran, Pei Zhang</p>
                    <p><strong>Published:</strong> 7/22/2025</p>
                    <p><strong>Categories:</strong> physics.flu-dyn, cs.LG</p>
                    <p><strong>Summary:</strong> Turbulence plays a crucial role in multiphysics applications, including aerodynamics, fusion, and combustion. Accurately capturing turbulences multiscale characteristics is essential for reliable predictions of multiphysics interactions, but remains a grand challenge even for exascale supercomputers and advanced deep learning models. The extreme-resolution data required to represent turbulence, ranging from billions to trillions of grid points, pose prohibitive computational costs for models based on architectures like vision transformers. To address this challenge, we introduce a multiscale hierarchical Turbulence Transformer that reduces sequence length from billions to a few millions and a novel RingX sequence parallelism approach that enables scalable long-context learning. We perform scaling and science runs on the Frontier supercomputer. Our approach demonstrates excellent performance up to 1.1 EFLOPS on 32,768 AMD GPUs, with a scaling efficiency of 94%. To our knowledge, this is the first AI model for turbulence that can capture small-scale eddies down to the dissipative range.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2507.16693v1" target="_blank">High-throughput Super-Resolution Imaging Chip based on Miniaturized Full-frequency Encoded-illumination</a></h3>
                    <p><strong>Authors:</strong> Xiaoyu Yang, Haonan Zhang, Feihong Lin, Mingwei Tang, Tawfique Hasan, Clemens F. Kaminski, Xu Liu, Qing Yang</p>
                    <p><strong>Published:</strong> 7/22/2025</p>
                    <p><strong>Categories:</strong> physics.optics</p>
                    <p><strong>Summary:</strong> A miniaturized full-frequency encoded illumination (mini-FEI) chip is presented for high-throughput super-resolution imaging using the spatial frequency shift (SFS) effect. A tunable full SFS scheme is achieved through propagating and evanescent wave. The multi-illumination modes are precisely and flexibly modulated by an encoded LED array. The light travels to the sample via a set of prisms, producing the super-resolution images with high signal-to-noise ratio (SNR). Mini-FEI super-resolution imaging reaches a resolution of 333 nm (~{\lambda}/4NA), close to the theoretical limit, while maintaining a large field of view (FOV) of ~1 mm2. The method is validated on label-free samples including USAF Target, Star Target, and onion root tip cells, all of which could be successfully reconstructed. Through the introduction of integrated LED arrays for evanescent wave excitation, expensive laser systems can be avoided and the system significantly miniaturized. The mini-FEI super-resolution imaging chip is simple and cost effective to fabricate and can be used in conjunction with any inverted brightfield microscope frame and thus has great potential for widespread use in scientific and industrial research environments.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2507.16685v1" target="_blank">VulGuard: An Unified Tool for Evaluating Just-In-Time Vulnerability Prediction Models</a></h3>
                    <p><strong>Authors:</strong> Duong Nguyen, Manh Tran-Duc, Thanh Le-Cong, Triet Huynh Minh Le, M. Ali Babar, Quyet-Thang Huynh</p>
                    <p><strong>Published:</strong> 7/22/2025</p>
                    <p><strong>Categories:</strong> cs.SE</p>
                    <p><strong>Summary:</strong> We present VulGuard, an automated tool designed to streamline the extraction, processing, and analysis of commits from GitHub repositories for Just-In-Time vulnerability prediction (JIT-VP) research. VulGuard automatically mines commit histories, extracts fine-grained code changes, commit messages, and software engineering metrics, and formats them for downstream analysis. In addition, it integrates several state-of-the-art vulnerability prediction models, allowing researchers to train, evaluate, and compare models with minimal setup. By supporting both repository-scale mining and model-level experimentation within a unified framework, VulGuard addresses key challenges in reproducibility and scalability in software security research. VulGuard can also be easily integrated into the CI/CD pipeline. We demonstrate the effectiveness of the tool in two influential open-source projects, FFmpeg and the Linux kernel, highlighting its potential to accelerate real-world JIT-VP research and promote standardized benchmarking. A demo video is available at: https://youtu.be/j96096-pxbs</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2507.16680v1" target="_blank">Latent Space Alignment for AI-Native MIMO Semantic Communications</a></h3>
                    <p><strong>Authors:</strong> Mario Edoardo Pandolfo, Simone Fiorellino, Emilio Calvanese Strinati, Paolo Di Lorenzo</p>
                    <p><strong>Published:</strong> 7/22/2025</p>
                    <p><strong>Categories:</strong> cs.LG, cs.IT, cs.NI, math.IT</p>
                    <p><strong>Summary:</strong> Semantic communications focus on prioritizing the understanding of the meaning behind transmitted data and ensuring the successful completion of tasks that motivate the exchange of information. However, when devices rely on different languages, logic, or internal representations, semantic mismatches may occur, potentially hindering mutual understanding. This paper introduces a novel approach to addressing latent space misalignment in semantic communications, exploiting multiple-input multiple-output (MIMO) communications. Specifically, our method learns a MIMO precoder/decoder pair that jointly performs latent space compression and semantic channel equalization, mitigating both semantic mismatches and physical channel impairments. We explore two solutions: (i) a linear model, optimized by solving a biconvex optimization problem via the alternating direction method of multipliers (ADMM); (ii) a neural network-based model, which learns semantic MIMO precoder/decoder under transmission power budget and complexity constraints. Numerical results demonstrate the effectiveness of the proposed approach in a goal-oriented semantic communication scenario, illustrating the main trade-offs between accuracy, communication burden, and complexity of the solutions.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2507.16676v1" target="_blank">Custom Algorithm-based Fault Tolerance for Attention Layers in Transformers</a></h3>
                    <p><strong>Authors:</strong> Vasileios Titopoulos, Kosmas Alexandridis, Giorgos Dimitrakopoulos</p>
                    <p><strong>Published:</strong> 7/22/2025</p>
                    <p><strong>Categories:</strong> cs.LG, cs.AR</p>
                    <p><strong>Summary:</strong> Transformers and large language models (LLMs), powered by the attention mechanism, have transformed numerous AI applications, driving the need for specialized hardware accelerators. A major challenge in these accelerators is efficiently detecting errors caused by random hardware faults. Traditional algorithm-based fault tolerance (ABFT) techniques verify individual matrix multiplications but fall short in handling the full attention mechanism, particularly due to intermediate softmax normalization. This work proposes Flash-ABFT, a novel method that computes an online checksum across the entire three-matrix product of query, key and value matrices, of an attention layer, including the softmax operation, with a single check. This approach significantly reduces overhead by eliminating redundant checks while maintaining high fault-detection accuracy. Experimental results demonstrate that Flash-ABFT incurs only 5.3% hardware area overhead and less than 1.9% energy overhead, making it a cost-effective and robust solution for error detection in attention accelerators.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2507.16670v1" target="_blank">Adaptive Inventory Strategies using Deep Reinforcement Learning for Dynamic Agri-Food Supply Chains</a></h3>
                    <p><strong>Authors:</strong> Amandeep Kaur, Gyan Prakash</p>
                    <p><strong>Published:</strong> 7/22/2025</p>
                    <p><strong>Categories:</strong> cs.AI</p>
                    <p><strong>Summary:</strong> Agricultural products are often subject to seasonal fluctuations in production and demand. Predicting and managing inventory levels in response to these variations can be challenging, leading to either excess inventory or stockouts. Additionally, the coordination among stakeholders at various level of food supply chain is not considered in the existing body of literature. To bridge these research gaps, this study focuses on inventory management of agri-food products under demand and lead time uncertainties. By implementing effective inventory replenishment policy results in maximize the overall profit throughout the supply chain. However, the complexity of the problem increases due to these uncertainties and shelf-life of the product, that makes challenging to implement traditional approaches to generate optimal set of solutions. Thus, the current study propose a novel Deep Reinforcement Learning (DRL) algorithm that combines the benefits of both value- and policy-based DRL approaches for inventory optimization under uncertainties. The proposed algorithm can incentivize collaboration among stakeholders by aligning their interests and objectives through shared optimization goal of maximizing profitability along the agri-food supply chain while considering perishability, and uncertainty simultaneously. By selecting optimal order quantities with continuous action space, the proposed algorithm effectively addresses the inventory optimization challenges. To rigorously evaluate this algorithm, the empirical data from fresh agricultural products supply chain inventory is considered. Experimental results corroborate the improved performance of the proposed inventory replenishment policy under stochastic demand patterns and lead time scenarios. The research findings hold managerial implications for policymakers to manage the inventory of agricultural products more effectively under uncertainty.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2507.16655v1" target="_blank">A comparison between behavioral similarity methods vs standard deviation method in predicting time series dataset, case study of finance market</a></h3>
                    <p><strong>Authors:</strong> Mahdi Goldani</p>
                    <p><strong>Published:</strong> 7/22/2025</p>
                    <p><strong>Categories:</strong> econ.TH</p>
                    <p><strong>Summary:</strong> In statistical modeling, prediction and explanation are two fundamental objectives. When the primary goal is forecasting, it is important to account for the inherent uncertainty associated with estimating unknown outcomes. Traditionally, confidence intervals constructed using standard deviations have served as a formal means to quantify this uncertainty and evaluate the closeness of predicted values to their true counterparts. This approach reflects an implicit aim to capture the behavioral similarity between observed and estimated values. However, advances in similarity based approaches present promising alternatives to conventional variance based techniques, particularly in contexts characterized by large datasets or a high number of explanatory variables. This study aims to investigate which methods either traditional or similarity based are capable of producing narrower confidence intervals under comparable conditions, thereby offering more precise and informative intervals. The dataset utilized in this study consists of U.S. mega cap companies, comprising 42 firms. Due to the high number of features, interdependencies among predictors are common, therefore, Ridge Regression is applied to address this issue. The research findings indicate that variance based method and LCSS exhibit the highest coverage among the analyzed methods, although they produce broader intervals. Conversely, DTW, Hausdorff, and TWED deliver narrower intervals, positioning them as the most accurate methods, despite their medium coverage rates. Ultimately, the trade off between interval width and coverage underscores the necessity for context aware decision making when selecting similarity based methods for confidence interval estimation in time series analysis.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2507.16649v1" target="_blank">From Profiling to Optimization: Unveiling the Profile Guided Optimization</a></h3>
                    <p><strong>Authors:</strong> Bingxin Liu, Yinghui Huang, Jianhua Gao, Jianjun Shi, Yongpeng Liu, Yipin Sun, Weixing Ji</p>
                    <p><strong>Published:</strong> 7/22/2025</p>
                    <p><strong>Categories:</strong> cs.PF</p>
                    <p><strong>Summary:</strong> Profile Guided Optimization (PGO) uses runtime profiling to direct compiler optimization decisions, effectively combining static analysis with actual execution behavior to enhance performance. Runtime profiles, collected through instrumentation or hardware- and software-assisted sampling, provide detailed insights into control flow, branch predictions, and memory access patterns. This survey systematically categorizes PGO research by profiling method (instrumentation vs. sampling), optimizations (compile time and link/post-link time), compiler integration (GCC, LLVM), and target architectures. Key algorithms and frameworks are shown in terms of design principles. Performance evaluation on representative examples demonstrates PGOs speedups, overheads, and integration maturity. Finally, we identify open challenges, such as reducing sampling overhead, dynamic input workloads, and supporting cross-architecture portability, and propose future research directions to low-overhead profiling and advanced compilers.</p>
                
            
                
                    <h3><a href="http://dx.doi.org/10.1109/BigData59044.2023.10386518" target="_blank">Towards Automated Regulatory Compliance Verification in Financial Auditing with Large Language Models</a></h3>
                    <p><strong>Authors:</strong> Armin Berger, Lars Hillebrand, David Leonhard, Tobias DeuÃŸer, Thiago Bell Felix de Oliveira, Tim Dilmaghani, Mohamed Khaled, Bernd Kliem, RÃ¼diger Loitz, Christian Bauckhage, Rafet Sifa</p>
                    <p><strong>Published:</strong> 7/22/2025</p>
                    <p><strong>Categories:</strong> cs.CL, cs.AI, cs.LG</p>
                    <p><strong>Summary:</strong> The auditing of financial documents, historically a labor-intensive process, stands on the precipice of transformation. AI-driven solutions have made inroads into streamlining this process by recommending pertinent text passages from financial reports to align with the legal requirements of accounting standards. However, a glaring limitation remains: these systems commonly fall short in verifying if the recommended excerpts indeed comply with the specific legal mandates. Hence, in this paper, we probe the efficiency of publicly available Large Language Models (LLMs) in the realm of regulatory compliance across different model configurations. We place particular emphasis on comparing cutting-edge open-source LLMs, such as Llama-2, with their proprietary counterparts like OpenAIs GPT models. This comparative analysis leverages two custom datasets provided by our partner PricewaterhouseCoopers (PwC) Germany. We find that the open-source Llama-2 70 billion model demonstrates outstanding performance in detecting non-compliance or true negative occurrences, beating all their proprietary counterparts. Nevertheless, proprietary models such as GPT-4 perform the best in a broad variety of scenarios, particularly in non-English contexts.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2507.16639v1" target="_blank">Benchmarking pig detection and tracking under diverse and challenging conditions</a></h3>
                    <p><strong>Authors:</strong> Jonathan Henrich, Christian Post, Maximilian Zilke, Parth Shiroya, Emma Chanut, Amir Mollazadeh Yamchi, Ramin Yahyapour, Thomas Kneib, Imke Traulsen</p>
                    <p><strong>Published:</strong> 7/22/2025</p>
                    <p><strong>Categories:</strong> cs.CV</p>
                    <p><strong>Summary:</strong> To ensure animal welfare and effective management in pig farming, monitoring individual behavior is a crucial prerequisite. While monitoring tasks have traditionally been carried out manually, advances in machine learning have made it possible to collect individualized information in an increasingly automated way. Central to these methods is the localization of animals across space (object detection) and time (multi-object tracking). Despite extensive research of these two tasks in pig farming, a systematic benchmarking study has not yet been conducted. In this work, we address this gap by curating two datasets: PigDetect for object detection and PigTrack for multi-object tracking. The datasets are based on diverse image and video material from realistic barn conditions, and include challenging scenarios such as occlusions or bad visibility. For object detection, we show that challenging training images improve detection performance beyond what is achievable with randomly sampled images alone. Comparing different approaches, we found that state-of-the-art models offer substantial improvements in detection quality over real-time alternatives. For multi-object tracking, we observed that SORT-based methods achieve superior detection performance compared to end-to-end trainable models. However, end-to-end models show better association performance, suggesting they could become strong alternatives in the future. We also investigate characteristic failure cases of end-to-end models, providing guidance for future improvements. The detection and tracking models trained on our datasets perform well in unseen pens, suggesting good generalization capabilities. This highlights the importance of high-quality training data. The datasets and research code are made publicly available to facilitate reproducibility, re-use and further development.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2507.16636v1" target="_blank">Physics-Informed Neural Networks for High-Precision Grad-Shafranov Equilibrium Reconstruction</a></h3>
                    <p><strong>Authors:</strong> Cuizhi Zhou, Kaien Zhu</p>
                    <p><strong>Published:</strong> 7/22/2025</p>
                    <p><strong>Categories:</strong> physics.plasm-ph</p>
                    <p><strong>Summary:</strong> The equilibrium reconstruction of plasma is a core step in real-time diagnostic tasks in fusion research. This paper explores a multi-stage Physics-Informed Neural Networks(PINNs) approach to solve the Grad-Shafranov equation, achieving high-precision solutions with an error magnitude of $O(10^{-8})$ between the output of the second-stage neural network and the analytical solution. Our results demonstrate that the multi-stage PINNs provides a reliable tool for plasma equilibrium reconstruction.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2507.16632v1" target="_blank">Step-Audio 2 Technical Report</a></h3>
                    <p><strong>Authors:</strong> Boyong Wu, Chao Yan, Chen Hu, Cheng Yi, Chengli Feng, Fei Tian, Feiyu Shen, Gang Yu, Haoyang Zhang, Jingbei Li, Mingrui Chen, Peng Liu, Wang You, Xiangyu Tony Zhang, Xingyuan Li, Xuerui Yang, Yayue Deng, Yechang Huang, Yuxin Li, Yuxin Zhang, Zhao You, Brian Li, Changyi Wan, Hanpeng Hu, Jiangjie Zhen, Siyu Chen, Song Yuan, Xuelin Zhang, Yimin Jiang, Yu Zhou, Yuxiang Yang, Bingxin Li, Buyun Ma, Changhe Song, Dongqing Pang, Guoqiang Hu, Haiyang Sun, Kang An, Na Wang, Shuli Gao, Wei Ji, Wen Li, Wen Sun, Xuan Wen, Yong Ren, Yuankai Ma, Yufan Lu, Bin Wang, Bo Li, Changxin Miao, Che Liu, Chen Xu, Dapeng Shi, Dingyuan Hu, Donghang Wu, Enle Liu, Guanzhe Huang, Gulin Yan, Han Zhang, Hao Nie, Haonan Jia, Hongyu Zhou, Jianjian Sun, Jiaoren Wu, Jie Wu, Jie Yang, Jin Yang, Junzhe Lin, Kaixiang Li, Lei Yang, Liying Shi, Li Zhou, Longlong Gu, Ming Li, Mingliang Li, Mingxiao Li, Nan Wu, Qi Han, Qinyuan Tan, Shaoliang Pang, Shengjie Fan, Siqi Liu, Tiancheng Cao, Wanying Lu, Wenqing He, Wuxun Xie, Xu Zhao, Xueqi Li, Yanbo Yu, Yang Yang, Yi Liu, Yifan Lu, Yilei Wang, Yuanhao Ding, Yuanwei Liang, Yuanwei Lu, Yuchu Luo, Yuhe Yin, Yumeng Zhan, Yuxiang Zhang, Zidong Yang, Zixin Zhang, Binxing Jiao, Daxin Jiang, Heung-Yeung Shum, Jiansheng Chen, Jing Li, Xiangyu Zhang, Yibo Zhu</p>
                    <p><strong>Published:</strong> 7/22/2025</p>
                    <p><strong>Categories:</strong> cs.CL, cs.SD, eess.AS</p>
                    <p><strong>Summary:</strong> This paper presents Step-Audio~2, an end-to-end multi-modal large language model designed for industry-strength audio understanding and speech conversation. By integrating a latent audio encoder and reasoning-centric reinforcement learning (RL), Step-Audio 2 achieves promising performance in automatic speech recognition (ASR) and audio understanding. To facilitate genuine end-to-end speech conversation, Step-Audio 2 incorporates the generation of discrete audio tokens into language modeling, significantly enhancing its responsiveness to paralinguistic information such as speaking styles and emotions. To effectively leverage the rich textual and acoustic knowledge in real-world data, Step-Audio 2 integrates retrieval-augmented generation (RAG) and is able to call external tools such as web search to mitigate hallucination and audio search to switch timbres. Trained on millions of hours of speech and audio data, Step-Audio 2 delivers intelligence and expressiveness across diverse conversational scenarios. Evaluation results demonstrate that Step-Audio 2 achieves state-of-the-art performance on various audio understanding and conversational benchmarks compared to other open-source and commercial solutions. Please visit https://github.com/stepfun-ai/Step-Audio2 for more information.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2507.16623v1" target="_blank">Automatic Fine-grained Segmentation-assisted Report Generation</a></h3>
                    <p><strong>Authors:</strong> Frederic Jonske, Constantin Seibold, Osman Alperen Koras, Fin Bahnsen, Marie Bauer, Amin Dada, Hamza Kalisch, Anton Schily, Jens Kleesiek</p>
                    <p><strong>Published:</strong> 7/22/2025</p>
                    <p><strong>Categories:</strong> cs.CV, cs.LG</p>
                    <p><strong>Summary:</strong> Reliable end-to-end clinical report generation has been a longstanding goal of medical ML research. The end goal for this process is to alleviate radiologists workloads and provide second opinions to clinicians or patients. Thus, a necessary prerequisite for report generation models is a strong general performance and some type of innate grounding capability, to convince clinicians or patients of the veracity of the generated reports. In this paper, we present ASaRG (\textbf{A}utomatic \textbf{S}egmentation-\textbf{a}ssisted \textbf{R}eport \textbf{G}eneration), an extension of the popular LLaVA architecture that aims to tackle both of these problems. ASaRG proposes to fuse intermediate features and fine-grained segmentation maps created by specialist radiological models into LLaVAs multi-modal projection layer via simple concatenation. With a small number of added parameters, our approach achieves a +0.89\% performance gain ($p=0.012$) in CE F1 score compared to the LLaVA baseline when using only intermediate features, and +2.77\% performance gain ($p0.001$) when adding a combination of intermediate features and fine-grained segmentation maps. Compared with COMG and ORID, two other report generation methods that utilize segmentations, the performance gain amounts to 6.98\% and 6.28\% in F1 score, respectively. ASaRG is not mutually exclusive with other changes made to the LLaVA architecture, potentially allowing our method to be combined with other advances in the field. Finally, the use of an arbitrary number of segmentations as part of the input demonstrably allows tracing elements of the report to the corresponding segmentation maps and verifying the groundedness of assessments. Our code will be made publicly available at a later date.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2507.16621v1" target="_blank">A Target-based Multi-LiDAR Multi-Camera Extrinsic Calibration System</a></h3>
                    <p><strong>Authors:</strong> Lorenzo Gentilini, Pierpaolo Serio, Valentina Donzella, Lorenzo Pollini</p>
                    <p><strong>Published:</strong> 7/22/2025</p>
                    <p><strong>Categories:</strong> cs.RO, cs.CV</p>
                    <p><strong>Summary:</strong> Extrinsic Calibration represents the cornerstone of autonomous driving. Its accuracy plays a crucial role in the perception pipeline, as any errors can have implications for the safety of the vehicle. Modern sensor systems collect different types of data from the environment, making it harder to align the data. To this end, we propose a target-based extrinsic calibration system tailored for a multi-LiDAR and multi-camera sensor suite. This system enables cross-calibration between LiDARs and cameras with limited prior knowledge using a custom ChArUco board and a tailored nonlinear optimization method. We test the system with real-world data gathered in a warehouse. Results demonstrated the effectiveness of the proposed method, highlighting the feasibility of a unique pipeline tailored for various types of sensors.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2507.16611v1" target="_blank">Smooth Games of Configuration in the Linear-Quadratic Setting</a></h3>
                    <p><strong>Authors:</strong> Jesse Milzman, Jeffrey Mao, Giuseppe Loianno</p>
                    <p><strong>Published:</strong> 7/22/2025</p>
                    <p><strong>Categories:</strong> cs.MA, cs.GT</p>
                    <p><strong>Summary:</strong> Dynamic game theory offers a toolbox for formalizing and solving for both cooperative and non-cooperative strategies in multi-agent scenarios. However, the optimal configuration of such games remains largely unexplored. While there is existing literature on the parametrization of dynamic games, little research examines this parametrization from a strategic perspective where each agents configuration choice is influenced by the decisions of others. In this work, we introduce the concept of a game of configuration, providing a framework for the strategic fine-tuning of differential games. We define a game of configuration as a two-stage game within the setting of finite-horizon, affine-quadratic, AQ, differential games. In the first stage, each player chooses their corresponding configuration parameter, which will impact their dynamics and costs in the second stage. We provide the subgame perfect solution concept and a method for computing first stage cost gradients over the configuration space. This then allows us to formulate a gradient-based method for searching for local solutions to the configuration game, as well as provide necessary conditions for equilibrium configurations over their downstream (second stage) trajectories. We conclude by demonstrating the effectiveness of our approach in example AQ systems, both zero-sum and general-sum.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2507.16598v1" target="_blank">Depression as a disorder of distributional coding</a></h3>
                    <p><strong>Authors:</strong> Matthew Botvinick, Zeb Kurth-Nelson, Timothy Muller, Will Dabney</p>
                    <p><strong>Published:</strong> 7/22/2025</p>
                    <p><strong>Categories:</strong> q-bio.NC</p>
                    <p><strong>Summary:</strong> Major depressive disorder persistently stands as a major public health problem. While some progress has been made toward effective treatments, the neural mechanisms that give rise to the disorder remain poorly understood. In this Perspective, we put forward a new theory of the pathophysiology of depression. More precisely, we spotlight three previously separate bodies of research, showing how they can be fit together into a previously overlooked larger picture. The first piece of the puzzle is provided by pathophysiology research implicating dopamine in depression. The second piece, coming from computational psychiatry, links depression with a special form of reinforcement learning. The third and final piece involves recent work at the intersection of artificial intelligence and basic neuroscience research, indicating that the brain may represent value using a distributional code. Fitting these three pieces together yields a new model of depressions pathophysiology, which spans circuit, systems, computational and behavioral levels, opening up new directions for research.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2507.16596v1" target="_blank">A Multimodal Deviation Perceiving Framework for Weakly-Supervised Temporal Forgery Localization</a></h3>
                    <p><strong>Authors:</strong> Wenbo Xu, Junyan Wu, Wei Lu, Xiangyang Luo, Qian Wang</p>
                    <p><strong>Published:</strong> 7/22/2025</p>
                    <p><strong>Categories:</strong> cs.CV</p>
                    <p><strong>Summary:</strong> Current researches on Deepfake forensics often treat detection as a classification task or temporal forgery localization problem, which are usually restrictive, time-consuming, and challenging to scale for large datasets. To resolve these issues, we present a multimodal deviation perceiving framework for weakly-supervised temporal forgery localization (MDP), which aims to identify temporal partial forged segments using only video-level annotations. The MDP proposes a novel multimodal interaction mechanism (MI) and an extensible deviation perceiving loss to perceive multimodal deviation, which achieves the refined start and end timestamps localization of forged segments. Specifically, MI introduces a temporal property preserving cross-modal attention to measure the relevance between the visual and audio modalities in the probabilistic embedding space. It could identify the inter-modality deviation and construct comprehensive video features for temporal forgery localization. To explore further temporal deviation for weakly-supervised learning, an extensible deviation perceiving loss has been proposed, aiming at enlarging the deviation of adjacent segments of the forged samples and reducing that of genuine samples. Extensive experiments demonstrate the effectiveness of the proposed framework and achieve comparable results to fully-supervised approaches in several evaluation metrics.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2507.16594v1" target="_blank">An Experimental Study of Split-Learning TinyML on Ultra-Low-Power Edge/IoT Nodes</a></h3>
                    <p><strong>Authors:</strong> Zied Jenhani, Mounir Bensalem, Jasenka DizdareviÄ‡, Admela Jukan</p>
                    <p><strong>Published:</strong> 7/22/2025</p>
                    <p><strong>Categories:</strong> cs.NI, cs.AI, cs.DC</p>
                    <p><strong>Summary:</strong> Running deep learning inference directly on ultra-low-power edge/IoT nodes has been limited by the tight memory and compute budgets of microcontrollers. Split learning (SL) addresses this limitation in which it executes part of the inference process on the sensor and off-loads the remainder to a companion device. In the context of constrained devices and the related impact of low-power, over-the-air transport protocols, the performance of split learning remains largely unexplored. TO the best of our knowledge, this paper presents the first end-to-end TinyML + SL testbed built on Espressif ESP32-S3 boards, designed to benchmark the over-the-air performance of split learning TinyML in edge/IoT environments. We benchmark the performance of a MobileNetV2 image recognition model, which is quantized to 8-bit integers, partitioned, and delivered to the nodes via over-the-air updates. The intermediate activations are exchanged through different wireless communication methods: ESP-NOW, BLE, and traditional UDP/IP and TCP/IP, enabling a head-to-head comparison on identical hardware. Measurements show that splitting the model after block_16_project_BN layer generates a 5.66 kB tensor that traverses the link in 3.2 ms, when UDP is used, achieving a steady-state round-trip latency of 5.8 s. ESP-NOW presents the most favorable RTT performance 3.7 s; BLE extends battery life further but increases latency beyond 10s.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2507.16586v1" target="_blank">AI for Better UX in Computer-Aided Engineering: Is Academia Catching Up with Industry Demands? A Multivocal Literature Review</a></h3>
                    <p><strong>Authors:</strong> Choro Ulan Uulu, Mikhail Kulyabin, Layan Etaiwi, Nuno Miguel Martins Pacheco, Jan Joosten, Kerstin RÃ¶se, Filippos Petridis, Jan Bosch, Helena HolmstrÃ¶m Olsson</p>
                    <p><strong>Published:</strong> 7/22/2025</p>
                    <p><strong>Categories:</strong> cs.HC, cs.AI, cs.SE</p>
                    <p><strong>Summary:</strong> Computer-Aided Engineering (CAE) enables simulation experts to optimize complex models, but faces challenges in user experience (UX) that limit efficiency and accessibility. While artificial intelligence (AI) has demonstrated potential to enhance CAE processes, research integrating these fields with a focus on UX remains fragmented. This paper presents a multivocal literature review (MLR) examining how AI enhances UX in CAE software across both academic research and industry implementations. Our analysis reveals significant gaps between academic explorations and industry applications, with companies actively implementing LLMs, adaptive UIs, and recommender systems while academic research focuses primarily on technical capabilities without UX validation. Key findings demonstrate opportunities in AI-powered guidance, adaptive interfaces, and workflow automation that remain underexplored in current research. By mapping the intersection of these domains, this study provides a foundation for future work to address the identified research gaps and advance the integration of AI to improve CAE user experience.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2507.16581v1" target="_blank">On Expansions of Monadic Second-Order Logic with Dynamical Predicates</a></h3>
                    <p><strong>Authors:</strong> Joris Nieuwveld, JoÃ«l Ouaknine</p>
                    <p><strong>Published:</strong> 7/22/2025</p>
                    <p><strong>Categories:</strong> cs.LO, math.NT, 11B37 11J86 11B40 11K16, F.4.0; G.2.0</p>
                    <p><strong>Summary:</strong> Expansions of the monadic second-order (MSO) theory of the structure $\langle \mathbb{N} ;  \rangle$ have been a fertile and active area of research ever since the publication of the seminal papers of B\uchi and Elgot  Rabin on the subject in the 1960s. In the present paper, we establish decidability of the MSO theory of $\langle \mathbb{N} ; ,P \rangle$, where $P$ ranges over a large class of unary dynamical predicates, i.e., sets of non-negative values assumed by certain integer linear recurrence sequences. One of our key technical tools is the novel concept of (effective) prodisjunctivity, which we expect may also find independent applications further afield.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2507.16562v1" target="_blank">Evaluating Social Acceptance of eXtended Reality (XR) Agent Technology: A User Study (Extended Version)</a></h3>
                    <p><strong>Authors:</strong> Megha Quamara, Viktor Schmuck, Cristina Iani, Axel Primavesi, Alexander Plaum, Luca Vigano</p>
                    <p><strong>Published:</strong> 7/22/2025</p>
                    <p><strong>Categories:</strong> cs.HC, cs.AI</p>
                    <p><strong>Summary:</strong> In this paper, we present the findings of a user study that evaluated the social acceptance of eXtended Reality (XR) agent technology, focusing on a remotely accessible, web-based XR training system developed for journalists. This system involves user interaction with a virtual avatar, enabled by a modular toolkit. The interactions are designed to provide tailored training for journalists in digital-remote settings, especially for sensitive or dangerous scenarios, without requiring specialized end-user equipment like headsets. Our research adapts and extends the Almere model, representing social acceptance through existing attributes such as perceived ease of use and perceived usefulness, along with added ones like dependability and security in the user-agent interaction. The XR agent was tested through a controlled experiment in a real-world setting, with data collected on users perceptions. Our findings, based on quantitative and qualitative measurements involving questionnaires, contribute to the understanding of user perceptions and acceptance of XR agent solutions within a specific social context, while also identifying areas for the improvement of XR systems.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2507.16559v1" target="_blank">Comparative validation of surgical phase recognition, instrument keypoint estimation, and instrument instance segmentation in endoscopy: Results of the PhaKIR 2024 challenge</a></h3>
                    <p><strong>Authors:</strong> Tobias Rueckert, David Rauber, Raphaela Maerkl, Leonard Klausmann, Suemeyye R. Yildiran, Max Gutbrod, Danilo Weber Nunes, Alvaro Fernandez Moreno, Imanol Luengo, Danail Stoyanov, Nicolas Toussaint, Enki Cho, Hyeon Bae Kim, Oh Sung Choo, Ka Young Kim, Seong Tae Kim, GonÃ§alo Arantes, Kehan Song, Jianjun Zhu, Junchen Xiong, Tingyi Lin, Shunsuke Kikuchi, Hiroki Matsuzaki, Atsushi Kouno, JoÃ£o Renato Ribeiro Manesco, JoÃ£o Paulo Papa, Tae-Min Choi, Tae Kyeong Jeong, Juyoun Park, Oluwatosin Alabi, Meng Wei, Tom Vercauteren, Runzhi Wu, Mengya Xu, An Wang, Long Bai, Hongliang Ren, Amine Yamlahi, Jakob Hennighausen, Lena Maier-Hein, Satoshi Kondo, Satoshi Kasai, Kousuke Hirasawa, Shu Yang, Yihui Wang, Hao Chen, Santiago RodrÃ­guez, NicolÃ¡s Aparicio, Leonardo Manrique, Juan Camilo Lyons, Olivia Hosie, NicolÃ¡s Ayobi, Pablo ArbelÃ¡ez, Yiping Li, Yasmina Al Khalil, Sahar Nasirihaghighi, Stefanie Speidel, Daniel Rueckert, Hubertus Feussner, Dirk Wilhelm, Christoph Palm</p>
                    <p><strong>Published:</strong> 7/22/2025</p>
                    <p><strong>Categories:</strong> cs.CV</p>
                    <p><strong>Summary:</strong> Reliable recognition and localization of surgical instruments in endoscopic video recordings are foundational for a wide range of applications in computer- and robot-assisted minimally invasive surgery (RAMIS), including surgical training, skill assessment, and autonomous assistance. However, robust performance under real-world conditions remains a significant challenge. Incorporating surgical context - such as the current procedural phase - has emerged as a promising strategy to improve robustness and interpretability. To address these challenges, we organized the Surgical Procedure Phase, Keypoint, and Instrument Recognition (PhaKIR) sub-challenge as part of the Endoscopic Vision (EndoVis) challenge at MICCAI 2024. We introduced a novel, multi-center dataset comprising thirteen full-length laparoscopic cholecystectomy videos collected from three distinct medical institutions, with unified annotations for three interrelated tasks: surgical phase recognition, instrument keypoint estimation, and instrument instance segmentation. Unlike existing datasets, ours enables joint investigation of instrument localization and procedural context within the same data while supporting the integration of temporal information across entire procedures. We report results and findings in accordance with the BIAS guidelines for biomedical image analysis challenges. The PhaKIR sub-challenge advances the field by providing a unique benchmark for developing temporally aware, context-driven methods in RAMIS and offers a high-quality resource to support future research in surgical scene understanding.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2507.16557v1" target="_blank">Exploring Gender Bias in Large Language Models: An In-depth Dive into the German Language</a></h3>
                    <p><strong>Authors:</strong> Kristin Gnadt, David Thulke, Simone Kopeinik, Ralf SchlÃ¼ter</p>
                    <p><strong>Published:</strong> 7/22/2025</p>
                    <p><strong>Categories:</strong> cs.CL, cs.LG</p>
                    <p><strong>Summary:</strong> In recent years, various methods have been proposed to evaluate gender bias in large language models (LLMs). A key challenge lies in the transferability of bias measurement methods initially developed for the English language when applied to other languages. This work aims to contribute to this research strand by presenting five German datasets for gender bias evaluation in LLMs. The datasets are grounded in well-established concepts of gender bias and are accessible through multiple methodologies. Our findings, reported for eight multilingual LLM models, reveal unique challenges associated with gender bias in German, including the ambiguous interpretation of male occupational terms and the influence of seemingly neutral nouns on gender perception. This work contributes to the understanding of gender bias in LLMs across languages and underscores the necessity for tailored evaluation frameworks.</p>
                
            
                
                    <h3><a href="http://dx.doi.org/10.1145/3748722" target="_blank">Optimization of DNN-based HSI Segmentation FPGA-based SoC for ADS: A Practical Approach</a></h3>
                    <p><strong>Authors:</strong> Jon GutiÃ©rrez-Zaballa, Koldo Basterretxea, Javier Echanobe</p>
                    <p><strong>Published:</strong> 7/22/2025</p>
                    <p><strong>Categories:</strong> cs.CV, cs.AI, cs.AR, cs.LG, eess.IV</p>
                    <p><strong>Summary:</strong> The use of HSI for autonomous navigation is a promising research field aimed at improving the accuracy and robustness of detection, tracking, and scene understanding systems based on vision sensors. Combining advanced computer algorithms, such as DNNs, with small-size snapshot HSI cameras enhances the reliability of these systems. HSI overcomes intrinsic limitations of greyscale and RGB imaging in depicting physical properties of targets, particularly regarding spectral reflectance and metamerism. Despite promising results in HSI-based vision developments, safety-critical systems like ADS demand strict constraints on latency, resource consumption, and security, motivating the shift of ML workloads to edge platforms. This involves a thorough software/hardware co-design scheme to distribute and optimize the tasks efficiently among the limited resources of computing platforms. With respect to inference, the over-parameterized nature of DNNs poses significant computational challenges for real-time on-the-edge deployment. In addition, the intensive data preprocessing required by HSI, which is frequently overlooked, must be carefully managed in terms of memory arrangement and inter-task communication to enable an efficient integrated pipeline design on a SoC. This work presents a set of optimization techniques for the practical co-design of a DNN-based HSI segmentation processor deployed on a FPGA-based SoC targeted at ADS, including key optimizations such as functional software/hardware task distribution, hardware-aware preprocessing, ML model compression, and a complete pipelined deployment. Applied compression techniques significantly reduce the complexity of the designed DNN to 24.34% of the original operations and to 1.02% of the original number of parameters, achieving a 2.86x speed-up in the inference task without noticeable degradation of the segmentation accuracy.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2507.16548v1" target="_blank">Alternative Loss Function in Evaluation of Transformer Models</a></h3>
                    <p><strong>Authors:</strong> Jakub MichaÅ„kÃ³w, PaweÅ‚ Sakowski, Robert Åšlepaczuk</p>
                    <p><strong>Published:</strong> 7/22/2025</p>
                    <p><strong>Categories:</strong> q-fin.CP, cs.LG, q-fin.TR</p>
                    <p><strong>Summary:</strong> The proper design and architecture of testing of machine learning models, especially in their application to quantitative finance problems, is crucial. The most important in this process is selecting an adequate loss function used for training, validation, estimation purposes, and tuning of hyperparameters. Therefore, in this research, through empirical experiments on equity and cryptocurrency assets, we introduce the Mean Absolute Directional Loss (MADL) function which is more adequate for optimizing forecast-generating models used in algorithmic investment strategies. The MADL function results are compared for Transformer and LSTM models and we show that almost in every case Transformer results are significantly better than those obtained with LSTM.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2507.16542v1" target="_blank">The Effect of Scale Consistency between Real and Virtual Spaces on Immersion in Exhibition Hybrid Spaces</a></h3>
                    <p><strong>Authors:</strong> Qiong Wu, Yan Dong, Zipeng Zhang, Ruochen Hu</p>
                    <p><strong>Published:</strong> 7/22/2025</p>
                    <p><strong>Categories:</strong> cs.HC</p>
                    <p><strong>Summary:</strong> In exhibition hybrid spaces, scale consistency between real and virtual spaces is crucial for user immersion. However, there is currently a lack of systematic research to determine appropriate virtual-to-real mapping ratios. This study developed an immersive interaction system based on Intel 3D Athlete Tracking body mapping technology. Two experiments investigated the impact of virtual space and virtual avatar scale on immersion. Experiment 1 investigated 30 participants preferences for virtual space scale, while Experiment 2 tested the effect of 6 different virtual avatar sizes (25%-150%) on immersion. A 5-point Likert scale was used to assess immersion, followed by analysis of variance and Tukey HSD post-hoc tests. Experiment 1 showed that participants preferred a virtual space ratio of 130% (mean 127.29%, SD 8.55%). Experiment 2 found that virtual avatar sizes within the 75%-100% range produced optimal immersion (p  0.05). Immersion decreased significantly when virtual avatar sizes deviated from users actual height (below 50% or above 125%). Participants were more sensitive to size changes in the 25%-75% range, while perception was weaker for changes in the 75%-100% range. Virtual environments slightly larger than real space (130%) and virtual avatars slightly smaller than users (75%-100%) optimize user immersion. These findings have been applied in the Intel Global Trade Center exhibition hall, demonstrating actionable insights for designing hybrid spaces that enhance immersion and coherence.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2507.16541v1" target="_blank">A Comprehensive Data-centric Overview of Federated Graph Learning</a></h3>
                    <p><strong>Authors:</strong> Zhengyu Wu, Xunkai Li, Yinlin Zhu, Zekai Chen, Guochen Yan, Yanyu Yan, Hao Zhang, Yuming Ai, Xinmo Jin, Rong-Hua Li, Guoren Wang</p>
                    <p><strong>Published:</strong> 7/22/2025</p>
                    <p><strong>Categories:</strong> cs.LG, cs.AI, cs.SI</p>
                    <p><strong>Summary:</strong> In the era of big data applications, Federated Graph Learning (FGL) has emerged as a prominent solution that reconcile the tradeoff between optimizing the collective intelligence between decentralized datasets holders and preserving sensitive information to maximum. Existing FGL surveys have contributed meaningfully but largely focus on integrating Federated Learning (FL) and Graph Machine Learning (GML), resulting in early stage taxonomies that emphasis on methodology and simulated scenarios. Notably, a data centric perspective, which systematically examines FGL methods through the lens of data properties and usage, remains unadapted to reorganize FGL research, yet it is critical to assess how FGL studies manage to tackle data centric constraints to enhance model performances. This survey propose a two-level data centric taxonomy: Data Characteristics, which categorizes studies based on the structural and distributional properties of datasets used in FGL, and Data Utilization, which analyzes the training procedures and techniques employed to overcome key data centric challenges. Each taxonomy level is defined by three orthogonal criteria, each representing a distinct data centric configuration. Beyond taxonomy, this survey examines FGL integration with Pretrained Large Models, showcases realistic applications, and highlights future direction aligned with emerging trends in GML.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2507.16534v1" target="_blank">Frontier AI Risk Management Framework in Practice: A Risk Analysis Technical Report</a></h3>
                    <p><strong>Authors:</strong> Shanghai AI Lab, :, Xiaoyang Chen, Yunhao Chen, Zeren Chen, Zhiyun Chen, Hanyun Cui, Yawen Duan, Jiaxuan Guo, Qi Guo, Xuhao Hu, Hong Huang, Lige Huang, Chunxiao Li, Juncheng Li, Qihao Lin, Dongrui Liu, Xinmin Liu, Zicheng Liu, Chaochao Lu, Xiaoya Lu, Jingjing Qu, Qibing Ren, Jing Shao, Jingwei Shi, Jingwei Sun, Peng Wang, Weibing Wang, Jia Xu, Lewen Yan, Xiao Yu, Yi Yu, Boxuan Zhang, Jie Zhang, Weichen Zhang, Zhijie Zheng, Tianyi Zhou, Bowen Zhou</p>
                    <p><strong>Published:</strong> 7/22/2025</p>
                    <p><strong>Categories:</strong> cs.AI, cs.CL, cs.CV, cs.LG</p>
                    <p><strong>Summary:</strong> To understand and identify the unprecedented risks posed by rapidly advancing artificial intelligence (AI) models, this report presents a comprehensive assessment of their frontier risks. Drawing on the E-T-C analysis (deployment environment, threat source, enabling capability) from the Frontier AI Risk Management Framework (v1.0) (SafeWork-F1-Framework), we identify critical risks in seven areas: cyber offense, biological and chemical risks, persuasion and manipulation, uncontrolled autonomous AI R\D, strategic deception and scheming, self-replication, and collusion. Guided by the AI-$45^\circ$ Law, we evaluate these risks using red lines (intolerable thresholds) and yellow lines (early warning indicators) to define risk zones: green (manageable risk for routine deployment and continuous monitoring), yellow (requiring strengthened mitigations and controlled deployment), and red (necessitating suspension of development and/or deployment). Experimental results show that all recent frontier AI models reside in green and yellow zones, without crossing red lines. Specifically, no evaluated models cross the yellow line for cyber offense or uncontrolled AI R\D risks. For self-replication, and strategic deception and scheming, most models remain in the green zone, except for certain reasoning models in the yellow zone. In persuasion and manipulation, most models are in the yellow zone due to their effective influence on humans. For biological and chemical risks, we are unable to rule out the possibility of most models residing in the yellow zone, although detailed threat modeling and in-depth assessment are required to make further claims. This work reflects our current understanding of AI frontier risks and urges collective action to mitigate these challenges.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2507.16521v1" target="_blank">Adaptive Transition State Refinement with Learned Equilibrium Flows</a></h3>
                    <p><strong>Authors:</strong> Samir Darouich, Vinh Tong, Tanja Bien, Johannes KÃ¤stner, Mathias Niepert</p>
                    <p><strong>Published:</strong> 7/22/2025</p>
                    <p><strong>Categories:</strong> physics.chem-ph, physics.comp-ph</p>
                    <p><strong>Summary:</strong> Identifying transition states (TSs), the high-energy configurations that molecules pass through during chemical reactions, is essential for understanding and designing chemical processes. However, accurately and efficiently identifying these states remains one of the most challenging problems in computational chemistry. In this work, we introduce a new generative AI approach that improves the quality of initial guesses for TS structures. Our method can be combined with a variety of existing techniques, including both machine learning models and fast, approximate quantum methods, to refine their predictions and bring them closer to chemically accurate results. Applied to TS guesses from a state-of-the-art machine learning model, our approach reduces the median structural error to just 0.088 $\unicode{x212B}$ and lowers the median absolute error in reaction barrier heights to 0.79 kcal mol$^{-1}$. When starting from a widely used tight-binding approximation, it increases the success rate of locating valid TSs by 41\% and speeds up high-level quantum optimization by a factor of three. By making TS searches more accurate, robust, and efficient, this method could accelerate reaction mechanism discovery and support the development of new materials, catalysts, and pharmaceuticals.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2507.16515v1" target="_blank">Introducing Quality Estimation to Machine Translation Post-editing Workflow: An Empirical Study on Its Usefulness</a></h3>
                    <p><strong>Authors:</strong> Siqi Liu, Guangrong Dai, Dechao Li</p>
                    <p><strong>Published:</strong> 7/22/2025</p>
                    <p><strong>Categories:</strong> cs.CL, cs.HC</p>
                    <p><strong>Summary:</strong> This preliminary study investigates the usefulness of sentence-level Quality Estimation (QE) in English-Chinese Machine Translation Post-Editing (MTPE), focusing on its impact on post-editing speed and student translators perceptions. It also explores the interaction effects between QE and MT quality, as well as between QE and translation expertise. The findings reveal that QE significantly reduces post-editing time. The examined interaction effects were not significant, suggesting that QE consistently improves MTPE efficiency across medium- and high-quality MT outputs and among student translators with varying levels of expertise. In addition to indicating potentially problematic segments, QE serves multiple functions in MTPE, such as validating translators evaluations of MT quality and enabling them to double-check translation outputs. However, interview data suggest that inaccurate QE may hinder post-editing processes. This research provides new insights into the strengths and limitations of QE, facilitating its more effective integration into MTPE workflows to enhance translators productivity.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2507.16507v1" target="_blank">Agentic RAG with Knowledge Graphs for Complex Multi-Hop Reasoning in Real-World Applications</a></h3>
                    <p><strong>Authors:</strong> Jean Lelong, Adnane Errazine, Annabelle Blangero</p>
                    <p><strong>Published:</strong> 7/22/2025</p>
                    <p><strong>Categories:</strong> cs.AI, cs.IR</p>
                    <p><strong>Summary:</strong> Conventional Retrieval-Augmented Generation (RAG) systems enhance Large Language Models (LLMs) but often fall short on complex queries, delivering limited, extractive answers and struggling with multiple targeted retrievals or navigating intricate entity relationships. This is a critical gap in knowledge-intensive domains. We introduce INRAExplorer, an agentic RAG system for exploring the scientific data of INRAE (Frances National Research Institute for Agriculture, Food and Environment). INRAExplorer employs an LLM-based agent with a multi-tool architecture to dynamically engage a rich knowledge base, through a comprehensive knowledge graph derived from open access INRAE publications. This design empowers INRAExplorer to conduct iterative, targeted queries, retrieve exhaustive datasets (e.g., all publications by an author), perform multi-hop reasoning, and deliver structured, comprehensive answers. INRAExplorer serves as a concrete illustration of enhancing knowledge interaction in specialized fields.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2507.16504v1" target="_blank">Probing Large $N_f$ Through Schemes</a></h3>
                    <p><strong>Authors:</strong> Alan Pinoy, Shahram Vatani</p>
                    <p><strong>Published:</strong> 7/22/2025</p>
                    <p><strong>Categories:</strong> hep-ph, hep-th</p>
                    <p><strong>Summary:</strong> We investigate the reliability of the large $N_f$ expansion of four-dimensional gauge-fermion quantum field theories, focusing on the structure and scheme dependence of the beta function. While the existence of a nontrivial UV fixed point at leading order in $1/N_f$ suggests the possibility of asymptotic safety, the absence of higher-order terms precludes robust conclusions. We analyze the impact of renormalization scheme transformations and show that higher-order corrections inevitably introduce increasingly singular contributions. We prove that at most one renormalization scheme can preserve the dominance of the leading contribution, rendering the truncation trustworthy; in all other schemes, higher-order terms dominate and the expansion becomes unreliable. This result places strong constraints on the physical interpretation of UV fixed points in large $N_f$ theories and emphasizes the need for resummation or non-perturbative control to establish asymptotic safety.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2507.16481v1" target="_blank">Guided Reinforcement Learning for Omnidirectional 3D Jumping in Quadruped Robots</a></h3>
                    <p><strong>Authors:</strong> Riccardo Bussola, Michele Focchi, Giulio Turrisi, Claudio Semini, Luigi Palopoli</p>
                    <p><strong>Published:</strong> 7/22/2025</p>
                    <p><strong>Categories:</strong> cs.RO, cs.SY, eess.SY</p>
                    <p><strong>Summary:</strong> Jumping poses a significant challenge for quadruped robots, despite being crucial for many operational scenarios. While optimisation methods exist for controlling such motions, they are often time-consuming and demand extensive knowledge of robot and terrain parameters, making them less robust in real-world scenarios. Reinforcement learning (RL) is emerging as a viable alternative, yet conventional end-to-end approaches lack efficiency in terms of sample complexity, requiring extensive training in simulations, and predictability of the final motion, which makes it difficult to certify the safety of the final motion. To overcome these limitations, this paper introduces a novel guided reinforcement learning approach that leverages physical intuition for efficient and explainable jumping, by combining B\ezier curves with a Uniformly Accelerated Rectilinear Motion (UARM) model. Extensive simulation and experimental results clearly demonstrate the advantages of our approach over existing alternatives.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2507.16480v1" target="_blank">Designing for Difference: How Human Characteristics Shape Perceptions of Collaborative Robots</a></h3>
                    <p><strong>Authors:</strong> Sabrina Livanec, Laura LondoÃ±o, Michael Gorki, Adrian RÃ¶fer, Abhinav Valada, Andrea Kiesel</p>
                    <p><strong>Published:</strong> 7/22/2025</p>
                    <p><strong>Categories:</strong> cs.RO, cs.AI, cs.CV, cs.ET, cs.SY, eess.SY</p>
                    <p><strong>Summary:</strong> The development of assistive robots for social collaboration raises critical questions about responsible and inclusive design, especially when interacting with individuals from protected groups such as those with disabilities or advanced age. Currently, research is scarce on how participants assess varying robot behaviors in combination with diverse human needs, likely since participants have limited real-world experience with advanced domestic robots. In the current study, we aim to address this gap while using methods that enable participants to assess robot behavior, as well as methods that support meaningful reflection despite limited experience. In an online study, 112 participants (from both experimental and control groups) evaluated 7 videos from a total of 28 variations of human-robot collaboration types. The experimental group first completed a cognitive-affective mapping (CAM) exercise on human-robot collaboration before providing their ratings. Although CAM reflection did not significantly affect overall ratings, it led to more pronounced assessments for certain combinations of robot behavior and human condition. Most importantly, the type of human-robot collaboration influences the assessment. Antisocial robot behavior was consistently rated as the lowest, while collaboration with aged individuals elicited more sensitive evaluations. Scenarios involving object handovers were viewed more positively than those without them. These findings suggest that both human characteristics and interaction paradigms influence the perceived acceptability of collaborative robots, underscoring the importance of prosocial design. They also highlight the potential of reflective methods, such as CAM, to elicit nuanced feedback, supporting the development of user-centered and socially responsible robotic systems tailored to diverse populations.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2507.16479v1" target="_blank">Arbitrage Tactics in the Local Markets via Hierarchical Multi-agent Reinforcement Learning</a></h3>
                    <p><strong>Authors:</strong> Haoyang Zhang, Mina Montazeri, Philipp Heer, Koen Kok, Nikolaos G. Paterakis</p>
                    <p><strong>Published:</strong> 7/22/2025</p>
                    <p><strong>Categories:</strong> eess.SY, cs.SY</p>
                    <p><strong>Summary:</strong> Strategic bidding tactics employed by prosumers in local markets, including the Local Electricity Market (LEM) and Local Flexibility Market (LFM), have attracted significant attention due to their potential to enhance economic benefits for market participants through optimized energy management and bidding. While existing research has explored strategic bidding in a single market with multi-agent reinforcement learning (MARL) algorithms, arbitrage opportunities across local markets remain unexplored. This paper introduces a hierarchical MARL (HMARL) algorithm designed to enable aggregator arbitrage across multiple local markets. The strategic behavior of these aggregators in local markets is modeled as a two-stage Markov game: the first stage involves the LEM, while the second stage encompasses both the LFM and the balancing market. To solve this two-stage Markov game, the HMARL framework assigns two sub-agents to each aggregator, a primary sub-agent and a secondary sub-agent. Without the arbitrage strategy, these sub-agents operate in silos, with the primary sub-agent focusing on first-stage profits and the secondary sub-agent on second-stage profits, each employing independent MARLs. On the contrary, when implementing the arbitrage strategy with the proposed HMARL, the sub-agents communicate and coordinate to perform arbitrage across multiple local markets, enhancing overall efficiency. The case study, conducted under a scenario where all aggregators employ the arbitrage strategy, shows that despite higher initial costs in the LEM, this strategy generates substantial savings in the LFM and the balancing market, resulting in a total profit increase of $40.6\%$ on average. This highlights the capability of the proposed HMARL to address the two-stage Markov game and facilitate arbitrage across local markets, thereby enhancing profitability for participants.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2507.16815v1" target="_blank">ThinkAct: Vision-Language-Action Reasoning via Reinforced Visual Latent Planning</a></h3>
                    <p><strong>Authors:</strong> Chi-Pin Huang, Yueh-Hua Wu, Min-Hung Chen, Yu-Chiang Frank Wang, Fu-En Yang</p>
                    <p><strong>Published:</strong> 7/22/2025</p>
                    <p><strong>Categories:</strong> cs.CV, cs.AI, cs.LG, cs.RO</p>
                    <p><strong>Summary:</strong> Vision-language-action (VLA) reasoning tasks require agents to interpret multimodal instructions, perform long-horizon planning, and act adaptively in dynamic environments. Existing approaches typically train VLA models in an end-to-end fashion, directly mapping inputs to actions without explicit reasoning, which hinders their ability to plan over multiple steps or adapt to complex task variations. In this paper, we propose ThinkAct, a dual-system framework that bridges high-level reasoning with low-level action execution via reinforced visual latent planning. ThinkAct trains a multimodal LLM to generate embodied reasoning plans guided by reinforcing action-aligned visual rewards based on goal completion and trajectory consistency. These reasoning plans are compressed into a visual plan latent that conditions a downstream action model for robust action execution on target environments. Extensive experiments on embodied reasoning and robot manipulation benchmarks demonstrate that ThinkAct enables few-shot adaptation, long-horizon planning, and self-correction behaviors in complex embodied AI tasks.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2507.16814v1" target="_blank">Semi-off-Policy Reinforcement Learning for Vision-Language Slow-thinking Reasoning</a></h3>
                    <p><strong>Authors:</strong> Junhao Shen, Haiteng Zhao, Yuzhe Gu, Songyang Gao, Kuikun Liu, Haian Huang, Jianfei Gao, Dahua Lin, Wenwei Zhang, Kai Chen</p>
                    <p><strong>Published:</strong> 7/22/2025</p>
                    <p><strong>Categories:</strong> cs.LG, cs.CV</p>
                    <p><strong>Summary:</strong> Enhancing large vision-language models (LVLMs) with visual slow-thinking reasoning is crucial for solving complex multimodal tasks. However, since LVLMs are mainly trained with vision-language alignment, it is difficult to adopt on-policy reinforcement learning (RL) to develop the slow thinking ability because the rollout space is restricted by its initial abilities. Off-policy RL offers a way to go beyond the current policy, but directly distilling trajectories from external models may cause visual hallucinations due to mismatched visual perception abilities across models. To address these issues, this paper proposes SOPHIA, a simple and scalable Semi-Off-Policy RL for vision-language slow-tHInking reAsoning. SOPHIA builds a semi-off-policy behavior model by combining on-policy visual understanding from a trainable LVLM with off-policy slow-thinking reasoning from a language model, assigns outcome-based rewards to reasoning, and propagates visual rewards backward. Then LVLM learns slow-thinking reasoning ability from the obtained reasoning trajectories using propagated rewards via off-policy RL algorithms. Extensive experiments with InternVL2.5 and InternVL3.0 with 8B and 38B sizes show the effectiveness of SOPHIA. Notably, SOPHIA improves InternVL3.0-38B by 8.50% in average, reaching state-of-the-art performance among open-source LVLMs on multiple multimodal reasoning benchmarks, and even outperforms some closed-source models (e.g., GPT-4.1) on the challenging MathVision and OlympiadBench, achieving 49.08% and 49.95% pass@1 accuracy, respectively. Analysis shows SOPHIA outperforms supervised fine-tuning and direct on-policy RL methods, offering a better policy initialization for further on-policy training.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2507.16812v1" target="_blank">MegaScience: Pushing the Frontiers of Post-Training Datasets for Science Reasoning</a></h3>
                    <p><strong>Authors:</strong> Run-Ze Fan, Zengzhi Wang, Pengfei Liu</p>
                    <p><strong>Published:</strong> 7/22/2025</p>
                    <p><strong>Categories:</strong> cs.CL, cs.AI, cs.LG</p>
                    <p><strong>Summary:</strong> Scientific reasoning is critical for developing AI scientists and supporting human researchers in advancing the frontiers of natural science discovery. However, the open-source community has primarily focused on mathematics and coding while neglecting the scientific domain, largely due to the absence of open, large-scale, high-quality, verifiable scientific reasoning datasets. To bridge this gap, we first present TextbookReasoning, an open dataset featuring truthful reference answers extracted from 12k university-level scientific textbooks, comprising 650k reasoning questions spanning 7 scientific disciplines. We further introduce MegaScience, a large-scale mixture of high-quality open-source datasets totaling 1.25 million instances, developed through systematic ablation studies that evaluate various data selection methodologies to identify the optimal subset for each publicly available scientific dataset. Meanwhile, we build a comprehensive evaluation system covering diverse subjects and question types across 15 benchmarks, incorporating comprehensive answer extraction strategies to ensure accurate evaluation metrics. Our experiments demonstrate that our datasets achieve superior performance and training efficiency with more concise response lengths compared to existing open-source scientific datasets. Furthermore, we train Llama3.1, Qwen2.5, and Qwen3 series base models on MegaScience, which significantly outperform the corresponding official instruct models in average performance. In addition, MegaScience exhibits greater effectiveness for larger and stronger models, suggesting a scaling benefit for scientific tuning. We release our data curation pipeline, evaluation system, datasets, and seven trained models to the community to advance scientific reasoning research.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2507.16808v1" target="_blank">Rethinking LLM-Based RTL Code Optimization Via Timing Logic Metamorphosis</a></h3>
                    <p><strong>Authors:</strong> Zhihao Xu, Bixin Li, Lulu Wang</p>
                    <p><strong>Published:</strong> 7/22/2025</p>
                    <p><strong>Categories:</strong> cs.SE, cs.AI, 68N19, 68T05, B.6.3; D.3.4; I.2.2; I.2.6</p>
                    <p><strong>Summary:</strong> Register Transfer Level(RTL) code optimization is crucial for achieving high performance and low power consumption in digital circuit design. However, traditional optimization methods often rely on manual tuning and heuristics, which can be time-consuming and error-prone. Recent studies proposed to leverage Large Language Models(LLMs) to assist in RTL code optimization. LLMs can generate optimized code snippets based on natural language descriptions, potentially speeding up the optimization process. However, existing approaches have not thoroughly evaluated the effectiveness of LLM-Based code optimization methods for RTL code with complex timing logic. To address this gap, we conducted a comprehensive empirical investigation to assess the capability of LLM-Based RTL code optimization methods in handling RTL code with complex timing logic. In this study, we first propose a new benchmark for RTL optimization evaluation. It comprises four subsets, each corresponding to a specific area of RTL code optimization. Then we introduce a method based on metamorphosis to systematically evaluate the effectiveness of LLM-Based RTL code optimization methods.Our key insight is that the optimization effectiveness should remain consistent for semantically equivalent but more complex code. After intensive experiments, we revealed several key findings. (1) LLM-Based RTL optimization methods can effectively optimize logic operations and outperform existing compiler-based methods. (2) LLM-Based RTL optimization methods do not perform better than existing compiler-based methods on RTL code with complex timing logic, particularly in timing control flow optimization and clock domain optimization. This is primarily attributed to the challenges LLMs face in understanding timing logic in RTL code. Based on these findings, we provide insights for further research in leveraging LLMs for RTL code optimization.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2507.16792v1" target="_blank">ChatChecker: A Framework for Dialogue System Testing and Evaluation Through Non-cooperative User Simulation</a></h3>
                    <p><strong>Authors:</strong> Roman Mayr, Michel Schimpf, Thomas BohnÃ©</p>
                    <p><strong>Published:</strong> 7/22/2025</p>
                    <p><strong>Categories:</strong> cs.AI</p>
                    <p><strong>Summary:</strong> While modern dialogue systems heavily rely on large language models (LLMs), their implementation often goes beyond pure LLM interaction. Developers integrate multiple LLMs, external tools, and databases. Therefore, assessment of the underlying LLM alone does not suffice, and the dialogue systems must be tested and evaluated as a whole. However, this remains a major challenge. With most previous work focusing on turn-level analysis, less attention has been paid to integrated dialogue-level quality assurance. To address this, we present ChatChecker, a framework for automated evaluation and testing of complex dialogue systems. ChatChecker uses LLMs to simulate diverse user interactions, identify dialogue breakdowns, and evaluate quality. Compared to previous approaches, our design reduces setup effort and is generalizable, as it does not require reference dialogues and is decoupled from the implementation of the target dialogue system. We improve breakdown detection performance over a prior LLM-based approach by including an error taxonomy in the prompt. Additionally, we propose a novel non-cooperative user simulator based on challenging personas that uncovers weaknesses in target dialogue systems more effectively. Through this, ChatChecker contributes to thorough and scalable testing. This enables both researchers and practitioners to accelerate the development of robust dialogue systems.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2507.16784v1" target="_blank">Beyond Context Limits: Subconscious Threads for Long-Horizon Reasoning</a></h3>
                    <p><strong>Authors:</strong> Hongyin Luo, Nathaniel Morgan, Tina Li, Derek Zhao, Ai Vy Ngo, Philip Schroeder, Lijie Yang, Assaf Ben-Kish, Jack OBrien, James Glass</p>
                    <p><strong>Published:</strong> 7/22/2025</p>
                    <p><strong>Categories:</strong> cs.CL</p>
                    <p><strong>Summary:</strong> To break the context limits of large language models (LLMs) that bottleneck reasoning accuracy and efficiency, we propose the Thread Inference Model (TIM), a family of LLMs trained for recursive and decompositional problem solving, and TIMRUN, an inference runtime enabling long-horizon structured reasoning beyond context limits. Together, TIM hosted on TIMRUN supports virtually unlimited working memory and multi-hop tool calls within a single language model inference, overcoming output limits, positional-embedding constraints, and GPU-memory bottlenecks. Performance is achieved by modeling natural language as reasoning trees measured by both length and depth instead of linear sequences. The reasoning trees consist of tasks with thoughts, recursive subtasks, and conclusions based on the concept we proposed in Schroeder et al, 2025. During generation, we maintain a working memory that retains only the key-value states of the most relevant context tokens, selected by a rule-based subtask-pruning mechanism, enabling reuse of positional embeddings and GPU memory pages throughout reasoning. Experimental results show that our system sustains high inference throughput, even when manipulating up to 90% of the KV cache in GPU memory. It also delivers accurate reasoning on mathematical tasks and handles information retrieval challenges that require long-horizon reasoning and multi-hop tool use.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2507.16781v1" target="_blank">Cooling Matters: Benchmarking Large Language Models and Vision-Language Models on Liquid-Cooled Versus Air-Cooled H100 GPU Systems</a></h3>
                    <p><strong>Authors:</strong> Imran Latif, Muhammad Ali Shafique, Hayat Ullah, Alex C. Newkirk, Xi Yu, Arslan Munir</p>
                    <p><strong>Published:</strong> 7/22/2025</p>
                    <p><strong>Categories:</strong> cs.DC</p>
                    <p><strong>Summary:</strong> The unprecedented growth in artificial intelligence (AI) workloads, recently dominated by large language models (LLMs) and vision-language models (VLMs), has intensified power and cooling demands in data centers. This study benchmarks LLMs and VLMs on two HGX nodes, each with 8x NVIDIA H100 graphics processing units (GPUs), using liquid and air cooling. Leveraging GPU Burn, Weights and Biases, and IPMItool, we collect detailed thermal, power, and computation data. Results show that the liquid-cooled systems maintain GPU temperatures between 41-50 degrees Celsius, while the air-cooled counterparts fluctuate between 54-72 degrees Celsius under load. This thermal stability of liquid-cooled systems yields 17 percent higher performance (54 TFLOPs per GPU vs. 46 TFLOPs per GPU), improved performance per watt, reduced energy overhead, and greater system efficiency than the air-cooled counterparts. These findings underscore the energy and sustainability benefits of liquid cooling, offering a compelling path forward for hyperscale data centers s</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2507.16773v1" target="_blank">When LLMs Copy to Think: Uncovering Copy-Guided Attacks in Reasoning LLMs</a></h3>
                    <p><strong>Authors:</strong> Yue Li, Xiao Li, Hao Wu, Yue Zhang, Fengyuan Xu, Xiuzhen Cheng, Sheng Zhong</p>
                    <p><strong>Published:</strong> 7/22/2025</p>
                    <p><strong>Categories:</strong> cs.CR</p>
                    <p><strong>Summary:</strong> Large Language Models (LLMs) have become integral to automated code analysis, enabling tasks such as vulnerability detection and code comprehension. However, their integration introduces novel attack surfaces. In this paper, we identify and investigate a new class of prompt-based attacks, termed Copy-Guided Attacks (CGA), which exploit the inherent copying tendencies of reasoning-capable LLMs. By injecting carefully crafted triggers into external code snippets, adversaries can induce the model to replicate malicious content during inference. This behavior enables two classes of vulnerabilities: inference length manipulation, where the model generates abnormally short or excessively long reasoning traces; and inference result manipulation, where the model produces misleading or incorrect conclusions. We formalize CGA as an optimization problem and propose a gradient-based approach to synthesize effective triggers. Empirical evaluation on state-of-the-art reasoning LLMs shows that CGA reliably induces infinite loops, premature termination, false refusals, and semantic distortions in code analysis tasks. While highly effective in targeted settings, we observe challenges in generalizing CGA across diverse prompts due to computational constraints, posing an open question for future research. Our findings expose a critical yet underexplored vulnerability in LLM-powered development pipelines and call for urgent advances in prompt-level defense mechanisms.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2507.16771v1" target="_blank">A Partitioned Sparse Variational Gaussian Process for Fast, Distributed Spatial Modeling</a></h3>
                    <p><strong>Authors:</strong> Michael Grosskopf, Kellin Rumsey, Ayan Biswas, Earl Lawrence</p>
                    <p><strong>Published:</strong> 7/22/2025</p>
                    <p><strong>Categories:</strong> cs.LG, stat.AP, stat.ML</p>
                    <p><strong>Summary:</strong> The next generation of Department of Energy supercomputers will be capable of exascale computation. For these machines, far more computation will be possible than that which can be saved to disk. As a result, users will be unable to rely on post-hoc access to data for uncertainty quantification and other statistical analyses and there will be an urgent need for sophisticated machine learning algorithms which can be trained in situ. Algorithms deployed in this setting must be highly scalable, memory efficient and capable of handling data which is distributed across nodes as spatially contiguous partitions. One suitable approach involves fitting a sparse variational Gaussian process (SVGP) model independently and in parallel to each spatial partition. The resulting model is scalable, efficient and generally accurate, but produces the undesirable effect of constructing discontinuous response surfaces due to the disagreement between neighboring models at their shared boundary. In this paper, we extend this idea by allowing for a small amount of communication between neighboring spatial partitions which encourages better alignment of the local models, leading to smoother spatial predictions and a better fit in general. Due to our decentralized communication scheme, the proposed extension remains highly scalable and adds very little overhead in terms of computation (and none, in terms of memory). We demonstrate this Partitioned SVGP (PSVGP) approach for the Energy Exascale Earth System Model (E3SM) and compare the results to the independent SVGP case.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2507.16761v1" target="_blank">Faithful, Interpretable Chest X-ray Diagnosis with Anti-Aliased B-cos Networks</a></h3>
                    <p><strong>Authors:</strong> Marcel Kleinmann, Shashank Agnihotri, Margret Keuper</p>
                    <p><strong>Published:</strong> 7/22/2025</p>
                    <p><strong>Categories:</strong> cs.CV, cs.LG</p>
                    <p><strong>Summary:</strong> Faithfulness and interpretability are essential for deploying deep neural networks (DNNs) in safety-critical domains such as medical imaging. B-cos networks offer a promising solution by replacing standard linear layers with a weight-input alignment mechanism, producing inherently interpretable, class-specific explanations without post-hoc methods. While maintaining diagnostic performance competitive with state-of-the-art DNNs, standard B-cos models suffer from severe aliasing artifacts in their explanation maps, making them unsuitable for clinical use where clarity is essential. Additionally, the original B-cos formulation is limited to multi-class settings, whereas chest X-ray analysis often requires multi-label classification due to co-occurring abnormalities. In this work, we address both limitations: (1) we introduce anti-aliasing strategies using FLCPooling (FLC) and BlurPool (BP) to significantly improve explanation quality, and (2) we extend B-cos networks to support multi-label classification. Our experiments on chest X-ray datasets demonstrate that the modified $\text{B-cos}_\text{FLC}$ and $\text{B-cos}_\text{BP}$ preserve strong predictive performance while providing faithful and artifact-free explanations suitable for clinical application in multi-label settings. Code available at: $\href{https://github.com/mkleinma/B-cos-medical-paper}{GitHub repository}$.</p>
                
            
                
                    <h3><a href="http://dx.doi.org/10.1038/s41563-025-02290-y" target="_blank">Atomic-scale Frustrated Josephson Coupling and Multi-condensate Visualization in FeSe</a></h3>
                    <p><strong>Authors:</strong> Nileema Sharma, Matthew Toole, James McKenzie, Sheng Ran, Xiaolong Liu</p>
                    <p><strong>Published:</strong> 7/22/2025</p>
                    <p><strong>Categories:</strong> cond-mat.supr-con, cond-mat.mtrl-sci</p>
                    <p><strong>Summary:</strong> In a Josephson junction involving multi-band superconductors, competition between inter-band and inter-junction Josephson coupling gives rise to frustration and spatial disjunction of superfluid densities among superconducting condensates. Such frustrated coupling manifests as quantum interference of Josephson currents from different tunneling channels and becomes tunable if channel transparency can be varied. To explore these unconventional effects in the prototypical $s^\pm$-wave superconductor FeSe, we use atomic resolution scanned Josephson tunneling microscopy SJTM for condensate resolved imaging and junction tuning -- capabilities unattainable in macroscopic Josephson devices with fixed characteristics. We quantitatively demonstrate frustrated Josephson tunneling by examining two tunneling inequalities. The relative transparency of two parallel tunneling pathways is found tunable, revealing a tendency towards a 0-pi transition with decreasing SJTM junction resistance. Simultaneous visualization of both superconducting condensates reveals anti correlated superfluid modulations, highlighting the role of inter-band scattering. Our study establishes SJTM as a powerful tool enabling new research frontiers of multi condensate superconductivity.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2507.16745v1" target="_blank">Artifacts in Halo Shapes: Imprints of the Initial Condition</a></h3>
                    <p><strong>Authors:</strong> Yu Yu, Zhao Chen</p>
                    <p><strong>Published:</strong> 7/22/2025</p>
                    <p><strong>Categories:</strong> astro-ph.CO</p>
                    <p><strong>Summary:</strong> Grid type pre-initial conditions are commonly used to initialize particle positions in cosmological simulations. While these conditions are known to produce noticeable numerical artifacts in void regions, their impact on halo properties has generally been assumed to be negligible. In this work, we employ multiple simulations to demonstrate that grid initialization induces statistically significant artifacts in halo shapes, despite the modest absolute amplitude ($\sim 1\%$) making them unimportant for most cosmological studies. We identify a redshift-dependent artificial alignment pattern: at low redshifts ($z2$), halo shapes preferentially orient away from the simulation boxs Cartesian axes, whereas their constituent particles initially exhibit alignment with these axes. We propose a mathematical hypothesis to explain this flipping behavior.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2507.16735v1" target="_blank">AI-enhanced conversational agents for personalized asthma support Factors for engagement, value and efficacy</a></h3>
                    <p><strong>Authors:</strong> Laura Moradbakhti, Dorian Peters, Jennifer K. Quint, BjÃ¶rn Schuller, Darren Cook, Rafael A. Calvo</p>
                    <p><strong>Published:</strong> 7/22/2025</p>
                    <p><strong>Categories:</strong> cs.HC, cs.AI, cs.CY, cs.ET, K.4.2; J.3</p>
                    <p><strong>Summary:</strong> Asthma-related deaths in the UK are the highest in Europe, and only 30% of patients access basic care. There is a need for alternative approaches to reaching people with asthma in order to provide health education, self-management support and bridges to care. Automated conversational agents (specifically, mobile chatbots) present opportunities for providing alternative and individually tailored access to health education, self-management support and risk self-assessment. But would patients engage with a chatbot, and what factors influence engagement? We present results from a patient survey (N=1257) devised by a team of asthma clinicians, patients, and technology developers, conducted to identify optimal factors for efficacy, value and engagement for a chatbot. Results indicate that most adults with asthma (53%) are interested in using a chatbot and the patients most likely to do so are those who believe their asthma is more serious and who are less confident about self-management. Results also indicate enthusiasm for 24/7 access, personalisation, and for WhatsApp as the preferred access method (compared to app, voice assistant, SMS or website). Obstacles to uptake include security/privacy concerns and skepticism of technological capabilities. We present detailed findings and consolidate these into 7 recommendations for developers for optimising efficacy of chatbot-based health support.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2507.16733v1" target="_blank">Generative Diffusion Models for Wireless Networks: Fundamental, Architecture, and State-of-the-Art</a></h3>
                    <p><strong>Authors:</strong> Dayu Fan, Rui Meng, Xiaodong Xu, Yiming Liu, Guoshun Nan, Chenyuan Feng, Shujun Han, Song Gao, Bingxuan Xu, Dusit Niyato, Tony Q. S. Quek, Ping Zhang</p>
                    <p><strong>Published:</strong> 7/22/2025</p>
                    <p><strong>Categories:</strong> eess.SP</p>
                    <p><strong>Summary:</strong> With the rapid development of Generative Artificial Intelligence (GAI) technology, Generative Diffusion Models (GDMs) have shown significant empowerment potential in the field of wireless networks due to advantages, such as noise resistance, training stability, controllability, and multimodal generation. Although there have been multiple studies focusing on GDMs for wireless networks, there is still a lack of comprehensive reviews on their technological evolution. Motivated by this, we systematically explore the application of GDMs in wireless networks. Firstly, starting from mathematical principles, we analyze technical advantages of GDMs and present six representative models. Furthermore, we propose the multi-layer wireless network architecture including sensing layer, transmission layer, application layer, and security plane. We also introduce the core mechanisms of GDM at each of the layers. Subsequently, we conduct a rigorous review on existing GDM-based schemes, with a focus on analyzing their innovative points, the role of GDMs, strengths, and weaknesses. Ultimately, we extract key challenges and provide potential solutions, with the aim of providing directional guidance for future research in this field.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2507.16731v1" target="_blank">Collaborative Inference and Learning between Edge SLMs and Cloud LLMs: A Survey of Algorithms, Execution, and Open Challenges</a></h3>
                    <p><strong>Authors:</strong> Senyao Li, Haozhao Wang, Wenchao Xu, Rui Zhang, Song Guo, Jingling Yuan, Xian Zhong, Tianwei Zhang, Ruixuan Li</p>
                    <p><strong>Published:</strong> 7/22/2025</p>
                    <p><strong>Categories:</strong> cs.DC</p>
                    <p><strong>Summary:</strong> As large language models (LLMs) evolve, deploying them solely in the cloud or compressing them for edge devices has become inadequate due to concerns about latency, privacy, cost, and personalization. This survey explores a collaborative paradigm in which cloud-based LLMs and edge-deployed small language models (SLMs) cooperate across both inference and training. We present a unified taxonomy of edge-cloud collaboration strategies. For inference, we categorize approaches into task assignment, task division, and mixture-based collaboration at both task and token granularity, encompassing adaptive scheduling, resource-aware offloading, speculative decoding, and modular routing. For training, we review distributed adaptation techniques, including parameter alignment, pruning, bidirectional distillation, and small-model-guided optimization. We further summarize datasets, benchmarks, and deployment cases, and highlight privacy-preserving methods and vertical applications. This survey provides the first systematic foundation for LLM-SLM collaboration, bridging system and algorithm co-design to enable efficient, scalable, and trustworthy edge-cloud intelligence.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2507.16729v1" target="_blank">Improving Model Classification by Optimizing the Training Dataset</a></h3>
                    <p><strong>Authors:</strong> Morad Tukan, Loay Mualem, Eitan Netzer, Liran Sigalat</p>
                    <p><strong>Published:</strong> 7/22/2025</p>
                    <p><strong>Categories:</strong> cs.LG</p>
                    <p><strong>Summary:</strong> In the era of data-centric AI, the ability to curate high-quality training data is as crucial as model design. Coresets offer a principled approach to data reduction, enabling efficient learning on large datasets through importance sampling. However, conventional sensitivity-based coreset construction often falls short in optimizing for classification performance metrics, e.g., $F1$ score, focusing instead on loss approximation. In this work, we present a systematic framework for tuning the coreset generation process to enhance downstream classification quality. Our method introduces new tunable parameters--including deterministic sampling, class-wise allocation, and refinement via active sampling, beyond traditional sensitivity scores. Through extensive experiments on diverse datasets and classifiers, we demonstrate that tuned coresets can significantly outperform both vanilla coresets and full dataset training on key classification metrics, offering an effective path towards better and more efficient model training.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2507.16727v1" target="_blank">Deliberative Searcher: Improving LLM Reliability via Reinforcement Learning with constraints</a></h3>
                    <p><strong>Authors:</strong> Zhenyun Yin, Shujie Wang, Xuhong Wang, Xingjun Ma, Yinchun Wang</p>
                    <p><strong>Published:</strong> 7/22/2025</p>
                    <p><strong>Categories:</strong> cs.AI</p>
                    <p><strong>Summary:</strong> Improving the reliability of large language models (LLMs) is critical for deploying them in real-world scenarios. In this paper, we propose \textbf{Deliberative Searcher}, the first framework to integrate certainty calibration with retrieval-based search for open-domain question answering. The agent performs multi-step reflection and verification over Wikipedia data and is trained with a reinforcement learning algorithm that optimizes for accuracy under a soft reliability constraint. Empirical results show that proposed method improves alignment between model confidence and correctness, leading to more trustworthy outputs. This paper will be continuously updated.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2507.16725v1" target="_blank">RAVine: Reality-Aligned Evaluation for Agentic Search</a></h3>
                    <p><strong>Authors:</strong> Yilong Xu, Xiang Long, Zhi Zheng, Jinhua Gao</p>
                    <p><strong>Published:</strong> 7/22/2025</p>
                    <p><strong>Categories:</strong> cs.CL, cs.AI, cs.IR</p>
                    <p><strong>Summary:</strong> Agentic search, as a more autonomous and adaptive paradigm of retrieval augmentation, is driving the evolution of intelligent search systems. However, existing evaluation frameworks fail to align well with the goals of agentic search. First, the complex queries commonly used in current benchmarks often deviate from realistic user search scenarios. Second, prior approaches tend to introduce noise when extracting ground truth for end-to-end evaluations, leading to distorted assessments at a fine-grained level. Third, most current frameworks focus solely on the quality of final answers, neglecting the evaluation of the iterative process inherent to agentic search. To address these limitations, we propose RAVine -- a Reality-Aligned eValuation framework for agentic LLMs with search. RAVine targets multi-point queries and long-form answers that better reflect user intents, and introduces an attributable ground truth construction strategy to enhance the accuracy of fine-grained evaluation. Moreover, RAVine examines models interaction with search tools throughout the iterative process, and accounts for factors of efficiency. We benchmark a series of models using RAVine and derive several insights, which we hope will contribute to advancing the development of agentic search systems. The code and datasets are available at https://github.com/SwordFaith/RAVine.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2507.16724v1" target="_blank">SALM: Spatial Audio Language Model with Structured Embeddings for Understanding and Editing</a></h3>
                    <p><strong>Authors:</strong> Jinbo Hu, Yin Cao, Ming Wu, Feiran Yang, Jun Yang</p>
                    <p><strong>Published:</strong> 7/22/2025</p>
                    <p><strong>Categories:</strong> cs.SD, eess.AS</p>
                    <p><strong>Summary:</strong> Spatial audio understanding is essential for accurately perceiving and interpreting acoustic environments. However, existing audio-language models struggle with processing spatial audio and perceiving spatial acoustic scenes. We introduce the Spatial Audio Language Model (SALM), a novel framework that bridges spatial audio and language via multi-modal contrastive learning. SALM consists of a text encoder and a dual-branch audio encoder, decomposing spatial sound into semantic and spatial components through structured audio embeddings. Key features of SALM include seamless alignment of spatial and text representations, separate and joint extraction of spatial and semantic information, zero-shot direction classification and robust support for spatial audio editing. Experimental results demonstrate that SALM effectively captures and aligns cross-modal representations. Furthermore, it supports advanced editing capabilities, such as altering directional audio using text-based embeddings.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2507.16717v1" target="_blank">Multi-objective Portfolio Optimization Via Gradient Descent</a></h3>
                    <p><strong>Authors:</strong> Christian Oliva, Pedro R. Ventura, Luis F. Lago-FernÃ¡ndez</p>
                    <p><strong>Published:</strong> 7/22/2025</p>
                    <p><strong>Categories:</strong> cs.CE, cs.LG</p>
                    <p><strong>Summary:</strong> Traditional approaches to portfolio optimization, often rooted in Modern Portfolio Theory and solved via quadratic programming or evolutionary algorithms, struggle with scalability or flexibility, especially in scenarios involving complex constraints, large datasets and/or multiple conflicting objectives. To address these challenges, we introduce a benchmark framework for multi-objective portfolio optimization (MPO) using gradient descent with automatic differentiation. Our method supports any optimization objective, such as minimizing risk measures (e.g., CVaR) or maximizing Sharpe ratio, along with realistic constraints, such as tracking error limits, UCITS regulations, or asset group restrictions. We have evaluated our framework across six experimental scenarios, from single-objective setups to complex multi-objective cases, and have compared its performance against standard solvers like CVXPY and SKFOLIO. Our results show that our method achieves competitive performance while offering enhanced flexibility for modeling multiple objectives and constraints. We aim to provide a practical and extensible tool for researchers and practitioners exploring advanced portfolio optimization problems in real-world conditions.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2507.16715v1" target="_blank">A Tutorial on MRI Reconstruction: From Modern Methods to Clinical Implications</a></h3>
                    <p><strong>Authors:</strong> Tolga Ã‡ukur, Salman U. H. Dar, Valiyeh Ansarian Nezhad, Yohan Jun, Tae Hyung Kim, Shohei Fujita, Berkin Bilgic</p>
                    <p><strong>Published:</strong> 7/22/2025</p>
                    <p><strong>Categories:</strong> eess.IV</p>
                    <p><strong>Summary:</strong> MRI is an indispensable clinical tool, offering a rich variety of tissue contrasts to support broad diagnostic and research applications. Clinical exams routinely acquire multiple structural sequences that provide complementary information for differential diagnosis, while research protocols often incorporate advanced functional, diffusion, spectroscopic, and relaxometry sequences to capture multidimensional insights into tissue structure and composition. However, these capabilities come at the cost of prolonged scan times, which reduce patient throughput, increase susceptibility to motion artifacts, and may require trade-offs in image quality or diagnostic scope. Over the last two decades, advances in image reconstruction algorithms--alongside improvements in hardware and pulse sequence design--have made it possible to accelerate acquisitions while preserving diagnostic quality. Central to this progress is the ability to incorporate prior information to regularize the solutions to the reconstruction problem. In this tutorial, we overview the basics of MRI reconstruction and highlight state-of-the-art approaches, beginning with classical methods that rely on explicit hand-crafted priors, and then turning to deep learning methods that leverage a combination of learned and crafted priors to further push the performance envelope. We also explore the translational aspects and eventual clinical implications of these methods. We conclude by discussing future directions to address remaining challenges in MRI reconstruction. The tutorial is accompanied by a Python toolbox (https://github.com/tutorial-MRI-recon/tutorial) to demonstrate select methods discussed in the article.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2507.16704v1" target="_blank">Screen2AX: Vision-Based Approach for Automatic macOS Accessibility Generation</a></h3>
                    <p><strong>Authors:</strong> Viktor Muryn, Marta Sumyk, Mariya Hirna, Sofiya Garkot, Maksym Shamrai</p>
                    <p><strong>Published:</strong> 7/22/2025</p>
                    <p><strong>Categories:</strong> cs.LG, cs.AI, cs.CV, cs.HC</p>
                    <p><strong>Summary:</strong> Desktop accessibility metadata enables AI agents to interpret screens and supports users who depend on tools like screen readers. Yet, many applications remain largely inaccessible due to incomplete or missing metadata provided by developers - our investigation shows that only 33% of applications on macOS offer full accessibility support. While recent work on structured screen representation has primarily addressed specific challenges, such as UI element detection or captioning, none has attempted to capture the full complexity of desktop interfaces by replicating their entire hierarchical structure. To bridge this gap, we introduce Screen2AX, the first framework to automatically create real-time, tree-structured accessibility metadata from a single screenshot. Our method uses vision-language and object detection models to detect, describe, and organize UI elements hierarchically, mirroring macOSs system-level accessibility structure. To tackle the limited availability of data for macOS desktop applications, we compiled and publicly released three datasets encompassing 112 macOS applications, each annotated for UI element detection, grouping, and hierarchical accessibility metadata alongside corresponding screenshots. Screen2AX accurately infers hierarchy trees, achieving a 77% F1 score in reconstructing a complete accessibility tree. Crucially, these hierarchy trees improve the ability of autonomous agents to interpret and interact with complex desktop interfaces. We introduce Screen2AX-Task, a benchmark specifically designed for evaluating autonomous agent task execution in macOS desktop environments. Using this benchmark, we demonstrate that Screen2AX delivers a 2.2x performance improvement over native accessibility representations and surpasses the state-of-the-art OmniParser V2 system on the ScreenSpot benchmark.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2507.16697v1" target="_blank">Pixel-Resolved Long-Context Learning for Turbulence at Exascale: Resolving Small-scale Eddies Toward the Viscous Limit</a></h3>
                    <p><strong>Authors:</strong> Junqi Yin, Mijanur Palash, M. Paul Laiu, Muralikrishnan Gopalakrishnan Meena, John Gounley, Stephen M. de Bruyn Kops, Feiyi Wang, Ramanan Sankaran, Pei Zhang</p>
                    <p><strong>Published:</strong> 7/22/2025</p>
                    <p><strong>Categories:</strong> physics.flu-dyn, cs.LG</p>
                    <p><strong>Summary:</strong> Turbulence plays a crucial role in multiphysics applications, including aerodynamics, fusion, and combustion. Accurately capturing turbulences multiscale characteristics is essential for reliable predictions of multiphysics interactions, but remains a grand challenge even for exascale supercomputers and advanced deep learning models. The extreme-resolution data required to represent turbulence, ranging from billions to trillions of grid points, pose prohibitive computational costs for models based on architectures like vision transformers. To address this challenge, we introduce a multiscale hierarchical Turbulence Transformer that reduces sequence length from billions to a few millions and a novel RingX sequence parallelism approach that enables scalable long-context learning. We perform scaling and science runs on the Frontier supercomputer. Our approach demonstrates excellent performance up to 1.1 EFLOPS on 32,768 AMD GPUs, with a scaling efficiency of 94%. To our knowledge, this is the first AI model for turbulence that can capture small-scale eddies down to the dissipative range.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2507.16693v1" target="_blank">High-throughput Super-Resolution Imaging Chip based on Miniaturized Full-frequency Encoded-illumination</a></h3>
                    <p><strong>Authors:</strong> Xiaoyu Yang, Haonan Zhang, Feihong Lin, Mingwei Tang, Tawfique Hasan, Clemens F. Kaminski, Xu Liu, Qing Yang</p>
                    <p><strong>Published:</strong> 7/22/2025</p>
                    <p><strong>Categories:</strong> physics.optics</p>
                    <p><strong>Summary:</strong> A miniaturized full-frequency encoded illumination (mini-FEI) chip is presented for high-throughput super-resolution imaging using the spatial frequency shift (SFS) effect. A tunable full SFS scheme is achieved through propagating and evanescent wave. The multi-illumination modes are precisely and flexibly modulated by an encoded LED array. The light travels to the sample via a set of prisms, producing the super-resolution images with high signal-to-noise ratio (SNR). Mini-FEI super-resolution imaging reaches a resolution of 333 nm (~{\lambda}/4NA), close to the theoretical limit, while maintaining a large field of view (FOV) of ~1 mm2. The method is validated on label-free samples including USAF Target, Star Target, and onion root tip cells, all of which could be successfully reconstructed. Through the introduction of integrated LED arrays for evanescent wave excitation, expensive laser systems can be avoided and the system significantly miniaturized. The mini-FEI super-resolution imaging chip is simple and cost effective to fabricate and can be used in conjunction with any inverted brightfield microscope frame and thus has great potential for widespread use in scientific and industrial research environments.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2507.16685v1" target="_blank">VulGuard: An Unified Tool for Evaluating Just-In-Time Vulnerability Prediction Models</a></h3>
                    <p><strong>Authors:</strong> Duong Nguyen, Manh Tran-Duc, Thanh Le-Cong, Triet Huynh Minh Le, M. Ali Babar, Quyet-Thang Huynh</p>
                    <p><strong>Published:</strong> 7/22/2025</p>
                    <p><strong>Categories:</strong> cs.SE</p>
                    <p><strong>Summary:</strong> We present VulGuard, an automated tool designed to streamline the extraction, processing, and analysis of commits from GitHub repositories for Just-In-Time vulnerability prediction (JIT-VP) research. VulGuard automatically mines commit histories, extracts fine-grained code changes, commit messages, and software engineering metrics, and formats them for downstream analysis. In addition, it integrates several state-of-the-art vulnerability prediction models, allowing researchers to train, evaluate, and compare models with minimal setup. By supporting both repository-scale mining and model-level experimentation within a unified framework, VulGuard addresses key challenges in reproducibility and scalability in software security research. VulGuard can also be easily integrated into the CI/CD pipeline. We demonstrate the effectiveness of the tool in two influential open-source projects, FFmpeg and the Linux kernel, highlighting its potential to accelerate real-world JIT-VP research and promote standardized benchmarking. A demo video is available at: https://youtu.be/j96096-pxbs</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2507.16680v1" target="_blank">Latent Space Alignment for AI-Native MIMO Semantic Communications</a></h3>
                    <p><strong>Authors:</strong> Mario Edoardo Pandolfo, Simone Fiorellino, Emilio Calvanese Strinati, Paolo Di Lorenzo</p>
                    <p><strong>Published:</strong> 7/22/2025</p>
                    <p><strong>Categories:</strong> cs.LG, cs.IT, cs.NI, math.IT</p>
                    <p><strong>Summary:</strong> Semantic communications focus on prioritizing the understanding of the meaning behind transmitted data and ensuring the successful completion of tasks that motivate the exchange of information. However, when devices rely on different languages, logic, or internal representations, semantic mismatches may occur, potentially hindering mutual understanding. This paper introduces a novel approach to addressing latent space misalignment in semantic communications, exploiting multiple-input multiple-output (MIMO) communications. Specifically, our method learns a MIMO precoder/decoder pair that jointly performs latent space compression and semantic channel equalization, mitigating both semantic mismatches and physical channel impairments. We explore two solutions: (i) a linear model, optimized by solving a biconvex optimization problem via the alternating direction method of multipliers (ADMM); (ii) a neural network-based model, which learns semantic MIMO precoder/decoder under transmission power budget and complexity constraints. Numerical results demonstrate the effectiveness of the proposed approach in a goal-oriented semantic communication scenario, illustrating the main trade-offs between accuracy, communication burden, and complexity of the solutions.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2507.16679v1" target="_blank">PICACO: Pluralistic In-Context Value Alignment of LLMs via Total Correlation Optimization</a></h3>
                    <p><strong>Authors:</strong> Han Jiang, Dongyao Zhu, Zhihua Wei, Xiaoyuan Yi, Ziang Xiao, Xing Xie</p>
                    <p><strong>Published:</strong> 7/22/2025</p>
                    <p><strong>Categories:</strong> cs.CL, cs.AI, cs.CY</p>
                    <p><strong>Summary:</strong> In-Context Learning has shown great potential for aligning Large Language Models (LLMs) with human values, helping reduce harmful outputs and accommodate diverse preferences without costly post-training, known as In-Context Alignment (ICA). However, LLMs comprehension of input prompts remains agnostic, limiting ICAs ability to address value tensions--human values are inherently pluralistic, often imposing conflicting demands, e.g., stimulation vs. tradition. Current ICA methods therefore face the Instruction Bottleneck challenge, where LLMs struggle to reconcile multiple intended values within a single prompt, leading to incomplete or biased alignment. To address this, we propose PICACO, a novel pluralistic ICA method. Without fine-tuning, PICACO optimizes a meta-instruction that navigates multiple values to better elicit LLMs understanding of them and improve their alignment. This is achieved by maximizing the total correlation between specified values and LLM responses, theoretically reinforcing value correlation while reducing distractive noise, resulting in effective value instructions. Extensive experiments on five value sets show that PICACO works well with both black-box and open-source LLMs, outperforms several recent strong baselines, and achieves a better balance across up to 8 distinct values.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2507.16676v1" target="_blank">Custom Algorithm-based Fault Tolerance for Attention Layers in Transformers</a></h3>
                    <p><strong>Authors:</strong> Vasileios Titopoulos, Kosmas Alexandridis, Giorgos Dimitrakopoulos</p>
                    <p><strong>Published:</strong> 7/22/2025</p>
                    <p><strong>Categories:</strong> cs.LG, cs.AR</p>
                    <p><strong>Summary:</strong> Transformers and large language models (LLMs), powered by the attention mechanism, have transformed numerous AI applications, driving the need for specialized hardware accelerators. A major challenge in these accelerators is efficiently detecting errors caused by random hardware faults. Traditional algorithm-based fault tolerance (ABFT) techniques verify individual matrix multiplications but fall short in handling the full attention mechanism, particularly due to intermediate softmax normalization. This work proposes Flash-ABFT, a novel method that computes an online checksum across the entire three-matrix product of query, key and value matrices, of an attention layer, including the softmax operation, with a single check. This approach significantly reduces overhead by eliminating redundant checks while maintaining high fault-detection accuracy. Experimental results demonstrate that Flash-ABFT incurs only 5.3% hardware area overhead and less than 1.9% energy overhead, making it a cost-effective and robust solution for error detection in attention accelerators.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2507.16674v1" target="_blank">GASPnet: Global Agreement to Synchronize Phases</a></h3>
                    <p><strong>Authors:</strong> Andrea Alamiaa, Sabine Muzellec, Thomas Serre, Rufin VanRullen</p>
                    <p><strong>Published:</strong> 7/22/2025</p>
                    <p><strong>Categories:</strong> cs.LG, q-bio.NC</p>
                    <p><strong>Summary:</strong> In recent years, Transformer architectures have revolutionized most fields of artificial intelligence, relying on an attentional mechanism based on the agreement between keys and queries to select and route information in the network. In previous work, we introduced a novel, brain-inspired architecture that leverages a similar implementation to achieve a global routing by agreement mechanism. Such a system modulates the networks activity by matching each neurons key with a single global query, pooled across the entire network. Acting as a global attentional system, this mechanism improves noise robustness over baseline levels but is insufficient for multi-classification tasks. Here, we improve on this work by proposing a novel mechanism that combines aspects of the Transformer attentional operations with a compelling neuroscience theory, namely, binding by synchrony. This theory proposes that the brain binds together features by synchronizing the temporal activity of neurons encoding those features. This allows the binding of features from the same object while efficiently disentangling those from distinct objects. We drew inspiration from this theory and incorporated angular phases into all layers of a convolutional network. After achieving phase alignment via Kuramoto dynamics, we use this approach to enhance operations between neurons with similar phases and suppresses those with opposite phases. We test the benefits of this mechanism on two datasets: one composed of pairs of digits and one composed of a combination of an MNIST item superimposed on a CIFAR-10 image. Our results reveal better accuracy than CNN networks, proving more robust to noise and with better generalization abilities. Overall, we propose a novel mechanism that addresses the visual binding problem in neural networks by leveraging the synergy between neuroscience and machine learning.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2507.16670v1" target="_blank">Adaptive Inventory Strategies using Deep Reinforcement Learning for Dynamic Agri-Food Supply Chains</a></h3>
                    <p><strong>Authors:</strong> Amandeep Kaur, Gyan Prakash</p>
                    <p><strong>Published:</strong> 7/22/2025</p>
                    <p><strong>Categories:</strong> cs.AI</p>
                    <p><strong>Summary:</strong> Agricultural products are often subject to seasonal fluctuations in production and demand. Predicting and managing inventory levels in response to these variations can be challenging, leading to either excess inventory or stockouts. Additionally, the coordination among stakeholders at various level of food supply chain is not considered in the existing body of literature. To bridge these research gaps, this study focuses on inventory management of agri-food products under demand and lead time uncertainties. By implementing effective inventory replenishment policy results in maximize the overall profit throughout the supply chain. However, the complexity of the problem increases due to these uncertainties and shelf-life of the product, that makes challenging to implement traditional approaches to generate optimal set of solutions. Thus, the current study propose a novel Deep Reinforcement Learning (DRL) algorithm that combines the benefits of both value- and policy-based DRL approaches for inventory optimization under uncertainties. The proposed algorithm can incentivize collaboration among stakeholders by aligning their interests and objectives through shared optimization goal of maximizing profitability along the agri-food supply chain while considering perishability, and uncertainty simultaneously. By selecting optimal order quantities with continuous action space, the proposed algorithm effectively addresses the inventory optimization challenges. To rigorously evaluate this algorithm, the empirical data from fresh agricultural products supply chain inventory is considered. Experimental results corroborate the improved performance of the proposed inventory replenishment policy under stochastic demand patterns and lead time scenarios. The research findings hold managerial implications for policymakers to manage the inventory of agricultural products more effectively under uncertainty.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2507.16663v1" target="_blank">Self-Contradiction as Self-Improvement: Mitigating the Generation-Understanding Gap in MLLMs</a></h3>
                    <p><strong>Authors:</strong> Yujin Han, Hao Chen, Andi Han, Zhiheng Wang, Xinyu Lin, Yingya Zhang, Shiwei Zhang, Difan Zou</p>
                    <p><strong>Published:</strong> 7/22/2025</p>
                    <p><strong>Categories:</strong> cs.CL, cs.AI</p>
                    <p><strong>Summary:</strong> Despite efforts to unify multimodal generation and understanding tasks in a single model, we show these MLLMs exhibit self-contradiction where generation produces images deemed misaligned with input prompts based on the models own understanding. We define a Nonunified score that quantifies such self-contradiction. Our empirical results reveal that the self-contradiction mainly arises from weak generation that fails to align with prompts, rather than misunderstanding. This capability asymmetry indicates the potential of leveraging self-contradiction for self-improvement, where the stronger model understanding guides the weaker generation to mitigate the generation-understanding gap. Applying standard post-training methods (e.g., SFT, DPO) with such internal supervision successfully improves both generation and unification. We discover a co-improvement effect on both generation and understanding when only fine-tuning the generation branch, a phenomenon known in pre-training but underexplored in post-training. Our analysis shows improvements stem from better detection of false positives that are previously incorrectly identified as prompt-aligned. Theoretically, we show the aligned training dynamics between generation and understanding allow reduced prompt-misaligned generations to also improve mismatch detection in the understanding branch. Additionally, the framework reveals a potential risk of co-degradation under poor supervision-an overlooked phenomenon that is empirically validated in our experiments. Notably, we find intrinsic metrics like Nonunified score cannot distinguish co-degradation from co-improvement, which highlights the necessity of data quality check. Finally, we propose a curriculum-based strategy based on our findings that gradually introduces harder samples as the model improves, leading to better unification and improved MLLM generation and understanding.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2507.16655v1" target="_blank">A comparison between behavioral similarity methods vs standard deviation method in predicting time series dataset, case study of finance market</a></h3>
                    <p><strong>Authors:</strong> Mahdi Goldani</p>
                    <p><strong>Published:</strong> 7/22/2025</p>
                    <p><strong>Categories:</strong> econ.TH</p>
                    <p><strong>Summary:</strong> In statistical modeling, prediction and explanation are two fundamental objectives. When the primary goal is forecasting, it is important to account for the inherent uncertainty associated with estimating unknown outcomes. Traditionally, confidence intervals constructed using standard deviations have served as a formal means to quantify this uncertainty and evaluate the closeness of predicted values to their true counterparts. This approach reflects an implicit aim to capture the behavioral similarity between observed and estimated values. However, advances in similarity based approaches present promising alternatives to conventional variance based techniques, particularly in contexts characterized by large datasets or a high number of explanatory variables. This study aims to investigate which methods either traditional or similarity based are capable of producing narrower confidence intervals under comparable conditions, thereby offering more precise and informative intervals. The dataset utilized in this study consists of U.S. mega cap companies, comprising 42 firms. Due to the high number of features, interdependencies among predictors are common, therefore, Ridge Regression is applied to address this issue. The research findings indicate that variance based method and LCSS exhibit the highest coverage among the analyzed methods, although they produce broader intervals. Conversely, DTW, Hausdorff, and TWED deliver narrower intervals, positioning them as the most accurate methods, despite their medium coverage rates. Ultimately, the trade off between interval width and coverage underscores the necessity for context aware decision making when selecting similarity based methods for confidence interval estimation in time series analysis.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2507.16649v1" target="_blank">From Profiling to Optimization: Unveiling the Profile Guided Optimization</a></h3>
                    <p><strong>Authors:</strong> Bingxin Liu, Yinghui Huang, Jianhua Gao, Jianjun Shi, Yongpeng Liu, Yipin Sun, Weixing Ji</p>
                    <p><strong>Published:</strong> 7/22/2025</p>
                    <p><strong>Categories:</strong> cs.PF</p>
                    <p><strong>Summary:</strong> Profile Guided Optimization (PGO) uses runtime profiling to direct compiler optimization decisions, effectively combining static analysis with actual execution behavior to enhance performance. Runtime profiles, collected through instrumentation or hardware- and software-assisted sampling, provide detailed insights into control flow, branch predictions, and memory access patterns. This survey systematically categorizes PGO research by profiling method (instrumentation vs. sampling), optimizations (compile time and link/post-link time), compiler integration (GCC, LLVM), and target architectures. Key algorithms and frameworks are shown in terms of design principles. Performance evaluation on representative examples demonstrates PGOs speedups, overheads, and integration maturity. Finally, we identify open challenges, such as reducing sampling overhead, dynamic input workloads, and supporting cross-architecture portability, and propose future research directions to low-overhead profiling and advanced compilers.</p>
                
            
                
                    <h3><a href="http://dx.doi.org/10.1109/BigData59044.2023.10386518" target="_blank">Towards Automated Regulatory Compliance Verification in Financial Auditing with Large Language Models</a></h3>
                    <p><strong>Authors:</strong> Armin Berger, Lars Hillebrand, David Leonhard, Tobias DeuÃŸer, Thiago Bell Felix de Oliveira, Tim Dilmaghani, Mohamed Khaled, Bernd Kliem, RÃ¼diger Loitz, Christian Bauckhage, Rafet Sifa</p>
                    <p><strong>Published:</strong> 7/22/2025</p>
                    <p><strong>Categories:</strong> cs.CL, cs.AI, cs.LG</p>
                    <p><strong>Summary:</strong> The auditing of financial documents, historically a labor-intensive process, stands on the precipice of transformation. AI-driven solutions have made inroads into streamlining this process by recommending pertinent text passages from financial reports to align with the legal requirements of accounting standards. However, a glaring limitation remains: these systems commonly fall short in verifying if the recommended excerpts indeed comply with the specific legal mandates. Hence, in this paper, we probe the efficiency of publicly available Large Language Models (LLMs) in the realm of regulatory compliance across different model configurations. We place particular emphasis on comparing cutting-edge open-source LLMs, such as Llama-2, with their proprietary counterparts like OpenAIs GPT models. This comparative analysis leverages two custom datasets provided by our partner PricewaterhouseCoopers (PwC) Germany. We find that the open-source Llama-2 70 billion model demonstrates outstanding performance in detecting non-compliance or true negative occurrences, beating all their proprietary counterparts. Nevertheless, proprietary models such as GPT-4 perform the best in a broad variety of scenarios, particularly in non-English contexts.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2507.16639v1" target="_blank">Benchmarking pig detection and tracking under diverse and challenging conditions</a></h3>
                    <p><strong>Authors:</strong> Jonathan Henrich, Christian Post, Maximilian Zilke, Parth Shiroya, Emma Chanut, Amir Mollazadeh Yamchi, Ramin Yahyapour, Thomas Kneib, Imke Traulsen</p>
                    <p><strong>Published:</strong> 7/22/2025</p>
                    <p><strong>Categories:</strong> cs.CV</p>
                    <p><strong>Summary:</strong> To ensure animal welfare and effective management in pig farming, monitoring individual behavior is a crucial prerequisite. While monitoring tasks have traditionally been carried out manually, advances in machine learning have made it possible to collect individualized information in an increasingly automated way. Central to these methods is the localization of animals across space (object detection) and time (multi-object tracking). Despite extensive research of these two tasks in pig farming, a systematic benchmarking study has not yet been conducted. In this work, we address this gap by curating two datasets: PigDetect for object detection and PigTrack for multi-object tracking. The datasets are based on diverse image and video material from realistic barn conditions, and include challenging scenarios such as occlusions or bad visibility. For object detection, we show that challenging training images improve detection performance beyond what is achievable with randomly sampled images alone. Comparing different approaches, we found that state-of-the-art models offer substantial improvements in detection quality over real-time alternatives. For multi-object tracking, we observed that SORT-based methods achieve superior detection performance compared to end-to-end trainable models. However, end-to-end models show better association performance, suggesting they could become strong alternatives in the future. We also investigate characteristic failure cases of end-to-end models, providing guidance for future improvements. The detection and tracking models trained on our datasets perform well in unseen pens, suggesting good generalization capabilities. This highlights the importance of high-quality training data. The datasets and research code are made publicly available to facilitate reproducibility, re-use and further development.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2507.16636v1" target="_blank">Physics-Informed Neural Networks for High-Precision Grad-Shafranov Equilibrium Reconstruction</a></h3>
                    <p><strong>Authors:</strong> Cuizhi Zhou, Kaien Zhu</p>
                    <p><strong>Published:</strong> 7/22/2025</p>
                    <p><strong>Categories:</strong> physics.plasm-ph</p>
                    <p><strong>Summary:</strong> The equilibrium reconstruction of plasma is a core step in real-time diagnostic tasks in fusion research. This paper explores a multi-stage Physics-Informed Neural Networks(PINNs) approach to solve the Grad-Shafranov equation, achieving high-precision solutions with an error magnitude of $O(10^{-8})$ between the output of the second-stage neural network and the analytical solution. Our results demonstrate that the multi-stage PINNs provides a reliable tool for plasma equilibrium reconstruction.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2507.16632v1" target="_blank">Step-Audio 2 Technical Report</a></h3>
                    <p><strong>Authors:</strong> Boyong Wu, Chao Yan, Chen Hu, Cheng Yi, Chengli Feng, Fei Tian, Feiyu Shen, Gang Yu, Haoyang Zhang, Jingbei Li, Mingrui Chen, Peng Liu, Wang You, Xiangyu Tony Zhang, Xingyuan Li, Xuerui Yang, Yayue Deng, Yechang Huang, Yuxin Li, Yuxin Zhang, Zhao You, Brian Li, Changyi Wan, Hanpeng Hu, Jiangjie Zhen, Siyu Chen, Song Yuan, Xuelin Zhang, Yimin Jiang, Yu Zhou, Yuxiang Yang, Bingxin Li, Buyun Ma, Changhe Song, Dongqing Pang, Guoqiang Hu, Haiyang Sun, Kang An, Na Wang, Shuli Gao, Wei Ji, Wen Li, Wen Sun, Xuan Wen, Yong Ren, Yuankai Ma, Yufan Lu, Bin Wang, Bo Li, Changxin Miao, Che Liu, Chen Xu, Dapeng Shi, Dingyuan Hu, Donghang Wu, Enle Liu, Guanzhe Huang, Gulin Yan, Han Zhang, Hao Nie, Haonan Jia, Hongyu Zhou, Jianjian Sun, Jiaoren Wu, Jie Wu, Jie Yang, Jin Yang, Junzhe Lin, Kaixiang Li, Lei Yang, Liying Shi, Li Zhou, Longlong Gu, Ming Li, Mingliang Li, Mingxiao Li, Nan Wu, Qi Han, Qinyuan Tan, Shaoliang Pang, Shengjie Fan, Siqi Liu, Tiancheng Cao, Wanying Lu, Wenqing He, Wuxun Xie, Xu Zhao, Xueqi Li, Yanbo Yu, Yang Yang, Yi Liu, Yifan Lu, Yilei Wang, Yuanhao Ding, Yuanwei Liang, Yuanwei Lu, Yuchu Luo, Yuhe Yin, Yumeng Zhan, Yuxiang Zhang, Zidong Yang, Zixin Zhang, Binxing Jiao, Daxin Jiang, Heung-Yeung Shum, Jiansheng Chen, Jing Li, Xiangyu Zhang, Yibo Zhu</p>
                    <p><strong>Published:</strong> 7/22/2025</p>
                    <p><strong>Categories:</strong> cs.CL, cs.SD, eess.AS</p>
                    <p><strong>Summary:</strong> This paper presents Step-Audio~2, an end-to-end multi-modal large language model designed for industry-strength audio understanding and speech conversation. By integrating a latent audio encoder and reasoning-centric reinforcement learning (RL), Step-Audio 2 achieves promising performance in automatic speech recognition (ASR) and audio understanding. To facilitate genuine end-to-end speech conversation, Step-Audio 2 incorporates the generation of discrete audio tokens into language modeling, significantly enhancing its responsiveness to paralinguistic information such as speaking styles and emotions. To effectively leverage the rich textual and acoustic knowledge in real-world data, Step-Audio 2 integrates retrieval-augmented generation (RAG) and is able to call external tools such as web search to mitigate hallucination and audio search to switch timbres. Trained on millions of hours of speech and audio data, Step-Audio 2 delivers intelligence and expressiveness across diverse conversational scenarios. Evaluation results demonstrate that Step-Audio 2 achieves state-of-the-art performance on various audio understanding and conversational benchmarks compared to other open-source and commercial solutions. Please visit https://github.com/stepfun-ai/Step-Audio2 for more information.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2507.16623v1" target="_blank">Automatic Fine-grained Segmentation-assisted Report Generation</a></h3>
                    <p><strong>Authors:</strong> Frederic Jonske, Constantin Seibold, Osman Alperen Koras, Fin Bahnsen, Marie Bauer, Amin Dada, Hamza Kalisch, Anton Schily, Jens Kleesiek</p>
                    <p><strong>Published:</strong> 7/22/2025</p>
                    <p><strong>Categories:</strong> cs.CV, cs.LG</p>
                    <p><strong>Summary:</strong> Reliable end-to-end clinical report generation has been a longstanding goal of medical ML research. The end goal for this process is to alleviate radiologists workloads and provide second opinions to clinicians or patients. Thus, a necessary prerequisite for report generation models is a strong general performance and some type of innate grounding capability, to convince clinicians or patients of the veracity of the generated reports. In this paper, we present ASaRG (\textbf{A}utomatic \textbf{S}egmentation-\textbf{a}ssisted \textbf{R}eport \textbf{G}eneration), an extension of the popular LLaVA architecture that aims to tackle both of these problems. ASaRG proposes to fuse intermediate features and fine-grained segmentation maps created by specialist radiological models into LLaVAs multi-modal projection layer via simple concatenation. With a small number of added parameters, our approach achieves a +0.89\% performance gain ($p=0.012$) in CE F1 score compared to the LLaVA baseline when using only intermediate features, and +2.77\% performance gain ($p0.001$) when adding a combination of intermediate features and fine-grained segmentation maps. Compared with COMG and ORID, two other report generation methods that utilize segmentations, the performance gain amounts to 6.98\% and 6.28\% in F1 score, respectively. ASaRG is not mutually exclusive with other changes made to the LLaVA architecture, potentially allowing our method to be combined with other advances in the field. Finally, the use of an arbitrary number of segmentations as part of the input demonstrably allows tracing elements of the report to the corresponding segmentation maps and verifying the groundedness of assessments. Our code will be made publicly available at a later date.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2507.16621v1" target="_blank">A Target-based Multi-LiDAR Multi-Camera Extrinsic Calibration System</a></h3>
                    <p><strong>Authors:</strong> Lorenzo Gentilini, Pierpaolo Serio, Valentina Donzella, Lorenzo Pollini</p>
                    <p><strong>Published:</strong> 7/22/2025</p>
                    <p><strong>Categories:</strong> cs.RO, cs.CV</p>
                    <p><strong>Summary:</strong> Extrinsic Calibration represents the cornerstone of autonomous driving. Its accuracy plays a crucial role in the perception pipeline, as any errors can have implications for the safety of the vehicle. Modern sensor systems collect different types of data from the environment, making it harder to align the data. To this end, we propose a target-based extrinsic calibration system tailored for a multi-LiDAR and multi-camera sensor suite. This system enables cross-calibration between LiDARs and cameras with limited prior knowledge using a custom ChArUco board and a tailored nonlinear optimization method. We test the system with real-world data gathered in a warehouse. Results demonstrated the effectiveness of the proposed method, highlighting the feasibility of a unique pipeline tailored for various types of sensors.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2507.16611v1" target="_blank">Smooth Games of Configuration in the Linear-Quadratic Setting</a></h3>
                    <p><strong>Authors:</strong> Jesse Milzman, Jeffrey Mao, Giuseppe Loianno</p>
                    <p><strong>Published:</strong> 7/22/2025</p>
                    <p><strong>Categories:</strong> cs.MA, cs.GT</p>
                    <p><strong>Summary:</strong> Dynamic game theory offers a toolbox for formalizing and solving for both cooperative and non-cooperative strategies in multi-agent scenarios. However, the optimal configuration of such games remains largely unexplored. While there is existing literature on the parametrization of dynamic games, little research examines this parametrization from a strategic perspective where each agents configuration choice is influenced by the decisions of others. In this work, we introduce the concept of a game of configuration, providing a framework for the strategic fine-tuning of differential games. We define a game of configuration as a two-stage game within the setting of finite-horizon, affine-quadratic, AQ, differential games. In the first stage, each player chooses their corresponding configuration parameter, which will impact their dynamics and costs in the second stage. We provide the subgame perfect solution concept and a method for computing first stage cost gradients over the configuration space. This then allows us to formulate a gradient-based method for searching for local solutions to the configuration game, as well as provide necessary conditions for equilibrium configurations over their downstream (second stage) trajectories. We conclude by demonstrating the effectiveness of our approach in example AQ systems, both zero-sum and general-sum.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2507.16608v1" target="_blank">Dyna3DGR: 4D Cardiac Motion Tracking with Dynamic 3D Gaussian Representation</a></h3>
                    <p><strong>Authors:</strong> Xueming Fu, Pei Wu, Yingtai Li, Xin Luo, Zihang Jiang, Junhao Mei, Jian Lu, Gao-Jun Teng, S. Kevin Zhou</p>
                    <p><strong>Published:</strong> 7/22/2025</p>
                    <p><strong>Categories:</strong> cs.CV</p>
                    <p><strong>Summary:</strong> Accurate analysis of cardiac motion is crucial for evaluating cardiac function. While dynamic cardiac magnetic resonance imaging (CMR) can capture detailed tissue motion throughout the cardiac cycle, the fine-grained 4D cardiac motion tracking remains challenging due to the homogeneous nature of myocardial tissue and the lack of distinctive features. Existing approaches can be broadly categorized into image based and representation-based, each with its limitations. Image-based methods, including both raditional and deep learning-based registration approaches, either struggle with topological consistency or rely heavily on extensive training data. Representation-based methods, while promising, often suffer from loss of image-level details. To address these limitations, we propose Dynamic 3D Gaussian Representation (Dyna3DGR), a novel framework that combines explicit 3D Gaussian representation with implicit neural motion field modeling. Our method simultaneously optimizes cardiac structure and motion in a self-supervised manner, eliminating the need for extensive training data or point-to-point correspondences. Through differentiable volumetric rendering, Dyna3DGR efficiently bridges continuous motion representation with image-space alignment while preserving both topological and temporal consistency. Comprehensive evaluations on the ACDC dataset demonstrate that our approach surpasses state-of-the-art deep learning-based diffeomorphic registration methods in tracking accuracy. The code will be available in https://github.com/windrise/Dyna3DGR.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2507.16598v1" target="_blank">Depression as a disorder of distributional coding</a></h3>
                    <p><strong>Authors:</strong> Matthew Botvinick, Zeb Kurth-Nelson, Timothy Muller, Will Dabney</p>
                    <p><strong>Published:</strong> 7/22/2025</p>
                    <p><strong>Categories:</strong> q-bio.NC</p>
                    <p><strong>Summary:</strong> Major depressive disorder persistently stands as a major public health problem. While some progress has been made toward effective treatments, the neural mechanisms that give rise to the disorder remain poorly understood. In this Perspective, we put forward a new theory of the pathophysiology of depression. More precisely, we spotlight three previously separate bodies of research, showing how they can be fit together into a previously overlooked larger picture. The first piece of the puzzle is provided by pathophysiology research implicating dopamine in depression. The second piece, coming from computational psychiatry, links depression with a special form of reinforcement learning. The third and final piece involves recent work at the intersection of artificial intelligence and basic neuroscience research, indicating that the brain may represent value using a distributional code. Fitting these three pieces together yields a new model of depressions pathophysiology, which spans circuit, systems, computational and behavioral levels, opening up new directions for research.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2507.16596v1" target="_blank">A Multimodal Deviation Perceiving Framework for Weakly-Supervised Temporal Forgery Localization</a></h3>
                    <p><strong>Authors:</strong> Wenbo Xu, Junyan Wu, Wei Lu, Xiangyang Luo, Qian Wang</p>
                    <p><strong>Published:</strong> 7/22/2025</p>
                    <p><strong>Categories:</strong> cs.CV</p>
                    <p><strong>Summary:</strong> Current researches on Deepfake forensics often treat detection as a classification task or temporal forgery localization problem, which are usually restrictive, time-consuming, and challenging to scale for large datasets. To resolve these issues, we present a multimodal deviation perceiving framework for weakly-supervised temporal forgery localization (MDP), which aims to identify temporal partial forged segments using only video-level annotations. The MDP proposes a novel multimodal interaction mechanism (MI) and an extensible deviation perceiving loss to perceive multimodal deviation, which achieves the refined start and end timestamps localization of forged segments. Specifically, MI introduces a temporal property preserving cross-modal attention to measure the relevance between the visual and audio modalities in the probabilistic embedding space. It could identify the inter-modality deviation and construct comprehensive video features for temporal forgery localization. To explore further temporal deviation for weakly-supervised learning, an extensible deviation perceiving loss has been proposed, aiming at enlarging the deviation of adjacent segments of the forged samples and reducing that of genuine samples. Extensive experiments demonstrate the effectiveness of the proposed framework and achieve comparable results to fully-supervised approaches in several evaluation metrics.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2507.16594v1" target="_blank">An Experimental Study of Split-Learning TinyML on Ultra-Low-Power Edge/IoT Nodes</a></h3>
                    <p><strong>Authors:</strong> Zied Jenhani, Mounir Bensalem, Jasenka DizdareviÄ‡, Admela Jukan</p>
                    <p><strong>Published:</strong> 7/22/2025</p>
                    <p><strong>Categories:</strong> cs.NI, cs.AI, cs.DC</p>
                    <p><strong>Summary:</strong> Running deep learning inference directly on ultra-low-power edge/IoT nodes has been limited by the tight memory and compute budgets of microcontrollers. Split learning (SL) addresses this limitation in which it executes part of the inference process on the sensor and off-loads the remainder to a companion device. In the context of constrained devices and the related impact of low-power, over-the-air transport protocols, the performance of split learning remains largely unexplored. TO the best of our knowledge, this paper presents the first end-to-end TinyML + SL testbed built on Espressif ESP32-S3 boards, designed to benchmark the over-the-air performance of split learning TinyML in edge/IoT environments. We benchmark the performance of a MobileNetV2 image recognition model, which is quantized to 8-bit integers, partitioned, and delivered to the nodes via over-the-air updates. The intermediate activations are exchanged through different wireless communication methods: ESP-NOW, BLE, and traditional UDP/IP and TCP/IP, enabling a head-to-head comparison on identical hardware. Measurements show that splitting the model after block_16_project_BN layer generates a 5.66 kB tensor that traverses the link in 3.2 ms, when UDP is used, achieving a steady-state round-trip latency of 5.8 s. ESP-NOW presents the most favorable RTT performance 3.7 s; BLE extends battery life further but increases latency beyond 10s.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2507.16586v1" target="_blank">AI for Better UX in Computer-Aided Engineering: Is Academia Catching Up with Industry Demands? A Multivocal Literature Review</a></h3>
                    <p><strong>Authors:</strong> Choro Ulan Uulu, Mikhail Kulyabin, Layan Etaiwi, Nuno Miguel Martins Pacheco, Jan Joosten, Kerstin RÃ¶se, Filippos Petridis, Jan Bosch, Helena HolmstrÃ¶m Olsson</p>
                    <p><strong>Published:</strong> 7/22/2025</p>
                    <p><strong>Categories:</strong> cs.HC, cs.AI, cs.SE</p>
                    <p><strong>Summary:</strong> Computer-Aided Engineering (CAE) enables simulation experts to optimize complex models, but faces challenges in user experience (UX) that limit efficiency and accessibility. While artificial intelligence (AI) has demonstrated potential to enhance CAE processes, research integrating these fields with a focus on UX remains fragmented. This paper presents a multivocal literature review (MLR) examining how AI enhances UX in CAE software across both academic research and industry implementations. Our analysis reveals significant gaps between academic explorations and industry applications, with companies actively implementing LLMs, adaptive UIs, and recommender systems while academic research focuses primarily on technical capabilities without UX validation. Key findings demonstrate opportunities in AI-powered guidance, adaptive interfaces, and workflow automation that remain underexplored in current research. By mapping the intersection of these domains, this study provides a foundation for future work to address the identified research gaps and advance the integration of AI to improve CAE user experience.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2507.16581v1" target="_blank">On Expansions of Monadic Second-Order Logic with Dynamical Predicates</a></h3>
                    <p><strong>Authors:</strong> Joris Nieuwveld, JoÃ«l Ouaknine</p>
                    <p><strong>Published:</strong> 7/22/2025</p>
                    <p><strong>Categories:</strong> cs.LO, math.NT, 11B37 11J86 11B40 11K16, F.4.0; G.2.0</p>
                    <p><strong>Summary:</strong> Expansions of the monadic second-order (MSO) theory of the structure $\langle \mathbb{N} ;  \rangle$ have been a fertile and active area of research ever since the publication of the seminal papers of B\uchi and Elgot  Rabin on the subject in the 1960s. In the present paper, we establish decidability of the MSO theory of $\langle \mathbb{N} ; ,P \rangle$, where $P$ ranges over a large class of unary dynamical predicates, i.e., sets of non-negative values assumed by certain integer linear recurrence sequences. One of our key technical tools is the novel concept of (effective) prodisjunctivity, which we expect may also find independent applications further afield.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2507.16576v1" target="_blank">From Text to Actionable Intelligence: Automating STIX Entity and Relationship Extraction</a></h3>
                    <p><strong>Authors:</strong> Ahmed Lekssays, Husrev Taha Sencar, Ting Yu</p>
                    <p><strong>Published:</strong> 7/22/2025</p>
                    <p><strong>Categories:</strong> cs.CR</p>
                    <p><strong>Summary:</strong> Sharing methods of attack and their effectiveness is a cornerstone of building robust defensive systems. Threat analysis reports, produced by various individuals and organizations, play a critical role in supporting security operations and combating emerging threats. To enhance the timeliness and automation of threat intelligence sharing, several standards have been established, with the Structured Threat Information Expression (STIX) framework emerging as one of the most widely adopted. However, generating STIX-compatible data from unstructured security text remains a largely manual, expert-driven process. To address this challenge, we introduce AZERG, a tool designed to assist security analysts in automatically generating structured STIX representations. To achieve this, we adapt general-purpose large language models for the specific task of extracting STIX-formatted threat data. To manage the complexity, the task is divided into four subtasks: entity detection (T1), entity type identification (T2), related pair detection (T3), and relationship type identification (T4). We apply task-specific fine-tuning to accurately extract relevant entities and infer their relationships in accordance with the STIX specification. To address the lack of training data, we compiled a comprehensive dataset with 4,011 entities and 2,075 relationships extracted from 141 full threat analysis reports, all annotated in alignment with the STIX standard. Our models achieved F1-scores of 84.43% for T1, 88.49% for T2, 95.47% for T3, and 84.60% for T4 in real-world scenarios. We validated their performance against a range of open- and closed-parameter models, as well as state-of-the-art methods, demonstrating improvements of 2-25% across tasks.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2507.16572v1" target="_blank">Pixels to Principles: Probing Intuitive Physics Understanding in Multimodal Language Models</a></h3>
                    <p><strong>Authors:</strong> Mohamad Ballout, Serwan Jassim, Elia Bruni</p>
                    <p><strong>Published:</strong> 7/22/2025</p>
                    <p><strong>Categories:</strong> cs.CL</p>
                    <p><strong>Summary:</strong> This paper presents a systematic evaluation of state-of-the-art multimodal large language models (MLLMs) on intuitive physics tasks using the GRASP and IntPhys 2 datasets. We assess the open-source models InternVL 2.5, Qwen 2.5 VL, LLaVA-OneVision, and the proprietary Gemini 2.0 Flash Thinking, finding that even the latest models struggle to reliably distinguish physically plausible from implausible scenarios. To go beyond performance metrics, we conduct a probing analysis of model embeddings, extracting intermediate representations at key processing stages to examine how well task-relevant information is preserved. Our results show that, depending on task difficulty, a critical vision-language misalignment can emerge: vision encoders successfully capture physical plausibility cues, but this information is not effectively utilized by the language model, leading to failures in reasoning. This misalignment suggests that the primary limitation of MLLMs in intuitive physics tasks is not the vision component but the ineffective integration of visual and linguistic information. Our findings highlight vision-language alignment as a key area for improvement, offering insights for future MLLMs development.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2507.16562v1" target="_blank">Evaluating Social Acceptance of eXtended Reality (XR) Agent Technology: A User Study (Extended Version)</a></h3>
                    <p><strong>Authors:</strong> Megha Quamara, Viktor Schmuck, Cristina Iani, Axel Primavesi, Alexander Plaum, Luca Vigano</p>
                    <p><strong>Published:</strong> 7/22/2025</p>
                    <p><strong>Categories:</strong> cs.HC, cs.AI</p>
                    <p><strong>Summary:</strong> In this paper, we present the findings of a user study that evaluated the social acceptance of eXtended Reality (XR) agent technology, focusing on a remotely accessible, web-based XR training system developed for journalists. This system involves user interaction with a virtual avatar, enabled by a modular toolkit. The interactions are designed to provide tailored training for journalists in digital-remote settings, especially for sensitive or dangerous scenarios, without requiring specialized end-user equipment like headsets. Our research adapts and extends the Almere model, representing social acceptance through existing attributes such as perceived ease of use and perceived usefulness, along with added ones like dependability and security in the user-agent interaction. The XR agent was tested through a controlled experiment in a real-world setting, with data collected on users perceptions. Our findings, based on quantitative and qualitative measurements involving questionnaires, contribute to the understanding of user perceptions and acceptance of XR agent solutions within a specific social context, while also identifying areas for the improvement of XR systems.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2507.16559v1" target="_blank">Comparative validation of surgical phase recognition, instrument keypoint estimation, and instrument instance segmentation in endoscopy: Results of the PhaKIR 2024 challenge</a></h3>
                    <p><strong>Authors:</strong> Tobias Rueckert, David Rauber, Raphaela Maerkl, Leonard Klausmann, Suemeyye R. Yildiran, Max Gutbrod, Danilo Weber Nunes, Alvaro Fernandez Moreno, Imanol Luengo, Danail Stoyanov, Nicolas Toussaint, Enki Cho, Hyeon Bae Kim, Oh Sung Choo, Ka Young Kim, Seong Tae Kim, GonÃ§alo Arantes, Kehan Song, Jianjun Zhu, Junchen Xiong, Tingyi Lin, Shunsuke Kikuchi, Hiroki Matsuzaki, Atsushi Kouno, JoÃ£o Renato Ribeiro Manesco, JoÃ£o Paulo Papa, Tae-Min Choi, Tae Kyeong Jeong, Juyoun Park, Oluwatosin Alabi, Meng Wei, Tom Vercauteren, Runzhi Wu, Mengya Xu, An Wang, Long Bai, Hongliang Ren, Amine Yamlahi, Jakob Hennighausen, Lena Maier-Hein, Satoshi Kondo, Satoshi Kasai, Kousuke Hirasawa, Shu Yang, Yihui Wang, Hao Chen, Santiago RodrÃ­guez, NicolÃ¡s Aparicio, Leonardo Manrique, Juan Camilo Lyons, Olivia Hosie, NicolÃ¡s Ayobi, Pablo ArbelÃ¡ez, Yiping Li, Yasmina Al Khalil, Sahar Nasirihaghighi, Stefanie Speidel, Daniel Rueckert, Hubertus Feussner, Dirk Wilhelm, Christoph Palm</p>
                    <p><strong>Published:</strong> 7/22/2025</p>
                    <p><strong>Categories:</strong> cs.CV</p>
                    <p><strong>Summary:</strong> Reliable recognition and localization of surgical instruments in endoscopic video recordings are foundational for a wide range of applications in computer- and robot-assisted minimally invasive surgery (RAMIS), including surgical training, skill assessment, and autonomous assistance. However, robust performance under real-world conditions remains a significant challenge. Incorporating surgical context - such as the current procedural phase - has emerged as a promising strategy to improve robustness and interpretability. To address these challenges, we organized the Surgical Procedure Phase, Keypoint, and Instrument Recognition (PhaKIR) sub-challenge as part of the Endoscopic Vision (EndoVis) challenge at MICCAI 2024. We introduced a novel, multi-center dataset comprising thirteen full-length laparoscopic cholecystectomy videos collected from three distinct medical institutions, with unified annotations for three interrelated tasks: surgical phase recognition, instrument keypoint estimation, and instrument instance segmentation. Unlike existing datasets, ours enables joint investigation of instrument localization and procedural context within the same data while supporting the integration of temporal information across entire procedures. We report results and findings in accordance with the BIAS guidelines for biomedical image analysis challenges. The PhaKIR sub-challenge advances the field by providing a unique benchmark for developing temporally aware, context-driven methods in RAMIS and offers a high-quality resource to support future research in surgical scene understanding.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2507.16815v1" target="_blank">ThinkAct: Vision-Language-Action Reasoning via Reinforced Visual Latent Planning</a></h3>
                    <p><strong>Authors:</strong> Chi-Pin Huang, Yueh-Hua Wu, Min-Hung Chen, Yu-Chiang Frank Wang, Fu-En Yang</p>
                    <p><strong>Published:</strong> 7/22/2025</p>
                    <p><strong>Categories:</strong> cs.CV, cs.AI, cs.LG, cs.RO</p>
                    <p><strong>Summary:</strong> Vision-language-action (VLA) reasoning tasks require agents to interpret multimodal instructions, perform long-horizon planning, and act adaptively in dynamic environments. Existing approaches typically train VLA models in an end-to-end fashion, directly mapping inputs to actions without explicit reasoning, which hinders their ability to plan over multiple steps or adapt to complex task variations. In this paper, we propose ThinkAct, a dual-system framework that bridges high-level reasoning with low-level action execution via reinforced visual latent planning. ThinkAct trains a multimodal LLM to generate embodied reasoning plans guided by reinforcing action-aligned visual rewards based on goal completion and trajectory consistency. These reasoning plans are compressed into a visual plan latent that conditions a downstream action model for robust action execution on target environments. Extensive experiments on embodied reasoning and robot manipulation benchmarks demonstrate that ThinkAct enables few-shot adaptation, long-horizon planning, and self-correction behaviors in complex embodied AI tasks.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2507.16814v1" target="_blank">Semi-off-Policy Reinforcement Learning for Vision-Language Slow-thinking Reasoning</a></h3>
                    <p><strong>Authors:</strong> Junhao Shen, Haiteng Zhao, Yuzhe Gu, Songyang Gao, Kuikun Liu, Haian Huang, Jianfei Gao, Dahua Lin, Wenwei Zhang, Kai Chen</p>
                    <p><strong>Published:</strong> 7/22/2025</p>
                    <p><strong>Categories:</strong> cs.LG, cs.CV</p>
                    <p><strong>Summary:</strong> Enhancing large vision-language models (LVLMs) with visual slow-thinking reasoning is crucial for solving complex multimodal tasks. However, since LVLMs are mainly trained with vision-language alignment, it is difficult to adopt on-policy reinforcement learning (RL) to develop the slow thinking ability because the rollout space is restricted by its initial abilities. Off-policy RL offers a way to go beyond the current policy, but directly distilling trajectories from external models may cause visual hallucinations due to mismatched visual perception abilities across models. To address these issues, this paper proposes SOPHIA, a simple and scalable Semi-Off-Policy RL for vision-language slow-tHInking reAsoning. SOPHIA builds a semi-off-policy behavior model by combining on-policy visual understanding from a trainable LVLM with off-policy slow-thinking reasoning from a language model, assigns outcome-based rewards to reasoning, and propagates visual rewards backward. Then LVLM learns slow-thinking reasoning ability from the obtained reasoning trajectories using propagated rewards via off-policy RL algorithms. Extensive experiments with InternVL2.5 and InternVL3.0 with 8B and 38B sizes show the effectiveness of SOPHIA. Notably, SOPHIA improves InternVL3.0-38B by 8.50% in average, reaching state-of-the-art performance among open-source LVLMs on multiple multimodal reasoning benchmarks, and even outperforms some closed-source models (e.g., GPT-4.1) on the challenging MathVision and OlympiadBench, achieving 49.08% and 49.95% pass@1 accuracy, respectively. Analysis shows SOPHIA outperforms supervised fine-tuning and direct on-policy RL methods, offering a better policy initialization for further on-policy training.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2507.16813v1" target="_blank">HOComp: Interaction-Aware Human-Object Composition</a></h3>
                    <p><strong>Authors:</strong> Dong Liang, Jinyuan Jia, Yuhao Liu, Rynson W. H. Lau</p>
                    <p><strong>Published:</strong> 7/22/2025</p>
                    <p><strong>Categories:</strong> cs.CV</p>
                    <p><strong>Summary:</strong> While existing image-guided composition methods may help insert a foreground object onto a user-specified region of a background image, achieving natural blending inside the region with the rest of the image unchanged, we observe that these existing methods often struggle in synthesizing seamless interaction-aware compositions when the task involves human-object interactions. In this paper, we first propose HOComp, a novel approach for compositing a foreground object onto a human-centric background image, while ensuring harmonious interactions between the foreground object and the background person and their consistent appearances. Our approach includes two key designs: (1) MLLMs-driven Region-based Pose Guidance (MRPG), which utilizes MLLMs to identify the interaction region as well as the interaction type (e.g., holding and lefting) to provide coarse-to-fine constraints to the generated pose for the interaction while incorporating human pose landmarks to track action variations and enforcing fine-grained pose constraints; and (2) Detail-Consistent Appearance Preservation (DCAP), which unifies a shape-aware attention modulation mechanism, a multi-view appearance loss, and a background consistency loss to ensure consistent shapes/textures of the foreground and faithful reproduction of the background human. We then propose the first dataset, named Interaction-aware Human-Object Composition (IHOC), for the task. Experimental results on our dataset show that HOComp effectively generates harmonious human-object interactions with consistent appearances, and outperforms relevant methods qualitatively and quantitatively.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2507.16812v1" target="_blank">MegaScience: Pushing the Frontiers of Post-Training Datasets for Science Reasoning</a></h3>
                    <p><strong>Authors:</strong> Run-Ze Fan, Zengzhi Wang, Pengfei Liu</p>
                    <p><strong>Published:</strong> 7/22/2025</p>
                    <p><strong>Categories:</strong> cs.CL, cs.AI, cs.LG</p>
                    <p><strong>Summary:</strong> Scientific reasoning is critical for developing AI scientists and supporting human researchers in advancing the frontiers of natural science discovery. However, the open-source community has primarily focused on mathematics and coding while neglecting the scientific domain, largely due to the absence of open, large-scale, high-quality, verifiable scientific reasoning datasets. To bridge this gap, we first present TextbookReasoning, an open dataset featuring truthful reference answers extracted from 12k university-level scientific textbooks, comprising 650k reasoning questions spanning 7 scientific disciplines. We further introduce MegaScience, a large-scale mixture of high-quality open-source datasets totaling 1.25 million instances, developed through systematic ablation studies that evaluate various data selection methodologies to identify the optimal subset for each publicly available scientific dataset. Meanwhile, we build a comprehensive evaluation system covering diverse subjects and question types across 15 benchmarks, incorporating comprehensive answer extraction strategies to ensure accurate evaluation metrics. Our experiments demonstrate that our datasets achieve superior performance and training efficiency with more concise response lengths compared to existing open-source scientific datasets. Furthermore, we train Llama3.1, Qwen2.5, and Qwen3 series base models on MegaScience, which significantly outperform the corresponding official instruct models in average performance. In addition, MegaScience exhibits greater effectiveness for larger and stronger models, suggesting a scaling benefit for scientific tuning. We release our data curation pipeline, evaluation system, datasets, and seven trained models to the community to advance scientific reasoning research.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2507.16810v1" target="_blank">The inverse initial data problem for anisotropic Navier-Stokes equations via Legendre time reduction method</a></h3>
                    <p><strong>Authors:</strong> Cong B. Van, Thuy T. Le, Loc H. Nguyen</p>
                    <p><strong>Published:</strong> 7/22/2025</p>
                    <p><strong>Categories:</strong> math.NA, cs.NA</p>
                    <p><strong>Summary:</strong> We consider the inverse initial data problem for the compressible anisotropic Navier-Stokes equations, where the goal is to reconstruct the initial velocity field from lateral boundary observations. This problem arises in applications where direct measurements of internal fluid states are unavailable. We introduce a novel computational framework based on Legendre time reduction, which projects the velocity field onto an exponentially weighted Legendre basis in time. This transformation reduces the original time-dependent inverse problem to a coupled, time-independent elliptic system. The resulting reduced model is solved iteratively using a Picard iteration and a stabilized least-squares formulation under noisy boundary data. Numerical experiments in two dimensions confirm that the method accurately and robustly reconstructs initial velocity fields, even in the presence of significant measurement noise and complex anisotropic structures. This approach offers a flexible and computationally tractable alternative for inverse modeling in fluid dynamics with anisotropic media.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2507.16809v1" target="_blank">LingBench++: A Linguistically-Informed Benchmark and Reasoning Framework for Multi-Step and Cross-Cultural Inference with LLMs</a></h3>
                    <p><strong>Authors:</strong> Da-Chen Lian, Ri-Sheng Huang, Pin-Er Chen, Chunki Lim, You-Kuan Lin, Guan-Yu Tseng, Zi-Cheng Yang, Shu-Kai Hsieh</p>
                    <p><strong>Published:</strong> 7/22/2025</p>
                    <p><strong>Categories:</strong> cs.CL</p>
                    <p><strong>Summary:</strong> We propose LingBench++, a linguistically-informed benchmark and reasoning framework designed to evaluate large language models (LLMs) on complex linguistic tasks inspired by the International Linguistics Olympiad (IOL). Unlike prior benchmarks that focus solely on final answer accuracy, LingBench++ provides structured reasoning traces, stepwise evaluation protocols, and rich typological metadata across over 90 low-resource and cross-cultural languages. We further develop a multi-agent architecture integrating grammatical knowledge retrieval, tool-augmented reasoning, and deliberate hypothesis testing. Through systematic comparisons of baseline and our proposed agentic models, we demonstrate that models equipped with external knowledge sources and iterative reasoning outperform single-pass approaches in both accuracy and interpretability. LingBench++ offers a comprehensive foundation for advancing linguistically grounded, culturally informed, and cognitively plausible reasoning in LLMs.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2507.16806v1" target="_blank">Beyond Binary Rewards: Training LMs to Reason About Their Uncertainty</a></h3>
                    <p><strong>Authors:</strong> Mehul Damani, Isha Puri, Stewart Slocum, Idan Shenfeld, Leshem Choshen, Yoon Kim, Jacob Andreas</p>
                    <p><strong>Published:</strong> 7/22/2025</p>
                    <p><strong>Categories:</strong> cs.LG, cs.AI, cs.CL</p>
                    <p><strong>Summary:</strong> When language models (LMs) are trained via reinforcement learning (RL) to generate natural language reasoning chains, their performance improves on a variety of difficult question answering tasks. Today, almost all successful applications of RL for reasoning use binary reward functions that evaluate the correctness of LM outputs. Because such reward functions do not penalize guessing or low-confidence outputs, they often have the unintended side-effect of degrading calibration and increasing the rate at which LMs generate incorrect responses (or hallucinate) in other problem domains. This paper describes RLCR (Reinforcement Learning with Calibration Rewards), an approach to training reasoning models that jointly improves accuracy and calibrated confidence estimation. During RLCR, LMs generate both predictions and numerical confidence estimates after reasoning. They are trained to optimize a reward function that augments a binary correctness score with a Brier score -- a scoring rule for confidence estimates that incentivizes calibrated prediction. We first prove that this reward function (or any analogous reward function that uses a bounded, proper scoring rule) yields models whose predictions are both accurate and well-calibrated. We next show that across diverse datasets, RLCR substantially improves calibration with no loss in accuracy, on both in-domain and out-of-domain evaluations -- outperforming both ordinary RL training and classifiers trained to assign post-hoc confidence scores. While ordinary RL hurts calibration, RLCR improves it. Finally, we demonstrate that verbalized confidence can be leveraged at test time to improve accuracy and calibration via confidence-weighted scaling methods. Our results show that explicitly optimizing for calibration can produce more generally reliable reasoning models.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2507.16803v1" target="_blank">MultiTaskDeltaNet: Change Detection-based Image Segmentation for Operando ETEM with Application to Carbon Gasification Kinetics</a></h3>
                    <p><strong>Authors:</strong> Yushuo Niu, Tianyu Li, Yuanyuan Zhu, Qian Yang</p>
                    <p><strong>Published:</strong> 7/22/2025</p>
                    <p><strong>Categories:</strong> eess.IV, cs.CV</p>
                    <p><strong>Summary:</strong> Transforming in-situ transmission electron microscopy (TEM) imaging into a tool for spatially-resolved operando characterization of solid-state reactions requires automated, high-precision semantic segmentation of dynamically evolving features. However, traditional deep learning methods for semantic segmentation often encounter limitations due to the scarcity of labeled data, visually ambiguous features of interest, and small-object scenarios. To tackle these challenges, we introduce MultiTaskDeltaNet (MTDN), a novel deep learning architecture that creatively reconceptualizes the segmentation task as a change detection problem. By implementing a unique Siamese network with a U-Net backbone and using paired images to capture feature changes, MTDN effectively utilizes minimal data to produce high-quality segmentations. Furthermore, MTDN utilizes a multi-task learning strategy to leverage correlations between physical features of interest. In an evaluation using data from in-situ environmental TEM (ETEM) videos of filamentous carbon gasification, MTDN demonstrated a significant advantage over conventional segmentation models, particularly in accurately delineating fine structural features. Notably, MTDN achieved a 10.22% performance improvement over conventional segmentation models in predicting small and visually ambiguous physical features. This work bridges several key gaps between deep learning and practical TEM image analysis, advancing automated characterization of nanomaterials in complex experimental settings.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2507.16802v1" target="_blank">Agentar-Fin-R1: Enhancing Financial Intelligence through Domain Expertise, Training Efficiency, and Advanced Reasoning</a></h3>
                    <p><strong>Authors:</strong> Yanjun Zheng, Xiyang Du, Longfei Liao, Xiaoke Zhao, Zhaowen Zhou, Bo Zhang, Jiawei Liu, Xiang Qi, Zhe Li, Zhiqiang Zhang, Wang Wei, Peng Zhang</p>
                    <p><strong>Published:</strong> 7/22/2025</p>
                    <p><strong>Categories:</strong> cs.CL, cs.LG</p>
                    <p><strong>Summary:</strong> Large Language Models (LLMs) demonstrate tremendous potential in the financial domain, yet existing models often fall short in scenarios demanding robust reasoning capabilities, stringent trustworthiness requirements, and efficient adaptation to task-specific needs. We introduce the Agentar-Fin-R1 series of financial large language models (8B and 32B parameters), specifically engineered based on the Qwen3 foundation model to enhance reasoning capabilities, reliability, and domain specialization for financial applications. Our optimization approach integrates a high-quality, systematic financial task taxonomy with a comprehensive multi-layered trustworthiness assurance framework. This framework encompasses high-quality trustworthy knowledge engineering, multi-agent trustworthy data synthesis, and rigorous data validation governance. Through label-guided automated difficulty-aware optimization, tow-stage learning processes, and detailed attribution systems, we achieve substantial improvements in training efficiency. Our models undergo comprehensive evaluation on mainstream financial benchmarks including FinEva, FinEval, and FinanceIQ, as well as general reasoning datasets such as MATH-500 and GPQA. To thoroughly assess real-world deployment capabilities, we innovatively propose the Finova evaluation benchmark, which focuses on agent-level financial reasoning and compliance verification. Experimental results demonstrate that Agentar-Fin-R1 not only achieves state-of-the-art performance on financial tasks but also exhibits exceptional general reasoning capabilities, validating its effectiveness as a trustworthy solution for high-stakes financial applications.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2507.16801v1" target="_blank">Decoding Translation-Related Functional Sequences in 5UTRs Using Interpretable Deep Learning Models</a></h3>
                    <p><strong>Authors:</strong> Yuxi Lin, Yaxue Fang, Zehong Zhang, Zhouwu Liu, Siyun Zhong, Fulong Yu</p>
                    <p><strong>Published:</strong> 7/22/2025</p>
                    <p><strong>Categories:</strong> q-bio.QM, cs.AI</p>
                    <p><strong>Summary:</strong> Understanding how 5 untranslated regions (5UTRs) regulate mRNA translation is critical for controlling protein expression and designing effective therapeutic mRNAs. While recent deep learning models have shown promise in predicting translational efficiency from 5UTR sequences, most are constrained by fixed input lengths and limited interpretability. We introduce UTR-STCNet, a Transformer-based architecture for flexible and biologically grounded modeling of variable-length 5UTRs. UTR-STCNet integrates a Saliency-Aware Token Clustering (SATC) module that iteratively aggregates nucleotide tokens into multi-scale, semantically meaningful units based on saliency scores. A Saliency-Guided Transformer (SGT) block then captures both local and distal regulatory dependencies using a lightweight attention mechanism. This combined architecture achieves efficient and interpretable modeling without input truncation or increased computational cost. Evaluated across three benchmark datasets, UTR-STCNet consistently outperforms state-of-the-art baselines in predicting mean ribosome load (MRL), a key proxy for translational efficiency. Moreover, the model recovers known functional elements such as upstream AUGs and Kozak motifs, highlighting its potential for mechanistic insight into translation regulation.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2507.16800v1" target="_blank">Entanglement Entropy of Quantum Corners</a></h3>
                    <p><strong>Authors:</strong> Luca Ciambelli, Jerzy Kowalski-Glikman, Ludovic Varrin</p>
                    <p><strong>Published:</strong> 7/22/2025</p>
                    <p><strong>Categories:</strong> hep-th, gr-qc</p>
                    <p><strong>Summary:</strong> In gravitational theories with boundaries, diffeomorphisms can become physical and acquire a non-vanishing Noether charge. Using the covariant phase space formalism, on shell of the gravitational constraints, the latter localizes on codimension-$2$ surfaces, the corners. The corner proposal asserts that these charges, and their algebras, must be important ingredients of any quantum gravity theory. In this manuscript, we continue the study of quantum corner symmetries and algebras by computing the entanglement entropy and quantum informational properties of quantum states abiding to the quantum representations of corners in the framework of $2$-dimensional gravity. We do so for two classes of states: the vacuum and coherent states, properly defined. We then apply our results to JT gravity, seen as the dimensional reduction of $4$d near extremal black holes. There, we demonstrate that the entanglement entropy of some coherent quantum gravity states -- states admitting a semiclassical description -- scales like the dilaton, reproducing the semiclassical area law behavior and further solidifying the quantum informational nature of entropy of quantum corners. We then study general states and their gluing procedure, finding a formula for the entanglement entropy based entirely on the representation theory of $2$d quantum corners.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2507.16799v1" target="_blank">Test-Time-Matching: Decouple Personality, Memory, and Linguistic Style in LLM-based Role-Playing Language Agent</a></h3>
                    <p><strong>Authors:</strong> Xiaoyu Zhan, Xinyu Fu, Hao Sun, Yuanqi Li, Jie Guo, Yanwen Guo</p>
                    <p><strong>Published:</strong> 7/22/2025</p>
                    <p><strong>Categories:</strong> cs.CL</p>
                    <p><strong>Summary:</strong> The rapid advancement of large language models (LLMs) has enabled role-playing language agents to demonstrate significant potential in various applications. However, relying solely on prompts and contextual inputs often proves insufficient for achieving deep immersion in specific roles, particularly well-known fictional or public figures. On the other hand, fine-tuning-based approaches face limitations due to the challenges associated with data collection and the computational resources required for training, thereby restricting their broader applicability. To address these issues, we propose Test-Time-Matching (TTM), a training-free role-playing framework through test-time scaling and context engineering. TTM uses LLM agents to automatically decouple a characters features into personality, memory, and linguistic style. Our framework involves a structured, three-stage generation pipeline that utilizes these features for controlled role-playing. It achieves high-fidelity role-playing performance, also enables seamless combinations across diverse linguistic styles and even variations in personality and memory. We evaluate our framework through human assessment, and the results demonstrate that our method achieves the outstanding performance in generating expressive and stylistically consistent character dialogues.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2507.16798v1" target="_blank">From heteroclinic loops to homoclinic snaking in reversible systems: rigorous forcing through computer-assisted proofs</a></h3>
                    <p><strong>Authors:</strong> Jan Bouwe van den Berg, Gabriel William Duchesne, Jean-Philippe Lessard</p>
                    <p><strong>Published:</strong> 7/22/2025</p>
                    <p><strong>Categories:</strong> math.DS, cs.NA, math.AP, math.NA, 34C37 (Primary) 37M21, 35B36, 65G40, 65T40, 42A10, 37C79 (Secondary)</p>
                    <p><strong>Summary:</strong> Homoclinic snaking is a widespread phenomenon observed in many pattern-forming systems. Demonstrating its occurrence in non-perturbative regimes has proven difficult, although a forcing theory has been developed based on the identification of patterned front solutions. These heteroclinic solutions are themselves challenging to analyze due to the nonlinear nature of the problem. In this paper, we use computer-assisted proofs to find parameterized loops of heteroclinic connections between equilibria and periodic orbits in time reversible systems. This leads to a proof of homoclinic snaking in both the Swift-Hohenberg and Gray-Scott problems. Our results demonstrate that computer-assisted proofs of continuous families of connecting orbits in nonlinear dynamical systems are a powerful tool for understanding global dynamics and their dependence on parameters.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2507.16797v1" target="_blank">No-go theorems for logical gates on product quantum codes</a></h3>
                    <p><strong>Authors:</strong> Xiaozhen Fu, Han Zheng, Zimu Li, Zi-Wen Liu</p>
                    <p><strong>Published:</strong> 7/22/2025</p>
                    <p><strong>Categories:</strong> quant-ph</p>
                    <p><strong>Summary:</strong> Quantum error-correcting codes are essential to the implementation of fault-tolerant quantum computation. Homological products of classical codes offer a versatile framework for constructing quantum error-correcting codes with desirable properties, especially quantum low-density parity check (qLDPC) codes. Based on extensions of the Bravyi--K\{o}nig theorem that encompass codes without geometric locality, we establish a series of general no-go theorems for fault-tolerant logical gates supported by hypergraph product codes. Specifically, we show that non-Clifford logical gates cannot be implemented transversally on hypergraph product codes of all product dimensions, and that the dimensions impose various limitations on the accessible level of the Clifford hierarchy gates by constant-depth local circuits. We also discuss examples both with and without geometric locality which attain the Clifford hierarchy bounds. Our results reveal fundamental restrictions on logical gates originating from highly general algebraic structures, extending beyond existing knowledge only in geometrically local, finite logical qubits, transversal, or 2-dimensional product cases, and may guide the vital study of fault-tolerant quantum computation with qLDPC codes.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2507.16795v1" target="_blank">Steering Out-of-Distribution Generalization with Concept Ablation Fine-Tuning</a></h3>
                    <p><strong>Authors:</strong> Helena Casademunt, Caden Juang, Adam Karvonen, Samuel Marks, Senthooran Rajamanoharan, Neel Nanda</p>
                    <p><strong>Published:</strong> 7/22/2025</p>
                    <p><strong>Categories:</strong> cs.LG, cs.AI, cs.CL</p>
                    <p><strong>Summary:</strong> Fine-tuning large language models (LLMs) can lead to unintended out-of-distribution generalization. Standard approaches to this problem rely on modifying training data, for example by adding data that better specify the intended generalization. However, this is not always practical. We introduce Concept Ablation Fine-Tuning (CAFT), a technique that leverages interpretability tools to control how LLMs generalize from fine-tuning, without needing to modify the training data or otherwise use data from the target distribution. Given a set of directions in an LLMs latent space corresponding to undesired concepts, CAFT works by ablating these concepts with linear projections during fine-tuning, steering the model away from unintended generalizations. We successfully apply CAFT to three fine-tuning tasks, including emergent misalignment, a phenomenon where LLMs fine-tuned on a narrow task generalize to give egregiously misaligned responses to general questions. Without any changes to the fine-tuning data, CAFT reduces misaligned responses by 10x without degrading performance on the training distribution. Overall, CAFT represents a novel approach for steering LLM generalization without modifying training data.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2507.16793v1" target="_blank">MTU: The Multifunction Tree Unit in zkSpeed for Accelerating HyperPlonk</a></h3>
                    <p><strong>Authors:</strong> Jianqiao Mo, Alhad Daftardar, Joey Ah-kiow, Kaiyue Guo, Benedikt BÃ¼nz, Siddharth Garg, Brandon Reagen</p>
                    <p><strong>Published:</strong> 7/22/2025</p>
                    <p><strong>Categories:</strong> cs.AR</p>
                    <p><strong>Summary:</strong> Zero-Knowledge Proofs (ZKPs) are critical for privacy preservation and verifiable computation. Many ZKPs rely on kernels such as the SumCheck protocol and Merkle Tree commitments, which enable their security properties. These kernels exhibit balanced binary tree computational patterns, which enable efficient hardware acceleration. Prior work has investigated accelerating these kernels as part of an overarching ZKP protocol; however, a focused study of how to best exploit the underlying tree pattern for hardware efficiency remains limited. We conduct a systematic evaluation of these tree-based workloads under different traversal strategies, analyzing performance on multi-threaded CPUs and a hardware accelerator, the Multifunction Tree Unit (MTU). We introduce a hardware-friendly Hybrid Traversal for binary tree that improves parallelism and scalability while significantly reducing memory traffic on hardware. Our results show that MTU achieves up to 1478$\times$ speedup over CPU at DDR-level bandwidth and that our hybrid traversal outperforms as standalone approach by up to 3$\times$. These findings offer practical guidance for designing efficient hardware accelerators for ZKP workloads with binary tree structures.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2507.16791v1" target="_blank">Super-capacitors interfaced with quantum dots at the electrolyte/electrode interface: capacitance gain and fluorescence line-width narrowing</a></h3>
                    <p><strong>Authors:</strong> H. Grebel</p>
                    <p><strong>Published:</strong> 7/22/2025</p>
                    <p><strong>Categories:</strong> physics.chem-ph, physics.app-ph</p>
                    <p><strong>Summary:</strong> In the past, we have observed an overall capacitance enhancement in super capacitors (S-C) when placing small amount of conductive colloids at the electrolyte/electrode interface with a mass ratio of 1:5000 to the electrode mass. The capacitance peaked at particular colloid concentration and the enhancement was attributed to local field effects by formation of an array of colloids at the interface. Here we show that fluorescing semiconductor quantum dots (QDs) at the interface exhibit similar effects on the cell capacitance with a capacitance amplification, measure by cyclic voltammetry (C-V), of more than 2.5 at a scan rate of 100 mV/s. Embedding QDs at the electrolyte/electrode interface has an added value that it may be further enhanced by white light and indeed this is the case here. The effect was also correlated with resonating effect (large signal enhancement) and in the case of wet samples, with line narrowing of the dots fluorescence; the latter indicates a substantial fluorescence gain. Probing the electrolyte/electrode interface with fluorescing materials adds to our basic knowledge of the interface and could be useful for light-sensitive S-C cells.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2507.16790v1" target="_blank">Enhancing Domain Diversity in Synthetic Data Face Recognition with Dataset Fusion</a></h3>
                    <p><strong>Authors:</strong> Anjith George, Sebastien Marcel</p>
                    <p><strong>Published:</strong> 7/22/2025</p>
                    <p><strong>Categories:</strong> cs.CV</p>
                    <p><strong>Summary:</strong> While the accuracy of face recognition systems has improved significantly in recent years, the datasets used to train these models are often collected through web crawling without the explicit consent of users, raising ethical and privacy concerns. To address this, many recent approaches have explored the use of synthetic data for training face recognition models. However, these models typically underperform compared to those trained on real-world data. A common limitation is that a single generator model is often used to create the entire synthetic dataset, leading to model-specific artifacts that may cause overfitting to the generators inherent biases and artifacts. In this work, we propose a solution by combining two state-of-the-art synthetic face datasets generated using architecturally distinct backbones. This fusion reduces model-specific artifacts, enhances diversity in pose, lighting, and demographics, and implicitly regularizes the face recognition model by emphasizing identity-relevant features. We evaluate the performance of models trained on this combined dataset using standard face recognition benchmarks and demonstrate that our approach achieves superior performance across many of these benchmarks.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2507.16787v1" target="_blank">Quantum thermodynamics in a rotating BTZ black hole spacetime</a></h3>
                    <p><strong>Authors:</strong> Wenjing Chen, Yixuan Ma, Si-Wei Han, Zihao Wang, Jun Feng</p>
                    <p><strong>Published:</strong> 7/22/2025</p>
                    <p><strong>Categories:</strong> hep-th, gr-qc, quant-ph</p>
                    <p><strong>Summary:</strong> We address the problem of the thermalization process for an Unruh-DeWitt (UDW) detector outside a BTZ black hole, from a perspective of quantum thermodynamics. In the context of an open quantum system, we derive the complete dynamics of the detector, which encodes a complicated response to scalar background fields. Using various information theory tools, such as quantum relative entropy, quantum heat, coherence, quantum Fisher information, and quantum speed of evolution, we examined three quantum thermodynamic laws for the UDW detector, where the influences from BTZ angular momentum and Hawking radiation are investigated. In particular, based on information geometry theory, we find an intrinsic asymmetry in the detectors thermolization process as it undergoes Hawking radiation from the BTZ black hole. In particular, we find that the detector consistently heats faster than it cools, analogous to the quantum Mpemba effect for nonequilibrium systems. Moreover, we demonstrate that the spin of a black hole significantly influences the magnitude of the asymmetry, while preserving the dominance of heating over cooling.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2507.16786v1" target="_blank">Broadband Relaxation Dynamics of Boron-Vacancy Centers in Hexagonal Boron Nitride</a></h3>
                    <p><strong>Authors:</strong> Abhishek Bharatbhai Solanki, Yueh-Chun Wu, Hamza Ather, Priyo Adhikary, Aravindh Shankar, Ian Gallagher, Xingyu Gao, Owen M. Matthiessen, Demid Sychev, Alexei Lagoutchev, Tongcang Li, Yong P. Chen, Vladimir M. Shalaev, Benjamin Lawrie, Pramey Upadhyaya</p>
                    <p><strong>Published:</strong> 7/22/2025</p>
                    <p><strong>Categories:</strong> quant-ph</p>
                    <p><strong>Summary:</strong> The negatively charged boron vacancy center ($\mathrm{V_B^-}$) in hexagonal boron nitride ($\mathrm{hBN}$) has attracted attention for its potential applications in quantum sensing. While GHz-scale sensing at low magnetic fields has been demonstrated with these defects, their behavior at high fields remains largely unexplored. We investigate the spin relaxation dynamics of $\mathrm{V_B^-}$ centers over temperatures of $15-250$ K and magnetic fields of up to $7$ T, corresponding to a ground-state splitting of $\sim 200$ GHz. Our results uncover distinct relaxation regimes, transitioning from spin-spin-interaction-driven and disorder-induced stretched exponential dynamics at low temperatures and fields to relaxation dominated by single-phonon processes at elevated magnetic fields. We extract temperature- and magnetic-field-dependent scaling behaviors of the relaxation rate to provide a quantitative picture of the interactions between $\mathrm{V_B^-}$ centers and their environment. Our results pave the way towards high-field, sub-terahertz quantum sensors based on two-dimensional spin-defect platforms.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2507.16784v1" target="_blank">Beyond Context Limits: Subconscious Threads for Long-Horizon Reasoning</a></h3>
                    <p><strong>Authors:</strong> Hongyin Luo, Nathaniel Morgan, Tina Li, Derek Zhao, Ai Vy Ngo, Philip Schroeder, Lijie Yang, Assaf Ben-Kish, Jack OBrien, James Glass</p>
                    <p><strong>Published:</strong> 7/22/2025</p>
                    <p><strong>Categories:</strong> cs.CL</p>
                    <p><strong>Summary:</strong> To break the context limits of large language models (LLMs) that bottleneck reasoning accuracy and efficiency, we propose the Thread Inference Model (TIM), a family of LLMs trained for recursive and decompositional problem solving, and TIMRUN, an inference runtime enabling long-horizon structured reasoning beyond context limits. Together, TIM hosted on TIMRUN supports virtually unlimited working memory and multi-hop tool calls within a single language model inference, overcoming output limits, positional-embedding constraints, and GPU-memory bottlenecks. Performance is achieved by modeling natural language as reasoning trees measured by both length and depth instead of linear sequences. The reasoning trees consist of tasks with thoughts, recursive subtasks, and conclusions based on the concept we proposed in Schroeder et al, 2025. During generation, we maintain a working memory that retains only the key-value states of the most relevant context tokens, selected by a rule-based subtask-pruning mechanism, enabling reuse of positional embeddings and GPU memory pages throughout reasoning. Experimental results show that our system sustains high inference throughput, even when manipulating up to 90% of the KV cache in GPU memory. It also delivers accurate reasoning on mathematical tasks and handles information retrieval challenges that require long-horizon reasoning and multi-hop tool use.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2507.16783v1" target="_blank">Quantum teleportation of an elemental silicon nanophotonic CNOT gate</a></h3>
                    <p><strong>Authors:</strong> Kai-Chi Chang, Xiang Cheng, Felix Ribuot-Hirsch, Murat Can Sarihan, Yujie Chen, Jaime Gonzalo Flor Flores, Mingbin Yu, Patrick Guo-Qiang Lo, Dim-Lee Kwong, Chee Wei Wong</p>
                    <p><strong>Published:</strong> 7/22/2025</p>
                    <p><strong>Categories:</strong> quant-ph</p>
                    <p><strong>Summary:</strong> Large-scale quantum computers possess the capacity to effectively tackle practical problems that can be insurmountable for classical computers. The main challenge in building these quantum computers is to realize scalable modules for remote qubits and entanglement. By assembling small, specialized parts into a larger architecture, the modular approach mitigates complexity and uncertainty. Such a distributed architecture requires non-local quantum gate operations between remote qubits. An essential method for implementing such operations, known as quantum gate teleportation, requires only local operations, classical communication, and shared entanglement. Till today, the quantum gate teleportation using a photonic chip has remained elusive. Here we experimentally demonstrate the quantum teleportation of an on-chip controlled-NOT (CNOT) gate, assisted with the scalable silicon chip platform, high-fidelity local quantum logic gates, linear optical components, post-selected entanglement, and coincidence measurements from photonic qubits. First, we measure and characterize our teleported chip-scale CNOT gate with an average truth table fidelity of 93.1 +- 0.3%. Second, for different input polarization states, we obtain an average quantum state fidelity of 87.0 +- 2.2% with our teleported on-chip CNOT gate. Third, we use our non-local CNOT gate for remote entanglement creation of four Bell states, with an average quantum state fidelity of 86.2 +- 0.8%. Fourthly, we fully characterize our teleported on-chip CNOT gate with a quantum process fidelity 83.1 +- 2.0%, and an average non-local CNOT gate fidelity of 86.5 +- 2.2%. Our teleported photonic on-chip quantum logic gate could be extended both to multiple qubits and chip-scale modules towards fault-tolerant and large-scale distributed quantum computation.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2507.16782v1" target="_blank">Task-Specific Zero-shot Quantization-Aware Training for Object Detection</a></h3>
                    <p><strong>Authors:</strong> Changhao Li, Xinrui Chen, Ji Wang, Kang Zhao, Jianfei Chen</p>
                    <p><strong>Published:</strong> 7/22/2025</p>
                    <p><strong>Categories:</strong> cs.CV</p>
                    <p><strong>Summary:</strong> Quantization is a key technique to reduce network size and computational complexity by representing the network parameters with a lower precision. Traditional quantization methods rely on access to original training data, which is often restricted due to privacy concerns or security challenges. Zero-shot Quantization (ZSQ) addresses this by using synthetic data generated from pre-trained models, eliminating the need for real training data. Recently, ZSQ has been extended to object detection. However, existing methods use unlabeled task-agnostic synthetic images that lack the specific information required for object detection, leading to suboptimal performance. In this paper, we propose a novel task-specific ZSQ framework for object detection networks, which consists of two main stages. First, we introduce a bounding box and category sampling strategy to synthesize a task-specific calibration set from the pre-trained network, reconstructing object locations, sizes, and category distributions without any prior knowledge. Second, we integrate task-specific training into the knowledge distillation process to restore the performance of quantized detection networks. Extensive experiments conducted on the MS-COCO and Pascal VOC datasets demonstrate the efficiency and state-of-the-art performance of our method. Our code is publicly available at: https://github.com/DFQ-Dojo/dfq-toolkit .</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2507.16781v1" target="_blank">Cooling Matters: Benchmarking Large Language Models and Vision-Language Models on Liquid-Cooled Versus Air-Cooled H100 GPU Systems</a></h3>
                    <p><strong>Authors:</strong> Imran Latif, Muhammad Ali Shafique, Hayat Ullah, Alex C. Newkirk, Xi Yu, Arslan Munir</p>
                    <p><strong>Published:</strong> 7/22/2025</p>
                    <p><strong>Categories:</strong> cs.DC</p>
                    <p><strong>Summary:</strong> The unprecedented growth in artificial intelligence (AI) workloads, recently dominated by large language models (LLMs) and vision-language models (VLMs), has intensified power and cooling demands in data centers. This study benchmarks LLMs and VLMs on two HGX nodes, each with 8x NVIDIA H100 graphics processing units (GPUs), using liquid and air cooling. Leveraging GPU Burn, Weights and Biases, and IPMItool, we collect detailed thermal, power, and computation data. Results show that the liquid-cooled systems maintain GPU temperatures between 41-50 degrees Celsius, while the air-cooled counterparts fluctuate between 54-72 degrees Celsius under load. This thermal stability of liquid-cooled systems yields 17 percent higher performance (54 TFLOPs per GPU vs. 46 TFLOPs per GPU), improved performance per watt, reduced energy overhead, and greater system efficiency than the air-cooled counterparts. These findings underscore the energy and sustainability benefits of liquid cooling, offering a compelling path forward for hyperscale data centers s</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2507.16780v1" target="_blank">On the consistency of non-commutative geometry inspired Reissner-NordstrÃ¶m black hole solution</a></h3>
                    <p><strong>Authors:</strong> Gokhan Alkac, Murat Mesta, Gonul Unal</p>
                    <p><strong>Published:</strong> 7/22/2025</p>
                    <p><strong>Categories:</strong> gr-qc, hep-th</p>
                    <p><strong>Summary:</strong> We revisit the non-commutative geometry inspired Reissner-Nordstr\{o}m black hole solution obtained by smearing the point sources with a Gaussian distribution. We show that while the form of the metric function and the physical properties derived from that remain valid, not all the components of Einstein equations are satisfied. We construct an improved energy-momentum tensor that consistently satisfies Einstein equations and show that it leads to a different prediction for the region where the strong energy condition is violated. For certain choice of parameters, our proposal predicts the violation of the energy condition outside the Cauchy horizon, which might be important for observational signatures.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2507.16779v1" target="_blank">Improving U-Net Confidence on TEM Image Data with L2-Regularization, Transfer Learning, and Deep Fine-Tuning</a></h3>
                    <p><strong>Authors:</strong> Aiden Ochoa, Xinyuan Xu, Xing Wang</p>
                    <p><strong>Published:</strong> 7/22/2025</p>
                    <p><strong>Categories:</strong> eess.IV, cs.CV</p>
                    <p><strong>Summary:</strong> With ever-increasing data volumes, it is essential to develop automated approaches for identifying nanoscale defects in transmission electron microscopy (TEM) images. However, compared to features in conventional photographs, nanoscale defects in TEM images exhibit far greater variation due to the complex contrast mechanisms and intricate defect structures. These challenges often result in much less labeled data and higher rates of annotation errors, posing significant obstacles to improving machine learning model performance for TEM image analysis. To address these limitations, we examined transfer learning by leveraging large, pre-trained models used for natural images. We demonstrated that by using the pre-trained encoder and L2-regularization, semantically complex features are ignored in favor of simpler, more reliable cues, substantially improving the model performance. However, this improvement cannot be captured by conventional evaluation metrics such as F1-score, which can be skewed by human annotation errors treated as ground truth. Instead, we introduced novel evaluation metrics that are independent of the annotation accuracy. Using grain boundary detection in UO2 TEM images as a case study, we found that our approach led to a 57% improvement in defect detection rate, which is a robust and holistic measure of model performance on the TEM dataset used in this work. Finally, we showed that model self-confidence is only achieved through transfer learning and fine-tuning of very deep layers.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2507.16773v1" target="_blank">When LLMs Copy to Think: Uncovering Copy-Guided Attacks in Reasoning LLMs</a></h3>
                    <p><strong>Authors:</strong> Yue Li, Xiao Li, Hao Wu, Yue Zhang, Fengyuan Xu, Xiuzhen Cheng, Sheng Zhong</p>
                    <p><strong>Published:</strong> 7/22/2025</p>
                    <p><strong>Categories:</strong> cs.CR</p>
                    <p><strong>Summary:</strong> Large Language Models (LLMs) have become integral to automated code analysis, enabling tasks such as vulnerability detection and code comprehension. However, their integration introduces novel attack surfaces. In this paper, we identify and investigate a new class of prompt-based attacks, termed Copy-Guided Attacks (CGA), which exploit the inherent copying tendencies of reasoning-capable LLMs. By injecting carefully crafted triggers into external code snippets, adversaries can induce the model to replicate malicious content during inference. This behavior enables two classes of vulnerabilities: inference length manipulation, where the model generates abnormally short or excessively long reasoning traces; and inference result manipulation, where the model produces misleading or incorrect conclusions. We formalize CGA as an optimization problem and propose a gradient-based approach to synthesize effective triggers. Empirical evaluation on state-of-the-art reasoning LLMs shows that CGA reliably induces infinite loops, premature termination, false refusals, and semantic distortions in code analysis tasks. While highly effective in targeted settings, we observe challenges in generalizing CGA across diverse prompts due to computational constraints, posing an open question for future research. Our findings expose a critical yet underexplored vulnerability in LLM-powered development pipelines and call for urgent advances in prompt-level defense mechanisms.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2507.16771v1" target="_blank">A Partitioned Sparse Variational Gaussian Process for Fast, Distributed Spatial Modeling</a></h3>
                    <p><strong>Authors:</strong> Michael Grosskopf, Kellin Rumsey, Ayan Biswas, Earl Lawrence</p>
                    <p><strong>Published:</strong> 7/22/2025</p>
                    <p><strong>Categories:</strong> cs.LG, stat.AP, stat.ML</p>
                    <p><strong>Summary:</strong> The next generation of Department of Energy supercomputers will be capable of exascale computation. For these machines, far more computation will be possible than that which can be saved to disk. As a result, users will be unable to rely on post-hoc access to data for uncertainty quantification and other statistical analyses and there will be an urgent need for sophisticated machine learning algorithms which can be trained in situ. Algorithms deployed in this setting must be highly scalable, memory efficient and capable of handling data which is distributed across nodes as spatially contiguous partitions. One suitable approach involves fitting a sparse variational Gaussian process (SVGP) model independently and in parallel to each spatial partition. The resulting model is scalable, efficient and generally accurate, but produces the undesirable effect of constructing discontinuous response surfaces due to the disagreement between neighboring models at their shared boundary. In this paper, we extend this idea by allowing for a small amount of communication between neighboring spatial partitions which encourages better alignment of the local models, leading to smoother spatial predictions and a better fit in general. Due to our decentralized communication scheme, the proposed extension remains highly scalable and adds very little overhead in terms of computation (and none, in terms of memory). We demonstrate this Partitioned SVGP (PSVGP) approach for the Energy Exascale Earth System Model (E3SM) and compare the results to the independent SVGP case.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2507.16763v1" target="_blank">Generalized non-reciprocal phase transitions in multipopulation systems</a></h3>
                    <p><strong>Authors:</strong> Cheyne Weis, Ryo Hanai</p>
                    <p><strong>Published:</strong> 7/22/2025</p>
                    <p><strong>Categories:</strong> cond-mat.soft, cond-mat.stat-mech</p>
                    <p><strong>Summary:</strong> Non-reciprocal interactions are prevalent in various complex systems leading to phenomena that cannot be described by traditional equilibrium statistical physics. Although non-reciprocally interacting systems composed of two populations have been closely studied, the physics of non-reciprocal systems with a general number of populations is not well explored despite the potential relevance to biological systems, active matter, and driven-dissipative quantum materials. In this work, we investigate the generic features of the phases and phase transitions that emerge in $O(2)$ symmetric many-body systems with multiple non-reciprocally coupled populations, applicable to microscopic models such as networks of oscillators, flocking models, and more generally systems where each agent has a phase variable. Using symmetry and topology of the possible orbits, we systematically show that a rich variety of time-dependent phases and phase transitions arise. Examples include multipopulation chiral phases that are distinct from their two-population counterparts that emerge via a phase transition characterized by critical exceptional points, as well as limit cycle saddle-node bifurcation and Hopf bifurcation. Interestingly, we find a phase transition that dynamically restores the $\mathbb{Z}_2$ symmetry occurs via a homoclinic orbit bifurcation, where the two $\mathbb{Z}_2$ broken orbits merge at the phase transition point, providing a general route to homoclinic chaos in the order parameter dynamics for $N\geq4$ populations. The results contribute to the understanding of the novel collective behavior and provide formalism for classifying dynamic phases and their transitions in systems driven by non-reciprocal interactions.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2507.16761v1" target="_blank">Faithful, Interpretable Chest X-ray Diagnosis with Anti-Aliased B-cos Networks</a></h3>
                    <p><strong>Authors:</strong> Marcel Kleinmann, Shashank Agnihotri, Margret Keuper</p>
                    <p><strong>Published:</strong> 7/22/2025</p>
                    <p><strong>Categories:</strong> cs.CV, cs.LG</p>
                    <p><strong>Summary:</strong> Faithfulness and interpretability are essential for deploying deep neural networks (DNNs) in safety-critical domains such as medical imaging. B-cos networks offer a promising solution by replacing standard linear layers with a weight-input alignment mechanism, producing inherently interpretable, class-specific explanations without post-hoc methods. While maintaining diagnostic performance competitive with state-of-the-art DNNs, standard B-cos models suffer from severe aliasing artifacts in their explanation maps, making them unsuitable for clinical use where clarity is essential. Additionally, the original B-cos formulation is limited to multi-class settings, whereas chest X-ray analysis often requires multi-label classification due to co-occurring abnormalities. In this work, we address both limitations: (1) we introduce anti-aliasing strategies using FLCPooling (FLC) and BlurPool (BP) to significantly improve explanation quality, and (2) we extend B-cos networks to support multi-label classification. Our experiments on chest X-ray datasets demonstrate that the modified $\text{B-cos}_\text{FLC}$ and $\text{B-cos}_\text{BP}$ preserve strong predictive performance while providing faithful and artifact-free explanations suitable for clinical application in multi-label settings. Code available at: $\href{https://github.com/mkleinma/B-cos-medical-paper}{GitHub repository}$.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2507.16760v1" target="_blank">Ultracold high-spin $Î£$-state polar molecules for new physics searches</a></h3>
                    <p><strong>Authors:</strong> Alessio Ciamei, Adam Koza, Marcin Gronowski, MichaÅ‚ Tomza</p>
                    <p><strong>Published:</strong> 7/22/2025</p>
                    <p><strong>Categories:</strong> cond-mat.quant-gas, physics.atom-ph</p>
                    <p><strong>Summary:</strong> We propose high-spin $\Sigma$-state polar molecules assembled from ultracold atoms to probe charge-parity violating physics beyond the Standard Model. We identify YbCr as a prime candidate to search for the electric dipole moment of the electron. We show that the combination of relativistic ytterbium and high-spin chromium, amenable to magneto-association, leads to molecules with easy-to-polarize parity doublets and large intramolecular electric fields. Based on \textit{ab initio} results for molecular constants, we predict a sensitivity of $\delta d_{\textrm{e}}= ( 6 \times 10^{-31} / \sqrt{n_{\mathrm{day}}})\,e\,\mathrm{cm}$ via standard spin-precession measurements, we assess the experimental feasibility, and discuss potential extensions to more advanced quantum control as well as searches of the nuclear magnetic quadrupole moment. This work paves the way to next-generation searches for new physics with ultracold molecules in both the leptonic and hadronic sectors.</p>
                
            
                
                    <h3><a href="http://dx.doi.org/10.1038/s41563-025-02290-y" target="_blank">Atomic-scale Frustrated Josephson Coupling and Multi-condensate Visualization in FeSe</a></h3>
                    <p><strong>Authors:</strong> Nileema Sharma, Matthew Toole, James McKenzie, Sheng Ran, Xiaolong Liu</p>
                    <p><strong>Published:</strong> 7/22/2025</p>
                    <p><strong>Categories:</strong> cond-mat.supr-con, cond-mat.mtrl-sci</p>
                    <p><strong>Summary:</strong> In a Josephson junction involving multi-band superconductors, competition between inter-band and inter-junction Josephson coupling gives rise to frustration and spatial disjunction of superfluid densities among superconducting condensates. Such frustrated coupling manifests as quantum interference of Josephson currents from different tunneling channels and becomes tunable if channel transparency can be varied. To explore these unconventional effects in the prototypical $s^\pm$-wave superconductor FeSe, we use atomic resolution scanned Josephson tunneling microscopy SJTM for condensate resolved imaging and junction tuning -- capabilities unattainable in macroscopic Josephson devices with fixed characteristics. We quantitatively demonstrate frustrated Josephson tunneling by examining two tunneling inequalities. The relative transparency of two parallel tunneling pathways is found tunable, revealing a tendency towards a 0-pi transition with decreasing SJTM junction resistance. Simultaneous visualization of both superconducting condensates reveals anti correlated superfluid modulations, highlighting the role of inter-band scattering. Our study establishes SJTM as a powerful tool enabling new research frontiers of multi condensate superconductivity.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2507.16756v1" target="_blank">Efficient Bayesian Inference for Discretely Observed Continuous Time Markov Chains</a></h3>
                    <p><strong>Authors:</strong> Tao Tang, Lachlan Astfalck, David Dunson</p>
                    <p><strong>Published:</strong> 7/22/2025</p>
                    <p><strong>Categories:</strong> stat.ME, stat.CO</p>
                    <p><strong>Summary:</strong> Inference for continuous-time Markov chains (CTMCs) becomes challenging when the process is only observed at discrete time points. The exact likelihood is intractable, and existing methods often struggle even in medium-dimensional state-spaces. We propose a scalable Bayesian framework for CTMC inference based on a pseudo-likelihood that bypasses the need for the full intractable likelihood. Our approach jointly estimates the probability transition matrix and a biorthogonal spectral decomposition of the generator, enabling an efficient Gibbs sampling procedure that obeys embeddability. Existing methods typically integrate out the unobserved transitions, which becomes computationally burdensome as the number of data or dimensions increase. The computational cost of our method is near-invariant in the number of data and scales well to medium-high dimensions. We justify our pseudo-likelihood approach by establishing theoretical guarantees, including a Bernstein-von Mises theorem for the probability transition matrix and posterior consistency for the spectral parameters of the generator. Through simulation and applications, we showcase the flexibility and robustness of our approach, offering a tractable and scalable approach to Bayesian inference for CTMCs.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2507.16755v1" target="_blank">The GameTheory package for Macaulay2</a></h3>
                    <p><strong>Authors:</strong> Erin Connelly, Vincenzo Galgano, Zhuang He, Giacomo Maletto, Elke Neuhaus, Irem Portakal, Hannah Tillmann-Morris, Chenyang Zhao</p>
                    <p><strong>Published:</strong> 7/22/2025</p>
                    <p><strong>Categories:</strong> math.AG, 14A10, 91A06, 52B05, 62R01, 14P05</p>
                    <p><strong>Summary:</strong> We describe the GameTheory package version 1.0 for computing equilibria in game theory available since version 1.25.05 of Macaulay2. We briefly explain the four equilibrium notions, Nash, correlated, dependency, and conditional independence, and demonstrate their implementation in the package with examples.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2507.16753v1" target="_blank">CMP: A Composable Meta Prompt for SAM-Based Cross-Domain Few-Shot Segmentation</a></h3>
                    <p><strong>Authors:</strong> Shuai Chen, Fanman Meng, Chunjin Yang, Haoran Wei, Chenhao Wu, Qingbo Wu, Hongliang Li</p>
                    <p><strong>Published:</strong> 7/22/2025</p>
                    <p><strong>Categories:</strong> cs.CV</p>
                    <p><strong>Summary:</strong> Cross-Domain Few-Shot Segmentation (CD-FSS) remains challenging due to limited data and domain shifts. Recent foundation models like the Segment Anything Model (SAM) have shown remarkable zero-shot generalization capability in general segmentation tasks, making it a promising solution for few-shot scenarios. However, adapting SAM to CD-FSS faces two critical challenges: reliance on manual prompt and limited cross-domain ability. Therefore, we propose the Composable Meta-Prompt (CMP) framework that introduces three key modules: (i) the Reference Complement and Transformation (RCT) module for semantic expansion, (ii) the Composable Meta-Prompt Generation (CMPG) module for automated meta-prompt synthesis, and (iii) the Frequency-Aware Interaction (FAI) module for domain discrepancy mitigation. Evaluations across four cross-domain datasets demonstrate CMPs state-of-the-art performance, achieving 71.8\% and 74.5\% mIoU in 1-shot and 5-shot scenarios respectively.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2507.16751v1" target="_blank">Many-Body Physics from Spin-Phonon Coupling in Rydberg Atom Arrays</a></h3>
                    <p><strong>Authors:</strong> Shuo Zhang, Langxuan Chen, Pengfei Zhang</p>
                    <p><strong>Published:</strong> 7/22/2025</p>
                    <p><strong>Categories:</strong> cond-mat.quant-gas, quant-ph</p>
                    <p><strong>Summary:</strong> The rapid advancement of quantum science and technology has established Rydberg atom arrays as a premier platform for exploring quantum many-body physics with exceptional precision and controllability. Traditionally, each atom is modeled as a spin degree of freedom with its spatial motion effectively frozen. This simplification has facilitated the discovery of a rich variety of novel equilibrium and non-equilibrium phases, including $\mathbb{Z}_{\text{N}}$ symmetry-breaking orders and quantum scars. In this work, we investigate the consequences of incorporating atomic vibrations in optical tweezers, which give rise to spin-phonon coupling. For systems in thermal equilibrium, we find that this coupling leads to a new symmetry-breaking phase in the weak driving limit, as a result of induced three-spin interactions. Furthermore, we show that the violation of quantum thermalization in $\mathbb{Z}_2$-ordered states is suppressed when spin-phonon coupling is introduced. Our results are readily testable in state-of-the-art Rydberg atom array experiments.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2507.16750v1" target="_blank">Global finite energy solutions of the Maxwell-scalar field system on the Einstein cylinder</a></h3>
                    <p><strong>Authors:</strong> Jean-Philippe Nicolas, Grigalius Taujanskas</p>
                    <p><strong>Published:</strong> 7/22/2025</p>
                    <p><strong>Categories:</strong> math.AP, gr-qc, 35A01, 35D30, 35P25, 35R05, 35R01, 35Q75, 83C99</p>
                    <p><strong>Summary:</strong> We prove the existence and uniqueness of global finite energy solutions of the Maxwell-scalar field system in Lorenz gauge on the Einstein cylinder. Our method is a combination of a conformal patching argument, the finite energy existence theorem in Lorenz gauge on Minkowski space of Selberg and Tesfahun, a careful localization of finite energy data, and null form estimates of Foschi-Klainerman type. Although we prove that the energy-carrying components of the solution maintain regularity, due to the incompleteness of the null structure in Lorenz gauge and the nature of our foliation-change arguments we find small losses of regularity in both the scalar field and the potential.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2507.16749v1" target="_blank">Bootstrapped Control Limits for Score-Based Concept Drift Control Charts</a></h3>
                    <p><strong>Authors:</strong> Jiezhong Wu, Daniel W. Apley</p>
                    <p><strong>Published:</strong> 7/22/2025</p>
                    <p><strong>Categories:</strong> stat.ME, stat.ML</p>
                    <p><strong>Summary:</strong> Monitoring for changes in a predictive relationship represented by a fitted supervised learning model (aka concept drift detection) is a widespread problem, e.g., for retrospective analysis to determine whether the predictive relationship was stable over the training data, for prospective analysis to determine when it is time to update the predictive model, for quality control of processes whose behavior can be characterized by a predictive relationship, etc. A general and powerful Fisher score-based concept drift approach has recently been proposed, in which concept drift detection reduces to detecting changes in the mean of the models score vector using a multivariate exponentially weighted moving average (MEWMA). To implement the approach, the initial data must be split into two subsets. The first subset serves as the training sample to which the model is fit, and the second subset serves as an out-of-sample test set from which the MEWMA control limit (CL) is determined. In this paper, we develop a novel bootstrap procedure for computing the CL. Our bootstrap CL provides much more accurate control of false-alarm rate, especially when the sample size and/or false-alarm rate is small. It also allows the entire initial sample to be used for training, resulting in a more accurate fitted supervised learning model. We show that a standard nested bootstrap (inner loop accounting for future data variability and outer loop accounting for training sample variability) substantially underestimates variability and develop a 632-like correction that appropriately accounts for this. We demonstrate the advantages with numerical examples.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2507.16748v1" target="_blank">Unpacking Ambiguity: The Interaction of Polysemous Discourse Markers and Non-DM Signals</a></h3>
                    <p><strong>Authors:</strong> Jingni Wu, Amir Zeldes</p>
                    <p><strong>Published:</strong> 7/22/2025</p>
                    <p><strong>Categories:</strong> cs.CL</p>
                    <p><strong>Summary:</strong> Discourse markers (DMs) like but or then are crucial for creating coherence in discourse, yet they are often replaced by or co-occur with non-DMs (in the morning can mean the same as then), and both can be ambiguous (since can refer to time or cause). The interaction mechanism between such signals remains unclear but pivotal for their disambiguation. In this paper we investigate the relationship between DM polysemy and co-occurrence of non-DM signals in English, as well as the influence of genre on these patterns. Using the framework of eRST, we propose a graded definition of DM polysemy, and conduct correlation and regression analyses to examine whether polysemous DMs are accompanied by more numerous and diverse non-DM signals. Our findings reveal that while polysemous DMs do co-occur with more diverse non-DMs, the total number of co-occurring signals does not necessarily increase. Moreover, genre plays a significant role in shaping DM-signal interactions.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2507.16746v1" target="_blank">Zebra-CoT: A Dataset for Interleaved Vision Language Reasoning</a></h3>
                    <p><strong>Authors:</strong> Ang Li, Charles Wang, Kaiyu Yue, Zikui Cai, Ollie Liu, Deqing Fu, Peng Guo, Wang Bill Zhu, Vatsal Sharan, Robin Jia, Willie Neiswanger, Furong Huang, Tom Goldstein, Micah Goldblum</p>
                    <p><strong>Published:</strong> 7/22/2025</p>
                    <p><strong>Categories:</strong> cs.CV, cs.CL, cs.LG</p>
                    <p><strong>Summary:</strong> Humans often use visual aids, for example diagrams or sketches, when solving complex problems. Training multimodal models to do the same, known as Visual Chain of Thought (Visual CoT), is challenging due to: (1) poor off-the-shelf visual CoT performance, which hinders reinforcement learning, and (2) the lack of high-quality visual CoT training data. We introduce $\textbf{Zebra-CoT}$, a diverse large-scale dataset with 182,384 samples, containing logically coherent interleaved text-image reasoning traces. We focus on four categories of tasks where sketching or visual reasoning is especially natural, spanning scientific questions such as geometry, physics, and algorithms; 2D visual reasoning tasks like visual search and jigsaw puzzles; 3D reasoning tasks including 3D multi-hop inference, embodied and robot planning; visual logic problems and strategic games like chess. Fine-tuning the Anole-7B model on the Zebra-CoT training corpus results in an improvement of +12% in our test-set accuracy and yields up to +13% performance gain on standard VLM benchmark evaluations. Fine-tuning Bagel-7B yields a model that generates high-quality interleaved visual reasoning chains, underscoring Zebra-CoTs effectiveness for developing multimodal reasoning abilities. We open-source our dataset and models to support development and evaluation of visual CoT.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2507.16744v1" target="_blank">Notes from the bulk</a></h3>
                    <p><strong>Authors:</strong> Erica Bertolini</p>
                    <p><strong>Published:</strong> 7/22/2025</p>
                    <p><strong>Categories:</strong> hep-th, cond-mat.mes-hall, cond-mat.str-el, gr-qc</p>
                    <p><strong>Summary:</strong> The scope of this Ph.D thesis is to study the effects of the presence of a boundary from a Quantum Field Theoretical perspective, searching for new physics and explanations of observed phenomena. In particular, thanks to the formal QFT setting, the issue of the existence of local, accelerated, edge modes in Hall systems is analyzed and understood in terms of the bulk-to-boundary approach as related to a curved background in topological QFTs with boundary. Within this formalism the induced metric on the boundary can be associated to the ad hoc potential introduced in the phenomenological models in order to obtain such non-constant edge velocities. This also leads to the prediction of local modes for Topological Insulators, and Quantum Spin Hall systems in general. The paradigm for which only topological QFTs have a physical content on the boundary is broken, and also non-Topological Quantum Field Theories such as fracton models and Linearized Gravity are shown to have non-trivial boundary dynamics. Indeed due to the breaking of their defining symmetry both models have a current algebra of the Kac-Moody type on the boundary. In the case of fractons this algebra is in a generalized form, which also appears in some kinds of higher order Topological Insulators, a sign of a possible relation between these materials and edge states of fracton quasiparticles. Concerning the theory of Linearized Gravity, instead, the algebra is a standard Kac-Moody one, whose presence was suspected, but never proved before. Physical results on the boundary range between condensed matter, elasticity and (massive) gravity models. A collateral result, which enrich this Thesis, is the building of a new covariant QFT for fractons with a peculiar gauge structure. This new model better highlight the properties of these quasiparticles.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2507.16743v1" target="_blank">Denoising-While-Completing Network (DWCNet): Robust Point Cloud Completion Under Corruption</a></h3>
                    <p><strong>Authors:</strong> Keneni W. Tesema, Lyndon Hill, Mark W. Jones, Gary K. L. Tam</p>
                    <p><strong>Published:</strong> 7/22/2025</p>
                    <p><strong>Categories:</strong> cs.CV</p>
                    <p><strong>Summary:</strong> Point cloud completion is crucial for 3D computer vision tasks in autonomous driving, augmented reality, and robotics. However, obtaining clean and complete point clouds from real-world environments is challenging due to noise and occlusions. Consequently, most existing completion networks -- trained on synthetic data -- struggle with real-world degradations. In this work, we tackle the problem of completing and denoising highly corrupted partial point clouds affected by multiple simultaneous degradations. To benchmark robustness, we introduce the Corrupted Point Cloud Completion Dataset (CPCCD), which highlights the limitations of current methods under diverse corruptions. Building on these insights, we propose DWCNet (Denoising-While-Completing Network), a completion framework enhanced with a Noise Management Module (NMM) that leverages contrastive learning and self-attention to suppress noise and model structural relationships. DWCNet achieves state-of-the-art performance on both clean and corrupted, synthetic and real-world datasets. The dataset and code will be publicly available at https://github.com/keneniwt/DWCNET-Robust-Point-Cloud-Completion-against-Corruptions</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2507.16742v1" target="_blank">Multiparameter estimation with position-momentum correlated Gaussian probes</a></h3>
                    <p><strong>Authors:</strong> JoÃ£o C. P. Porto, Carlos H. S. Vieira, Pedro R. Dieguez, Irismar G. da Paz, Lucas S. Marinho</p>
                    <p><strong>Published:</strong> 7/22/2025</p>
                    <p><strong>Categories:</strong> quant-ph</p>
                    <p><strong>Summary:</strong> Gaussian quantum probes have been widely used in quantum metrology and thermometry, where the goal is to estimate the temperature of an environment with which the probe interacts. It was recently shown that introducing initial position-momentum (PM) correlations in such probes can enhance the estimation precision compared to standard, uncorrelated Gaussian states. Motivated by these findings, we investigate whether PM correlations can also be advantageous in a simultaneous estimation setting, specifically, when estimating both the PM correlations themselves and the effective environment temperature that interacts with the probe. Using the Quantum Fisher Information Matrix, we derive new precision bounds for this joint estimation task. Additionally, we demonstrate that such correlations can serve as a resource to improve temperature estimation within this multiparameter context. Finally, we analyze the compatibility between the two parameters, establishing conditions under which the derived bounds can be saturated.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2507.16741v1" target="_blank">Parametric Amplification of Spin-Motion Coupling in Three-Dimensional Trapped-Ion Crystals</a></h3>
                    <p><strong>Authors:</strong> Samarth Hawaldar, N. Nikhil, Ana Maria Rey, John J. Bollinger, Athreya Shankar</p>
                    <p><strong>Published:</strong> 7/22/2025</p>
                    <p><strong>Categories:</strong> quant-ph</p>
                    <p><strong>Summary:</strong> Three-dimensional (3D) crystals offer a route to scale up trapped ion systems for quantum sensing and quantum simulation applications. However, engineering coherent spin-motion couplings and effective spin-spin interactions in large crystals poses technical challenges associated with decoherence and prolonged timescales to generate appreciable entanglement. Here, we explore the possibility to speed up these interactions in 3D crystals via parametric amplification. We derive a general Hamiltonian for the parametric amplification of spin-motion coupling that is applicable to crystals of any dimension in both rf Paul traps and Penning traps. Unlike in lower dimensional crystals, we find that the ability to faithfully (uniformly) amplify the spin-spin interactions in 3D crystals depends on the physical implementation of the spin-motion coupling. We consider the light-shift (LS) gate, and the so-called phase-insensitive and phase-sensitive M{\o}lmer-S{\o}rensen (MS) gates, and find that only the latter gate can be faithfully amplified in general 3D crystals. We discuss a situation where non-uniform amplification can be advantageous. We also reconsider the impact of counter-rotating terms on parametric amplification and find that they are not as detrimental as previous studies suggest.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2507.16737v1" target="_blank">Computational aspects of the trace norm contraction coefficient</a></h3>
                    <p><strong>Authors:</strong> Idris Delsol, Omar Fawzi, Jan Kochanowski, Akshay Ramachandran</p>
                    <p><strong>Published:</strong> 7/22/2025</p>
                    <p><strong>Categories:</strong> quant-ph, cs.CC</p>
                    <p><strong>Summary:</strong> We show that approximating the trace norm contraction coefficient of a quantum channel within a constant factor is NP-hard. Equivalently, this shows that determining the optimal success probability for encoding a bit in a quantum system undergoing noise is NP-hard. This contrasts with the classical analogue of this problem that can clearly by solved efficiently. Our hardness results also hold for deciding if the contraction coefficient is equal to 1. As a consequence, we show that deciding if a non-commutative graph has an independence number of at least 2 is NP-hard. In addition, we establish a converging hierarchy of semidefinite programming upper bounds on the contraction coefficient.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2507.16736v1" target="_blank">DFR: A Decompose-Fuse-Reconstruct Framework for Multi-Modal Few-Shot Segmentation</a></h3>
                    <p><strong>Authors:</strong> Shuai Chen, Fanman Meng, Xiwei Zhang, Haoran Wei, Chenhao Wu, Qingbo Wu, Hongliang Li</p>
                    <p><strong>Published:</strong> 7/22/2025</p>
                    <p><strong>Categories:</strong> cs.CV</p>
                    <p><strong>Summary:</strong> This paper presents DFR (Decompose, Fuse and Reconstruct), a novel framework that addresses the fundamental challenge of effectively utilizing multi-modal guidance in few-shot segmentation (FSS). While existing approaches primarily rely on visual support samples or textual descriptions, their single or dual-modal paradigms limit exploitation of rich perceptual information available in real-world scenarios. To overcome this limitation, the proposed approach leverages the Segment Anything Model (SAM) to systematically integrate visual, textual, and audio modalities for enhanced semantic understanding. The DFR framework introduces three key innovations: 1) Multi-modal Decompose: a hierarchical decomposition scheme that extracts visual region proposals via SAM, expands textual semantics into fine-grained descriptors, and processes audio features for contextual enrichment; 2) Multi-modal Contrastive Fuse: a fusion strategy employing contrastive learning to maintain consistency across visual, textual, and audio modalities while enabling dynamic semantic interactions between foreground and background features; 3) Dual-path Reconstruct: an adaptive integration mechanism combining semantic guidance from tri-modal fused tokens with geometric cues from multi-modal location priors. Extensive experiments across visual, textual, and audio modalities under both synthetic and real settings demonstrate DFRs substantial performance improvements over state-of-the-art methods.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2507.16735v1" target="_blank">AI-enhanced conversational agents for personalized asthma support Factors for engagement, value and efficacy</a></h3>
                    <p><strong>Authors:</strong> Laura Moradbakhti, Dorian Peters, Jennifer K. Quint, BjÃ¶rn Schuller, Darren Cook, Rafael A. Calvo</p>
                    <p><strong>Published:</strong> 7/22/2025</p>
                    <p><strong>Categories:</strong> cs.HC, cs.AI, cs.CY, cs.ET, K.4.2; J.3</p>
                    <p><strong>Summary:</strong> Asthma-related deaths in the UK are the highest in Europe, and only 30% of patients access basic care. There is a need for alternative approaches to reaching people with asthma in order to provide health education, self-management support and bridges to care. Automated conversational agents (specifically, mobile chatbots) present opportunities for providing alternative and individually tailored access to health education, self-management support and risk self-assessment. But would patients engage with a chatbot, and what factors influence engagement? We present results from a patient survey (N=1257) devised by a team of asthma clinicians, patients, and technology developers, conducted to identify optimal factors for efficacy, value and engagement for a chatbot. Results indicate that most adults with asthma (53%) are interested in using a chatbot and the patients most likely to do so are those who believe their asthma is more serious and who are less confident about self-management. Results also indicate enthusiasm for 24/7 access, personalisation, and for WhatsApp as the preferred access method (compared to app, voice assistant, SMS or website). Obstacles to uptake include security/privacy concerns and skepticism of technological capabilities. We present detailed findings and consolidate these into 7 recommendations for developers for optimising efficacy of chatbot-based health support.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2507.16732v1" target="_blank">HarmonPaint: Harmonized Training-Free Diffusion Inpainting</a></h3>
                    <p><strong>Authors:</strong> Ying Li, Xinzhe Li, Yong Du, Yangyang Xu, Junyu Dong, Shengfeng He</p>
                    <p><strong>Published:</strong> 7/22/2025</p>
                    <p><strong>Categories:</strong> cs.CV</p>
                    <p><strong>Summary:</strong> Existing inpainting methods often require extensive retraining or fine-tuning to integrate new content seamlessly, yet they struggle to maintain coherence in both structure and style between inpainted regions and the surrounding background. Motivated by these limitations, we introduce HarmonPaint, a training-free inpainting framework that seamlessly integrates with the attention mechanisms of diffusion models to achieve high-quality, harmonized image inpainting without any form of training. By leveraging masking strategies within self-attention, HarmonPaint ensures structural fidelity without model retraining or fine-tuning. Additionally, we exploit intrinsic diffusion model properties to transfer style information from unmasked to masked regions, achieving a harmonious integration of styles. Extensive experiments demonstrate the effectiveness of HarmonPaint across diverse scenes and styles, validating its versatility and performance.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2507.16731v1" target="_blank">Collaborative Inference and Learning between Edge SLMs and Cloud LLMs: A Survey of Algorithms, Execution, and Open Challenges</a></h3>
                    <p><strong>Authors:</strong> Senyao Li, Haozhao Wang, Wenchao Xu, Rui Zhang, Song Guo, Jingling Yuan, Xian Zhong, Tianwei Zhang, Ruixuan Li</p>
                    <p><strong>Published:</strong> 7/22/2025</p>
                    <p><strong>Categories:</strong> cs.DC</p>
                    <p><strong>Summary:</strong> As large language models (LLMs) evolve, deploying them solely in the cloud or compressing them for edge devices has become inadequate due to concerns about latency, privacy, cost, and personalization. This survey explores a collaborative paradigm in which cloud-based LLMs and edge-deployed small language models (SLMs) cooperate across both inference and training. We present a unified taxonomy of edge-cloud collaboration strategies. For inference, we categorize approaches into task assignment, task division, and mixture-based collaboration at both task and token granularity, encompassing adaptive scheduling, resource-aware offloading, speculative decoding, and modular routing. For training, we review distributed adaptation techniques, including parameter alignment, pruning, bidirectional distillation, and small-model-guided optimization. We further summarize datasets, benchmarks, and deployment cases, and highlight privacy-preserving methods and vertical applications. This survey provides the first systematic foundation for LLM-SLM collaboration, bridging system and algorithm co-design to enable efficient, scalable, and trustworthy edge-cloud intelligence.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2507.16725v1" target="_blank">RAVine: Reality-Aligned Evaluation for Agentic Search</a></h3>
                    <p><strong>Authors:</strong> Yilong Xu, Xiang Long, Zhi Zheng, Jinhua Gao</p>
                    <p><strong>Published:</strong> 7/22/2025</p>
                    <p><strong>Categories:</strong> cs.CL, cs.AI, cs.IR</p>
                    <p><strong>Summary:</strong> Agentic search, as a more autonomous and adaptive paradigm of retrieval augmentation, is driving the evolution of intelligent search systems. However, existing evaluation frameworks fail to align well with the goals of agentic search. First, the complex queries commonly used in current benchmarks often deviate from realistic user search scenarios. Second, prior approaches tend to introduce noise when extracting ground truth for end-to-end evaluations, leading to distorted assessments at a fine-grained level. Third, most current frameworks focus solely on the quality of final answers, neglecting the evaluation of the iterative process inherent to agentic search. To address these limitations, we propose RAVine -- a Reality-Aligned eValuation framework for agentic LLMs with search. RAVine targets multi-point queries and long-form answers that better reflect user intents, and introduces an attributable ground truth construction strategy to enhance the accuracy of fine-grained evaluation. Moreover, RAVine examines models interaction with search tools throughout the iterative process, and accounts for factors of efficiency. We benchmark a series of models using RAVine and derive several insights, which we hope will contribute to advancing the development of agentic search systems. The code and datasets are available at https://github.com/SwordFaith/RAVine.</p>
                
            
        
        
            <p><em>Generated by AI News Agent</em></p>
        
    

