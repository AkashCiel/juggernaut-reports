
    
        <h1>ðŸ¤– AI Research News Report</h1>
        <p>Sunday, September 21, 2025</p>
        <p>Topics: ai alignment research, quantum computing</p>
    
    
        20Research Papers
        2Topics Covered
        YesAI Summary
    
    
    
        <h2>ðŸ¤– AI Summary</h2>
        <h2>ai alignment research</h2>
<h3>Most Important Trends</h3>
<ol>
<li><p><strong>Advancements in Domain Adaptation and Calibration:</strong> Theres a noticeable trend toward improving AI models domain adaptability and calibration, especially in complex environments like open-vocabulary semantic segmentation and medical imaging. This is reflected in efforts to enhance models performance without direct access to source data, and to ensure reliability in predictions through better calibration techniques.</p>
</li>
<li><p><strong>Cross-Modal Learning:</strong> Researchers are increasingly leveraging cross-modal learning strategies, as seen in event-based monocular depth estimation and multimodal piano performance analysis. These efforts aim to integrate different types of sensory data to improve overall model understanding and performance.</p>
</li>
<li><p><strong>Scalability and Data Utilization:</strong> There is a significant focus on scaling AI systems, as evidenced by large-scale datasets for computer use agents and frameworks for mitigating data contamination in large language models. These trends highlight the importance of robust data foundations for advancing AI capabilities.</p>
</li>
<li><p><strong>AI in Real-World Applications:</strong> AIs role in practical applications like trajectory prediction for autonomous systems and assessing historical oppression using language models underscores the push to apply AI in complex, real-world scenarios.</p>
</li>
</ol>
<h3>Breakthroughs</h3>
<ol>
<li><p><strong>VocAlign Framework:</strong> The introduction of a novel source-free domain adaptation framework, VocAlign, for semantic segmentation in vision-language models represents a significant breakthrough. It enhances pseudo-label generation and achieves notable performance improvements on key benchmarks.</p>
</li>
<li><p><strong>CalibPrompt for Med-VLMs:</strong> The development of CalibPrompt addresses calibration challenges in medical vision-language models, improving prediction reliability without compromising accuracy, which is crucial for clinical applications.</p>
</li>
<li><p><strong>Cross-Modal Distillation for Depth Estimation:</strong> The innovative use of cross-modal distillation to generate dense proxy labels from event cameras marks a breakthrough in overcoming data limitations in depth estimation, achieving state-of-the-art results without requiring expensive annotations.</p>
</li>
<li><p><strong>ScaleCUA for Open-Source CUAs:</strong> ScaleCUAs development showcases a major advancement in scaling computer use agents across platforms, achieving significant performance gains and setting new standards in multiple task domains.</p>
</li>
<li><p><strong>LNE-Blocking Framework:</strong> The LNE-Blocking framework offers a novel solution for mitigating data contamination in large language models, effectively restoring model performance and ensuring fair benchmarking.</p>
</li>
</ol>
<h3>Implications</h3>
<ol>
<li><p><strong>Enhanced Model Adaptability and Reliability:</strong> The improvements in domain adaptation and calibration can make AI models more reliable and versatile, beneficial for applications in dynamic and critical environments such as autonomous driving, medical diagnostics, and human-computer interaction.</p>
</li>
<li><p><strong>Improved Real-World AI Applications:</strong> The breakthroughs in cross-modal learning and trajectory prediction expand AIs potential in real-world applications, enhancing systems ability to handle complex, noisy data and improving safety and decision-making.</p>
</li>
<li><p><strong>Scalability and Accessibility:</strong> The focus on scalable datasets and open-source models, as seen with ScaleCUA and LNE-Blocking, promotes broader accessibility and the potential for widespread adoption of advanced AI systems in various fields.</p>
</li>
<li><p><strong>Increased Awareness of AI Vulnerabilities:</strong> The identification of vulnerabilities in XR systems integrated with large language models highlights the need for robust security measures, pushing the community to develop better protection mechanisms.</p>
</li>
<li><p><strong>Interdisciplinary Applications of AI:</strong> The use of AI to assess historical oppression and enhance multimodal experiences like piano performance indicates a growing interdisciplinary approach, integrating AI with social sciences and the arts to address complex societal challenges.</p>
</li>
</ol>
<p>These trends and breakthroughs signify important steps toward more aligned, reliable, and applicable AI systems, addressing both technical and societal challenges.</p>
<p><em>Based on 10 research papers</em></p>

<h2>quantum computing</h2>
<p>The research papers provided do not directly pertain to quantum computing, but rather cover a variety of topics in machine learning, computer vision, cosmology, and other fields. However, I will summarize each paper with respect to its own field of study.</p>
<ol>
<li><p><strong>Vocabulary Alignment for Source-Free Domain Adaptation:</strong></p>
<ul>
<li><strong>Trends:</strong> There is an increasing focus on improving machine learning models adaptability to different datasets without needing direct access to the source data.</li>
<li><strong>Breakthroughs:</strong> The VocAlign framework uses a novel vocabulary alignment strategy and low-rank adaptation to enhance open-vocabulary semantic segmentation.</li>
<li><strong>Implications:</strong> This approach reduces computational demands and improves performance, setting a new benchmark for source-free domain adaptation.</li>
</ul>
</li>
<li><p><strong>Calibration-Aware Prompt Learning for Medical Vision-Language Models:</strong></p>
<ul>
<li><strong>Trends:</strong> Theres a growing need to address the calibration of medical AI models to ensure reliable predictions.</li>
<li><strong>Breakthroughs:</strong> The introduction of CalibPrompt for tuning models enhances the reliability of medical vision-language models by aligning accuracy with confidence levels.</li>
<li><strong>Implications:</strong> Improved calibration can lead to more trustworthy AI applications in clinical settings, enhancing decision-making processes.</li>
</ul>
</li>
<li><p><strong>Event-Based Monocular Depth Estimation:</strong></p>
<ul>
<li><strong>Trends:</strong> Leveraging event cameras for depth estimation is gaining traction due to their efficiency in dynamic environments.</li>
<li><strong>Breakthroughs:</strong> A cross-modal distillation paradigm using Vision Foundation Models provides a new method for generating dense depth labels without extensive annotations.</li>
<li><strong>Implications:</strong> This approach could simplify data requirements and improve depth estimation in real-world applications, such as robotics and autonomous vehicles.</li>
</ul>
</li>
<li><p><strong>Cosmic Pairwise Velocities in Non-Linear Structure Formation:</strong></p>
<ul>
<li><strong>Trends:</strong> Understanding cosmic structure formation through dark matter velocities is crucial for cosmological research.</li>
<li><strong>Breakthroughs:</strong> The study provides a robust method for measuring pairwise velocities and explores their sensitivity to cosmological parameters.</li>
<li><strong>Implications:</strong> This research could enhance the precision of cosmological models and contribute to our understanding of dark energy.</li>
</ul>
</li>
<li><p><strong>Multimodal Piano Performance Dataset Acquisition:</strong></p>
<ul>
<li><strong>Trends:</strong> Theres increasing interest in analyzing the multimodal aspects of musical performance.</li>
<li><strong>Breakthroughs:</strong> The development of web toolkits for data acquisition and fingering annotation facilitates the study of piano performances.</li>
<li><strong>Implications:</strong> This could streamline the process of gathering large-scale data, aiding research in music technology and performance analysis.</li>
</ul>
</li>
<li><p><strong>Scaling Open-Source Computer Use Agents:</strong></p>
<ul>
<li><strong>Trends:</strong> The demand for autonomous computer agents that can operate across platforms is rising.</li>
<li><strong>Breakthroughs:</strong> ScaleCUA introduces a large-scale dataset and framework for training computer use agents, achieving state-of-the-art performance.</li>
<li><strong>Implications:</strong> The release of this resource could accelerate the development of more versatile computer use agents.</li>
</ul>
</li>
<li><p><strong>Multi-View Stereo with Confidence-Aware Diffusion Model:</strong></p>
<ul>
<li><strong>Trends:</strong> Enhancing the computational efficiency of 3D reconstruction methods is a key focus.</li>
<li><strong>Breakthroughs:</strong> The integration of diffusion models into multi-view stereo processes offers improved performance and efficiency.</li>
<li><strong>Implications:</strong> This advancement holds potential for applications in fields like virtual reality and 3D modeling.</li>
</ul>
</li>
<li><p><strong>Contamination Mitigation in Large Language Models:</strong></p>
<ul>
<li><strong>Trends:</strong> Addressing data contamination in AI training datasets is increasingly important for fair model evaluation.</li>
<li><strong>Breakthroughs:</strong> The LNE-Blocking framework restores model performance by identifying and mitigating contamination effects.</li>
<li><strong>Implications:</strong> This approach could lead to more reliable benchmarking of AI models, fostering trust in their capabilities.</li>
</ul>
</li>
<li><p><strong>Out-of-Sight Trajectories in Autonomous Systems:</strong></p>
<ul>
<li><strong>Trends:</strong> Predicting trajectories in environments with limited visibility is crucial for autonomous systems.</li>
<li><strong>Breakthroughs:</strong> The OST task and its extensions improve trajectory prediction using unsupervised denoising techniques.</li>
<li><strong>Implications:</strong> This work could enhance safety and performance in autonomous driving and robotics.</li>
</ul>
</li>
<li><p><strong>Geometric Image Caption Synthesis:</strong></p>
<ul>
<li><strong>Trends:</strong> Theres a need for models that can handle complex geometric reasoning tasks.</li>
<li><strong>Breakthroughs:</strong> A data generation pipeline using reinforcement learning enhances geometric problem-solving in multimodal models.</li>
<li><strong>Implications:</strong> Improved task generalization could expand the applicability of AI in fields requiring complex reasoning, such as engineering and design.</li>
</ul>
</li>
</ol>
<p>Each of these papers contributes to advancing their respective fields, with implications for improving the efficiency, reliability, and applicability of AI and technology solutions.</p>
<p><em>Based on 10 research papers</em></p>

        Generated by OpenAI GPT-4o-mini
    
    
    
    
    
        <h2>ðŸ“„ Research Papers (20)</h2>
        
            
                Lost in Translation? Vocabulary Alignment for Source-Free Domain Adaptation in Open-Vocabulary Semantic Segmentation
                
                    <strong>Authors:</strong> Silvio Mazzucco, Carl Persson, Mattia Segu, Pier Luigi Dovesi, Federico Tombari, Luc Van Gool, Matteo Poggi
                
                
                    <strong>Abstract:</strong> We introduce VocAlign, a novel source-free domain adaptation framework specifically designed for VLMs in open-vocabulary semantic segmentation. Our method adopts a student-teacher paradigm enhanced with a vocabulary alignment strategy, which improves pseudo-label generation by incorporating additional class concepts. To ensure efficiency, we use Low-Rank Adaptation (LoRA) to fine-tune the model, preserving its original capabilities while minimizing computational overhead. In addition, we propose a Top-K class selection mechanism for the student model, which significantly reduces memory requirements while further improving adaptation performance. Our approach achieves a notable 6.11 mIoU improvement on the CityScapes dataset and demonstrates superior performance on zero-shot segmentation benchmarks, setting a new standard for source-free adaptation in the open-vocabulary setting.
                
                
                    <strong>Published:</strong> 2025-09-18T17:59:58Z
                    <a href="http://arxiv.org/abs/2509.15225v1" target="_blank">ðŸ“„ View Paper</a>
                
            
        
            
                Calibration-Aware Prompt Learning for Medical Vision-Language Models
                
                    <strong>Authors:</strong> Abhishek Basu, Fahad Shamshad, Ashshak Sharifdeen, Karthik Nandakumar, Muhammad Haris Khan
                
                
                    <strong>Abstract:</strong> Medical Vision-Language Models (Med-VLMs) have demonstrated remarkable performance across diverse medical imaging tasks by leveraging large-scale image-text pretraining. However, their confidence calibration is largely unexplored, and so remains a significant challenge. As such, miscalibrated predictions can lead to overconfident errors, undermining clinical trust and decision-making reliability. To address this, we introduce CalibPrompt, the first framework to calibrate Med-VLMs during prompt tuning. CalibPrompt optimizes a small set of learnable prompts with carefully designed calibration objectives under scarce labeled data regime. First, we study a regularizer that attempts to align the smoothed accuracy with the predicted model confidences. Second, we introduce an angular separation loss to maximize textual feature proximity toward improving the reliability in confidence estimates of multimodal Med-VLMs. Extensive experiments on four publicly available Med-VLMs and five diverse medical imaging datasets reveal that CalibPrompt consistently improves calibration without drastically affecting clean accuracy. Our code is available at https://github.com/iabh1shekbasu/CalibPrompt.
                
                
                    <strong>Published:</strong> 2025-09-18T17:59:58Z
                    <a href="http://arxiv.org/abs/2509.15226v1" target="_blank">ðŸ“„ View Paper</a>
                
            
        
            
                Depth AnyEvent: A Cross-Modal Distillation Paradigm for Event-Based Monocular Depth Estimation
                
                    <strong>Authors:</strong> Luca Bartolomei, Enrico Mannocci, Fabio Tosi, Matteo Poggi, Stefano Mattoccia
                
                
                    <strong>Abstract:</strong> Event cameras capture sparse, high-temporal-resolution visual information, making them particularly suitable for challenging environments with high-speed motion and strongly varying lighting conditions. However, the lack of large datasets with dense ground-truth depth annotations hinders learning-based monocular depth estimation from event data. To address this limitation, we propose a cross-modal distillation paradigm to generate dense proxy labels leveraging a Vision Foundation Model (VFM). Our strategy requires an event stream spatially aligned with RGB frames, a simple setup even available off-the-shelf, and exploits the robustness of large-scale VFMs. Additionally, we propose to adapt VFMs, either a vanilla one like Depth Anything v2 (DAv2), or deriving from it a novel recurrent architecture to infer depth from monocular event cameras. We evaluate our approach with synthetic and real-world datasets, demonstrating that i) our cross-modal paradigm achieves competitive performance compared to fully supervised methods without requiring expensive depth annotations, and ii) our VFM-based models achieve state-of-the-art performance.
                
                
                    <strong>Published:</strong> 2025-09-18T17:59:51Z
                    <a href="http://arxiv.org/abs/2509.15224v1" target="_blank">ðŸ“„ View Paper</a>
                
            
        
            
                Two Web Toolkits for Multimodal Piano Performance Dataset Acquisition and Fingering Annotation
                
                    <strong>Authors:</strong> Junhyung Park, Yonghyun Kim, Joonhyung Bae, Kirak Kim, Taegyun Kwon, Alexander Lerch, Juhan Nam
                
                
                    <strong>Abstract:</strong> Piano performance is a multimodal activity that intrinsically combines physical actions with the acoustic rendition. Despite growing research interest in analyzing the multimodal nature of piano performance, the laborious process of acquiring large-scale multimodal data remains a significant bottleneck, hindering further progress in this field. To overcome this barrier, we present an integrated web toolkit comprising two graphical user interfaces (GUIs): (i) PiaRec, which supports the synchronized acquisition of audio, video, MIDI, and performance metadata. (ii) ASDF, which enables the efficient annotation of performer fingering from the visual data. Collectively, this system can streamline the acquisition of multimodal piano performance datasets.
                
                
                    <strong>Published:</strong> 2025-09-18T17:59:24Z
                    <a href="http://arxiv.org/abs/2509.15222v1" target="_blank">ðŸ“„ View Paper</a>
                
            
        
            
                ScaleCUA: Scaling Open-Source Computer Use Agents with Cross-Platform Data
                
                    <strong>Authors:</strong> Zhaoyang Liu, JingJing Xie, Zichen Ding, Zehao Li, Bowen Yang, Zhenyu Wu, Xuehui Wang, Qiushi Sun, Shi Liu, Weiyun Wang, Shenglong Ye, Qingyun Li, Zeyue Tian, Gen Luo, Xiangyu Yue, Biqing Qi, Kai Chen, Bowen Zhou, Yu Qiao, Qifeng Chen, Wenhai Wang
                
                
                    <strong>Abstract:</strong> Vision-Language Models (VLMs) have enabled computer use agents (CUAs) that operate GUIs autonomously, showing great potential, yet progress is limited by the lack of large-scale, open-source computer use data and foundation models. In this work, we introduce ScaleCUA, a step toward scaling open-source CUAs. It offers a large-scale dataset spanning 6 operating systems and 3 task domains, built via a closed-loop pipeline uniting automated agents with human experts. Trained on this scaled-up data, ScaleCUA can operate seamlessly across platforms. Specifically, it delivers strong gains over baselines (+26.6 on WebArena-Lite-v2, +10.7 on ScreenSpot-Pro) and sets new state-of-the-art results (94.4% on MMBench-GUI L1-Hard, 60.6% on OSWorld-G, 47.4% on WebArena-Lite-v2). These findings underscore the power of data-driven scaling for general-purpose computer use agents. We will release data, models, and code to advance future research: https://github.com/OpenGVLab/ScaleCUA.
                
                
                    <strong>Published:</strong> 2025-09-18T17:59:22Z
                    <a href="http://arxiv.org/abs/2509.15221v1" target="_blank">ðŸ“„ View Paper</a>
                
            
        
            
                LNE-Blocking: An Efficient Framework for Contamination Mitigation Evaluation on Large Language Models
                
                    <strong>Authors:</strong> Ruijie Hou, Yueyang Jiao, Hanxu Hu, Yingming Li, Wai Lam, Huajian Zhang, Hongyuan Lu
                
                
                    <strong>Abstract:</strong> The problem of data contamination is now almost inevitable during the development of large language models (LLMs), with the training data commonly integrating those evaluation benchmarks even unintentionally. This problem subsequently makes it hard to benchmark LLMs fairly. Instead of constructing contamination-free datasets (quite hard), we propose a novel framework, \textbf{LNE-Blocking}, to restore model performance prior to contamination on potentially leaked datasets. Our framework consists of two components: contamination detection and disruption operation. For the prompt, the framework first uses the contamination detection method, \textbf{LNE}, to assess the extent of contamination in the model. Based on this, it adjusts the intensity of the disruption operation, \textbf{Blocking}, to elicit non-memorized responses from the model. Our framework is the first to efficiently restore the models greedy decoding performance. This comes with a strong performance on multiple datasets with potential leakage risks, and it consistently achieves stable recovery results across different models and varying levels of data contamination. We release the code at https://github.com/RuijieH/LNE-Blocking to facilitate research.
                
                
                    <strong>Published:</strong> 2025-09-18T17:59:16Z
                    <a href="http://arxiv.org/abs/2509.15218v1" target="_blank">ðŸ“„ View Paper</a>
                
            
        
            
                Out-of-Sight Trajectories: Tracking, Fusion, and Prediction
                
                    <strong>Authors:</strong> Haichao Zhang, Yi Xu, Yun Fu
                
                
                    <strong>Abstract:</strong> Trajectory prediction is a critical task in computer vision and autonomous systems, playing a key role in autonomous driving, robotics, surveillance, and virtual reality. Existing methods often rely on complete and noise-free observational data, overlooking the challenges associated with out-of-sight objects and the inherent noise in sensor data caused by limited camera coverage, obstructions, and the absence of ground truth for denoised trajectories. These limitations pose safety risks and hinder reliable prediction in real-world scenarios. In this extended work, we present advancements in Out-of-Sight Trajectory (OST), a novel task that predicts the noise-free visual trajectories of out-of-sight objects using noisy sensor data. Building on our previous research, we broaden the scope of Out-of-Sight Trajectory Prediction (OOSTraj) to include pedestrians and vehicles, extending its applicability to autonomous driving, robotics, surveillance, and virtual reality. Our enhanced Vision-Positioning Denoising Module leverages camera calibration to establish a vision-positioning mapping, addressing the lack of visual references, while effectively denoising noisy sensor data in an unsupervised manner. Through extensive evaluations on the Vi-Fi and JRDB datasets, our approach achieves state-of-the-art performance in both trajectory denoising and prediction, significantly surpassing previous baselines. Additionally, we introduce comparisons with traditional denoising methods, such as Kalman filtering, and adapt recent trajectory prediction models to our task, providing a comprehensive benchmark. This work represents the first initiative to integrate vision-positioning projection for denoising noisy sensor trajectories of out-of-sight agents, paving the way for future advances. The code and preprocessed datasets are available at github.com/Hai-chao-Zhang/OST
                
                
                    <strong>Published:</strong> 2025-09-18T17:59:16Z
                    <a href="http://arxiv.org/abs/2509.15219v1" target="_blank">ðŸ“„ View Paper</a>
                
            
        
            
                Assessing Historical Structural Oppression Worldwide via Rule-Guided Prompting of Large Language Models
                
                    <strong>Authors:</strong> Sreejato Chatterjee, Linh Tran, Quoc Duy Nguyen, Roni Kirson, Drue Hamlin, Harvest Aquino, Hanjia Lyu, Jiebo Luo, Timothy Dye
                
                
                    <strong>Abstract:</strong> Traditional efforts to measure historical structural oppression struggle with cross-national validity due to the unique, locally specified histories of exclusion, colonization, and social status in each country, and often have relied on structured indices that privilege material resources while overlooking lived, identity-based exclusion. We introduce a novel framework for oppression measurement that leverages Large Language Models (LLMs) to generate context-sensitive scores of lived historical disadvantage across diverse geopolitical settings. Using unstructured self-identified ethnicity utterances from a multilingual COVID-19 global study, we design rule-guided prompting strategies that encourage models to produce interpretable, theoretically grounded estimations of oppression. We systematically evaluate these strategies across multiple state-of-the-art LLMs. Our results demonstrate that LLMs, when guided by explicit rules, can capture nuanced forms of identity-based historical oppression within nations. This approach provides a complementary measurement tool that highlights dimensions of systemic exclusion, offering a scalable, cross-cultural lens for understanding how oppression manifests in data-driven research and public health contexts. To support reproducible evaluation, we release an open-sourced benchmark dataset for assessing LLMs on oppression measurement (https://github.com/chattergpt/llm-oppression-benchmark).
                
                
                    <strong>Published:</strong> 2025-09-18T17:59:05Z
                    <a href="http://arxiv.org/abs/2509.15216v1" target="_blank">ðŸ“„ View Paper</a>
                
            
        
            
                Evil Vizier: Vulnerabilities of LLM-Integrated XR Systems
                
                    <strong>Authors:</strong> Yicheng Zhang, Zijian Huang, Sophie Chen, Erfan Shayegani, Jiasi Chen, Nael Abu-Ghazaleh
                
                
                    <strong>Abstract:</strong> Extended reality (XR) applications increasingly integrate Large Language Models (LLMs) to enhance user experience, scene understanding, and even generate executable XR content, and are often called AI glasses. Despite these potential benefits, the integrated XR-LLM pipeline makes XR applications vulnerable to new forms of attacks. In this paper, we analyze LLM-Integated XR systems in the literature and in practice and categorize them along different dimensions from a systems perspective. Building on this categorization, we identify a common threat model and demonstrate a series of proof-of-concept attacks on multiple XR platforms that employ various LLM models (Meta Quest 3, Meta Ray-Ban, Android, and Microsoft HoloLens 2 running Llama and GPT models). Although these platforms each implement LLM integration differently, they share vulnerabilities where an attacker can modify the public context surrounding a legitimate LLM query, resulting in erroneous visual or auditory feedback to users, thus compromising their safety or privacy, sowing confusion, or other harmful effects. To defend against these threats, we discuss mitigation strategies and best practices for developers, including an initial defense prototype, and call on the community to develop new protection mechanisms to mitigate these risks.
                
                
                    <strong>Published:</strong> 2025-09-18T17:58:15Z
                    <a href="http://arxiv.org/abs/2509.15213v1" target="_blank">ðŸ“„ View Paper</a>
                
            
        
            
                Localized In-Plane Cavity Optomechanics in MEMS
                
                    <strong>Authors:</strong> Sasan Rahmanian
                
                
                    <strong>Abstract:</strong> This study demonstrates the realization of localized in-plane optomechanical microcavities embedded within an electrostatic MEMS architecture. The system consists of a curved, clamped-clamped microbeam, fabricated on a silicon-on-insulator (SOI) wafer. A green laser emitted from a Laser Doppler Vibrometer (LDV), is directed perpendicularly onto the device under a vacuum pressure of 7 mTorr, with the beam aligned to fill the gap between the movable microbeam and its adjacent side fixed mirror. This configuration forms localized cavity optomechanical resonators that enable the generation of optomechanical soliton frequency combs through phonon lasing without electrical excitation. The optomechanical resonators dynamics are examined through experiments and numerical simulations. First, the experimental findings unveil that in electrostatic MEMS structures, the two reflective electrodes positioned to form a capacitive gap can inadvertently form localized cavities. These cavities significantly affect optical readouts, as the photodetected signal encodes contributions from both Doppler-shifted electromagnetic waves and light scattered from the intracavity optical field. This dual contributions can distort mechanical response interpretation unless appropriately filtered. Second, experiments show that optical pumping at various positions along the microbeam induces periodic pulse trains with distinct free spectral ranges (FSRs), each corresponding to different mechanical modes. Our results present the generation of solitary optical wavepackets using in-plane localized Fabry-P\erot microcavities formed within a MEMS device. The results suggest a path toward chip-scale, soliton frequency combs generators featuring frequency spacing on the order of kilohertz, without relying on integrated fiber optics.
                
                
                    <strong>Published:</strong> 2025-09-18T17:54:45Z
                    <a href="http://arxiv.org/abs/2509.15203v1" target="_blank">ðŸ“„ View Paper</a>
                
            
        
            
                Lost in Translation? Vocabulary Alignment for Source-Free Domain Adaptation in Open-Vocabulary Semantic Segmentation
                
                    <strong>Authors:</strong> Silvio Mazzucco, Carl Persson, Mattia Segu, Pier Luigi Dovesi, Federico Tombari, Luc Van Gool, Matteo Poggi
                
                
                    <strong>Abstract:</strong> We introduce VocAlign, a novel source-free domain adaptation framework specifically designed for VLMs in open-vocabulary semantic segmentation. Our method adopts a student-teacher paradigm enhanced with a vocabulary alignment strategy, which improves pseudo-label generation by incorporating additional class concepts. To ensure efficiency, we use Low-Rank Adaptation (LoRA) to fine-tune the model, preserving its original capabilities while minimizing computational overhead. In addition, we propose a Top-K class selection mechanism for the student model, which significantly reduces memory requirements while further improving adaptation performance. Our approach achieves a notable 6.11 mIoU improvement on the CityScapes dataset and demonstrates superior performance on zero-shot segmentation benchmarks, setting a new standard for source-free adaptation in the open-vocabulary setting.
                
                
                    <strong>Published:</strong> 2025-09-18T17:59:58Z
                    <a href="http://arxiv.org/abs/2509.15225v1" target="_blank">ðŸ“„ View Paper</a>
                
            
        
            
                Calibration-Aware Prompt Learning for Medical Vision-Language Models
                
                    <strong>Authors:</strong> Abhishek Basu, Fahad Shamshad, Ashshak Sharifdeen, Karthik Nandakumar, Muhammad Haris Khan
                
                
                    <strong>Abstract:</strong> Medical Vision-Language Models (Med-VLMs) have demonstrated remarkable performance across diverse medical imaging tasks by leveraging large-scale image-text pretraining. However, their confidence calibration is largely unexplored, and so remains a significant challenge. As such, miscalibrated predictions can lead to overconfident errors, undermining clinical trust and decision-making reliability. To address this, we introduce CalibPrompt, the first framework to calibrate Med-VLMs during prompt tuning. CalibPrompt optimizes a small set of learnable prompts with carefully designed calibration objectives under scarce labeled data regime. First, we study a regularizer that attempts to align the smoothed accuracy with the predicted model confidences. Second, we introduce an angular separation loss to maximize textual feature proximity toward improving the reliability in confidence estimates of multimodal Med-VLMs. Extensive experiments on four publicly available Med-VLMs and five diverse medical imaging datasets reveal that CalibPrompt consistently improves calibration without drastically affecting clean accuracy. Our code is available at https://github.com/iabh1shekbasu/CalibPrompt.
                
                
                    <strong>Published:</strong> 2025-09-18T17:59:58Z
                    <a href="http://arxiv.org/abs/2509.15226v1" target="_blank">ðŸ“„ View Paper</a>
                
            
        
            
                Depth AnyEvent: A Cross-Modal Distillation Paradigm for Event-Based Monocular Depth Estimation
                
                    <strong>Authors:</strong> Luca Bartolomei, Enrico Mannocci, Fabio Tosi, Matteo Poggi, Stefano Mattoccia
                
                
                    <strong>Abstract:</strong> Event cameras capture sparse, high-temporal-resolution visual information, making them particularly suitable for challenging environments with high-speed motion and strongly varying lighting conditions. However, the lack of large datasets with dense ground-truth depth annotations hinders learning-based monocular depth estimation from event data. To address this limitation, we propose a cross-modal distillation paradigm to generate dense proxy labels leveraging a Vision Foundation Model (VFM). Our strategy requires an event stream spatially aligned with RGB frames, a simple setup even available off-the-shelf, and exploits the robustness of large-scale VFMs. Additionally, we propose to adapt VFMs, either a vanilla one like Depth Anything v2 (DAv2), or deriving from it a novel recurrent architecture to infer depth from monocular event cameras. We evaluate our approach with synthetic and real-world datasets, demonstrating that i) our cross-modal paradigm achieves competitive performance compared to fully supervised methods without requiring expensive depth annotations, and ii) our VFM-based models achieve state-of-the-art performance.
                
                
                    <strong>Published:</strong> 2025-09-18T17:59:51Z
                    <a href="http://arxiv.org/abs/2509.15224v1" target="_blank">ðŸ“„ View Paper</a>
                
            
        
            
                Parameter sensitivity of cosmic pairwise velocities in the non-linear regime of structure formation
                
                    <strong>Authors:</strong> Jorge Enrique GarcÃ­a-Farieta, HÃ©ctor J. HortÃºa
                
                
                    <strong>Abstract:</strong> The peculiar velocities of dark matter tracers drive the growth of cosmic structures, providing a sensitive test of cosmological models and strengthening constraints on the nature of dark energy. In this work, we investigate the mean pairwise velocities, $v_{12}$, of dark matter tracers as a cosmological probe in the non-linear regime of cosmic structure formation. Using N-body dark matter-only simulations, we measure $v_{12}$ for pair separations up to 50 $h^{-1}$Mpc and model it by solving the pair conservation equation for a self-gravitating particle system, along with various prescriptions of the nonlinear matter power spectrum. We quantified the sensitivity of $v_{12}$ to variations in key cosmological parameters such as $\Omega_{\mathrm{m}}$, $\sigma_8$, $h$, $M_\nu$, and $w$. Our parameter inference analysis using MCMC shows sub-11% agreement with simulation data, with notable degeneracies, particularly between $\Omega_\mathrm{m}$ and $\sigma_8$. We further compute the stable clustering crossing scale across redshifts $z=0$, $0.5$, and $1$, assessing its dependence on cosmology. Among the tested power spectrum modeling approaches, we find that the CSSTEmu emulator provides the most accurate predictions, with deviations below 5% for $r  10$ $h^{-1}$Mpc at $z=0.5$. Our results are validated using independent simulation suites, demonstrating that our framework offers a robust method for extracting cosmological constraints from upcoming peculiar velocity data.
                
                
                    <strong>Published:</strong> 2025-09-18T17:59:30Z
                    <a href="http://arxiv.org/abs/2509.15223v1" target="_blank">ðŸ“„ View Paper</a>
                
            
        
            
                Two Web Toolkits for Multimodal Piano Performance Dataset Acquisition and Fingering Annotation
                
                    <strong>Authors:</strong> Junhyung Park, Yonghyun Kim, Joonhyung Bae, Kirak Kim, Taegyun Kwon, Alexander Lerch, Juhan Nam
                
                
                    <strong>Abstract:</strong> Piano performance is a multimodal activity that intrinsically combines physical actions with the acoustic rendition. Despite growing research interest in analyzing the multimodal nature of piano performance, the laborious process of acquiring large-scale multimodal data remains a significant bottleneck, hindering further progress in this field. To overcome this barrier, we present an integrated web toolkit comprising two graphical user interfaces (GUIs): (i) PiaRec, which supports the synchronized acquisition of audio, video, MIDI, and performance metadata. (ii) ASDF, which enables the efficient annotation of performer fingering from the visual data. Collectively, this system can streamline the acquisition of multimodal piano performance datasets.
                
                
                    <strong>Published:</strong> 2025-09-18T17:59:24Z
                    <a href="http://arxiv.org/abs/2509.15222v1" target="_blank">ðŸ“„ View Paper</a>
                
            
        
            
                ScaleCUA: Scaling Open-Source Computer Use Agents with Cross-Platform Data
                
                    <strong>Authors:</strong> Zhaoyang Liu, JingJing Xie, Zichen Ding, Zehao Li, Bowen Yang, Zhenyu Wu, Xuehui Wang, Qiushi Sun, Shi Liu, Weiyun Wang, Shenglong Ye, Qingyun Li, Zeyue Tian, Gen Luo, Xiangyu Yue, Biqing Qi, Kai Chen, Bowen Zhou, Yu Qiao, Qifeng Chen, Wenhai Wang
                
                
                    <strong>Abstract:</strong> Vision-Language Models (VLMs) have enabled computer use agents (CUAs) that operate GUIs autonomously, showing great potential, yet progress is limited by the lack of large-scale, open-source computer use data and foundation models. In this work, we introduce ScaleCUA, a step toward scaling open-source CUAs. It offers a large-scale dataset spanning 6 operating systems and 3 task domains, built via a closed-loop pipeline uniting automated agents with human experts. Trained on this scaled-up data, ScaleCUA can operate seamlessly across platforms. Specifically, it delivers strong gains over baselines (+26.6 on WebArena-Lite-v2, +10.7 on ScreenSpot-Pro) and sets new state-of-the-art results (94.4% on MMBench-GUI L1-Hard, 60.6% on OSWorld-G, 47.4% on WebArena-Lite-v2). These findings underscore the power of data-driven scaling for general-purpose computer use agents. We will release data, models, and code to advance future research: https://github.com/OpenGVLab/ScaleCUA.
                
                
                    <strong>Published:</strong> 2025-09-18T17:59:22Z
                    <a href="http://arxiv.org/abs/2509.15221v1" target="_blank">ðŸ“„ View Paper</a>
                
            
        
            
                Lightweight and Accurate Multi-View Stereo with Confidence-Aware Diffusion Model
                
                    <strong>Authors:</strong> Fangjinhua Wang, Qingshan Xu, Yew-Soon Ong, Marc Pollefeys
                
                
                    <strong>Abstract:</strong> To reconstruct the 3D geometry from calibrated images, learning-based multi-view stereo (MVS) methods typically perform multi-view depth estimation and then fuse depth maps into a mesh or point cloud. To improve the computational efficiency, many methods initialize a coarse depth map and then gradually refine it in higher resolutions. Recently, diffusion models achieve great success in generation tasks. Starting from a random noise, diffusion models gradually recover the sample with an iterative denoising process. In this paper, we propose a novel MVS framework, which introduces diffusion models in MVS. Specifically, we formulate depth refinement as a conditional diffusion process. Considering the discriminative characteristic of depth estimation, we design a condition encoder to guide the diffusion process. To improve efficiency, we propose a novel diffusion network combining lightweight 2D U-Net and convolutional GRU. Moreover, we propose a novel confidence-based sampling strategy to adaptively sample depth hypotheses based on the confidence estimated by diffusion model. Based on our novel MVS framework, we propose two novel MVS methods, DiffMVS and CasDiffMVS. DiffMVS achieves competitive performance with state-of-the-art efficiency in run-time and GPU memory. CasDiffMVS achieves state-of-the-art performance on DTU, Tanks  Temples and ETH3D. Code is available at: https://github.com/cvg/diffmvs.
                
                
                    <strong>Published:</strong> 2025-09-18T17:59:19Z
                    <a href="http://dx.doi.org/10.1109/TPAMI.2025.3597148" target="_blank">ðŸ“„ View Paper</a>
                
            
        
            
                LNE-Blocking: An Efficient Framework for Contamination Mitigation Evaluation on Large Language Models
                
                    <strong>Authors:</strong> Ruijie Hou, Yueyang Jiao, Hanxu Hu, Yingming Li, Wai Lam, Huajian Zhang, Hongyuan Lu
                
                
                    <strong>Abstract:</strong> The problem of data contamination is now almost inevitable during the development of large language models (LLMs), with the training data commonly integrating those evaluation benchmarks even unintentionally. This problem subsequently makes it hard to benchmark LLMs fairly. Instead of constructing contamination-free datasets (quite hard), we propose a novel framework, \textbf{LNE-Blocking}, to restore model performance prior to contamination on potentially leaked datasets. Our framework consists of two components: contamination detection and disruption operation. For the prompt, the framework first uses the contamination detection method, \textbf{LNE}, to assess the extent of contamination in the model. Based on this, it adjusts the intensity of the disruption operation, \textbf{Blocking}, to elicit non-memorized responses from the model. Our framework is the first to efficiently restore the models greedy decoding performance. This comes with a strong performance on multiple datasets with potential leakage risks, and it consistently achieves stable recovery results across different models and varying levels of data contamination. We release the code at https://github.com/RuijieH/LNE-Blocking to facilitate research.
                
                
                    <strong>Published:</strong> 2025-09-18T17:59:16Z
                    <a href="http://arxiv.org/abs/2509.15218v1" target="_blank">ðŸ“„ View Paper</a>
                
            
        
            
                Out-of-Sight Trajectories: Tracking, Fusion, and Prediction
                
                    <strong>Authors:</strong> Haichao Zhang, Yi Xu, Yun Fu
                
                
                    <strong>Abstract:</strong> Trajectory prediction is a critical task in computer vision and autonomous systems, playing a key role in autonomous driving, robotics, surveillance, and virtual reality. Existing methods often rely on complete and noise-free observational data, overlooking the challenges associated with out-of-sight objects and the inherent noise in sensor data caused by limited camera coverage, obstructions, and the absence of ground truth for denoised trajectories. These limitations pose safety risks and hinder reliable prediction in real-world scenarios. In this extended work, we present advancements in Out-of-Sight Trajectory (OST), a novel task that predicts the noise-free visual trajectories of out-of-sight objects using noisy sensor data. Building on our previous research, we broaden the scope of Out-of-Sight Trajectory Prediction (OOSTraj) to include pedestrians and vehicles, extending its applicability to autonomous driving, robotics, surveillance, and virtual reality. Our enhanced Vision-Positioning Denoising Module leverages camera calibration to establish a vision-positioning mapping, addressing the lack of visual references, while effectively denoising noisy sensor data in an unsupervised manner. Through extensive evaluations on the Vi-Fi and JRDB datasets, our approach achieves state-of-the-art performance in both trajectory denoising and prediction, significantly surpassing previous baselines. Additionally, we introduce comparisons with traditional denoising methods, such as Kalman filtering, and adapt recent trajectory prediction models to our task, providing a comprehensive benchmark. This work represents the first initiative to integrate vision-positioning projection for denoising noisy sensor trajectories of out-of-sight agents, paving the way for future advances. The code and preprocessed datasets are available at github.com/Hai-chao-Zhang/OST
                
                
                    <strong>Published:</strong> 2025-09-18T17:59:16Z
                    <a href="http://arxiv.org/abs/2509.15219v1" target="_blank">ðŸ“„ View Paper</a>
                
            
        
            
                Generalizable Geometric Image Caption Synthesis
                
                    <strong>Authors:</strong> Yue Xin, Wenyuan Wang, Rui Pan, Ruida Wang, Howard Meng, Renjie Pi, Shizhe Diao, Tong Zhang
                
                
                    <strong>Abstract:</strong> Multimodal large language models have various practical applications that demand strong reasoning abilities. Despite recent advancements, these models still struggle to solve complex geometric problems. A key challenge stems from the lack of high-quality image-text pair datasets for understanding geometric images. Furthermore, most template-based data synthesis pipelines typically fail to generalize to questions beyond their predefined templates. In this paper, we bridge this gap by introducing a complementary process of Reinforcement Learning with Verifiable Rewards (RLVR) into the data generation pipeline. By adopting RLVR to refine captions for geometric images synthesized from 50 basic geometric relations and using reward signals derived from mathematical problem-solving tasks, our pipeline successfully captures the key features of geometry problem-solving. This enables better task generalization and yields non-trivial improvements. Furthermore, even in out-of-distribution scenarios, the generated dataset enhances the general reasoning capabilities of multimodal large language models, yielding accuracy improvements of $2.8\%\text{-}4.8\%$ in statistics, arithmetic, algebraic, and numerical tasks with non-geometric input images of MathVista and MathVerse, along with $2.4\%\text{-}3.9\%$ improvements in Art, Design, Tech, and Engineering tasks in MMMU.
                
                
                    <strong>Published:</strong> 2025-09-18T17:59:11Z
                    <a href="http://arxiv.org/abs/2509.15217v1" target="_blank">ðŸ“„ View Paper</a>
                
            
        
    
    
        <p><em>Generated by AI News Agent</em></p>
    

