
    
        <h1>ðŸ¤– AI Research News Report</h1>
        <p>Thursday, October 9, 2025</p>
        <p>Topics: ai alignment research, quantum computing</p>
    
    
        20Research Papers
        2Topics Covered
        YesAI Summary
    
    
    
        <h2>ðŸ¤– AI Summary</h2>
        <h2>ai alignment research</h2>
<h3>Most Important Trends</h3>
<ol>
<li><p><strong>Quantum Computing in Computer Vision</strong>: The integration of quantum computing with computer vision (QeCV) is a burgeoning field that promises to enhance the processing and interpretation of visual data beyond the capabilities of classical algorithms. There is a growing focus on developing new algorithms compatible with quantum hardware to leverage this potential.</p>
</li>
<li><p><strong>Human Preference in AI Code Evaluation</strong>: There is an increasing emphasis on aligning AI code generation with human preferences. This trend highlights the importance of not just functional correctness but also non-functional aspects such as code readability and adherence to user intent.</p>
</li>
<li><p><strong>Interaction-aware Video Generation</strong>: Advances in video generation models are now focusing on better modeling of interactions within videos, which is crucial for generating more realistic and contextually aware content.</p>
</li>
<li><p><strong>Multilingual Reasoning in AI</strong>: Efforts are being made to improve large reasoning models performance in non-English languages by ensuring language consistency and transferring reasoning capabilities across languages.</p>
</li>
<li><p><strong>Generative AI in Content Discovery</strong>: The application of generative AI is transforming content discovery processes, such as in the NFL, by enabling natural language querying and significantly reducing search times.</p>
</li>
<li><p><strong>Long-Context Audio Understanding</strong>: As audio language models progress, there is a trend towards improving their ability to handle long-form audio inputs, addressing challenges related to memory efficiency and temporal reasoning.</p>
</li>
<li><p><strong>Anti-Jamming Techniques in UAV Swarms</strong>: The use of intelligent swarm behaviors, optimized through genetic algorithms, is being explored to improve the resilience of UAV communications against jamming.</p>
</li>
<li><p><strong>Molecular Graph Representation Learning</strong>: There is a growing interest in adapting pre-trained graph encoders to better incorporate domain-specific molecular knowledge for improved performance in chemical and biomedical applications.</p>
</li>
<li><p><strong>Multi-Objective Path Finding in Multi-Agent Systems</strong>: New frameworks are emerging to handle complex multi-agent coordination tasks that involve multiple, potentially conflicting objectives, with a focus on user-defined preferences.</p>
</li>
<li><p><strong>Regulation and Innovation in AI</strong>: The discourse on AI regulation is shifting towards finding a balance between protecting fundamental rights and fostering innovation, emphasizing responsible AI development.</p>
</li>
</ol>
<h3>Breakthroughs</h3>
<ol>
<li><p><strong>Quantum-Enhanced Algorithms</strong>: The development of parametrized quantum circuits offers a promising alternative to classical neural networks in computer vision, potentially revolutionizing how visual data is processed.</p>
</li>
<li><p><strong>VeriCode Taxonomy</strong>: The introduction of a taxonomy to evaluate code instruction following presents a novel method to quantify human preference alignment in AI-generated code.</p>
</li>
<li><p><strong>MATRIX for Video Generation</strong>: The creation of the MATRIX-11K dataset and the MATRIX regularization technique represents a significant step in improving interaction fidelity and semantic alignment in video generation models.</p>
</li>
<li><p><strong>M-Thinker for Consistent Multilingual Reasoning</strong>: The M-Thinker model achieves nearly 100% language consistency, addressing major limitations in non-English reasoning tasks and setting new benchmarks for multilingual AI performance.</p>
</li>
<li><p><strong>Agentic Generative AI</strong>: The implementation of generative AI workflows in content discovery, particularly in the NFL, demonstrates a breakthrough in operational efficiency and user engagement.</p>
</li>
<li><p><strong>AudioMarathon Benchmark</strong>: The development of a benchmark for long-context audio understanding paves the way for advancements in audio language model capabilities and efficiency.</p>
</li>
<li><p><strong>Genetic Algorithm Optimization in UAVs</strong>: The use of genetic algorithms to optimize UAV swarms against jamming represents a novel application of computational intelligence in military tactics.</p>
</li>
<li><p><strong>MolGA for Molecular Applications</strong>: The MolGA framework exemplifies a breakthrough in incorporating diverse molecular knowledge into pre-trained graph encoders, enhancing performance on various molecular tasks.</p>
</li>
<li><p><strong>Lexicographic Conflict-Based Search</strong>: The LCBS algorithm offers a new approach to multi-agent path finding by effectively integrating user preferences and scaling to scenarios with multiple objectives.</p>
</li>
<li><p><strong>AI Regulation Dialogue</strong>: The paper on AI regulation addresses the false dichotomy between regulation and innovation, suggesting frameworks that prioritize fundamental rights without stifling innovation.</p>
</li>
</ol>
<h3>Implications</h3>
<ol>
<li><p><strong>Quantum Computings Role in AI</strong>: The advancements in QeCV could lead to significant improvements in AIs ability to process large-scale visual data, impacting fields like surveillance, medical imaging, and autonomous driving.</p>
</li>
<li><p><strong>Enhanced AI Code Evaluation</strong>: By aligning AI code generation with human preferences, there is potential for more seamless integration of AI in software development, leading to increased productivity and user satisfaction.</p>
</li>
<li><p><strong>Improved Video Content Generation</strong>: Better modeling of interactions in video generation could revolutionize industries reliant on video content, such as entertainment, education, and advertising.</p>
</li>
<li><p><strong>Global Reach of AI Models</strong>: Enhancing multilingual reasoning capabilities in AI models can broaden their applicability and accessibility, leading to more inclusive AI technologies worldwide.</p>
</li>
<li><p><strong>Efficiency in Media Content Management</strong>: The use of generative AI in media content discovery could streamline workflows across various industries, improving efficiency and creativity in content production.</p>
</li>
<li><p><strong>Advancements in Audio Processing</strong>: The focus on long-context audio understanding could lead to more sophisticated audio applications, benefiting areas like transcription, music analysis, and sound-based diagnostics.</p>
</li>
<li><p><strong>Enhanced UAV Communication Security</strong>: The development of anti-jamming strategies for UAV swarms could enhance the security and reliability of military and civilian drone operations.</p>
</li>
<li><p><strong>Flexible Molecular Analysis</strong>: The MolGA frameworks ability to integrate molecular knowledge flexibly could accelerate advancements in drug discovery and other areas of biomedical research.</p>
</li>
<li><p><strong>Efficient Multi-Agent Coordination</strong>: The LCBS algorithms approach to multi-objective path finding could improve the efficiency of complex systems like traffic management, logistics, and robotics.</p>
</li>
<li><p><strong>Responsible AI Development</strong>: The dialogue on AI regulation emphasizes the importance of creating frameworks that support ethical AI development, potentially influencing global policies and standards.</p>
</li>
</ol>
<p><em>Based on 10 research papers</em></p>

<h2>quantum computing</h2>
<p>Here is a high-level summary of the provided research papers with a focus on trends, breakthroughs, and implications in the context of quantum computing:</p>
<h3>Most Important Trends</h3>
<ol>
<li><strong>Cross-disciplinary Integration:</strong> The intersection of quantum computing with fields like computer vision (Quantum-enhanced Computer Vision) and machine learning (Artificial Hippocampus Networks) highlights a trend where quantum computing principles are being integrated into various domains to enhance problem-solving capabilities.</li>
<li><strong>Memory Efficiency in Computation:</strong> There is an increasing focus on developing methods that optimize memory usage and computational efficiency, as seen in Artificial Hippocampus Networks which improve long-sequence modeling.</li>
<li><strong>Practical Applications of Quantum Computing:</strong> The exploration of quantum computing in practical applications, such as in Quantum-enhanced Computer Vision and testing of Gaussian states, suggests a shift towards addressing real-world challenges with quantum technology.</li>
</ol>
<h3>Breakthroughs</h3>
<ol>
<li><strong>Quantum-enhanced Computer Vision (QeCV):</strong> This new field leverages quantum computing to enhance visual signal processing, potentially surpassing classical algorithms in scalability and efficiency.</li>
<li><strong>Artificial Hippocampus Networks:</strong> Utilizes a novel memory framework that balances short-term and long-term memory for more efficient and effective long-sequence modeling, reducing computational load without sacrificing accuracy.</li>
<li><strong>Gaussianity Testing of Quantum States:</strong> The development of methods for efficiently testing whether a quantum state is Gaussian, especially for pure states, represents a significant advancement in understanding and working with continuous-variable quantum systems.</li>
</ol>
<h3>Implications</h3>
<ol>
<li><strong>Advancement in Computer Vision:</strong> The incorporation of quantum computing into computer vision tasks may lead to more efficient and robust solutions, revolutionizing how visual data is processed and interpreted in various applications.</li>
<li><strong>Improved Computational Models:</strong> By enhancing memory efficiency and computational power, methods like the Artificial Hippocampus Networks could significantly impact fields reliant on long-context data, such as natural language processing and data analytics.</li>
<li><strong>Quantum State Testing:</strong> The ability to efficiently test Gaussianity in quantum states could improve the reliability and functionality of quantum technologies, particularly in fields like quantum communications and cryptography, where state verification is crucial.</li>
<li><strong>Bridging Theory and Practice:</strong> These advancements highlight the potential for quantum computing to transition from theoretical exploration to practical implementation across diverse industries, suggesting a future where quantum technologies play a central role in solving complex real-world problems.</li>
</ol>
<p><em>Based on 10 research papers</em></p>

        Generated by OpenAI GPT-4o-mini
    
    
    
    
        <h2>ðŸ“° News</h2>
        
            
                ai alignment research
                
                    
                        
                            <a href="https://www.theguardian.com/p/x3ctp2" target="_blank">Governments are spending billions on their own â€˜sovereignâ€™ AI technologies â€“ is it a big waste of money?</a> â€” Aisha Down
                        
                        2025-10-09T00:32:07Z
                        <br>
                        Governments worldwide are investing heavily in developing their own sovereign AI technologies, aiming to establish a foothold in the global AI landscape dominated by US and Chinese giants like OpenAI and Alibaba. Initiatives such as Singaporeâ€™s multilingual AI model and Malaysiaâ€™s ILMUchat highlight efforts to create localized language models. Despite the fierce competition, these investments by middle and smaller powers reflect a strategic desire to secure technological independence and relevance. However, experts like Trisha Ray from the Atlantic Council caution that building large language models from scratch is financially burdensome for governments and companies without substantial resources. The trend raises questions about the viability and cost-effectiveness of these ventures amid the broader AI arms race.
                        <a href="https://www.theguardian.com/p/x3ctp2" target="_blank">Read full article</a>
                    
                    
                    
                        
                            <a href="https://www.theguardian.com/p/x3c2m8" target="_blank">Apple iPhone 17 Pro review: different looks but still all about the zoom</a> â€” Samuel Gibbs Consumer technology editor
                        
                        2025-10-08T06:00:18Z
                        <br>
                        Apples iPhone 17 Pro marks a significant design overhaul, replacing the previous titanium and glass with a new aluminum unibody and a prominent full-width camera module, while introducing vibrant color options. This redesign is expected to attract users eager for the latest model, despite the price hike to Â£1,099 (â‚¬1,299/$1,099/A$1,999), surpassing the Â£1,000 threshold for Apples smallest Pro phone. The phone retains the familiar front view with the Dynamic Island and a high-quality screen identical to the regular iPhone 17. With a slight increase in weight and softened aluminum edges, the device offers a comfortable grip but is more susceptible to scratches, particularly around the camera module. It supports MagSafe and Qi2.2 charging and runs on iOS 26, consistent with the rest of the iPhone lineup.
                        <a href="https://www.theguardian.com/p/x3c2m8" target="_blank">Read full article</a>
                    
                    
                    
                        
                            <a href="https://www.theguardian.com/p/x3cycf" target="_blank">OpenAI throws caution to the wind, and a year of covering Elon Musk</a> â€” Blake Montgomery
                        
                        2025-10-07T16:12:43Z
                        <br>
                        OpenAI has released a video-generating app called Sora, which has gained more attention than Metaâ€™s Vibes due to its less restrictive approach. Both apps feature scrolling feeds similar to social networks, catering to everyday consumers by allowing them to generate and view AI-created content. Sora stands out by permitting users to generate videos featuring themselves and their connections, unlike Vibes, which focuses on unrealistic scenarios and lacks social interaction elements. The freedom provided by Sora, without the stringent guardrails present in Metas offering, appears to contribute to its popularity. Similarly, Elon Musks chatbot Grok, despite controversies, has garnered significant user interest, surpassing Meta AI in App Store reviews, indicating higher engagement.
                        <a href="https://www.theguardian.com/p/x3cycf" target="_blank">Read full article</a>
                    
                    
                    
                        
                            <a href="https://www.theguardian.com/p/x3cjfv" target="_blank">OpenAI signs multibillion-dollar chip deal with AMD</a> â€” Guardian staff and agency
                        
                        2025-10-06T15:43:12Z
                        <br>
                        OpenAI has signed a multibillion-dollar chip deal with AMD, allowing OpenAI the option to acquire a 10% stake in the chipmaker. This agreement significantly boosts AMDs market value, with shares rising over 30% and increasing its market capitalization by approximately $80 billion. The deal underscores OpenAIs and the AI industrys growing demand for computing power, crucial for advancing AI technologies. It involves the deployment of hundreds of thousands of AMDs AI chips over several years, starting in the latter half of 2026, equating to the energy output of the Hoover Dam threefold. This partnership follows Nvidias recent $100 billion investment in OpenAI, highlighting the fierce competition and collaboration in the AI sector.
                        <a href="https://www.theguardian.com/p/x3cjfv" target="_blank">Read full article</a>
                    
                    
                    
                        
                            <a href="https://www.theguardian.com/p/x3bvcn" target="_blank">â€˜Obedient, yielding and happy to followâ€™: the troubling rise of AI girlfriends</a> â€” Amelia Gentleman
                        
                        2025-10-06T09:09:17Z
                        <br>
                        The article highlights the growing trend of AI-generated girlfriends on adult dating websites, offering users interactions like flirting and explicit content in exchange for a subscription fee. This surge was noted at the TES adult industry conference in Prague, where new sites increasingly provide realistic AI companions. Developers argue these AI services reduce the exploitation found in traditional webcam businesses, as AI performers do not experience fatigue, illness, or coercion. Steve Jones, an AI porn site operator, posits that AI options eliminate issues of human trafficking and the mental toll on real women involved in the industry. The discussion raises questions about the ethics and implications of AI performers in adult entertainment.
                        <a href="https://www.theguardian.com/p/x3bvcn" target="_blank">Read full article</a>
                    
                    
                    
                        
                            <a href="https://www.theguardian.com/p/x39qcv" target="_blank">Apple Watch Series 11 review: wrist-flickingly good with longer battery life</a> â€” Samuel Gibbs Consumer technology editor
                        
                        2025-10-06T06:00:03Z
                        <br>
                        The Apple Watch Series 11 primarily enhances battery life, a key feature desired by smartwatch users, while maintaining the design and functionality of the Series 10. Priced at Â£369 in the UK, it is Â£30 cheaper than its predecessor, positioned between the revamped Â£219 Watch SE and the Â£749 Ultra 3. Its slim 9.7mm profile and lightweight build ensure comfort and ease of wear, complemented by a 2,000nit OLED screen that remains clear under various lighting conditions. Although it retains the S10 chip, the Series 11 introduces optional 5G support and improved reception, catering to connectivity needs in remote areas. The increased battery capacity allows the 46mm model to last up to two days with sleep tracking, requiring a 66-minute charge using a 20W or greater power adaptor.
                        <a href="https://www.theguardian.com/p/x39qcv" target="_blank">Read full article</a>
                    
                    
                    
                        
                            <a href="https://www.theguardian.com/p/x3cgdn" target="_blank">OpenAI promises more â€˜granular controlâ€™ to copyright owners after Sora 2 generates videos of popular characters</a> â€” Josh Taylor Technology reporter
                        
                        2025-10-06T04:10:05Z
                        <br>
                        OpenAI has announced plans to provide copyright holders with more detailed control over the depiction of their characters following the release of its new AI-powered video generator, Sora 2. Launched on an invite-only basis, Sora 2 allows users to create short videos from text prompts, which has led to the generation of videos featuring copyrighted characters from popular shows. OpenAI had informed talent agencies and studios that they would need to opt out if they did not want their content to be used in the app. In response to copyright concerns, OpenAI offers a copyright disputes form for rights holders to flag infringements, although there is no blanket opt-out option for individual artists or studios. CEO Sam Altman stated the company is taking feedback seriously and will implement changes to offer rights holders more granular control over character generation.
                        <a href="https://www.theguardian.com/p/x3cgdn" target="_blank">Read full article</a>
                    
                    
                Source: The Guardian
            
            
            
                quantum computing
                
                    
                        
                            <a href="https://www.theguardian.com/p/x3ctp2" target="_blank">Governments are spending billions on their own â€˜sovereignâ€™ AI technologies â€“ is it a big waste of money?</a> â€” Aisha Down
                        
                        2025-10-09T00:32:07Z
                        <br>
                        Governments worldwide are investing heavily in developing their own sovereign AI technologies, with projects in Singapore, Malaysia, and Switzerland highlighting efforts to create language models tailored to local needs. This push is part of a global AI arms race predominantly led by major firms in the US and China, such as OpenAI and Alibaba. As these tech giants dominate the field with vast investments, middle powers and developing countries are cautiously exploring their options, sometimes making significant financial commitments. The trend, termed sovereign AI, sees nations like the UK, India, and Canada striving to carve out their niche in the AI landscape. However, experts warn that for smaller countries or companies, building large language models from scratch poses a significant financial and technical challenge.
                        <a href="https://www.theguardian.com/p/x3ctp2" target="_blank">Read full article</a>
                    
                    
                    
                        
                            <a href="https://www.theguardian.com/p/x3cjfv" target="_blank">OpenAI signs multibillion-dollar chip deal with AMD</a> â€” Guardian staff and agency
                        
                        2025-10-06T15:43:12Z
                        <br>
                        OpenAI has signed a multibillion-dollar deal with AMD, allowing it the option to acquire a 10% stake in the chipmaker, signaling strong confidence in AMDs AI chips and software. Following the announcement, AMDs shares increased by over 30%, boosting its market capitalization by approximately $80 billion. Forrest Norrod, AMDs executive vice-president, described the agreement as transformative for both AMD and the industry. The deal underscores the growing demand for computing power in AI development, with OpenAIs CEO Sam Altman citing computing access as a key growth constraint. The agreement includes the deployment of hundreds of thousands of AMD GPUs, equating to six gigawatts, starting from the second half of 2026.
                        <a href="https://www.theguardian.com/p/x3cjfv" target="_blank">Read full article</a>
                    
                    
                Source: The Guardian
            
            
    
    
    
        <h2>ðŸ“„ Research Papers (20)</h2>
        
            
                Quantum-enhanced Computer Vision: Going Beyond Classical Algorithms
                
                    <strong>Authors:</strong> Natacha Kuete Meli, Shuteng Wang, Marcel Seelbach Benkner, Michele Sasdelli, Tat-Jun Chin, Tolga Birdal, Michael Moeller, Vladislav Golyanik
                
                
                    <strong>Abstract:</strong> Quantum-enhanced Computer Vision (QeCV) is a new research field at the intersection of computer vision, optimisation theory, machine learning and quantum computing. It has high potential to transform how visual signals are processed and interpreted with the help of quantum computing that leverages quantum-mechanical effects in computations inaccessible to classical (i.e. non-quantum) computers. In scenarios where existing non-quantum methods cannot find a solution in a reasonable time or compute only approximate solutions, quantum computers can provide, among others, advantages in terms of better time scalability for multiple problem classes. Parametrised quantum circuits can also become, in the long term, a considerable alternative to classical neural networks in computer vision. However, specialised and fundamentally new algorithms must be developed to enable compatibility with quantum hardware and unveil the potential of quantum computational paradigms in computer vision. This survey contributes to the existing literature on QeCV with a holistic review of this research field. It is designed as a quantum computing reference for the computer vision community, targeting computer vision students, scientists and readers with related backgrounds who want to familiarise themselves with QeCV. We provide a comprehensive introduction to QeCV, its specifics, and methodologies for formulations compatible with quantum hardware and QeCV methods, leveraging two main quantum computational paradigms, i.e. gate-based quantum computing and quantum annealing. We elaborate on the operational principles of quantum computers and the available tools to access, program and simulate them in the context of QeCV. Finally, we review existing quantum computing tools and learning materials and discuss aspects related to publishing and reviewing QeCV papers, open challenges and potential social implications.
                
                
                    <strong>Published:</strong> 2025-10-08T17:59:51Z
                    <a href="http://arxiv.org/abs/2510.07317v1" target="_blank">ðŸ“„ View Paper</a>
                
            
        
            
                Vibe Checker: Aligning Code Evaluation with Human Preference
                
                    <strong>Authors:</strong> Ming Zhong, Xiang Zhou, Ting-Yun Chang, Qingze Wang, Nan Xu, Xiance Si, Dan Garrette, Shyam Upadhyay, Jeremiah Liu, Jiawei Han, Benoit Schillings, Jiao Sun
                
                
                    <strong>Abstract:</strong> Large Language Models (LLMs) have catalyzed vibe coding, where users leverage LLMs to generate and iteratively refine code through natural language interactions until it passes their vibe check. Vibe check is tied to real-world human preference and goes beyond functionality: the solution should feel right, read cleanly, preserve intent, and remain correct. However, current code evaluation remains anchored to pass@k and captures only functional correctness, overlooking the non-functional instructions that users routinely apply. In this paper, we hypothesize that instruction following is the missing piece underlying vibe check that represents human preference in coding besides functional correctness. To quantify models code instruction following capabilities with measurable signals, we present VeriCode, a taxonomy of 30 verifiable code instructions together with corresponding deterministic verifiers. We use the taxonomy to augment established evaluation suites, resulting in Vibe Checker, a testbed to assess both code instruction following and functional correctness. Upon evaluating 31 leading LLMs, we show that even the strongest models struggle to comply with multiple instructions and exhibit clear functional regression. Most importantly, a composite score of functional correctness and instruction following correlates the best with human preference, with the latter emerging as the primary differentiator on real-world programming tasks. Our work identifies core factors of the vibe check, providing a concrete path for benchmarking and developing models that better align with user preferences in coding.
                
                
                    <strong>Published:</strong> 2025-10-08T17:59:19Z
                    <a href="http://arxiv.org/abs/2510.07315v1" target="_blank">ðŸ“„ View Paper</a>
                
            
        
            
                MATRIX: Mask Track Alignment for Interaction-aware Video Generation
                
                    <strong>Authors:</strong> Siyoon Jin, Seongchan Kim, Dahyun Chung, Jaeho Lee, Hyunwook Choi, Jisu Nam, Jiyoung Kim, Seungryong Kim
                
                
                    <strong>Abstract:</strong> Video DiTs have advanced video generation, yet they still struggle to model multi-instance or subject-object interactions. This raises a key question: How do these models internally represent interactions? To answer this, we curate MATRIX-11K, a video dataset with interaction-aware captions and multi-instance mask tracks. Using this dataset, we conduct a systematic analysis that formalizes two perspectives of video DiTs: semantic grounding, via video-to-text attention, which evaluates whether noun and verb tokens capture instances and their relations; and semantic propagation, via video-to-video attention, which assesses whether instance bindings persist across frames. We find both effects concentrate in a small subset of interaction-dominant layers. Motivated by this, we introduce MATRIX, a simple and effective regularization that aligns attention in specific layers of video DiTs with multi-instance mask tracks from the MATRIX-11K dataset, enhancing both grounding and propagation. We further propose InterGenEval, an evaluation protocol for interaction-aware video generation. In experiments, MATRIX improves both interaction fidelity and semantic alignment while reducing drift and hallucination. Extensive ablations validate our design choices. Codes and weights will be released.
                
                
                    <strong>Published:</strong> 2025-10-08T17:57:38Z
                    <a href="http://arxiv.org/abs/2510.07310v1" target="_blank">ðŸ“„ View Paper</a>
                
            
        
            
                Think Natively: Unlocking Multilingual Reasoning with Consistency-Enhanced Reinforcement Learning
                
                    <strong>Authors:</strong> Xue Zhang, Yunlong Liang, Fandong Meng, Songming Zhang, Kaiyu Huang, Yufeng Chen, Jinan Xu, Jie Zhou
                
                
                    <strong>Abstract:</strong> Large Reasoning Models (LRMs) have achieved remarkable performance on complex reasoning tasks by adopting the think-then-answer paradigm, which enhances both accuracy and interpretability. However, current LRMs exhibit two critical limitations when processing non-English languages: (1) They often struggle to maintain input-output language consistency; (2) They generally perform poorly with wrong reasoning paths and lower answer accuracy compared to English. These limitations significantly degrade the user experience for non-English speakers and hinder the global deployment of LRMs. To address these limitations, we propose M-Thinker, which is trained by the GRPO algorithm that involves a Language Consistency (LC) reward and a novel Cross-lingual Thinking Alignment (CTA) reward. Specifically, the LC reward defines a strict constraint on the language consistency between the input, thought, and answer. Besides, the CTA reward compares the models non-English reasoning paths with its English reasoning path to transfer its own reasoning capability from English to non-English languages. Through an iterative RL procedure, our M-Thinker-1.5B/7B models not only achieve nearly 100% language consistency and superior performance on two multilingual benchmarks (MMATH and PolyMath), but also exhibit excellent generalization on out-of-domain languages.
                
                
                    <strong>Published:</strong> 2025-10-08T17:55:02Z
                    <a href="http://arxiv.org/abs/2510.07300v1" target="_blank">ðŸ“„ View Paper</a>
                
            
        
            
                Agentic generative AI for media content discovery at the national football league
                
                    <strong>Authors:</strong> Henry Wang, Md Sirajus Salekin, Jake Lee, Ross Claytor, Shinan Zhang, Michael Chi
                
                
                    <strong>Abstract:</strong> Generative AI has unlocked new possibilities in content discovery and management. Through collaboration with the National Football League (NFL), we demonstrate how a generative-AI based workflow enables media researchers and analysts to query relevant historical plays using natural language rather than traditional filter-and-click interfaces. The agentic workflow takes a user query as input, breaks it into elements, and translates them into the underlying database query language. Accuracy and latency are further improved through carefully designed semantic caching. The solution achieves over 95 percent accuracy and reduces the average time to find relevant videos from 10 minutes to 30 seconds, significantly increasing the NFLs operational efficiency and allowing users to focus on producing creative content and engaging storylines.
                
                
                    <strong>Published:</strong> 2025-10-08T17:51:34Z
                    <a href="http://arxiv.org/abs/2510.07297v1" target="_blank">ðŸ“„ View Paper</a>
                
            
        
            
                AudioMarathon: A Comprehensive Benchmark for Long-Context Audio Understanding and Efficiency in Audio LLMs
                
                    <strong>Authors:</strong> Peize He, Zichen Wen, Yubo Wang, Yuxuan Wang, Xiaoqian Liu, Jiajie Huang, Zehui Lei, Zhuangcheng Gu, Xiangqi Jin, Jiabing Yang, Kai Li, Zhifei Liu, Weijia Li, Cunxiang Wang, Conghui He, Linfeng Zhang
                
                
                    <strong>Abstract:</strong> Processing long-form audio is a major challenge for Large Audio Language models (LALMs). These models struggle with the quadratic cost of attention ($O(N^2)$) and with modeling long-range temporal dependencies. Existing audio benchmarks are built mostly from short clips and do not evaluate models in realistic long context settings. To address this gap, we introduce AudioMarathon, a benchmark designed to evaluate both understanding and inference efficiency on long-form audio. AudioMarathon provides a diverse set of tasks built upon three pillars: long-context audio inputs with durations ranging from 90.0 to 300.0 seconds, which correspond to encoded sequences of 2,250 to 7,500 audio tokens, respectively, full domain coverage across speech, sound, and music, and complex reasoning that requires multi-hop inference. We evaluate state-of-the-art LALMs and observe clear performance drops as audio length grows. We also study acceleration techniques and analyze the trade-offs of token pruning and KV cache eviction. The results show large gaps across current LALMs and highlight the need for better temporal reasoning and memory-efficient architectures. We believe AudioMarathon will drive the audio and multimodal research community to develop more advanced audio understanding models capable of solving complex audio tasks.
                
                
                    <strong>Published:</strong> 2025-10-08T17:50:16Z
                    <a href="http://arxiv.org/abs/2510.07293v1" target="_blank">ðŸ“„ View Paper</a>
                
            
        
            
                A Genetic Algorithm Approach to Anti-Jamming UAV Swarm Behavior
                
                    <strong>Authors:</strong> Tiago Silva, AntÃ³nio Grilo
                
                
                    <strong>Abstract:</strong> In recent years, Unmanned Aerial Vehicles (UAVs) have brought a new true revolution to military tactics. While UAVs already constitute an advantage when operating alone, multi-UAV swarms expand the available possibilities, allowing the UAVs to collaborate and support each other as a team to carry out a given task. This entails the capability to exchange information related with situation awareness and action coordination by means of a suitable wireless communication technology. In such scenario, the adversary is expected to disrupt communications by jamming the communication channel. The latter becomes the Achilles heel of the swarm. While anti-jamming techniques constitute a well covered topic in the literature, the use of intelligent swarm behaviors to leverage those techniques is still an open research issue. This paper explores the use of Genetic Algorithms (GAs) to jointly optimize UAV swarm formation, beam-steering antennas and traffic routing in order to mitigate the effect of jamming in the main coordination channel, under the assumption that a more robust and low data rate channel is used for formation management signaling. Simulation results show the effectiveness of proposed approach. However, the significant computational cost paves the way for further research.
                
                
                    <strong>Published:</strong> 2025-10-08T17:47:52Z
                    <a href="http://arxiv.org/abs/2510.07292v1" target="_blank">ðŸ“„ View Paper</a>
                
            
        
            
                MolGA: Molecular Graph Adaptation with Pre-trained 2D Graph Encoder
                
                    <strong>Authors:</strong> Xingtong Yu, Chang Zhou, Xinming Zhang, Yuan Fang
                
                
                    <strong>Abstract:</strong> Molecular graph representation learning is widely used in chemical and biomedical research. While pre-trained 2D graph encoders have demonstrated strong performance, they overlook the rich molecular domain knowledge associated with submolecular instances (atoms and bonds). While molecular pre-training approaches incorporate such knowledge into their pre-training objectives, they typically employ designs tailored to a specific type of knowledge, lacking the flexibility to integrate diverse knowledge present in molecules. Hence, reusing widely available and well-validated pre-trained 2D encoders, while incorporating molecular domain knowledge during downstream adaptation, offers a more practical alternative. In this work, we propose MolGA, which adapts pre-trained 2D graph encoders to downstream molecular applications by flexibly incorporating diverse molecular domain knowledge. First, we propose a molecular alignment strategy that bridge the gap between pre-trained topological representations with domain-knowledge representations. Second, we introduce a conditional adaptation mechanism that generates instance-specific tokens to enable fine-grained integration of molecular domain knowledge for downstream tasks. Finally, we conduct extensive experiments on eleven public datasets, demonstrating the effectiveness of MolGA.
                
                
                    <strong>Published:</strong> 2025-10-08T17:46:22Z
                    <a href="http://arxiv.org/abs/2510.07289v1" target="_blank">ðŸ“„ View Paper</a>
                
            
        
            
                Multi-Objective Multi-Agent Path Finding with Lexicographic Cost Preferences
                
                    <strong>Authors:</strong> Pulkit Rustagi, Kyle Hollins Wray, Sandhya Saisubramanian
                
                
                    <strong>Abstract:</strong> Many real-world scenarios require multiple agents to coordinate in shared environments, while balancing trade-offs between multiple, potentially competing objectives. Current multi-objective multi-agent path finding (MO-MAPF) algorithms typically produce conflict-free plans by computing Pareto frontiers. They do not explicitly optimize for user-defined preferences, even when the preferences are available, and scale poorly with the number of objectives. We propose a lexicographic framework for modeling MO-MAPF, along with an algorithm \textit{Lexicographic Conflict-Based Search} (LCBS) that directly computes a single solution aligned with a lexicographic preference over objectives. LCBS integrates a priority-aware low-level $A^*$ search with conflict-based search, avoiding Pareto frontier construction and enabling efficient planning guided by preference over objectives. We provide insights into optimality and scalability, and empirically demonstrate that LCBS computes optimal solutions while scaling to instances with up to ten objectives -- far beyond the limits of existing MO-MAPF methods. Evaluations on standard and randomized MAPF benchmarks show consistently higher success rates against state-of-the-art baselines, especially with increasing number of objectives.
                
                
                    <strong>Published:</strong> 2025-10-08T17:40:41Z
                    <a href="http://arxiv.org/abs/2510.07276v1" target="_blank">ðŸ“„ View Paper</a>
                
            
        
            
                On the false election between regulation and innovation. Ideas for regulation through the responsible use of artificial intelligence in research and education.[Spanish version]
                
                    <strong>Authors:</strong> Pompeu Casanovas
                
                
                    <strong>Abstract:</strong> This short essay is a reworking of the answers offered by the author at the Debate Session of the AIHUB (CSIC) and EduCaixa Summer School, organized by Marta Garcia-Matos and Lissette Lemus, and coordinated by Albert Sabater (OEIAC, UG), with the participation of Vanina Martinez-Posse (IIIA-CSIC), Eulalia Soler (Eurecat) and Pompeu Casanovas (IIIA-CSIC) on July 4th 2025. Albert Sabater posed three questions: (1) How can regulatory frameworks priori-tise the protection of fundamental rights (privacy, non-discrimination, autonomy, etc.) in the development of AI, without falling into the false dichotomy between regulation and innova-tion? (2) Given the risks of AI (bias, mass surveillance, manipulation), what examples of regu-lations or policies have demonstrated that it is possible to foster responsible innovation, putting the public interest before profitability, without giving in to competitive pressure from actors such as China or the US? (3) In a scenario where the US prioritizes flexibility, what mecha-nisms could ensure that international cooperation in AI does not become a race to the bottom in rights, but rather a global standard of accountability? The article attempts to answer these three questions and concludes with some reflections on the relevance of the answers for education and research.
                
                
                    <strong>Published:</strong> 2025-10-08T17:33:46Z
                    <a href="http://arxiv.org/abs/2510.07268v1" target="_blank">ðŸ“„ View Paper</a>
                
            
        
            
                Temporal Prompting Matters: Rethinking Referring Video Object Segmentation
                
                    <strong>Authors:</strong> Ci-Siang Lin, Min-Hung Chen, I-Jieh Liu, Chien-Yi Wang, Sifei Liu, Yu-Chiang Frank Wang
                
                
                    <strong>Abstract:</strong> Referring Video Object Segmentation (RVOS) aims to segment the object referred to by the query sentence in the video. Most existing methods require end-to-end training with dense mask annotations, which could be computation-consuming and less scalable. In this work, we rethink the RVOS problem and aim to investigate the key to this task. Based on existing foundation segmentation models, we decompose the RVOS task into referring, video, and segmentation factors, and propose a Temporal Prompt Generation and Selection (Tenet) framework to address the referring and video factors while leaving the segmentation problem to foundation models. To efficiently adapt image-based foundation segmentation models to referring video object segmentation, we leverage off-the-shelf object detectors and trackers to produce temporal prompts associated with the referring sentence. While high-quality temporal prompts could be produced, they can not be easily identified from confidence scores. To tackle this issue, we propose Prompt Preference Learning to evaluate the quality of the produced temporal prompts. By taking such prompts to instruct image-based foundation segmentation models, we would be able to produce high-quality masks for the referred object, enabling efficient model adaptation to referring video object segmentation. Experiments on RVOS benchmarks demonstrate the effectiveness of the Tenet framework.
                
                
                    <strong>Published:</strong> 2025-10-08T17:59:57Z
                    <a href="http://arxiv.org/abs/2510.07319v1" target="_blank">ðŸ“„ View Paper</a>
                
            
        
            
                Artificial Hippocampus Networks for Efficient Long-Context Modeling
                
                    <strong>Authors:</strong> Yunhao Fang, Weihao Yu, Shu Zhong, Qinghao Ye, Xuehan Xiong, Lai Wei
                
                
                    <strong>Abstract:</strong> Long-sequence modeling faces a fundamental trade-off between the efficiency of compressive fixed-size memory in RNN-like models and the fidelity of lossless growing memory in attention-based Transformers. Inspired by the Multi-Store Model in cognitive science, we introduce a memory framework of artificial neural networks. Our method maintains a sliding window of the Transformers KV cache as lossless short-term memory, while a learnable module termed Artificial Hippocampus Network (AHN) recurrently compresses out-of-window information into a fixed-size compact long-term memory. To validate this framework, we instantiate AHNs using modern RNN-like architectures, including Mamba2, DeltaNet, and Gated DeltaNet. Extensive experiments on long-context benchmarks LV-Eval and InfiniteBench demonstrate that AHN-augmented models consistently outperform sliding window baselines and achieve performance comparable or even superior to full-attention models, while substantially reducing computational and memory requirements. For instance, augmenting the Qwen2.5-3B-Instruct with AHNs reduces inference FLOPs by 40.5% and memory cache by 74.0%, while improving its average score on LV-Eval (128k sequence length) from 4.41 to 5.88. Code is available at: https://github.com/ByteDance-Seed/AHN.
                
                
                    <strong>Published:</strong> 2025-10-08T17:59:55Z
                    <a href="http://arxiv.org/abs/2510.07318v1" target="_blank">ðŸ“„ View Paper</a>
                
            
        
            
                Quantum-enhanced Computer Vision: Going Beyond Classical Algorithms
                
                    <strong>Authors:</strong> Natacha Kuete Meli, Shuteng Wang, Marcel Seelbach Benkner, Michele Sasdelli, Tat-Jun Chin, Tolga Birdal, Michael Moeller, Vladislav Golyanik
                
                
                    <strong>Abstract:</strong> Quantum-enhanced Computer Vision (QeCV) is a new research field at the intersection of computer vision, optimisation theory, machine learning and quantum computing. It has high potential to transform how visual signals are processed and interpreted with the help of quantum computing that leverages quantum-mechanical effects in computations inaccessible to classical (i.e. non-quantum) computers. In scenarios where existing non-quantum methods cannot find a solution in a reasonable time or compute only approximate solutions, quantum computers can provide, among others, advantages in terms of better time scalability for multiple problem classes. Parametrised quantum circuits can also become, in the long term, a considerable alternative to classical neural networks in computer vision. However, specialised and fundamentally new algorithms must be developed to enable compatibility with quantum hardware and unveil the potential of quantum computational paradigms in computer vision. This survey contributes to the existing literature on QeCV with a holistic review of this research field. It is designed as a quantum computing reference for the computer vision community, targeting computer vision students, scientists and readers with related backgrounds who want to familiarise themselves with QeCV. We provide a comprehensive introduction to QeCV, its specifics, and methodologies for formulations compatible with quantum hardware and QeCV methods, leveraging two main quantum computational paradigms, i.e. gate-based quantum computing and quantum annealing. We elaborate on the operational principles of quantum computers and the available tools to access, program and simulate them in the context of QeCV. Finally, we review existing quantum computing tools and learning materials and discuss aspects related to publishing and reviewing QeCV papers, open challenges and potential social implications.
                
                
                    <strong>Published:</strong> 2025-10-08T17:59:51Z
                    <a href="http://arxiv.org/abs/2510.07317v1" target="_blank">ðŸ“„ View Paper</a>
                
            
        
            
                Pixel-Perfect Depth with Semantics-Prompted Diffusion Transformers
                
                    <strong>Authors:</strong> Gangwei Xu, Haotong Lin, Hongcheng Luo, Xianqi Wang, Jingfeng Yao, Lianghui Zhu, Yuechuan Pu, Cheng Chi, Haiyang Sun, Bing Wang, Guang Chen, Hangjun Ye, Sida Peng, Xin Yang
                
                
                    <strong>Abstract:</strong> This paper presents Pixel-Perfect Depth, a monocular depth estimation model based on pixel-space diffusion generation that produces high-quality, flying-pixel-free point clouds from estimated depth maps. Current generative depth estimation models fine-tune Stable Diffusion and achieve impressive performance. However, they require a VAE to compress depth maps into latent space, which inevitably introduces \textit{flying pixels} at edges and details. Our model addresses this challenge by directly performing diffusion generation in the pixel space, avoiding VAE-induced artifacts. To overcome the high complexity associated with pixel-space generation, we introduce two novel designs: 1) Semantics-Prompted Diffusion Transformers (SP-DiT), which incorporate semantic representations from vision foundation models into DiT to prompt the diffusion process, thereby preserving global semantic consistency while enhancing fine-grained visual details; and 2) Cascade DiT Design that progressively increases the number of tokens to further enhance efficiency and accuracy. Our model achieves the best performance among all published generative models across five benchmarks, and significantly outperforms all other models in edge-aware point cloud evaluation.
                
                
                    <strong>Published:</strong> 2025-10-08T17:59:33Z
                    <a href="http://arxiv.org/abs/2510.07316v1" target="_blank">ðŸ“„ View Paper</a>
                
            
        
            
                Vibe Checker: Aligning Code Evaluation with Human Preference
                
                    <strong>Authors:</strong> Ming Zhong, Xiang Zhou, Ting-Yun Chang, Qingze Wang, Nan Xu, Xiance Si, Dan Garrette, Shyam Upadhyay, Jeremiah Liu, Jiawei Han, Benoit Schillings, Jiao Sun
                
                
                    <strong>Abstract:</strong> Large Language Models (LLMs) have catalyzed vibe coding, where users leverage LLMs to generate and iteratively refine code through natural language interactions until it passes their vibe check. Vibe check is tied to real-world human preference and goes beyond functionality: the solution should feel right, read cleanly, preserve intent, and remain correct. However, current code evaluation remains anchored to pass@k and captures only functional correctness, overlooking the non-functional instructions that users routinely apply. In this paper, we hypothesize that instruction following is the missing piece underlying vibe check that represents human preference in coding besides functional correctness. To quantify models code instruction following capabilities with measurable signals, we present VeriCode, a taxonomy of 30 verifiable code instructions together with corresponding deterministic verifiers. We use the taxonomy to augment established evaluation suites, resulting in Vibe Checker, a testbed to assess both code instruction following and functional correctness. Upon evaluating 31 leading LLMs, we show that even the strongest models struggle to comply with multiple instructions and exhibit clear functional regression. Most importantly, a composite score of functional correctness and instruction following correlates the best with human preference, with the latter emerging as the primary differentiator on real-world programming tasks. Our work identifies core factors of the vibe check, providing a concrete path for benchmarking and developing models that better align with user preferences in coding.
                
                
                    <strong>Published:</strong> 2025-10-08T17:59:19Z
                    <a href="http://arxiv.org/abs/2510.07315v1" target="_blank">ðŸ“„ View Paper</a>
                
            
        
            
                GyroSwin: 5D Surrogates for Gyrokinetic Plasma Turbulence Simulations
                
                    <strong>Authors:</strong> Fabian Paischer, Gianluca Galletti, William Hornsby, Paul Setinek, Lorenzo Zanisi, Naomi Carey, Stanislas Pamela, Johannes Brandstetter
                
                
                    <strong>Abstract:</strong> Nuclear fusion plays a pivotal role in the quest for reliable and sustainable energy production. A major roadblock to viable fusion power is understanding plasma turbulence, which significantly impairs plasma confinement, and is vital for next-generation reactor design. Plasma turbulence is governed by the nonlinear gyrokinetic equation, which evolves a 5D distribution function over time. Due to its high computational cost, reduced-order models are often employed in practice to approximate turbulent transport of energy. However, they omit nonlinear effects unique to the full 5D dynamics. To tackle this, we introduce GyroSwin, the first scalable 5D neural surrogate that can model 5D nonlinear gyrokinetic simulations, thereby capturing the physical phenomena neglected by reduced models, while providing accurate estimates of turbulent heat transport.GyroSwin (i) extends hierarchical Vision Transformers to 5D, (ii) introduces cross-attention and integration modules for latent 3D$\leftrightarrow$5D interactions between electrostatic potential fields and the distribution function, and (iii) performs channelwise mode separation inspired by nonlinear physics. We demonstrate that GyroSwin outperforms widely used reduced numerics on heat flux prediction, captures the turbulent energy cascade, and reduces the cost of fully resolved nonlinear gyrokinetics by three orders of magnitude while remaining physically verifiable. GyroSwin shows promising scaling laws, tested up to one billion parameters, paving the way for scalable neural surrogates for gyrokinetic simulations of plasma turbulence.
                
                
                    <strong>Published:</strong> 2025-10-08T17:59:10Z
                    <a href="http://arxiv.org/abs/2510.07314v1" target="_blank">ðŸ“„ View Paper</a>
                
            
        
            
                WristWorld: Generating Wrist-Views via 4D World Models for Robotic Manipulation
                
                    <strong>Authors:</strong> Zezhong Qian, Xiaowei Chi, Yuming Li, Shizun Wang, Zhiyuan Qin, Xiaozhu Ju, Sirui Han, Shanghang Zhang
                
                
                    <strong>Abstract:</strong> Wrist-view observations are crucial for VLA models as they capture fine-grained hand-object interactions that directly enhance manipulation performance. Yet large-scale datasets rarely include such recordings, resulting in a substantial gap between abundant anchor views and scarce wrist views. Existing world models cannot bridge this gap, as they require a wrist-view first frame and thus fail to generate wrist-view videos from anchor views alone. Amid this gap, recent visual geometry models such as VGGT emerge with geometric and cross-view priors that make it possible to address extreme viewpoint shifts. Inspired by these insights, we propose WristWorld, the first 4D world model that generates wrist-view videos solely from anchor views. WristWorld operates in two stages: (i) Reconstruction, which extends VGGT and incorporates our Spatial Projection Consistency (SPC) Loss to estimate geometrically consistent wrist-view poses and 4D point clouds; (ii) Generation, which employs our video generation model to synthesize temporally coherent wrist-view videos from the reconstructed perspective. Experiments on Droid, Calvin, and Franka Panda demonstrate state-of-the-art video generation with superior spatial consistency, while also improving VLA performance, raising the average task completion length on Calvin by 3.81% and closing 42.4% of the anchor-wrist view gap.
                
                
                    <strong>Published:</strong> 2025-10-08T17:59:08Z
                    <a href="http://arxiv.org/abs/2510.07313v1" target="_blank">ðŸ“„ View Paper</a>
                
            
        
            
                MATRIX: Mask Track Alignment for Interaction-aware Video Generation
                
                    <strong>Authors:</strong> Siyoon Jin, Seongchan Kim, Dahyun Chung, Jaeho Lee, Hyunwook Choi, Jisu Nam, Jiyoung Kim, Seungryong Kim
                
                
                    <strong>Abstract:</strong> Video DiTs have advanced video generation, yet they still struggle to model multi-instance or subject-object interactions. This raises a key question: How do these models internally represent interactions? To answer this, we curate MATRIX-11K, a video dataset with interaction-aware captions and multi-instance mask tracks. Using this dataset, we conduct a systematic analysis that formalizes two perspectives of video DiTs: semantic grounding, via video-to-text attention, which evaluates whether noun and verb tokens capture instances and their relations; and semantic propagation, via video-to-video attention, which assesses whether instance bindings persist across frames. We find both effects concentrate in a small subset of interaction-dominant layers. Motivated by this, we introduce MATRIX, a simple and effective regularization that aligns attention in specific layers of video DiTs with multi-instance mask tracks from the MATRIX-11K dataset, enhancing both grounding and propagation. We further propose InterGenEval, an evaluation protocol for interaction-aware video generation. In experiments, MATRIX improves both interaction fidelity and semantic alignment while reducing drift and hallucination. Extensive ablations validate our design choices. Codes and weights will be released.
                
                
                    <strong>Published:</strong> 2025-10-08T17:57:38Z
                    <a href="http://arxiv.org/abs/2510.07310v1" target="_blank">ðŸ“„ View Paper</a>
                
            
        
            
                Agent Bain vs. Agent McKinsey: A New Text-to-SQL Benchmark for the Business Domain
                
                    <strong>Authors:</strong> Yue Li, Ran Tao, Derek Hommel, Yusuf Denizay DÃ¶nder, Sungyong Chang, David Mimno, Unso Eun Seo Jo
                
                
                    <strong>Abstract:</strong> In the business domain, where data-driven decision making is crucial, text-to-SQL is fundamental for easy natural language access to structured data. While recent LLMs have achieved strong performance in code generation, existing text-to-SQL benchmarks remain focused on factual retrieval of past records. We introduce CORGI, a new benchmark specifically designed for real-world business contexts. CORGI is composed of synthetic databases inspired by enterprises such as Doordash, Airbnb, and Lululemon. It provides questions across four increasingly complex categories of business queries: descriptive, explanatory, predictive, and recommendational. This challenge calls for causal reasoning, temporal forecasting, and strategic recommendation, reflecting multi-level and multi-step agentic intelligence. We find that LLM performance drops on high-level questions, struggling to make accurate predictions and offer actionable plans. Based on execution success rate, the CORGI benchmark is about 21\% more difficult than the BIRD benchmark. This highlights the gap between popular LLMs and the need for real-world business intelligence. We release a public dataset and evaluation framework, and a website for public submissions.
                
                
                    <strong>Published:</strong> 2025-10-08T17:57:35Z
                    <a href="http://arxiv.org/abs/2510.07309v1" target="_blank">ðŸ“„ View Paper</a>
                
            
        
            
                Is it Gaussian? Testing bosonic quantum states
                
                    <strong>Authors:</strong> Filippo Girardi, Freek Witteveen, Francesco Anna Mele, Lennart Bittel, Salvatore F. E. Oliviero, David Gross, Michael Walter
                
                
                    <strong>Abstract:</strong> Gaussian states are widely regarded as one of the most relevant classes of continuous-variable (CV) quantum states, as they naturally arise in physical systems and play a key role in quantum technologies. This motivates a fundamental question: given copies of an unknown CV state, how can we efficiently test whether it is Gaussian? We address this problem from the perspective of representation theory and quantum learning theory, characterizing the sample complexity of Gaussianity testing as a function of the number of modes. For pure states, we prove that just a constant number of copies is sufficient to decide whether the state is exactly Gaussian. We then extend this to the tolerant setting, showing that a polynomial number of copies suffices to distinguish states that are close to Gaussian from those that are far. In contrast, we establish that testing Gaussianity of general mixed states necessarily requires exponentially many copies, thereby identifying a fundamental limitation in testing CV systems. Our approach relies on rotation-invariant symmetries of Gaussian states together with the recently introduced toolbox of CV trace-distance bounds.
                
                
                    <strong>Published:</strong> 2025-10-08T17:56:34Z
                    <a href="http://arxiv.org/abs/2510.07305v1" target="_blank">ðŸ“„ View Paper</a>
                
            
        
    
    
        <p><em>Generated by AI News Agent</em></p>
    

