
    
        <h1>ðŸ¤– AI Research Report</h1>
        
            <strong>Date:</strong> 2025-09-03<br>
            <strong>Topics:</strong> ai alignment research, quantum computing<br>
            <strong>Papers Found:</strong> 60
        
        
        
            
                <h2>ðŸ¤– AI Summary</h2>
                <h2>ai alignment research</h2>
<p>Certainly! Heres a high-level summary of the research papers provided, organized according to the specified format:</p>
<h3>Most Important Trends</h3>
<ol>
<li><p><strong>Multi-label Learning in AI</strong>: The trend towards recognizing the complexity and ambiguity in data, such as visual event recognition, is evident. This involves reformulating problems to accommodate multiple valid interpretations (e.g., multi-label learning in situation recognition).</p>
</li>
<li><p><strong>Integration of Privacy and Fairness</strong>: Theres a significant push to integrate privacy-preserving techniques and fairness in AI models, particularly for sensitive data like healthcare. This is exemplified by methods like FLIP, which combine differential privacy with fairness in data generation.</p>
</li>
<li><p><strong>Interdisciplinary Approaches in AI Development</strong>: The intersection of AI with other disciplines is growing, such as combining AI with network optimization for supply chain planning or using AI in the context of social simulations to study online communities.</p>
</li>
<li><p><strong>Enhanced AI Security</strong>: The focus on AI security is increasing, with research into vulnerabilities such as prompt injection and data poisoning, highlighting the need for robust defenses in AI systems.</p>
</li>
<li><p><strong>AI in Domain-Specific Applications</strong>: Theres a trend towards developing AI solutions tailored for specific domains, such as medical AI frameworks that leverage model diversity or AI tools for visual sound source localization in complex audio-visual environments.</p>
</li>
</ol>
<h3>Breakthroughs</h3>
<ol>
<li><p><strong>Novel Frameworks for AI Challenges</strong>: The introduction of frameworks like the Graph Enhanced Verb Multilayer Perceptron for multi-label learning, and FLIP for task-agnostic fair and private data generation, represents significant advancements in addressing complex AI challenges.</p>
</li>
<li><p><strong>Integration of LLMs with Traditional Models</strong>: Combining large language models with traditional optimization techniques, as seen in supply chain planning, offers new pathways for enhancing decision support systems.</p>
</li>
<li><p><strong>AI for Social Simulation</strong>: Using LLMs to simulate social platforms, as demonstrated in the Voat community case study, represents a novel application of AI to understand and predict social dynamics and toxicity.</p>
</li>
<li><p><strong>Robustness in Security and Privacy</strong>: The development of frameworks like Auto-SNL for real-time ML inference in high-data environments showcases breakthroughs in ensuring security and efficiency in AI applications.</p>
</li>
<li><p><strong>AI-Driven Creativity</strong>: The exploration of AI strategies for creative writing, such as multi-agent refined rewards and LLM-as-a-Judge, highlights innovative approaches to enhancing creative output in language models.</p>
</li>
</ol>
<h3>Implications</h3>
<ol>
<li><p><strong>Improved AI Alignment</strong>: The advancements in multi-label learning and privacy-fairness integration suggest a move towards more aligned AI models that can better understand and represent complex real-world scenarios.</p>
</li>
<li><p><strong>Ethical AI Development</strong>: The focus on fairness, privacy, and security in AI systems underscores the importance of ethical considerations, leading to more responsible AI development and deployment.</p>
</li>
<li><p><strong>Enhanced Collaboration Across Disciplines</strong>: The interdisciplinary nature of recent AI research indicates a broader collaboration across fields, which could lead to more comprehensive and effective AI solutions.</p>
</li>
<li><p><strong>Security and Trust in AI</strong>: The emphasis on detecting and mitigating security vulnerabilities in AI systems highlights the need for building trust and reliability in AI applications, crucial for their widespread adoption.</p>
</li>
<li><p><strong>Scalability and Accessibility</strong>: The development of efficient AI frameworks and tools, such as those enhancing creative capabilities in smaller models, points towards making AI technology more scalable and accessible to a wider audience.</p>
</li>
</ol>
<p>These insights collectively emphasize the ongoing efforts and advancements in aligning AI systems more closely with human values and ethical standards, addressing critical challenges such as ambiguity, privacy, fairness, and security.</p>
<p><em>Based on 50 research papers</em></p>

<h2>quantum computing</h2>
<p>The papers provided do not directly focus on quantum computing; they span a variety of fields, including physics, computer science, and engineering. However, I will attempt to summarize their relevance and potential connections to quantum computing, where applicable.</p>
<h3>Most Important Trends</h3>
<ol>
<li><strong>Interdisciplinary Approaches</strong>: The research spans multiple domains such as physics, computer vision, and space systems, showcasing a trend of interdisciplinary approaches. This reflects the broader scientific trend of integrating diverse methodologies to tackle complex problems.</li>
<li><strong>Advanced Computational Models</strong>: Several papers highlight the use of advanced computational models, including large language models, graph neural networks, and the integration of holographic principles to explore various phenomena, indicating a trend towards leveraging high-level computational tools across different research areas.</li>
<li><strong>Real-Time Data Integration</strong>: The emphasis on real-time data processing and adaptive systems, as seen in the cryogenic fluid management framework, is indicative of a trend towards developing systems that can dynamically integrate and respond to new data inputs.</li>
</ol>
<h3>Breakthroughs</h3>
<ol>
<li><strong>Baryon Number Violation</strong>: The study on baryon number violation presents a novel computation of this phenomenon at zero temperature, challenging existing thermal sphaleron models and potentially informing future quantum field theories.</li>
<li><strong>DriveQA and Model Sensitivity</strong>: The development of DriveQA, a benchmark for autonomous driving knowledge, reveals significant breakthroughs in understanding model weaknesses and improving fine-tuning processes for large language models.</li>
<li><strong>Holographic Entropy Cone</strong>: The work on holographic entropy cones provides a new characterization of these structures, enhancing our understanding of entanglement entropy in quantum gravity frameworks.</li>
<li><strong>Multi-Agent Systems for Clinical Decision Making</strong>: The introduction of a multi-agent system for clinical problem detection illustrates a breakthrough in creating more robust and interpretable AI systems for high-stakes environments.</li>
</ol>
<h3>Implications</h3>
<ol>
<li><strong>Quantum Field Theory</strong>: The findings on baryon number violation could have significant implications for the development of quantum field theories, offering new insights into fundamental particle interactions at the quantum level.</li>
<li><strong>AI and Autonomous Systems</strong>: The improvements in AI models, as demonstrated in DriveQA and the multi-agent clinical systems, suggest potential applications in quantum computing for optimizing algorithms and decision-making processes in complex systems.</li>
<li><strong>Understanding Quantum Entanglement</strong>: The insights gained from studying holographic entropy cones contribute to a deeper understanding of quantum entanglement, which is crucial for advancing quantum computing technologies.</li>
<li><strong>Space System Operations</strong>: The adaptive real-time forecasting framework for cryogenic fluids can inform similar strategies in quantum systems where precise control and prediction of quantum states are necessary.</li>
</ol>
<p>While these papers do not directly address quantum computing, their findings and methodologies may inform and inspire future research within the field, particularly in areas involving complex system modeling and real-time data processing.</p>
<p><em>Based on 10 research papers</em></p>

            
        
        
        <h2>ðŸ“š Research Papers</h2>
        
                
                    <h3><a href="http://arxiv.org/abs/2508.21816v1" target="_blank">The Demon is in Ambiguity: Revisiting Situation Recognition with Single Positive Multi-Label Learning</a></h3>
                    <p><strong>Authors:</strong> Yiming Lin, Yuchen Niu, Shang Wang, Kaizhu Huang, Qiufeng Wang, Xiao-Bo Jin</p>
                    <p><strong>Published:</strong> 8/29/2025</p>
                    <p><strong>Categories:</strong> cs.CV, cs.AI</p>
                    <p><strong>Summary:</strong> Context recognition (SR) is a fundamental task in computer vision that aims to extract structured semantic summaries from images by identifying key events and their associated entities. Specifically, given an input image, the model must first classify the main visual events (verb classification), then identify the participating entities and their semantic roles (semantic role labeling), and finally localize these entities in the image (semantic role localization). Existing methods treat verb classification as a single-label problem, but we show through a comprehensive analysis that this formulation fails to address the inherent ambiguity in visual event recognition, as multiple verb categories may reasonably describe the same image. This paper makes three key contributions: First, we reveal through empirical analysis that verb classification is inherently a multi-label problem due to the ubiquitous semantic overlap between verb categories. Second, given the impracticality of fully annotating large-scale datasets with multiple labels, we propose to reformulate verb classification as a single positive multi-label learning (SPMLL) problem - a novel perspective in SR research. Third, we design a comprehensive multi-label evaluation benchmark for SR that is carefully designed to fairly evaluate model performance in a multi-label setting. To address the challenges of SPMLL, we futher develop the Graph Enhanced Verb Multilayer Perceptron (GE-VerbMLP), which combines graph neural networks to capture label correlations and adversarial training to optimize decision boundaries. Extensive experiments on real-world datasets show that our approach achieves more than 3\% MAP improvement while remaining competitive on traditional top-1 and top-5 accuracy metrics.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.21815v1" target="_blank">Achieving Hilbert-Schmidt Independence Under RÃ©nyi Differential Privacy for Fair and Private Data Generation</a></h3>
                    <p><strong>Authors:</strong> Tobias Hyrup, Emmanouil Panagiotou, Arjun Roy, Arthur Zimek, Eirini Ntoutsi, Peter Schneider-Kamp</p>
                    <p><strong>Published:</strong> 8/29/2025</p>
                    <p><strong>Categories:</strong> cs.LG</p>
                    <p><strong>Summary:</strong> As privacy regulations such as the GDPR and HIPAA and responsibility frameworks for artificial intelligence such as the AI Act gain traction, the ethical and responsible use of real-world data faces increasing constraints. Synthetic data generation has emerged as a promising solution to risk-aware data sharing and model development, particularly for tabular datasets that are foundational to sensitive domains such as healthcare. To address both privacy and fairness concerns in this setting, we propose FLIP (Fair Latent Intervention under Privacy guarantees), a transformer-based variational autoencoder augmented with latent diffusion to generate heterogeneous tabular data. Unlike the typical setup in fairness-aware data generation, we assume a task-agnostic setup, not reliant on a fixed, defined downstream task, thus offering broader applicability. To ensure privacy, FLIP employs R\enyi differential privacy (RDP) constraints during training and addresses fairness in the input space with RDP-compatible balanced sampling that accounts for group-specific noise levels across multiple sampling rates. In the latent space, we promote fairness by aligning neuron activation patterns across protected groups using Centered Kernel Alignment (CKA), a similarity measure extending the Hilbert-Schmidt Independence Criterion (HSIC). This alignment encourages statistical independence between latent representations and the protected feature. Empirical results demonstrate that FLIP effectively provides significant fairness improvements for task-agnostic fairness and across diverse downstream tasks under differential privacy constraints.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.21811v1" target="_blank">The Integration of Agile Methodologies in DevOps Practices within the Information Technology Industry</a></h3>
                    <p><strong>Authors:</strong> Ashley Hourigan, Ridewaan Hanslo</p>
                    <p><strong>Published:</strong> 8/29/2025</p>
                    <p><strong>Categories:</strong> cs.SE, 68, D.2.9</p>
                    <p><strong>Summary:</strong> The demand for rapid software delivery in the Information Technology (IT) industry has significantly intensified, emphasising the need for faster software products and service releases with enhanced features to meet customer expectations. Agile methodologies are replacing traditional approaches such as Waterfall, where flexibility, iterative development and adaptation to change are favoured over rigid planning and execution. DevOps, a subsequent evolution from Agile, emphasises collaborative efforts in development and operations teams, focusing on continuous integration and deployment to deliver resilient and high-quality software products and services. This study aims to critically assess both Agile and DevOps practices in the IT industry to identify the feasibility and applicability of Agile methods in DevOps practices. Eleven semi-structured interviews were conducted with Agile and DevOps practitioners in varying capacities across several sectors within the IT industry. Through thematic analysis, 51 unique codes were extracted and synthesised into 19 themes that reported on each phase of the DevOps lifecycle, specifically regarding the integration and implementation of Agile methods into DevOps practices. Based on the findings, a new understanding detailing the interrelationship of Agile methods in DevOps practices was discussed that met the research objectives.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.21788v1" target="_blank">Going over Fine Web with a Fine-Tooth Comb: Technical Report of Indexing Fine Web for Problematic Content Search and Retrieval</a></h3>
                    <p><strong>Authors:</strong> InÃ©s Altemir Marinas, Anastasiia Kucherenko, Andrei Kucharavy</p>
                    <p><strong>Published:</strong> 8/29/2025</p>
                    <p><strong>Categories:</strong> cs.CL, cs.AI, cs.IR</p>
                    <p><strong>Summary:</strong> Large language models (LLMs) rely heavily on web-scale datasets like Common Crawl, which provides over 80\% of training data for some modern models. However, the indiscriminate nature of web crawling raises challenges in data quality, safety, and ethics. Despite the critical importance of training data quality, prior research on harmful content has been limited to small samples due to computational constraints. This project presents a framework for indexing and analyzing LLM training datasets using an ElasticSearch-based pipeline. We apply it to SwissAIs FineWeb-2 corpus (1.5TB, four languages), achieving fast query performance--most searches in milliseconds, all under 2 seconds. Our work demonstrates real-time dataset analysis, offering practical tools for safer, more accountable AI systems.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.21786v1" target="_blank">The Spontaneous Cascading Mechanism Behind Critical Phenomena in Self-Coupled Lasers</a></h3>
                    <p><strong>Authors:</strong> Jiaoqing Wang, Yael Kfir-Cohen, Chenni Xu, Bnaya Gross, Aswathy Sundaresan, Shlomo Havlin, Patrick Sebbah</p>
                    <p><strong>Published:</strong> 8/29/2025</p>
                    <p><strong>Categories:</strong> physics.optics, physics.soc-ph</p>
                    <p><strong>Summary:</strong> The basic physics of lasers is characterized by a second-order continuous phase transition at the critical lasing threshold. Nevertheless, laser bistability with abrupt transitions has been reported in some laser systems, but its underlying mechanism has never been explored. Here we study experimentally and theoretically a novel nonlinearly self-coupled laser system. We show both experimentally and theoretically that this system experiences spontaneous cascading that yields an abrupt mixed-order transition. At the critical point, a long-lived cascading plateau is observed, characterized by a critical branching factor equal to one. When deviating from criticality, the branching factor departs monotonically from one. The critical scaling close to and at the critical point resembles similar phenomena observed recently in other interdependent systems, suggesting a common universal cascading origin for abrupt transitions. Our results shed light on the cascading mechanism of abrupt transitions in laser systems, which can be utilized for future research and applications.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.21783v1" target="_blank">QoS-Aware Proportional Fairness Scheduling for Multi-Flow 5G UEs: A Smart Factory Perspective</a></h3>
                    <p><strong>Authors:</strong> Mohamed Seliem, Utz Roedig, Cormac Sreenan, Dirk Pesch</p>
                    <p><strong>Published:</strong> 8/29/2025</p>
                    <p><strong>Categories:</strong> cs.NI</p>
                    <p><strong>Summary:</strong> Private 5G networks are emerging as key enablers for smart factories, where a single device often handles multiple concurrent traffic flows with distinct Quality of Service (QoS) requirements. Existing simulation frameworks, however, lack the fidelity to model such multi-flow behavior at the QoS Flow Identifier (QFI) level. This paper addresses this gap by extending Simu5G to support per-QFI modeling and by introducing a novel QoS-aware Proportional Fairness (QoS-PF) scheduler. The scheduler dynamically balances delay, Guaranteed Bit Rate (GBR), and priority metrics to optimize resource allocation across heterogeneous flows. We evaluate the proposed approach in a realistic smart factory scenario featuring edge-hosted machine vision, real-time control loops, and bulk data transfer. Results show that QoS-PF improves deadline adherence and fairness without compromising throughput. All extensions are implemented in a modular and open-source manner to support future research. Our work provides both a methodological and architectural foundation for simulating and analyzing advanced QoS policies in industrial 5G deployments.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.21774v1" target="_blank">Improving Biomedical Knowledge Graph Quality: A Community Approach</a></h3>
                    <p><strong>Authors:</strong> Katherina G Cortes, Shilpa Sundar, Sarah Gehrke, Keenan Manpearl, Junxia Lin, Daniel Robert Korn, Harry Caufield, Kevin Schaper, Justin Reese, Kushal Koirala, Lawrence E Hunter, E. Kathleen Carter, Marcello DeLuca, Arjun Krishnan, Chris Mungall, Melissa Haendel</p>
                    <p><strong>Published:</strong> 8/29/2025</p>
                    <p><strong>Categories:</strong> q-bio.OT</p>
                    <p><strong>Summary:</strong> Biomedical knowledge graphs (KGs) are widely used across research and translational settings, yet their design decisions and implementation are often opaque. Unlike ontologies that more frequently adhere to established creation principles, biomedical KGs lack consistent practices for construction, documentation, and dissemination. To address this gap, we introduce a set of evaluation criteria grounded in widely accepted data standards and principles from related fields. We apply these criteria to 16 biomedical KGs, revealing that even those that appear to align with best practices often obscure essential information required for external reuse. Moreover, biomedical KGs, despite pursuing similar goals and ingesting the same sources in some cases, display substantial variation in models, source integration, and terminology for node types. Reaping the potential benefits of knowledge graphs for biomedical research while reducing wasted effort requires community-wide adoption of shared criteria and maturation of standards such as BioLink and KGX. Such improvements in transparency and standardization are essential for creating long-term reusability, improving comparability across resources, and enhancing the overall utility of KGs within biomedicine.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.21762v1" target="_blank">Reasoning-Intensive Regression</a></h3>
                    <p><strong>Authors:</strong> Diane Tchuindjo, Omar Khattab</p>
                    <p><strong>Published:</strong> 8/29/2025</p>
                    <p><strong>Categories:</strong> cs.CL, cs.AI</p>
                    <p><strong>Summary:</strong> AI researchers and practitioners increasingly apply large language models (LLMs) to what we call reasoning-intensive regression (RiR), i.e. deducing subtle numerical properties from text. Unlike standard language regression tasks, e.g. for sentiment or similarity, RiR often appears instead in ad-hoc problems like rubric-based scoring or domain-specific retrieval, where much deeper analysis of text is required while only limited task-specific training data and computation are available. We cast three realistic problems as RiR tasks to establish an initial benchmark, and use that to test our hypothesis that prompting frozen LLMs and finetuning Transformer encoders via gradient descent will both often struggle in RiR. We then propose MENTAT, a simple and lightweight method that combines batch-reflective prompt optimization with neural ensemble learning. MENTAT achieves up to 65% improvement over both baselines, though substantial room remains for future advances in RiR.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.21761v1" target="_blank">Learning from Silence and Noise for Visual Sound Source Localization</a></h3>
                    <p><strong>Authors:</strong> Xavier Juanola, Giovana Morais, Magdalena Fuentes, Gloria Haro</p>
                    <p><strong>Published:</strong> 8/29/2025</p>
                    <p><strong>Categories:</strong> cs.CV, cs.MM</p>
                    <p><strong>Summary:</strong> Visual sound source localization is a fundamental perception task that aims to detect the location of sounding sources in a video given its audio. Despite recent progress, we identify two shortcomings in current methods: 1) most approaches perform poorly in cases with low audio-visual semantic correspondence such as silence, noise, and offscreen sounds, i.e. in the presence of negative audio; and 2) most prior evaluations are limited to positive cases, where both datasets and metrics convey scenarios with a single visible sound source in the scene. To address this, we introduce three key contributions. First, we propose a new training strategy that incorporates silence and noise, which improves performance in positive cases, while being more robust against negative sounds. Our resulting self-supervised model, SSL-SaN, achieves state-of-the-art performance compared to other self-supervised models, both in sound localization and cross-modal retrieval. Second, we propose a new metric that quantifies the trade-off between alignment and separability of auditory and visual features across positive and negative audio-visual pairs. Third, we present IS3+, an extended and improved version of the IS3 synthetic dataset with negative audio. Our data, metrics and code are available on the https://xavijuanola.github.io/SSL-SaN/.</p>
                
            
                
                    <h3><a href="http://dx.doi.org/10.3847/1538-4357/adff7f" target="_blank">The Impact of Enhanced EUV Flux on the Upper Atmosphere of Earth-like Exoplanets</a></h3>
                    <p><strong>Authors:</strong> Lukas Hanson, Ofer Cohen, Aaron Ridley, Alex Glocer</p>
                    <p><strong>Published:</strong> 8/29/2025</p>
                    <p><strong>Categories:</strong> astro-ph.EP</p>
                    <p><strong>Summary:</strong> Identifying Earth-like planets outside out solar system is a leading research goal in astronomy, but determining if candidate planets have atmospheres, and more importantly if they can retain atmospheres, is still out of reach. In this paper, we present our study on the impact of enhanced EUV flux on the stability and escape of the upper atmosphere of an Earth-like exoplanet using the Global Ionosphere and Thermosphere Model (GITM). We also investigate the differences between one- and three-dimensional solutions. We use a baseline case of EUV flux experienced at the Earth, and multiplying this flux by a constant factor going up to 50. Our results show a clear evidence of an inflated and elevated ionosphere due to enhanced EUV flux, and they provide a detailed picture of how different heating and cooling rates, as well as the conductivity are changing at each EUV flux level. Our results also demonstrate that one-dimensional solutions are limited in their ability to capture a global atmosphere that are not uniform. We find that a threshold EUV flux level for a stable atmosphere occurs around a factor of 10 times the baseline level, where EUV fluxes above this level indicate a rapidly escaping atmosphere. This threshold EUV flux translates to about 0.3AU for a planet orbiting the Sun. Thus, our findings indicate that an Earth-like exoplanet orbiting its host star in a close-in orbit is likely to lose its atmosphere quickly.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.21740v1" target="_blank">Operational Validation of Large-Language-Model Agent Social Simulation: Evidence from Voat v/technology</a></h3>
                    <p><strong>Authors:</strong> Aleksandar TomaÅ¡eviÄ‡, Darja CvetkoviÄ‡, Sara Major, Slobodan MaletiÄ‡, Miroslav AnÄ‘elkoviÄ‡, Ana VraniÄ‡, Boris Stupovski, DuÅ¡an VudragoviÄ‡, Aleksandar BogojeviÄ‡, Marija MitroviÄ‡ Dankulov</p>
                    <p><strong>Published:</strong> 8/29/2025</p>
                    <p><strong>Categories:</strong> cs.CY, cs.SI, physics.soc-ph</p>
                    <p><strong>Summary:</strong> Large Language Models (LLMs) enable generative social simulations that can capture culturally informed, norm-guided interaction on online social platforms. We build a technology community simulation modeled on Voat, a Reddit-like alt-right news aggregator and discussion platform active from 2014 to 2020. Using the YSocial framework, we seed the simulation with a fixed catalog of technology links sampled from Voats shared URLs (covering 30+ domains) and calibrate parameters to Voats v/technology using samples from the MADOC dataset. Agents use a base, uncensored model (Dolphin 3.0, based on Llama 3.1 8B) and concise personas (demographics, political leaning, interests, education, toxicity propensity) to generate posts, replies, and reactions under platform rules for link and text submissions, threaded replies and daily activity cycles. We run a 30-day simulation and evaluate operational validity by comparing distributions and structures with matched Voat data: activity patterns, interaction networks, toxicity, and topic coverage. Results indicate familiar online regularities: similar activity rhythms, heavy-tailed participation, sparse low-clustering interaction networks, core-periphery structure, topical alignment with Voat, and elevated toxicity. Limitations of the current study include the stateless agent design and evaluation based on a single 30-day run, which constrains external validity and variance estimates. The simulation generates realistic discussions, often featuring toxic language, primarily centered on technology topics such as Big Tech and AI. This approach offers a valuable method for examining toxicity dynamics and testing moderation strategies within a controlled environment.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.21739v1" target="_blank">Neural Network Acceleration on MPSoC board: Integrating SLACs SNL, Rogue Software and Auto-SNL</a></h3>
                    <p><strong>Authors:</strong> Hamza Ezzaoui Rahali, Abhilasha Dave, Larry Ruckman, Mohammad Mehdi Rahimifar, Audrey C. Therrien, James J. Russel, Ryan T. Herbst</p>
                    <p><strong>Published:</strong> 8/29/2025</p>
                    <p><strong>Categories:</strong> cs.LG, cs.AI, cs.AR</p>
                    <p><strong>Summary:</strong> The LCLS-II Free Electron Laser (FEL) will generate X-ray pulses for beamline experiments at rates of up to 1~MHz, with detectors producing data throughputs exceeding 1 TB/s. Managing such massive data streams presents significant challenges, as transmission and storage infrastructures become prohibitively expensive. Machine learning (ML) offers a promising solution for real-time data reduction, but conventional implementations introduce excessive latency, making them unsuitable for high-speed experimental environments. To address these challenges, SLAC developed the SLAC Neural Network Library (SNL), a specialized framework designed to deploy real-time ML inference models on Field-Programmable Gate Arrays (FPGA). SNLs key feature is the ability to dynamically update model weights without requiring FPGA resynthesis, enhancing flexibility for adaptive learning applications. To further enhance usability and accessibility, we introduce Auto-SNL, a Python extension that streamlines the process of converting Python-based neural network models into SNL-compatible high-level synthesis code. This paper presents a benchmark comparison against hls4ml, the current state-of-the-art tool, across multiple neural network architectures, fixed-point precisions, and synthesis configurations targeting a Xilinx ZCU102 FPGA. The results showed that SNL achieves competitive or superior latency in most tested architectures, while in some cases also offering FPGA resource savings. This adaptation demonstrates SNLs versatility, opening new opportunities for researchers and academics in fields such as high-energy physics, medical imaging, robotics, and many more.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.21738v1" target="_blank">From Drone Imagery to Livability Mapping: AI-powered Environment Perception in Rural China</a></h3>
                    <p><strong>Authors:</strong> Weihuan Deng, Yaofu Huang, Luan Chen, Xun Li, Yao Yao</p>
                    <p><strong>Published:</strong> 8/29/2025</p>
                    <p><strong>Categories:</strong> cs.CY, cs.CV</p>
                    <p><strong>Summary:</strong> With the deepening of poverty alleviation and rural revitalization strategies, improving the rural living environment and enhancing the quality of life have become key priorities. Rural livability is a key indicator for measuring the effectiveness of these efforts. Current measurement approaches face significant limitations, as questionnaire-based methods are difficult to scale, while urban-oriented visual perception methods are poorly suited for rural contexts. In this paper, a rural-specific livability assessment framework was proposed based on drone imagery and multimodal large language models (MLLMs). To comprehensively assess village livability, this study first used a top-down approach to collect large-scale drone imagery of 1,766 villages in 146 counties across China. In terms of the model framework, an efficient image comparison mechanism was developed, incorporating binary search interpolation to determine effective image pairs while reducing comparison iterations. Building on expert knowledge, a chain-of-thought prompting suitable for nationwide rural livability measurement was constructed, considering both living quality and ecological habitability dimensions. This approach enhanced the rationality and reliability of the livability assessment. Finally, this study characterized the spatial heterogeneity of rural livability across China and thoroughly analyzed its influential factors. The results show that: (1) The rural livability in China demonstrates a dual-core-periphery spatial pattern, radiating outward from Sichuan and Zhejiang provinces with declining gradients; (2) Among various influential factors, government fiscal expenditure emerged as the core determinant, with each unit increase corresponding to a 3.9 - 4.9 unit enhancement in livability. The findings provide valuable insights for rural construction policy-making.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.21736v1" target="_blank">MicroLabVR: Interactive 3D Visualization of Simulated Spatiotemporal Microbiome Data in Virtual Reality</a></h3>
                    <p><strong>Authors:</strong> Simon Burbach, Maria Maleshkova, Florian Centler, Tanja Joan Schmidt</p>
                    <p><strong>Published:</strong> 8/29/2025</p>
                    <p><strong>Categories:</strong> cs.HC, cs.CE, cs.GR, q-bio.CB, q-bio.MN</p>
                    <p><strong>Summary:</strong> Microbiomes are a vital part of the human body, engaging in tasks like food digestion and immune defense. Their structure and function must be understood in order to promote host health and facilitate swift recovery during disease. Due to the difficulties in experimentally studying these systems in situ, more research is being conducted in the field of mathematical modeling. Visualizing spatiotemporal data is challenging, and current tools that simulate microbial communities spatial and temporal development often only provide limited functionalities, often requiring expert knowledge to generate useful results. To overcome these limitations, we provide a user-friendly tool to interactively explore spatiotemporal simulation data, called MicroLabVR, which transfers spatial data into virtual reality (VR) while following guidelines to enhance user experience (UX). With MicroLabVR, users can import CSV datasets containing population growth, substance concentration development, and metabolic flux distribution data. The implemented visualization methods allow users to evaluate the dataset in a VR environment interactively. MicroLabVR aims to improve data analysis for the user by allowing the exploration of microbiome data in their spatial context.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.21733v1" target="_blank">Developer Insights into Designing AI-Based Computer Perception Tools</a></h3>
                    <p><strong>Authors:</strong> Maya Guhan, Meghan E. Hurley, Eric A. Storch, John Herrington, Casey Zampella, Julia Parish-Morris, Gabriel LÃ¡zaro-MuÃ±oz, Kristin Kostick-Quenet</p>
                    <p><strong>Published:</strong> 8/29/2025</p>
                    <p><strong>Categories:</strong> cs.HC, cs.AI, cs.CY</p>
                    <p><strong>Summary:</strong> Artificial intelligence (AI)-based computer perception (CP) technologies use mobile sensors to collect behavioral and physiological data for clinical decision-making. These tools can reshape how clinical knowledge is generated and interpreted. However, effective integration of these tools into clinical workflows depends on how developers balance clinical utility with user acceptability and trustworthiness. Our study presents findings from 20 in-depth interviews with developers of AI-based CP tools. Interviews were transcribed and inductive, thematic analysis was performed to identify 4 key design priorities: 1) to account for context and ensure explainability for both patients and clinicians; 2) align tools with existing clinical workflows; 3) appropriately customize to relevant stakeholders for usability and acceptability; and 4) push the boundaries of innovation while aligning with established paradigms. Our findings highlight that developers view themselves as not merely technical architects but also ethical stewards, designing tools that are both acceptable by users and epistemically responsible (prioritizing objectivity and pushing clinical knowledge forward). We offer the following suggestions to help achieve this balance: documenting how design choices around customization are made, defining limits for customization choices, transparently conveying information about outputs, and investing in user training. Achieving these goals will require interdisciplinary collaboration between developers, clinicians, and ethicists.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.21730v1" target="_blank">Freeze and Conquer: Reusable Ansatz for Solving the Traveling Salesman Problem</a></h3>
                    <p><strong>Authors:</strong> Fabrizio Fagiolo, Nicolo Vescera</p>
                    <p><strong>Published:</strong> 8/29/2025</p>
                    <p><strong>Categories:</strong> cs.AI</p>
                    <p><strong>Summary:</strong> In this paper we present a variational algorithm for the Traveling Salesman Problem (TSP) that combines (i) a compact encoding of permutations, which reduces the qubit requirement too, (ii) an optimize-freeze-reuse strategy: where the circuit topology (``Ansatz) is first optimized on a training instance by Simulated Annealing (SA), then ``frozen and re-used on novel instances, limited to a rapid re-optimization of only the circuit parameters. This pipeline eliminates costly structural research in testing, making the procedure immediately implementable on NISQ hardware. On a set of $40$ randomly generated symmetric instances that span $4 - 7$ cities, the resulting Ansatz achieves an average optimal trip sampling probability of $100\%$ for 4 city cases, $90\%$ for 5 city cases and $80\%$ for 6 city cases. With 7 cities the success rate drops markedly to an average of $\sim 20\%$, revealing the onset of scalability limitations of the proposed method. The results show robust generalization ability for moderate problem sizes and indicate how freezing the Ansatz can dramatically reduce time-to-solution without degrading solution quality. The paper also discusses scalability limitations, the impact of ``warm-start initialization of parameters, and prospects for extension to more complex problems, such as Vehicle Routing and Job-Shop Scheduling.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.21728v1" target="_blank">AI-powered full-data set search for new physics in ultraperipheral and diffractive events</a></h3>
                    <p><strong>Authors:</strong> Simone Ragoni, Brianna Kinkaid, Janet Seger, Christopher Anson, David Tlusty</p>
                    <p><strong>Published:</strong> 8/29/2025</p>
                    <p><strong>Categories:</strong> hep-ph, hep-ex</p>
                    <p><strong>Summary:</strong> We present possible strategies for anomaly detection of rare particle decays and exotic hadrons, such as pentaquarks, in low-background environments such as those characteristic of diffractive events and ultraperipheral \pp, \pA, or \AAcoll collisions at the CERN Large Hadron Collider (LHC). Our models are trained with toy samples representing the UPC processes measured until now by the ALICE Collaboration. When samples containing rare processes such as $\jpsi\rightarrow4\pi$ and pentaquark production, where the number of injected pentaquark events is estimated based on current experimentally available upper limits, and those for $\jpsi\rightarrow4\pi$ are estimated through the branching ratio of the decay channel, are analyzed, the rare processes are flagged as anomalous by the models. This approach demonstrates the applicability of such a technique for searches for new physics in the current and future data sets at collider experiments with high purity, while also allowing for the measurement of upper limits for the production of exotica.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.21722v1" target="_blank">Inferring Effects of Major Events through Discontinuity Forecasting of Population Anxiety</a></h3>
                    <p><strong>Authors:</strong> Siddharth Mangalik, Ojas Deshpande, Adithya V. Ganesan, Sean A. P. Clouston, H. Andrew Schwartz</p>
                    <p><strong>Published:</strong> 8/29/2025</p>
                    <p><strong>Categories:</strong> cs.LG</p>
                    <p><strong>Summary:</strong> Estimating community-specific mental health effects of local events is vital for public health policy. While forecasting mental health scores alone offers limited insights into the impact of events on community well-being, quasi-experimental designs like the Longitudinal Regression Discontinuity Design (LRDD) from econometrics help researchers derive more effects that are more likely to be causal from observational data. LRDDs aim to extrapolate the size of changes in an outcome (e.g. a discontinuity in running scores for anxiety) due to a time-specific event. Here, we propose adapting LRDDs beyond traditional forecasting into a statistical learning framework whereby future discontinuities (i.e. time-specific shifts) and changes in slope (i.e. linear trajectories) are estimated given a locations history of the score, dynamic covariates (other running assessments), and exogenous variables (static representations). Applying our framework to predict discontinuities in the anxiety of US counties from COVID-19 events, we found the task was difficult but more achievable as the sophistication of models was increased, with the best results coming from integrating exogenous and dynamic covariates. Our approach shows strong improvement ($r=+.46$ for discontinuity and $r = +.65$ for slope) over traditional static community representations. Discontinuity forecasting raises new possibilities for estimating the idiosyncratic effects of potential future or hypothetical events on specific communities.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.21714v1" target="_blank">Application of Super-Sampling to Microscopy Images Produces Image Resolution below Optical Diffraction Limit</a></h3>
                    <p><strong>Authors:</strong> James N. Caron</p>
                    <p><strong>Published:</strong> 8/29/2025</p>
                    <p><strong>Categories:</strong> physics.optics</p>
                    <p><strong>Summary:</strong> Image Phase Alignment Super-Sampling (ImPASS) is a computational imaging algorithm for converting a sequence of displaced low-resolution images into a single high-resolution image. The method consists of a unique combination of Phase Correlation image registration and SeDDaRA blind deconvolution. The method has previously been validated in simulations and applied successfully to images captured in a laboratory setting. As discussed here, the performance of ImPASS surpasses similar methods that provide quantitative results. ImPASS is applied for the first time to images taken by a widefield microscope, requiring no customization other than a translation stage, to determine if this approach can subceed the diffraction limit for this application. The 80-frame image sets had as targets a slide with a slice of Porcine Cornea, and a standard US Air Force resolution chart, providing quantitative and quantitative assessments. The sets were up-sampled by a factor of eight, aligned, combined, and processed. The measurement revealed that image resolution improved by a factor of 2.68 and subceeded the diffraction limit by a factor of 1.79.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.21698v1" target="_blank">Unidentified and Confounded? Understanding Two-Tower Models for Unbiased Learning to Rank (Extended Abstract)</a></h3>
                    <p><strong>Authors:</strong> Philipp Hager, Onno Zoeter, Maarten de Rijke</p>
                    <p><strong>Published:</strong> 8/29/2025</p>
                    <p><strong>Categories:</strong> cs.IR</p>
                    <p><strong>Summary:</strong> Additive two-tower models are popular learning-to-rank methods for handling biased user feedback in industry settings. Recent studies, however, report a concerning phenomenon: training two-tower models on clicks collected by well-performing production systems leads to decreased ranking performance. This paper investigates two recent explanations for this observation: confounding effects from logging policies and model identifiability issues. We theoretically analyze the identifiability conditions of two-tower models, showing that either document swaps across positions or overlapping feature distributions are required to recover model parameters from clicks. We also investigate the effect of logging policies on two-tower models, finding that they introduce no bias when models perfectly capture user behavior. However, logging policies can amplify biases when models imperfectly capture user behavior, particularly when prediction errors correlate with document placement across positions. We propose a sample weighting technique to mitigate these effects and provide actionable insights for researchers and practitioners using two-tower models.</p>
                
            
                
                    <h3><a href="http://dx.doi.org/10.1016/j.asr.2024.06.053" target="_blank">The cosmic-ray spectrum in the PeV to EeV energy range</a></h3>
                    <p><strong>Authors:</strong> Donghwa Kang, Andreas Haungs</p>
                    <p><strong>Published:</strong> 8/29/2025</p>
                    <p><strong>Categories:</strong> astro-ph.HE</p>
                    <p><strong>Summary:</strong> Cosmic rays around the so-called knee in the spectrum at around PeV primary energy are generally galactic in origin. Observations on the form of their energy spectrum and their mass composition are fundamental tools to understand the origin, acceleration and propagation mechanism of high-energy cosmic rays. In addition, it is required to find signatures to clarify the transition from galactic to extragalactic sources, which are believed to be responsible for the highest-energy cosmic rays above EeV. This brief review focuses on recent experimental results around the knee of the all-particle energy spectrum and composition in the energy range of the knee up to EeV energies.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.21689v1" target="_blank">Mapping like a Skeptic: Probabilistic BEV Projection for Online HD Mapping</a></h3>
                    <p><strong>Authors:</strong> Fatih ErdoÄŸan, Merve Rabia BarÄ±n, Fatma GÃ¼ney</p>
                    <p><strong>Published:</strong> 8/29/2025</p>
                    <p><strong>Categories:</strong> cs.CV</p>
                    <p><strong>Summary:</strong> Constructing high-definition (HD) maps from sensory input requires accurately mapping the road elements in image space to the Birds Eye View (BEV) space. The precision of this mapping directly impacts the quality of the final vectorized HD map. Existing HD mapping approaches outsource the projection to standard mapping techniques, such as attention-based ones. However, these methods struggle with accuracy due to generalization problems, often hallucinating non-existent road elements. Our key idea is to start with a geometric mapping based on camera parameters and adapt it to the scene to extract relevant map information from camera images. To implement this, we propose a novel probabilistic projection mechanism with confidence scores to (i) refine the mapping to better align with the scene and (ii) filter out irrelevant elements that should not influence HD map generation. In addition, we improve temporal processing by using confidence scores to selectively accumulate reliable information over time. Experiments on new splits of the nuScenes and Argoverse2 datasets demonstrate improved performance over state-of-the-art approaches, indicating better generalization. The improvements are particularly pronounced on nuScenes and in the challenging long perception range. Our code and model checkpoints are available at https://github.com/Fatih-Erdogan/mapping-like-skeptic .</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.21675v1" target="_blank">Is this chart lying to me? Automating the detection of misleading visualizations</a></h3>
                    <p><strong>Authors:</strong> Jonathan Tonglet, Jan Zimny, Tinne Tuytelaars, Iryna Gurevych</p>
                    <p><strong>Published:</strong> 8/29/2025</p>
                    <p><strong>Categories:</strong> cs.CL, cs.CV, cs.GR</p>
                    <p><strong>Summary:</strong> Misleading visualizations are a potent driver of misinformation on social media and the web. By violating chart design principles, they distort data and lead readers to draw inaccurate conclusions. Prior work has shown that both humans and multimodal large language models (MLLMs) are frequently deceived by such visualizations. Automatically detecting misleading visualizations and identifying the specific design rules they violate could help protect readers and reduce the spread of misinformation. However, the training and evaluation of AI models has been limited by the absence of large, diverse, and openly available datasets. In this work, we introduce Misviz, a benchmark of 2,604 real-world visualizations annotated with 12 types of misleaders. To support model training, we also release Misviz-synth, a synthetic dataset of 81,814 visualizations generated using Matplotlib and based on real-world data tables. We perform a comprehensive evaluation on both datasets using state-of-the-art MLLMs, rule-based systems, and fine-tuned classifiers. Our results reveal that the task remains highly challenging. We release Misviz, Misviz-synth, and the accompanying code.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.21669v1" target="_blank">Cybersecurity AI: Hacking the AI Hackers via Prompt Injection</a></h3>
                    <p><strong>Authors:</strong> VÃ­ctor Mayoral-Vilches, Per Mannermaa Rynning</p>
                    <p><strong>Published:</strong> 8/29/2025</p>
                    <p><strong>Categories:</strong> cs.CR</p>
                    <p><strong>Summary:</strong> We demonstrate how AI-powered cybersecurity tools can be turned against themselves through prompt injection attacks. Prompt injection is reminiscent of cross-site scripting (XSS): malicious text is hidden within seemingly trusted content, and when the system processes it, that text is transformed into unintended instructions. When AI agents designed to find and exploit vulnerabilities interact with malicious web servers, carefully crafted reponses can hijack their execution flow, potentially granting attackers system access. We present proof-of-concept exploits against the Cybersecurity AI (CAI) framework and its CLI tool, and detail our mitigations against such attacks in a multi-layered defense implementation. Our findings indicate that prompt injection is a recurring and systemic issue in LLM-based architectures, one that will require dedicated work to address, much as the security community has had to do with XSS in traditional web applications.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.21666v1" target="_blank">Harnessing IoT and Generative AI for Weather-Adaptive Learning in Climate Resilience Education</a></h3>
                    <p><strong>Authors:</strong> Imran S. A. Khan, Emmanuel G. Blanchard, SÃ©bastien George</p>
                    <p><strong>Published:</strong> 8/29/2025</p>
                    <p><strong>Categories:</strong> cs.HC, cs.AI, cs.CY, cs.LG, cs.SE</p>
                    <p><strong>Summary:</strong> This paper introduces the Future Atmospheric Conditions Training System (FACTS), a novel platform that advances climate resilience education through place-based, adaptive learning experiences. FACTS combines real-time atmospheric data collected by IoT sensors with curated resources from a Knowledge Base to dynamically generate localized learning challenges. Learner responses are analyzed by a Generative AI powered server, which delivers personalized feedback and adaptive support. Results from a user evaluation indicate that participants found the system both easy to use and effective for building knowledge related to climate resilience. These findings suggest that integrating IoT and Generative AI into atmospherically adaptive learning technologies holds significant promise for enhancing educational engagement and fostering climate awareness.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.21654v1" target="_blank">I Stolenly Swear That I Am Up to (No) Good: Design and Evaluation of Model Stealing Attacks</a></h3>
                    <p><strong>Authors:</strong> Daryna Oliynyk, Rudolf Mayer, Kathrin Grosse, Andreas Rauber</p>
                    <p><strong>Published:</strong> 8/29/2025</p>
                    <p><strong>Categories:</strong> cs.CR, cs.LG</p>
                    <p><strong>Summary:</strong> Model stealing attacks endanger the confidentiality of machine learning models offered as a service. Although these models are kept secret, a malicious party can query a model to label data samples and train their own substitute model, violating intellectual property. While novel attacks in the field are continually being published, their design and evaluations are not standardised, making it challenging to compare prior works and assess progress in the field. This paper is the first to address this gap by providing recommendations for designing and evaluating model stealing attacks. To this end, we study the largest group of attacks that rely on training a substitute model -- those attacking image classification models. We propose the first comprehensive threat model and develop a framework for attack comparison. Further, we analyse attack setups from related works to understand which tasks and models have been studied the most. Based on our findings, we present best practices for attack development before, during, and beyond experiments and derive an extensive list of open research questions regarding the evaluation of model stealing attacks. Our findings and recommendations also transfer to other problem domains, hence establishing the first generic evaluation methodology for model stealing attacks.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.21648v1" target="_blank">Leveraging Imperfection with MEDLEY A Multi-Model Approach Harnessing Bias in Medical AI</a></h3>
                    <p><strong>Authors:</strong> Farhad Abtahi, Mehdi Astaraki, Fernando Seoane</p>
                    <p><strong>Published:</strong> 8/29/2025</p>
                    <p><strong>Categories:</strong> cs.AI, 68T07, 68T09, 68T20 (Primary) 62P10, 62C20, 62H30 (Secondary)</p>
                    <p><strong>Summary:</strong> Bias in medical artificial intelligence is conventionally viewed as a defect requiring elimination. However, human reasoning inherently incorporates biases shaped by education, culture, and experience, suggesting their presence may be inevitable and potentially valuable. We propose MEDLEY (Medical Ensemble Diagnostic system with Leveraged diversitY), a conceptual framework that orchestrates multiple AI models while preserving their diverse outputs rather than collapsing them into a consensus. Unlike traditional approaches that suppress disagreement, MEDLEY documents model-specific biases as potential strengths and treats hallucinations as provisional hypotheses for clinician verification. A proof-of-concept demonstrator was developed using over 30 large language models, creating a minimum viable product that preserved both consensus and minority views in synthetic cases, making diagnostic uncertainty and latent biases transparent for clinical oversight. While not yet a validated clinical tool, the demonstration illustrates how structured diversity can enhance medical reasoning under clinician supervision. By reframing AI imperfection as a resource, MEDLEY offers a paradigm shift that opens new regulatory, ethical, and innovation pathways for developing trustworthy medical AI systems.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.21636v1" target="_blank">Detecting Stealthy Data Poisoning Attacks in AI Code Generators</a></h3>
                    <p><strong>Authors:</strong> Cristina Improta</p>
                    <p><strong>Published:</strong> 8/29/2025</p>
                    <p><strong>Categories:</strong> cs.CR, cs.SE</p>
                    <p><strong>Summary:</strong> Deep learning (DL) models for natural language-to-code generation have become integral to modern software development pipelines. However, their heavy reliance on large amounts of data, often collected from unsanitized online sources, exposes them to data poisoning attacks, where adversaries inject malicious samples to subtly bias model behavior. Recent targeted attacks silently replace secure code with semantically equivalent but vulnerable implementations without relying on explicit triggers to launch the attack, making it especially hard for detection methods to distinguish clean from poisoned samples. We present a systematic study on the effectiveness of existing poisoning detection methods under this stealthy threat model. Specifically, we perform targeted poisoning on three DL models (CodeBERT, CodeT5+, AST-T5), and evaluate spectral signatures analysis, activation clustering, and static analysis as defenses. Our results show that all methods struggle to detect triggerless poisoning, with representation-based approaches failing to isolate poisoned samples and static analysis suffering false positives and false negatives, highlighting the need for more robust, trigger-independent defenses for AI-assisted code generation.</p>
                
            
                
                    <h3><a href="http://dx.doi.org/10.1177/02783649251368909" target="_blank">The Rosario Dataset v2: Multimodal Dataset for Agricultural Robotics</a></h3>
                    <p><strong>Authors:</strong> Nicolas Soncini, Javier Cremona, Erica Vidal, Maximiliano GarcÃ­a, GastÃ³n Castro, TaihÃº Pire</p>
                    <p><strong>Published:</strong> 8/29/2025</p>
                    <p><strong>Categories:</strong> cs.RO, cs.CV, cs.SY, eess.SY, I.2.9</p>
                    <p><strong>Summary:</strong> We present a multi-modal dataset collected in a soybean crop field, comprising over two hours of recorded data from sensors such as stereo infrared camera, color camera, accelerometer, gyroscope, magnetometer, GNSS (Single Point Positioning, Real-Time Kinematic and Post-Processed Kinematic), and wheel odometry. This dataset captures key challenges inherent to robotics in agricultural environments, including variations in natural lighting, motion blur, rough terrain, and long, perceptually aliased sequences. By addressing these complexities, the dataset aims to support the development and benchmarking of advanced algorithms for localization, mapping, perception, and navigation in agricultural robotics. The platform and data collection system is designed to meet the key requirements for evaluating multi-modal SLAM systems, including hardware synchronization of sensors, 6-DOF ground truth and loops on long trajectories. We run multimodal state-of-the art SLAM methods on the dataset, showcasing the existing limitations in their application on agricultural settings. The dataset and utilities to work with it are released on https://cifasis.github.io/rosariov2/.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.21634v1" target="_blank">Human-Written vs. AI-Generated Code: A Large-Scale Study of Defects, Vulnerabilities, and Complexity</a></h3>
                    <p><strong>Authors:</strong> Domenico Cotroneo, Cristina Improta, Pietro Liguori</p>
                    <p><strong>Published:</strong> 8/29/2025</p>
                    <p><strong>Categories:</strong> cs.SE</p>
                    <p><strong>Summary:</strong> As AI code assistants become increasingly integrated into software development workflows, understanding how their code compares to human-written programs is critical for ensuring reliability, maintainability, and security. In this paper, we present a large-scale comparison of code authored by human developers and three state-of-the-art LLMs, i.e., ChatGPT, DeepSeek-Coder, and Qwen-Coder, on multiple dimensions of software quality: code defects, security vulnerabilities, and structural complexity. Our evaluation spans over 500k code samples in two widely used languages, Python and Java, classifying defects via Orthogonal Defect Classification and security vulnerabilities using the Common Weakness Enumeration. We find that AI-generated code is generally simpler and more repetitive, yet more prone to unused constructs and hardcoded debugging, while human-written code exhibits greater structural complexity and a higher concentration of maintainability issues. Notably, AI-generated code also contains more high-risk security vulnerabilities. These findings highlight the distinct defect profiles of AI- and human-authored code and underscore the need for specialized quality assurance practices in AI-assisted programming.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.21622v1" target="_blank">Integrating Large Language Models with Network Optimization for Interactive and Explainable Supply Chain Planning: A Real-World Case Study</a></h3>
                    <p><strong>Authors:</strong> Saravanan Venkatachalam</p>
                    <p><strong>Published:</strong> 8/29/2025</p>
                    <p><strong>Categories:</strong> cs.AI</p>
                    <p><strong>Summary:</strong> This paper presents an integrated framework that combines traditional network optimization models with large language models (LLMs) to deliver interactive, explainable, and role-aware decision support for supply chain planning. The proposed system bridges the gap between complex operations research outputs and business stakeholder understanding by generating natural language summaries, contextual visualizations, and tailored key performance indicators (KPIs). The core optimization model addresses tactical inventory redistribution across a network of distribution centers for multi-period and multi-item, using a mixed-integer formulation. The technical architecture incorporates AI agents, RESTful APIs, and a dynamic user interface to support real-time interaction, configuration updates, and simulation-based insights. A case study demonstrates how the system improves planning outcomes by preventing stockouts, reducing costs, and maintaining service levels. Future extensions include integrating private LLMs, transfer learning, reinforcement learning, and Bayesian neural networks to enhance explainability, adaptability, and real-time decision-making.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.21614v1" target="_blank">Energy Detection over Composite $Îº-Î¼$ Shadowed Fading Channels with Inverse Gaussian Distribution in Ultra mMTC Networks</a></h3>
                    <p><strong>Authors:</strong> He Huang, Zeping Sui, Zilong Liu, Wei Huang, Md. Noor-A-Rahim, Haishi Wang, Zhiheng Hu</p>
                    <p><strong>Published:</strong> 8/29/2025</p>
                    <p><strong>Categories:</strong> eess.SP</p>
                    <p><strong>Summary:</strong> This paper investigates the characteristics of energy detection (ED) over composite $\kappa$-$\mu$ shadowed fading channels in ultra machine-type communication (mMTC) networks. We have derived the closed-form expressions of the probability density function (PDF) of signal-to-noise ratio (SNR) based on the Inverse Gaussian (\emph{IG}) distribution. By adopting novel integration and mathematical transformation techniques, we derive a truncation-based closed-form expression for the average detection probability for the first time. It can be observed from our simulations that the number of propagation paths has a more pronounced effect on average detection probability compared to average SNR, which is in contrast to earlier studies that focus on device-to-device networks. It suggests that for 6G mMTC network design, we should consider enhancing transmitter-receiver placement and antenna alignment strategies, rather than relying solely on increasing the device-to-device average SNR.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.21597v1" target="_blank">Time-resolved 3D imaging opportunities with XMPI at ForMAX</a></h3>
                    <p><strong>Authors:</strong> Julia Katharina Rogalinski, Zisheng Yao, Yuhe Zhang, Zhe Hu, Korneliya Gordeyeva, Tomas RosÃ©n, Daniel SÃ¶derberg, Andrea Mazzolari, Jackson da Silva, Vahid Haghighat, Samuel A. McDonald, Kim NygÃ¥rd, Eleni Myrto Asimakopoulou, Pablo Villanueva-Perez</p>
                    <p><strong>Published:</strong> 8/29/2025</p>
                    <p><strong>Categories:</strong> physics.app-ph</p>
                    <p><strong>Summary:</strong> X-rays are commonly used in imaging experiments due to their penetration power, which enables non-destructive resolution of internal structures in samples that are opaque to visible light. Time-resolved X-ray tomography is the state-of-the-art method for obtaining volumetric 4D (3D + time) information by rotating the sample and acquiring projections from different angular viewpoints over time. This method enables studies to address a plethora of research questions across various scientific disciplines. However, it faces several limitations, such as incompatibility with single-shot experiments, challenges in rotating complex sample environments that restrict the achievable rotation speed or range, and the introduction of centrifugal forces that can affect the samples dynamics. These limitations can hinder and even preclude the study of certain dynamics. Here, we present an implementation of an alternative approach, X-ray Multi-Projection Imaging (XMPI), which eliminates the need for sample rotation. Instead, the direct incident X-ray beam is split into beamlets using beam splitting X-ray optics. These beamlets intersect at the sample position from different angular viewpoints, allowing multiple projections to be acquired simultaneously. We commissioned this setup at the ForMAX beamline at MAX IV. We present projections acquired from two different sample systems - fibers under mechanical load and particle suspension in multi-phase flow - with distinct spatial and temporal resolution requirements. We demonstrate the capabilities of the ForMAX XMPI setup using the detectors full dynamical range for the relevant sample-driven spatiotemporal resolutions: i) at least 12.5 kHz framerates with 4 micrometer pixel sizes (fibers) and ii) 40 Hz acquisitions with 1.3 micrometer pixel sizes (multi-phase flows), setting the basis for a permanent XMPI endstation at ForMAX.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.21589v1" target="_blank">Middo: Model-Informed Dynamic Data Optimization for Enhanced LLM Fine-Tuning via Closed-Loop Learning</a></h3>
                    <p><strong>Authors:</strong> Zinan Tang, Xin Gao, Qizhi Pei, Zhuoshi Pan, Mengzhang Cai, Jiang Wu, Conghui He, Lijun Wu</p>
                    <p><strong>Published:</strong> 8/29/2025</p>
                    <p><strong>Categories:</strong> cs.CL, cs.AI</p>
                    <p><strong>Summary:</strong> Supervised Fine-Tuning (SFT) Large Language Models (LLM) fundamentally rely on high-quality training data. While data selection and data synthesis are two common strategies to improve data quality, existing approaches often face limitations in static dataset curation that fail to adapt to evolving model capabilities. In this paper, we introduce Middo, a self-evolving Model-informed dynamic data optimization framework that uses model-aware data selection and context-preserving data refinement. Unlike conventional one-off filtering/synthesis methods, our framework establishes a closed-loop optimization system: (1) A self-referential diagnostic module proactively identifies suboptimal samples through tri-axial model signals - loss patterns (complexity), embedding cluster dynamics (diversity), and self-alignment scores (quality); (2) An adaptive optimization engine then transforms suboptimal samples into pedagogically valuable training points while preserving semantic integrity; (3) This optimization process continuously evolves with model capability through dynamic learning principles. Experiments on multiple benchmarks demonstrate that our \method consistently enhances the quality of seed data and boosts LLMs performance with improving accuracy by 7.15% on average while maintaining the original dataset scale. This work establishes a new paradigm for sustainable LLM training through dynamic human-AI co-evolution of data and models. Our datasets, models, and code are coming soon.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.21587v1" target="_blank">A Survey on Current Trends and Recent Advances in Text Anonymization</a></h3>
                    <p><strong>Authors:</strong> Tobias DeuÃŸer, Lorenz Sparrenberg, Armin Berger, Max HahnbÃ¼ck, Christian Bauckhage, Rafet Sifa</p>
                    <p><strong>Published:</strong> 8/29/2025</p>
                    <p><strong>Categories:</strong> cs.CL, cs.AI</p>
                    <p><strong>Summary:</strong> The proliferation of textual data containing sensitive personal information across various domains requires robust anonymization techniques to protect privacy and comply with regulations, while preserving data usability for diverse and crucial downstream tasks. This survey provides a comprehensive overview of current trends and recent advances in text anonymization techniques. We begin by discussing foundational approaches, primarily centered on Named Entity Recognition, before examining the transformative impact of Large Language Models, detailing their dual role as sophisticated anonymizers and potent de-anonymization threats. The survey further explores domain-specific challenges and tailored solutions in critical sectors such as healthcare, law, finance, and education. We investigate advanced methodologies incorporating formal privacy models and risk-aware frameworks, and address the specialized subfield of authorship anonymization. Additionally, we review evaluation frameworks, comprehensive metrics, benchmarks, and practical toolkits for real-world deployment of anonymization solutions. This review consolidates current knowledge, identifies emerging trends and persistent challenges, including the evolving privacy-utility trade-off, the need to address quasi-identifiers, and the implications of LLM capabilities, and aims to guide future research directions for both academics and practitioners in this field.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.21572v1" target="_blank">NewsReX: A More Efficient Approach to News Recommendation with Keras 3 and JAX</a></h3>
                    <p><strong>Authors:</strong> Igor L. R. Azevedo, Toyotaro Suzumura, Yuichiro Yasui</p>
                    <p><strong>Published:</strong> 8/29/2025</p>
                    <p><strong>Categories:</strong> cs.IR</p>
                    <p><strong>Summary:</strong> Reproducing and comparing results in news recommendation research has become increasingly difficult. This is due to a fragmented ecosystem of diverse codebases, varied configurations, and mainly due to resource-intensive models. We introduce NewsReX, an open-source library designed to streamline this process. Our key contribution is a modern implementation built on Keras 3 and JAX, which provides an increase in computational efficiency. Experiments show that NewsReX is faster than current implementations. To support broader research, we provide a straightforward guide and scripts for training models on custom datasets. We validated this functionality using a proprietary Japanese news dataset from Nikkei News, a leading Japanese media corporation renowned for its comprehensive coverage of business, economic, and financial news. NewsReX makes reproducing complex experiments faster and more accessible to a wider range of hardware making sure the speed up it also achieved for less powerful GPUs, like an 8GB RTX 3060 Ti. Beyond the library, this paper offers an analysis of key training parameters often overlooked in the literature, including the effect of different negative sampling strategies, the varying number of epochs, the impact of random batching, and more. This supplementary analysis serves as a valuable reference for future research, aiming to reduce redundant computation when comparing baselines and guide best practices. Code available at https://github.com/igor17400/NewsReX.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.21561v1" target="_blank">Summarize-Exemplify-Reflect: Data-driven Insight Distillation Empowers LLMs for Few-shot Tabular Classification</a></h3>
                    <p><strong>Authors:</strong> Yifei Yuan, Jiatong Li, Weijia Zhang, Mohammad Aliannejadi, Evangelos Kanoulas, Renjun Hu</p>
                    <p><strong>Published:</strong> 8/29/2025</p>
                    <p><strong>Categories:</strong> cs.LG, cs.CL</p>
                    <p><strong>Summary:</strong> Recent studies show the promise of large language models (LLMs) for few-shot tabular classification but highlight challenges due to the variability in structured data. To address this, we propose distilling data into actionable insights to enable robust and effective classification by LLMs. Drawing inspiration from human learning processes, we introduce InsightTab, an insight distillation framework guided by principles of divide-and-conquer, easy-first, and reflective learning. Our approach integrates rule summarization, strategic exemplification, and insight reflection through deep collaboration between LLMs and data modeling techniques. The obtained insights enable LLMs to better align their general knowledge and capabilities with the particular requirements of specific tabular tasks. We extensively evaluate InsightTab on nine datasets. The results demonstrate consistent improvement over state-of-the-art methods. Ablation studies further validate the principle-guided distillation process, while analyses emphasize InsightTabs effectiveness in leveraging labeled data and managing bias.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.21548v1" target="_blank">Quantum Leap in Finance: Economic Advantages, Security, and Post-Quantum Readiness</a></h3>
                    <p><strong>Authors:</strong> Gerhard Hellstern, Esra Yeniaras</p>
                    <p><strong>Published:</strong> 8/29/2025</p>
                    <p><strong>Categories:</strong> quant-ph</p>
                    <p><strong>Summary:</strong> This paper provides an in-depth review of the evolving role of quantum computing in the financial sector, emphasizing both its computational potential and cybersecurity implications. Distinguishing itself from existing surveys, this work integrates classical quantum computing applications - such as portfolio optimization, risk analysis, derivative pricing, and Monte Carlo simulations with a thorough examination of blockchain technologies and post-quantum cryptography (PQC), which are crucial for maintaining secure financial operations in the emerging quantum era. We propose a structured four-step framework to assess the feasibility and expected benefits of implementing quantum solutions in finance, considering factors such as computational scalability, error tolerance, data complexity, and practical implementability. This framework is applied to a series of representative financial scenarios to identify domains where quantum approaches can surpass classical techniques. Furthermore, the paper explores the vulnerabilities quantum computing introduces to digital finance-related applications and blockchain security, including risks to digital signatures, hash functions, and randomness generation, and discusses mitigation strategies through PQC and quantum-resilient alternatives of classical digital finance tools and blockchain architectures. By addressing both quantum blockchain, quantum key distribution (QKD) as well as quantum communication networks, his review presents a more holistic perspective than prior studies, offering actionable insights for researchers, financial practitioners, and policymakers navigating the intersection of quantum computing, blockchain, and secure financial systems.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.21540v1" target="_blank">HealthProcessAI: A Technical Framework and Proof-of-Concept for LLM-Enhanced Healthcare Process Mining</a></h3>
                    <p><strong>Authors:</strong> Eduardo Illueca-Fernandez, Kaile Chen, Fernando Seoane, Farhad Abtahi</p>
                    <p><strong>Published:</strong> 8/29/2025</p>
                    <p><strong>Categories:</strong> cs.AI</p>
                    <p><strong>Summary:</strong> Process mining has emerged as a powerful analytical technique for understanding complex healthcare workflows. However, its application faces significant barriers, including technical complexity, a lack of standardized approaches, and limited access to practical training resources. We introduce HealthProcessAI, a GenAI framework designed to simplify process mining applications in healthcare and epidemiology by providing a comprehensive wrapper around existing Python (PM4PY) and R (bupaR) libraries. To address unfamiliarity and improve accessibility, the framework integrates multiple Large Language Models (LLMs) for automated process map interpretation and report generation, helping translate technical analyses into outputs that diverse users can readily understand. We validated the framework using sepsis progression data as a proof-of-concept example and compared the outputs of five state-of-the-art LLM models through the OpenRouter platform. To test its functionality, the framework successfully processed sepsis data across four proof-of-concept scenarios, demonstrating robust technical performance and its capability to generate reports through automated LLM analysis. LLM evaluation using five independent LLMs as automated evaluators revealed distinct model strengths: Claude Sonnet-4 and Gemini 2.5-Pro achieved the highest consistency scores (3.79/4.0 and 3.65/4.0) when evaluated by automated LLM assessors. By integrating multiple Large Language Models (LLMs) for automated interpretation and report generation, the framework addresses widespread unfamiliarity with process mining outputs, making them more accessible to clinicians, data scientists, and researchers. This structured analytics and AI-driven interpretation combination represents a novel methodological advance in translating complex process mining results into potentially actionable insights for healthcare applications.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.21539v1" target="_blank">HCCM: Hierarchical Cross-Granularity Contrastive and Matching Learning for Natural Language-Guided Drones</a></h3>
                    <p><strong>Authors:</strong> Hao Ruan, Jinliang Lin, Yingxin Lai, Zhiming Luo, Shaozi Li</p>
                    <p><strong>Published:</strong> 8/29/2025</p>
                    <p><strong>Categories:</strong> cs.CV</p>
                    <p><strong>Summary:</strong> Natural Language-Guided Drones (NLGD) provide a novel paradigm for tasks such as target matching and navigation. However, the wide field of view and complex compositional semantics in drone scenarios pose challenges for vision-language understanding. Mainstream Vision-Language Models (VLMs) emphasize global alignment while lacking fine-grained semantics, and existing hierarchical methods depend on precise entity partitioning and strict containment, limiting effectiveness in dynamic environments. To address this, we propose the Hierarchical Cross-Granularity Contrastive and Matching learning (HCCM) framework with two components: (1) Region-Global Image-Text Contrastive Learning (RG-ITC), which avoids precise scene partitioning and captures hierarchical local-to-global semantics by contrasting local visual regions with global text and vice versa; (2) Region-Global Image-Text Matching (RG-ITM), which dispenses with rigid constraints and instead evaluates local semantic consistency within global cross-modal representations, enhancing compositional reasoning. Moreover, drone text descriptions are often incomplete or ambiguous, destabilizing alignment. HCCM introduces a Momentum Contrast and Distillation (MCD) mechanism to improve robustness. Experiments on GeoText-1652 show HCCM achieves state-of-the-art Recall@1 of 28.8% (image retrieval) and 14.7% (text retrieval). On the unseen ERA dataset, HCCM demonstrates strong zero-shot generalization with 39.93% mean recall (mR), outperforming fine-tuned baselines.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.21538v1" target="_blank">Adaptive extended Kalman filter and laser link acquisition in the detection of gravitational waves in space</a></h3>
                    <p><strong>Authors:</strong> Jinke Yang, Yong Xie, Yidi Fan, Pengcheng Wang, Xindong Liang, Haojie Li, Xue Wang, Zhao Cui, Jianjun Jia, Yucheng Tang, Yun Kau Lau</p>
                    <p><strong>Published:</strong> 8/29/2025</p>
                    <p><strong>Categories:</strong> astro-ph.IM, gr-qc</p>
                    <p><strong>Summary:</strong> An alternative, new laser link acquisition scheme for the triangular constellation of spacecraft (SCs) in deep space in the detection of gravitational waves is considered. In place of a wide field CCD camera in the initial stage of laser link acquisition adopted in the conventional scheme, an extended Kalman filter based on precision orbit determination is incorporated in the point ahead angle mechanism (PAAM) to steer the laser beam in such a way to narrow the uncertainty cone and at the same time avoids the heating problem generated by the CCD camera.A quadrant photodetector (QPD) based on the Differential Power Sensing (DPS) technique, which offers a higher dynamic range than differential wavefront sensing (DWS), is employed as the readout of the laser beam spot. The conventional two stages (coarse acquisition and fine acquisition) are integrated into a single control loop. The payload structure of the ATP control loop is simplified and numerical simulations, based on a colored measurement noise model that closely mimics the prospective on-orbit conditions, demonstrate that the AEKF significantly reduces the initial uncertainty region by predicting the point ahead angle (PAA) even when the worst case scenario in SC position (navigation) error is considered.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.21536v1" target="_blank">Triply Robust Panel Estimators</a></h3>
                    <p><strong>Authors:</strong> Susan Athey, Guido Imbens, Zhaonan Qu, Davide Viviano</p>
                    <p><strong>Published:</strong> 8/29/2025</p>
                    <p><strong>Categories:</strong> stat.ME</p>
                    <p><strong>Summary:</strong> This paper studies estimation of causal effects in a panel data setting. We introduce a new estimator, the Triply RObust Panel (TROP) estimator, that combines $(i)$ a flexible model for the potential outcomes based on a low-rank factor structure on top of a two-way-fixed effect specification, with $(ii)$ unit weights intended to upweight units similar to the treated units and $(iii)$ time weights intended to upweight time periods close to the treated time periods. We study the performance of the estimator in a set of simulations designed to closely match several commonly studied real data sets. We find that there is substantial variation in the performance of the estimators across the settings considered. The proposed estimator outperforms two-way-fixed-effect/difference-in-differences, synthetic control, matrix completion and synthetic-difference-in-differences estimators. We investigate what features of the data generating process lead to this performance, and assess the relative importance of the three components of the proposed estimator. We have two recommendations. Our preferred strategy is that researchers use simulations closely matched to the data they are interested in, along the lines discussed in this paper, to investigate which estimators work well in their particular setting. A simpler approach is to use more robust estimators such as synthetic difference-in-differences or the new triply robust panel estimator which we find to substantially outperform two-way fixed effect estimators in many empirically relevant settings.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.21535v1" target="_blank">Non-Take-Up of Unemployment Benefit II in Germany: A Longitudinal Perspective Using Administrative Data</a></h3>
                    <p><strong>Authors:</strong> JÃ¼rgen Wiemers</p>
                    <p><strong>Published:</strong> 8/29/2025</p>
                    <p><strong>Categories:</strong> econ.GN, q-fin.EC</p>
                    <p><strong>Summary:</strong> Extensive research demonstrates that many households eligible for means-tested benefits do not claim them, a phenomenon known as non-take-up. Empirical studies frequently conceptualise non-take-up as a rational decision, occurring when the perceived net utility of claiming is negative. Theoretically, long-term factors can substantially impact this decision. Despite the potential relevance of longitudinal aspects, evidence on their influence remains limited. This study addresses this gap by incorporating long-term factors in the analysis of non-take-up behaviour relating to Unemployment Benefit II (UB II), Germanys basic means-tested welfare programme. Using data from the German Panel Study Labour Market and Social Security (PASS) from 2008 to 2020, linked with administrative data from Germanys Federal Employment Agency (PASS-ADIAB), this study reconstructs households benefit receipt and income histories, even during non-survey periods. This allows modelling benefit non-take-up for eligible households using the duration and frequency of past benefit receipt. In addition, the use of administrative data mitigates bias from self-reported benefit receipt. Household eligibility for UB II is simulated using GETTSIM, an open-source microsimulation model, applied to the PASS dataset for the first time. Findings indicate that long-term factors significantly influence the probability of claiming UB II. Specifically, a longer history of benefit receipt increases this probability, whereas higher income potential and positive income shocks reduce it. Including long-term factors substantially affects the estimated impact of traditionally used determinants of non-take-up, indicating a potential misspecification in existing models that neglect them.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.21526v1" target="_blank">Chemical Control of Mechanical Anisotropy and Band Alignment in Perylene-based Two-dimensional MoS$_2$-Organic Hybrids</a></h3>
                    <p><strong>Authors:</strong> Mohammed El Amine Miloudi, Oliver KÃ¼hn</p>
                    <p><strong>Published:</strong> 8/29/2025</p>
                    <p><strong>Categories:</strong> cond-mat.mtrl-sci, cond-mat.mes-hall</p>
                    <p><strong>Summary:</strong> This study presents a comprehensive investigation of hybrid interfaces formed by monolayer MoS$_2$ coupled with the organic molecules perylene (P), perylene diimide (PDI), and perylene orange (PO). Using density functional theory, we demonstrate the extent to which the mechanical and electronic properties of a hybrid system can be altered by the chemical modification of a given chromophore. The three systems exhibit distinct differences due to their chemical composition and van der Waals contact enabled by their geometry. All systems are structurally stable. The binding energies follow the order PD$$P$$PO due to the large $\pi$-system (PD) and strong structural distortion (PO). Youngs modulus and Poissons ratio exhibit pronounced anisotropy in all cases. PO exhibits the greatest anisotropy due to steric effects and a permanent dipole, which introduce directionality to the molecule-surface interaction. Physisorption is accompanied by net charge transfer in the same order as the binding energies. The associated interfacial polarization results in a change in the work function compared to pristine MoS$_2$ in the order P$$PO$$PD. Finally, the presence of organic molecules introduces states into the MoS$_2$ energy gap, with the band alignment being either type II (P, PO) or type I (PD).</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.21517v1" target="_blank">Modeling Wise Decision Making: A Z-Number Fuzzy Framework Inspired by Phronesis</a></h3>
                    <p><strong>Authors:</strong> Sweta Kaman, Ankita Sharma, Romi Banerjee</p>
                    <p><strong>Published:</strong> 8/29/2025</p>
                    <p><strong>Categories:</strong> cs.AI</p>
                    <p><strong>Summary:</strong> Background: Wisdom is a superordinate construct that embraces perspective taking, reflectiveness, prosocial orientation, reflective empathetic action, and intellectual humility. Unlike conventional models of reasoning that are rigidly bound by binary thinking, wisdom unfolds in shades of ambiguity, requiring both graded evaluation and self-reflective humility. Current measures depend on self-reports and seldom reflect the humility and uncertainty inherent in wise reasoning. A computational framework that takes into account both multidimensionality and confidence has the potential to improve psychological science and allow humane AI. Method: We present a fuzzy inference system with Z numbers, each of the decisions being expressed in terms of a wisdom score (restriction) and confidence score (certainty). As part of this study, participants (N = 100) were exposed to culturally neutral pictorial moral dilemma tasks to which they generated think-aloud linguistic responses, which were mapped into five theoretically based components of wisdom. The scores of each individual component were combined using a base of 21 rules, with membership functions tuned via Gaussian kernel density estimation. Results: In a proof of concept study, the system produced dual attribute wisdom representations that correlated modestly but significantly with established scales while showing negligible relations with unrelated traits, supporting convergent and divergent validity. Contribution: The contribution is to formalize wisdom as a multidimensional, uncertainty-conscious construct, operationalized in the form of Z-numbers. In addition to progressing measurement in psychology, it calculates how fuzzy Z numbers can provide AI systems with interpretable, confidence-sensitive reasoning that affords a safe, middle ground between rigorous computation and human-like judgment.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.21516v1" target="_blank">A Combined Push-Pull Access Framework for Digital Twin Alignment and Anomaly Reporting</a></h3>
                    <p><strong>Authors:</strong> Federico Chiariotti, Fabio Saggese, Andrea Munari, Leonardo Badia, Petar Popovski</p>
                    <p><strong>Published:</strong> 8/29/2025</p>
                    <p><strong>Categories:</strong> cs.NI, 94A05, C.2.1; C.2.5</p>
                    <p><strong>Summary:</strong> A digital twin (DT) contains a set of virtual models of real systems and processes that are synchronized to their physical counterparts. This enables experimentation and examination of counterfactuals, simulating the consequences of decisions in real time. However, the DT accuracy relies on timely updates that maintain alignment with the real system. We can distinguish between: (i) pull-updates, which follow a request from the DT to the sensors, to decrease its drift from the physical state; (ii) push-updates, which are sent directly by the sensors since they represent urgent information, such as anomalies. In this work, we devise a push-pull scheduler (PPS) medium access framework, which dynamically allocates the communication resources used for these two types of updates. Our scheme strikes a balance in the trade-off between DT alignment in normal conditions and anomaly reporting, optimizing resource usage and reducing the drift age of incorrect information (AoII) by over 20% with respect to state-of-the-art solutions, while maintaining the same anomaly detection guarantees, as well as reducing the worst-case anomaly detection AoII from 70 ms to 20 ms when considering a 1 ms average drift AoII constraint.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.21499v1" target="_blank">Gromov hyperbolicity II: Dimension-free inner uniform estimates for quasigeodesics</a></h3>
                    <p><strong>Authors:</strong> Chang-Yu Guo, Manzi Huang, Yaxiang Li, Xiantao Wang</p>
                    <p><strong>Published:</strong> 8/29/2025</p>
                    <p><strong>Categories:</strong> math.CV, math.MG</p>
                    <p><strong>Summary:</strong> This is the second article of a series of our recent works, addressing an open question of Bonk-Heinonen-Koskela [3], to study the relationship between (inner) uniformality and Gromov hyperbolicity in infinite dimensional spaces. Our main focus of this paper is to establish a dimension-free inner uniform estimate for quasigeodesics. More precisely, we prove that a $c_0$-quasigeodesic in a $\delta$-Gromov hyperbolic $c$-John domain in $\mathbb{R}^n$ is $b$-inner uniform, for some constant $b$ depending only on $c_0$, $\delta$ and $c$, but not on the dimension $n$. The proof relies crucially on the techniques introduced by Guo-Huang-Wang in their recent work [arXiv:2502.02930, 2025]. In particular, we actually show that the above result holds in general Banach spaces, which answers affirmatively an open question of J. V\ais\al\a in [Analysis, 2004] and partially addresses the open question of Bonk-Heinonen-Koskela in [Asterisque, 2001]. As a byproduct of our main result, we obtain that a $c_0$-quasigeodesic in a $\delta$-Gromov hyperbolic $c$-John domain in $\mathbb{R}^n$ is a $b$-cone arc with a dimension-free constant $b=b(c_0,\delta,c)$. This resolves an open problem of J. Heinonen in [Rev. Math. Iberoam., 1989].</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.21488v1" target="_blank">Priors Matter: Addressing Misspecification in Bayesian Deep Q-Learning</a></h3>
                    <p><strong>Authors:</strong> Pascal R. van der Vaart, Neil Yorke-Smith, Matthijs T. J. Spaan</p>
                    <p><strong>Published:</strong> 8/29/2025</p>
                    <p><strong>Categories:</strong> cs.LG, cs.AI</p>
                    <p><strong>Summary:</strong> Uncertainty quantification in reinforcement learning can greatly improve exploration and robustness. Approximate Bayesian approaches have recently been popularized to quantify uncertainty in model-free algorithms. However, so far the focus has been on improving the accuracy of the posterior approximation, instead of studying the accuracy of the prior and likelihood assumptions underlying the posterior. In this work, we demonstrate that there is a cold posterior effect in Bayesian deep Q-learning, where contrary to theory, performance increases when reducing the temperature of the posterior. To identify and overcome likely causes, we challenge common assumptions made on the likelihood and priors in Bayesian model-free algorithms. We empirically study prior distributions and show through statistical tests that the common Gaussian likelihood assumption is frequently violated. We argue that developing more suitable likelihoods and priors should be a key focus in future Bayesian reinforcement learning research and we offer simple, implementable solutions for better priors in deep Q-learning that lead to more performant Bayesian algorithms.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.21484v2" target="_blank">Data-driven Discovery of Digital Twins in Biomedical Research</a></h3>
                    <p><strong>Authors:</strong> ClÃ©mence MÃ©tayer, Annabelle Ballesta, Julien Martinelli</p>
                    <p><strong>Published:</strong> 8/29/2025</p>
                    <p><strong>Categories:</strong> q-bio.QM, cs.LG, stat.ML</p>
                    <p><strong>Summary:</strong> Recent technological advances have expanded the availability of high-throughput biological datasets, enabling the reliable design of digital twins of biomedical systems or patients. Such computational tools represent key reaction networks driving perturbation or drug response and can guide drug discovery and personalized therapeutics. Yet, their development still relies on laborious data integration by the human modeler, so that automated approaches are critically needed. The success of data-driven system discovery in Physics, rooted in clean datasets and well-defined governing laws, has fueled interest in applying similar techniques in Biology, which presents unique challenges. Here, we reviewed methodologies for automatically inferring digital twins from biological time series, which mostly involve symbolic or sparse regression. We evaluate algorithms according to eight biological and methodological challenges, associated to noisy/incomplete data, multiple conditions, prior knowledge integration, latent variables, high dimensionality, unobserved variable derivatives, candidate library design, and uncertainty quantification. Upon these criteria, sparse regression generally outperformed symbolic regression, particularly when using Bayesian frameworks. We further highlight the emerging role of deep learning and large language models, which enable innovative prior knowledge integration, though the reliability and consistency of such approaches must be improved. While no single method addresses all challenges, we argue that progress in learning digital twins will come from hybrid and modular frameworks combining chemical reaction network-based mechanistic grounding, Bayesian uncertainty quantification, and the generative and knowledge integration capacities of deep learning. To support their development, we further propose a benchmarking framework to evaluate methods across all challenges.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.21476v1" target="_blank">Igniting Creative Writing in Small Language Models: LLM-as-a-Judge versus Multi-Agent Refined Rewards</a></h3>
                    <p><strong>Authors:</strong> Xiaolong Wei, Bo Lu, Xingyu Zhang, Zhejun Zhao, Dongdong Shen, Long Xia, Dawei Yin</p>
                    <p><strong>Published:</strong> 8/29/2025</p>
                    <p><strong>Categories:</strong> cs.CL, cs.AI</p>
                    <p><strong>Summary:</strong> Large Language Models (LLMs) have demonstrated remarkable creative writing capabilities, yet their substantial computational demands hinder widespread use. Enhancing Small Language Models (SLMs) offers a promising alternative, but current methods like Supervised Fine-Tuning (SFT) struggle with novelty, and Reinforcement Learning from Human Feedback (RLHF) is costly. This paper explores two distinct AI-driven reward strategies within a Reinforcement Learning from AI Feedback (RLAIF) framework to ignite the creative writing of a 7B-parameter SLM, specifically for generating Chinese greetings. The first strategy employs a RM trained on high-quality preference data curated by a novel multi-agent rejection sampling framework designed for creative tasks. The second, more novel strategy utilizes a principle-guided LLM-as-a-Judge, whose reward function is optimized via an adversarial training scheme with a reflection mechanism, to directly provide reward signals. Comprehensive experiments reveal that while both approaches significantly enhance creative output over baselines, the principle-guided LLM-as-a-Judge demonstrably yields superior generation quality. Furthermore, it offers notable advantages in training efficiency and reduced dependency on human-annotated data, presenting a more scalable and effective path towards creative SLMs. Our automated evaluation methods also exhibit strong alignment with human judgments. Our code and data are publicly available at https://github.com/weixiaolong94-hub/Igniting-Creative-Writing-in-Small-Language-Models.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.21825v1" target="_blank">Standard Model Baryon Number Violation at Zero Temperature from Higgs Bubble Collisions</a></h3>
                    <p><strong>Authors:</strong> Nabeen Bhusal, Simone Blasi, Martina Cataldi, Aleksandr Chatrchyan, Marco Gorghetto, Geraldine Servant</p>
                    <p><strong>Published:</strong> 8/29/2025</p>
                    <p><strong>Categories:</strong> hep-ph</p>
                    <p><strong>Summary:</strong> We compute for the first time baryon number violation at zero temperature from Higgs bubble collisions and find that it can be of the same order as that from thermal sphalerons in the symmetric phase at electroweak temperatures. We study the dependence of the rate of Chern--Simons number transitions on the shape of the scalar potential and on the Lorentz factor of the bubble walls at collision via large-scale (3+1)D lattice simulations of the Higgs doublet and SU(2) gauge fields. We estimate the resulting baryon asymmetry assuming some CP-violating source activated by the Higgs-field variation during the phase transition.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.21824v1" target="_blank">DriveQA: Passing the Driving Knowledge Test</a></h3>
                    <p><strong>Authors:</strong> Maolin Wei, Wanzhou Liu, Eshed Ohn-Bar</p>
                    <p><strong>Published:</strong> 8/29/2025</p>
                    <p><strong>Categories:</strong> cs.CV</p>
                    <p><strong>Summary:</strong> If a Large Language Model (LLM) were to take a driving knowledge test today, would it pass? Beyond standard spatial and visual question-answering (QA) tasks on current autonomous driving benchmarks, driving knowledge tests require a complete understanding of all traffic rules, signage, and right-of-way principles. To pass this test, human drivers must discern various edge cases that rarely appear in real-world datasets. In this work, we present DriveQA, an extensive open-source text and vision-based benchmark that exhaustively covers traffic regulations and scenarios. Through our experiments using DriveQA, we show that (1) state-of-the-art LLMs and Multimodal LLMs (MLLMs) perform well on basic traffic rules but exhibit significant weaknesses in numerical reasoning and complex right-of-way scenarios, traffic sign variations, and spatial layouts, (2) fine-tuning on DriveQA improves accuracy across multiple categories, particularly in regulatory sign recognition and intersection decision-making, (3) controlled variations in DriveQA-V provide insights into model sensitivity to environmental factors such as lighting, perspective, distance, and weather conditions, and (4) pretraining on DriveQA enhances downstream driving task performance, leading to improved results on real-world datasets such as nuScenes and BDD, while also demonstrating that models can internalize text and synthetic traffic knowledge to generalize effectively across downstream QA tasks.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.21823v1" target="_blank">A new characterization of the holographic entropy cone</a></h3>
                    <p><strong>Authors:</strong> Guglielmo Grimaldi, Matthew Headrick, Veronika E. Hubeny</p>
                    <p><strong>Published:</strong> 8/29/2025</p>
                    <p><strong>Categories:</strong> hep-th</p>
                    <p><strong>Summary:</strong> Entanglement entropies computed using the holographic Ryu-Takayanagi formula are known to obey an infinite set of linear inequalities, which define the so-called RT entropy cone. The general structure of this cone, or equivalently the set of all valid inequalities, is unknown. It is also unknown whether those same inequalities are also obeyed by entropies computed using the covariant Hubeny-Rangamani-Takayanagi formula, although significant evidence has accumulated that they are. Using Markov states, we develop a test of this conjecture in a heretofore unexplored regime. The test reduces to checking that a given inequality obeys a certain majorization property, which is easy to evaluate. We find that the RT inequalities pass this test and, surprisingly, \emph{only} RT inequalities do so. Our results not only provide strong new evidence that the HRT and RT cones coincide, but also offer a completely new characterization of that cone.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.21817v1" target="_blank">An Introduction to Gravitational Wave Theory</a></h3>
                    <p><strong>Authors:</strong> Simone Speziale, DaniÃ¨le A. Steer</p>
                    <p><strong>Published:</strong> 8/29/2025</p>
                    <p><strong>Categories:</strong> gr-qc, astro-ph.CO, hep-th</p>
                    <p><strong>Summary:</strong> Introduction to the theoretical foundations of gravitational waves: from general relativity to detection and binary system waveforms. Lecture notes prepared for the MaNiTou summer school on gravitational waves. Draft chapter for the CNRS contemporary Encyclopaedia Sciences to be published by ISTE.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.21816v1" target="_blank">The Demon is in Ambiguity: Revisiting Situation Recognition with Single Positive Multi-Label Learning</a></h3>
                    <p><strong>Authors:</strong> Yiming Lin, Yuchen Niu, Shang Wang, Kaizhu Huang, Qiufeng Wang, Xiao-Bo Jin</p>
                    <p><strong>Published:</strong> 8/29/2025</p>
                    <p><strong>Categories:</strong> cs.CV, cs.AI</p>
                    <p><strong>Summary:</strong> Context recognition (SR) is a fundamental task in computer vision that aims to extract structured semantic summaries from images by identifying key events and their associated entities. Specifically, given an input image, the model must first classify the main visual events (verb classification), then identify the participating entities and their semantic roles (semantic role labeling), and finally localize these entities in the image (semantic role localization). Existing methods treat verb classification as a single-label problem, but we show through a comprehensive analysis that this formulation fails to address the inherent ambiguity in visual event recognition, as multiple verb categories may reasonably describe the same image. This paper makes three key contributions: First, we reveal through empirical analysis that verb classification is inherently a multi-label problem due to the ubiquitous semantic overlap between verb categories. Second, given the impracticality of fully annotating large-scale datasets with multiple labels, we propose to reformulate verb classification as a single positive multi-label learning (SPMLL) problem - a novel perspective in SR research. Third, we design a comprehensive multi-label evaluation benchmark for SR that is carefully designed to fairly evaluate model performance in a multi-label setting. To address the challenges of SPMLL, we futher develop the Graph Enhanced Verb Multilayer Perceptron (GE-VerbMLP), which combines graph neural networks to capture label correlations and adversarial training to optimize decision boundaries. Extensive experiments on real-world datasets show that our approach achieves more than 3\% MAP improvement while remaining competitive on traditional top-1 and top-5 accuracy metrics.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.21809v1" target="_blank">VoCap: Video Object Captioning and Segmentation from Any Prompt</a></h3>
                    <p><strong>Authors:</strong> Jasper Uijlings, Xingyi Zhou, Xiuye Gu, Arsha Nagrani, Anurag Arnab, Alireza Fathi, David Ross, Cordelia Schmid</p>
                    <p><strong>Published:</strong> 8/29/2025</p>
                    <p><strong>Categories:</strong> cs.CV</p>
                    <p><strong>Summary:</strong> Understanding objects in videos in terms of fine-grained localization masks and detailed semantic properties is a fundamental task in video understanding. In this paper, we propose VoCap, a flexible video model that consumes a video and a prompt of various modalities (text, box or mask), and produces a spatio-temporal masklet with a corresponding object-centric caption. As such our model addresses simultaneously the tasks of promptable video object segmentation, referring expression segmentation, and object captioning. Since obtaining data for this task is tedious and expensive, we propose to annotate an existing large-scale segmentation dataset (SAV) with pseudo object captions. We do so by preprocessing videos with their ground-truth masks to highlight the object of interest and feed this to a large Vision Language Model (VLM). For an unbiased evaluation, we collect manual annotations on the validation set. We call the resulting dataset SAV-Caption. We train our VoCap model at scale on a SAV-Caption together with a mix of other image and video datasets. Our model yields state-of-the-art results on referring expression video object segmentation, is competitive on semi-supervised video object segmentation, and establishes a benchmark for video object captioning. Our dataset will be made available at https://github.com/google-deepmind/vocap.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.21808v1" target="_blank">Unitary induced channels and Tsirelsons problem</a></h3>
                    <p><strong>Authors:</strong> MichaÅ‚ Banacki, PaweÅ‚ Horodecki</p>
                    <p><strong>Published:</strong> 8/29/2025</p>
                    <p><strong>Categories:</strong> quant-ph, math-ph, math.MP, 81R15</p>
                    <p><strong>Summary:</strong> Motivated by a recent progress concerning quantum commuting and quantum tensor models of composed systems we investigate a notion of (generalized) unitary induced quantum channel. Using properties of Brown algebras we provide an equivalent characterization of discussed families in both tensor and commuting paradigms. In particular, we provide an equivalent formulation of Tsirelsons conjecture (Connes embedding problem) in terms of considered paradigms based on protocols which do not require measurements performed on infinite-dimensional subsystems. As a result we show that there is a difference between quantum commuting and quantum tensor models for generalized unitary induced channels.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.21807v1" target="_blank">Epsilon-saturation for stable graphs and Littlestone classes</a></h3>
                    <p><strong>Authors:</strong> Maryanthe Malliaris, Olga Medrano MartÃ­n del Campo, Shay Moran</p>
                    <p><strong>Published:</strong> 8/29/2025</p>
                    <p><strong>Categories:</strong> math.LO, cs.DM, cs.LO, math.CO</p>
                    <p><strong>Summary:</strong> Any Littlestone class, or stable graph, has finite sets which function as ``virtual elements: these can be seen from the learning side as representing hypotheses which are expressible as weighted majority opinions of hypotheses in the class, and from the model-theoretic side as an approximate finitary version of realizing types. We introduce and study the epsilon-saturation of a Littlestone class, or stable graph, which is essentially the closure of the class under inductively adding all such virtual elements. We characterize this closure and prove that under reasonable choices of parameters, it remains Littlestone (or stable), though not always of the same Littlestone dimension. This highlights some surprising phenomena having to do with regimes of epsilon and the relation between Littlestone/stability and VC dimension.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.21803v1" target="_blank">Automated Clinical Problem Detection from SOAP Notes using a Collaborative Multi-Agent LLM Architecture</a></h3>
                    <p><strong>Authors:</strong> Yeawon Lee, Xiaoyang Wang, Christopher C. Yang</p>
                    <p><strong>Published:</strong> 8/29/2025</p>
                    <p><strong>Categories:</strong> cs.AI, cs.MA</p>
                    <p><strong>Summary:</strong> Accurate interpretation of clinical narratives is critical for patient care, but the complexity of these notes makes automation challenging. While Large Language Models (LLMs) show promise, single-model approaches can lack the robustness required for high-stakes clinical tasks. We introduce a collaborative multi-agent system (MAS) that models a clinical consultation team to address this gap. The system is tasked with identifying clinical problems by analyzing only the Subjective (S) and Objective (O) sections of SOAP notes, simulating the diagnostic reasoning process of synthesizing raw data into an assessment. A Manager agent orchestrates a dynamically assigned team of specialist agents who engage in a hierarchical, iterative debate to reach a consensus. We evaluated our MAS against a single-agent baseline on a curated dataset of 420 MIMIC-III notes. The dynamic multi-agent configuration demonstrated consistently improved performance in identifying congestive heart failure, acute kidney injury, and sepsis. Qualitative analysis of the agent debates reveals that this structure effectively surfaces and weighs conflicting evidence, though it can occasionally be susceptible to groupthink. By modeling a clinical teams reasoning process, our system offers a promising path toward more accurate, robust, and interpretable clinical decision support tools.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.21802v1" target="_blank">An Adaptive Real-Time Forecasting Framework for Cryogenic Fluid Management in Space Systems</a></h3>
                    <p><strong>Authors:</strong> Qiyun Cheng, Huihua Yang, Wei Ji</p>
                    <p><strong>Published:</strong> 8/29/2025</p>
                    <p><strong>Categories:</strong> physics.flu-dyn</p>
                    <p><strong>Summary:</strong> Accurate real-time forecasting of cryogenic tank behavior is essential for the safe and efficient operation of propulsion and storage systems in future deep-space missions. While cryogenic fluid management (CFM) systems increasingly require autonomous capabilities, conventional simulation methods remain hindered by high computational cost, model imperfections, and sensitivity to unanticipated boundary condition changes. To address these limitations, this study proposes an Adaptive Real-Time Forecasting Framework for Cryogenic Propellant Management in Space Systems, featuring a lightweight, non-intrusive method named ARCTIC (Adaptive Real-time Cryogenic Tank Inference and Correction). ARCTIC integrates real-time sensor data with precomputed nodal simulations through a data-driven correction layer that dynamically refines forecast accuracy without modifying the underlying model. Two updating mechanisms, auto-calibration and observation and correction, enable continuous adaptation to evolving system states and transient disturbances. The method is first assessed through synthetic scenarios representing self-pressurization, sloshing, and periodic operations, then validated using experimental data from NASAs Multipurpose Hydrogen Test Bed and K-Site facilities. Results demonstrate that ARCTIC significantly improves forecast accuracy under model imperfections, data noise, and boundary fluctuations, offering a robust real-time forecasting capability to support autonomous CFM operations. The frameworks compatibility with existing simulation tools and its low computational overhead make it especially suited for onboard implementation in space systems requiring predictive autonomy.</p>
                
            
        
        
            <p><em>Generated by AI News Agent</em></p>
        
    

