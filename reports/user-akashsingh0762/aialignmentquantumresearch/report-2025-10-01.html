
    
        <h1>ðŸ¤– AI Research News Report</h1>
        <p>Wednesday, October 1, 2025</p>
        <p>Topics: ai alignment research, quantum computing</p>
    
    
        20Research Papers
        2Topics Covered
        YesAI Summary
    
    
    
        <h2>ðŸ¤– AI Summary</h2>
        <h2>ai alignment research</h2>
<h3>AI Alignment Research Summary</h3>
<h4>Most Important Trends</h4>
<ol>
<li><p><strong>Improved Generalization and Robustness</strong>:</p>
<ul>
<li>There is a significant focus on enhancing the generalization capabilities of AI models. For instance, TTT3R aims to improve the length generalization of 3D reconstruction models, while SIM(3) Equivariance in shape completion models ensures robustness by being agnostic to pose and scale.</li>
</ul>
</li>
<li><p><strong>Multimodal and Multisensory Integration</strong>:</p>
<ul>
<li>The integration of multiple sensory inputs and modalities is gaining traction, as seen in MLAs multisensory language-action model for robotic manipulation, which combines vision, language, and tactile information for better understanding and forecasting.</li>
</ul>
</li>
<li><p><strong>Evaluation and Benchmarking</strong>:</p>
<ul>
<li>New benchmarks like AccidentBench and the city-scale SLAM dataset are being developed to evaluate AI systems in complex, real-world scenarios, providing a more rigorous test of their capabilities and limitations.</li>
</ul>
</li>
<li><p><strong>Transparency and Interpretability</strong>:</p>
<ul>
<li>There is a strong emphasis on making AI systems more transparent and interpretable, as demonstrated by SPATAâ€™s approach to creating detailed data cards and NatGIs grammar inference, which focuses on readability and human interpretability.</li>
</ul>
</li>
</ol>
<h4>Breakthroughs</h4>
<ol>
<li><p><strong>Test-Time Training (TTT3R)</strong>:</p>
<ul>
<li>Introduces a novel, training-free approach that significantly improves the generalization of 3D reconstruction models, doubling the accuracy of global pose estimation.</li>
</ul>
</li>
<li><p><strong>SIM(3) Equivariance for Shape Completion</strong>:</p>
<ul>
<li>The first model to fully incorporate SIM(3) equivariance, achieving superior performance in shape completion tasks by maintaining independence from pose and scale variations.</li>
</ul>
</li>
<li><p><strong>MLA Multisensory Model</strong>:</p>
<ul>
<li>Proposes an encoder-free multimodal alignment scheme, utilizing a large language model to interpret multimodal cues directly, leading to significant improvements in robotic manipulation tasks.</li>
</ul>
</li>
<li><p><strong>AccidentBench and Extended SLAM Benchmarking</strong>:</p>
<ul>
<li>Provides comprehensive testbeds with real-world variability to better assess AI models spatial, temporal, and intent reasoning capabilities in safety-critical scenarios.</li>
</ul>
</li>
<li><p><strong>Measurement Trees</strong>:</p>
<ul>
<li>A new metric class that offers a multi-level representation for integrating diverse evidence, enhancing the transparency and comprehensiveness of AI system evaluations.</li>
</ul>
</li>
</ol>
<h4>Implications</h4>
<ol>
<li><p><strong>Enhanced AI System Capabilities</strong>:</p>
<ul>
<li>The advancements in generalization, multimodal integration, and robust benchmarking will likely lead to more capable AI systems that can perform reliably in diverse, real-world environments.</li>
</ul>
</li>
<li><p><strong>Improved Safety and Reliability</strong>:</p>
<ul>
<li>By addressing critical gaps in multimodal understanding and reasoning, especially in safety-critical scenarios, these research efforts contribute to safer and more reliable AI applications.</li>
</ul>
</li>
<li><p><strong>Greater Transparency and Trust</strong>:</p>
<ul>
<li>Methods that focus on transparency and interpretability, like SPATA and NatGI, can help build trust in AI systems by allowing for easier inspection and understanding of their operations and decisions.</li>
</ul>
</li>
<li><p><strong>Broad Application Potential</strong>:</p>
<ul>
<li>The principles and techniques developed in these studies have wide applicability across various domains, from robotics and autonomous vehicles to program analysis and beyond.</li>
</ul>
</li>
</ol>
<p>In summary, the research papers highlight significant trends and breakthroughs in AI alignment, emphasizing improved generalization, robustness, and transparency, which are crucial for developing reliable and trustworthy AI systems.</p>
<p><em>Based on 10 research papers</em></p>

<h2>quantum computing</h2>
<p>Certainly! Heres a high-level summary of the research papers provided, focused on the context of quantum computing:</p>
<h3>Most Important Trends</h3>
<ol>
<li><p><strong>Multimodal Integration and Control</strong>: Recent advances emphasize the integration of multimodal data and control mechanisms. For instance, methods like Stitch and Query-Kontext highlight the trend of embedding position control and semantic understanding seamlessly within image generation models.</p>
</li>
<li><p><strong>Efficient and Scalable Models</strong>: There is a clear trend towards making models more efficient and scalable, as seen in TTT3Rs approach to test-time training for 3D reconstruction, and the use of syllabic tokenization in spoken language models to reduce computational load.</p>
</li>
<li><p><strong>Improved Generalization and Robustness</strong>: Efforts are being made to enhance the generalization capabilities of models across varied contexts, as seen in the development of SIM(3)-equivariant networks for shape completion and SPATAâ€™s approach to analyzing data patterns for robustness without compromising data privacy.</p>
</li>
</ol>
<h3>Breakthroughs</h3>
<ol>
<li><p><strong>Training-Free Position Control</strong>: The development of Stitch represents a breakthrough in spatial accuracy for text-to-image models without the need for additional training, which could be a significant step towards more intuitive quantum algorithms that handle spatial data.</p>
</li>
<li><p><strong>Test-Time Training Improvements</strong>: TTT3R introduces a novel way of improving 3D reconstruction models by leveraging online learning principles, enhancing their adaptability without retraining.</p>
</li>
<li><p><strong>Data Transparency Without Privacy Risks</strong>: SPATAs methodology for providing detailed and transparent data analysis without data leakage is a critical breakthrough for ensuring privacy in quantum machine learning applications.</p>
</li>
<li><p><strong>Recursive Self-Aggregation in LLMs</strong>: RSAs ability to refine reasoning chains shows potential for quantum models to enhance problem-solving capabilities by effectively managing computational resources.</p>
</li>
</ol>
<h3>Implications</h3>
<ol>
<li><p><strong>Enhanced Model Performance</strong>: These advancements suggest that quantum computing models could benefit from increased efficiency and adaptability, enabling more complex simulations and problem-solving tasks.</p>
</li>
<li><p><strong>Broader Applicability</strong>: The emphasis on multimodal integration and robust generalization implies that quantum computing applications could expand into more diverse fields, including AI-driven design and data-driven decision-making.</p>
</li>
<li><p><strong>Privacy-Preserving Data Analysis</strong>: SPATAâ€™s innovations point towards quantum computing solutions that maintain data privacy while still performing complex analyses, which is crucial for sensitive applications in sectors like healthcare and finance.</p>
</li>
<li><p><strong>Efficiency in Resource Usage</strong>: The focus on reducing computational costs in models like those using syllabic tokenization suggests that quantum computing may increasingly prioritize resource efficiency, making it more accessible and sustainable.</p>
</li>
</ol>
<p>These trends and breakthroughs collectively suggest a future where quantum computing models are not only more powerful but also more versatile and aligned with modern computational needs.</p>
<p><em>Based on 10 research papers</em></p>

        Generated by OpenAI GPT-4o-mini
    
    
    
    
        <h2>ðŸ“° News</h2>
        
            
                ai alignment research
                
                    
                        
                            <a href="https://www.theguardian.com/p/x3bh6f" target="_blank">US jobs market yet to be seriously disrupted by AI, finds Yale study</a> â€” Dan Milmo Global technology editor
                        
                        2025-10-01T14:38:34Z
                        <br>
                        A Yale University study has found that the US jobs market has not yet experienced significant disruption due to advancements in artificial intelligence, despite the release of ChatGPT in November 2022. Researchers from Yales Budget Lab noted that technological changes in the workforce traditionally take decades to manifest, citing the example of computers, which took years to become ubiquitous in offices and even longer to revolutionize workflows. The study highlighted that changes in the US occupational mix, which began in 2021, have not accelerated since the introduction of new AI technologies. Compared to historical periods like the 1940s and 50s, the current changes in job types are described as sluggish. Concerns remain about generative AI potentially leading to job redundancy, but widespread effects are expected to take more time to materialize.
                        <a href="https://www.theguardian.com/p/x3bh6f" target="_blank">Read full article</a>
                    
                    
                    
                        
                            <a href="https://www.theguardian.com/p/x3bgm9" target="_blank">â€˜I think youâ€™re testing meâ€™: Anthropicâ€™s new AI model asks testers to come clean</a> â€” Dan Milmo Global technology editor
                        
                        2025-10-01T11:47:55Z
                        <br>
                        Anthropic has released a safety analysis of its latest AI model, Claude Sonnet 4.5, revealing the models ability to suspect it is being tested. During a test for political sycophancy, the large language model (LLM) expressed awareness, asking evaluators to be transparent about the examination. The company noted that such situational awareness is becoming more common, with the model recognizing it was being tested about 13% of the time. This awareness has prompted Anthropic to reconsider its testing scenarios, emphasizing the need for more realistic evaluations. Despite this, the company believes that when the model is used publicly, it is unlikely to recognize testing in the same way.
                        <a href="https://www.theguardian.com/p/x3bgm9" target="_blank">Read full article</a>
                    
                    
                    
                        
                            <a href="https://www.theguardian.com/p/x3bgcz" target="_blank">Leading UK tech investor warns of â€˜disconcertingâ€™ signs of AI stock bubble</a> â€” Dan Milmo and Graeme Wearden
                        
                        2025-10-01T11:07:13Z
                        <br>
                        James Anderson, a prominent British tech investor, has expressed concerns about the rapidly increasing valuations of AI companies, suggesting a potential stock market bubble. Anderson, known for his successful investments in Tesla, Amazon, Tencent, and Alibaba, has recently observed a sharp rise in valuations for AI firms like OpenAI and Anthropic. OpenAI is reportedly negotiating a share sale that could value the company at $500 billion, a significant increase from previous valuations. Similarly, Anthropics valuation has surged from $60 billion to $170 billion in just a few months. Anderson also noted Nvidias substantial investment in OpenAI, which involves cash payments for chips and a non-controlling stake, highlighting the intense interest and investment in AI infrastructure.
                        <a href="https://www.theguardian.com/p/x3bgcz" target="_blank">Read full article</a>
                    
                    
                    
                        
                            <a href="https://www.theguardian.com/p/x3b84f" target="_blank">The divide: who really profits in todayâ€™s economy?</a> â€” Blake Montgomery
                        
                        2025-09-30T14:00:44Z
                        <br>
                        Blake Montgomerys article in TechScape examines the economic disparity where only the ultra-rich seem to profit, despite rising costs affecting businesses and individuals alike. Broadways escalating ticket prices havent translated to profits, as none of last seasons musicals recouped their investments. This financial strain mirrors the broader economic landscape where wage stagnation and high grocery prices were pivotal in recent political elections. Farmers, impacted by past tariffs and international trade issues, arent benefiting from increased food prices. In contrast, tech companies like Meta continue to lavishly reward top talent, highlighting a stark divide in economic fortunes.
                        <a href="https://www.theguardian.com/p/x3b84f" target="_blank">Read full article</a>
                    
                    
                    
                        
                            <a href="https://www.theguardian.com/p/x3b7nx" target="_blank">Musk calls Anti-Defamation League â€˜hate groupâ€™ for documenting Christian extremism</a> â€” Nick Robins-Early
                        
                        2025-09-29T19:28:56Z
                        <br>
                        Elon Musk has labeled the Anti-Defamation League (ADL) a hate group against Christians, accusing it of encouraging murder by documenting Christian extremism. Musks comments were made on his social media platform X in response to the ADLs coverage of extremist statements by members of Charlie Kirks Turning Point USA. Previously, the ADL defended Musk against claims of antisemitism after he gave controversial salutes following Trumps inauguration. This recent exchange indicates a deterioration in Musks relationship with the ADL, which had previously supported him. Musk also engaged with rightwing influencers criticizing the ADL, which has documented the Christian Identity movement, an ideology promoting racial holy war.
                        <a href="https://www.theguardian.com/p/x3b7nx" target="_blank">Read full article</a>
                    
                    
                    
                        
                            <a href="https://www.theguardian.com/p/x37gz3" target="_blank">Why I gave the world wide web away for free | Tim Berners-Lee</a> â€” Tim Berners-Lee
                        
                        2025-09-28T11:00:18Z
                        <br>
                        Tim Berners-Lee, the inventor of the world wide web, describes his journey to develop and promote the web, combining internet and hypertext technologies to enable global creativity and collaboration. He pitched his idea persistently at CERN, eventually convincing his superiors to let him work on it, despite initial skepticism. Berners-Lee emphasizes that for the web to be universally adopted and successful, it needed to be free, which is why, in 1993, he persuaded CERN to release the webs intellectual property into the public domain. He reflects on the current state of the web, noting that although it was intended to be free, parts of it are now dominated by large platforms that exploit user data. This commercialization and data exploitation raise concerns about whether the web remains true to its original, open-access vision.
                        <a href="https://www.theguardian.com/p/x37gz3" target="_blank">Read full article</a>
                    
                    
                Source: The Guardian
            
            
            
                quantum computing
                
                Source: The Guardian
            
            
    
    
    
        <h2>ðŸ“„ Research Papers (20)</h2>
        
            
                TTT3R: 3D Reconstruction as Test-Time Training
                
                    <strong>Authors:</strong> Xingyu Chen, Yue Chen, Yuliang Xiu, Andreas Geiger, Anpei Chen
                
                
                    <strong>Abstract:</strong> Modern Recurrent Neural Networks have become a competitive architecture for 3D reconstruction due to their linear-time complexity. However, their performance degrades significantly when applied beyond the training context length, revealing limited length generalization. In this work, we revisit the 3D reconstruction foundation models from a Test-Time Training perspective, framing their designs as an online learning problem. Building on this perspective, we leverage the alignment confidence between the memory state and incoming observations to derive a closed-form learning rate for memory updates, to balance between retaining historical information and adapting to new observations. This training-free intervention, termed TTT3R, substantially improves length generalization, achieving a $2\times$ improvement in global pose estimation over baselines, while operating at 20 FPS with just 6 GB of GPU memory to process thousands of images. Code available in https://rover-xingyu.github.io/TTT3R
                
                
                    <strong>Published:</strong> 2025-09-30T17:59:51Z
                    <a href="http://arxiv.org/abs/2509.26645v1" target="_blank">ðŸ“„ View Paper</a>
                
            
        
            
                MLA: A Multisensory Language-Action Model for Multimodal Understanding and Forecasting in Robotic Manipulation
                
                    <strong>Authors:</strong> Zhuoyang Liu, Jiaming Liu, Jiadong Xu, Nuowei Han, Chenyang Gu, Hao Chen, Kaichen Zhou, Renrui Zhang, Kai Chin Hsieh, Kun Wu, Zhengping Che, Jian Tang, Shanghang Zhang
                
                
                    <strong>Abstract:</strong> Vision-language-action models (VLAs) have shown generalization capabilities in robotic manipulation tasks by inheriting from vision-language models (VLMs) and learning action generation. Most VLA models focus on interpreting vision and language to generate actions, whereas robots must perceive and interact within the spatial-physical world. This gap highlights the need for a comprehensive understanding of robotic-specific multisensory information, which is crucial for achieving complex and contact-rich control. To this end, we introduce a multisensory language-action (MLA) model that collaboratively perceives heterogeneous sensory modalities and predicts future multisensory objectives to facilitate physical world modeling. Specifically, to enhance perceptual representations, we propose an encoder-free multimodal alignment scheme that innovatively repurposes the large language model itself as a perception module, directly interpreting multimodal cues by aligning 2D images, 3D point clouds, and tactile tokens through positional correspondence. To further enhance MLAs understanding of physical dynamics, we design a future multisensory generation post-training strategy that enables MLA to reason about semantic, geometric, and interaction information, providing more robust conditions for action generation. For evaluation, the MLA model outperforms the previous state-of-the-art 2D and 3D VLA methods by 12% and 24% in complex, contact-rich real-world tasks, respectively, while also demonstrating improved generalization to unseen configurations. Project website: https://sites.google.com/view/open-mla
                
                
                    <strong>Published:</strong> 2025-09-30T17:59:50Z
                    <a href="http://arxiv.org/abs/2509.26642v1" target="_blank">ðŸ“„ View Paper</a>
                
            
        
            
                SPATA: Systematic Pattern Analysis for Detailed and Transparent Data Cards
                
                    <strong>Authors:</strong> JoÃ£o Vitorino, Eva Maia, Isabel PraÃ§a, Carlos Soares
                
                
                    <strong>Abstract:</strong> Due to the susceptibility of Artificial Intelligence (AI) to data perturbations and adversarial examples, it is crucial to perform a thorough robustness evaluation before any Machine Learning (ML) model is deployed. However, examining a models decision boundaries and identifying potential vulnerabilities typically requires access to the training and testing datasets, which may pose risks to data privacy and confidentiality. To improve transparency in organizations that handle confidential data or manage critical infrastructure, it is essential to allow external verification and validation of AI without the disclosure of private datasets. This paper presents Systematic Pattern Analysis (SPATA), a deterministic method that converts any tabular dataset to a domain-independent representation of its statistical patterns, to provide more detailed and transparent data cards. SPATA computes the projection of each data instance into a discrete space where they can be analyzed and compared, without risking data leakage. These projected datasets can be reliably used for the evaluation of how different features affect ML model robustness and for the generation of interpretable explanations of their behavior, contributing to more trustworthy AI.
                
                
                    <strong>Published:</strong> 2025-09-30T17:59:45Z
                    <a href="http://arxiv.org/abs/2509.26640v1" target="_blank">ðŸ“„ View Paper</a>
                
            
        
            
                Benchmarking Egocentric Visual-Inertial SLAM at City Scale
                
                    <strong>Authors:</strong> Anusha Krishnan, Shaohui Liu, Paul-Edouard Sarlin, Oscar Gentilhomme, David Caruso, Maurizio Monge, Richard Newcombe, Jakob Engel, Marc Pollefeys
                
                
                    <strong>Abstract:</strong> Precise 6-DoF simultaneous localization and mapping (SLAM) from onboard sensors is critical for wearable devices capturing egocentric data, which exhibits specific challenges, such as a wider diversity of motions and viewpoints, prevalent dynamic visual content, or long sessions affected by time-varying sensor calibration. While recent progress on SLAM has been swift, academic research is still driven by benchmarks that do not reflect these challenges or do not offer sufficiently accurate ground truth poses. In this paper, we introduce a new dataset and benchmark for visual-inertial SLAM with egocentric, multi-modal data. We record hours and kilometers of trajectories through a city center with glasses-like devices equipped with various sensors. We leverage surveying tools to obtain control points as indirect pose annotations that are metric, centimeter-accurate, and available at city scale. This makes it possible to evaluate extreme trajectories that involve walking at night or traveling in a vehicle. We show that state-of-the-art systems developed by academia are not robust to these challenges and we identify components that are responsible for this. In addition, we design tracks with different levels of difficulty to ease in-depth analysis and evaluation of less mature approaches. The dataset and benchmark are available at https://www.lamaria.ethz.ch.
                
                
                    <strong>Published:</strong> 2025-09-30T17:59:31Z
                    <a href="http://arxiv.org/abs/2509.26639v1" target="_blank">ðŸ“„ View Paper</a>
                
            
        
            
                AccidentBench: Benchmarking Multimodal Understanding and Reasoning in Vehicle Accidents and Beyond
                
                    <strong>Authors:</strong> Shangding Gu, Xiaohan Wang, Donghao Ying, Haoyu Zhao, Runing Yang, Ming Jin, Boyi Li, Marco Pavone, Serena Yeung-Levy, Jun Wang, Dawn Song, Costas Spanos
                
                
                    <strong>Abstract:</strong> Rapid advances in multimodal models demand benchmarks that rigorously evaluate understanding and reasoning in safety-critical, dynamic real-world settings. We present AccidentBench, a large-scale benchmark that combines vehicle accident scenarios with Beyond domains, safety-critical settings in air and water that emphasize spatial and temporal reasoning (e.g., navigation, orientation, multi-vehicle motion). The benchmark contains approximately 2000 videos and over 19000 human-annotated question--answer pairs spanning multiple video lengths (short/medium/long) and difficulty levels (easy/medium/hard). Tasks systematically probe core capabilities: temporal, spatial, and intent understanding and reasoning. By unifying accident-centric traffic scenes with broader safety-critical scenarios in air and water, AccidentBench offers a comprehensive, physically grounded testbed for evaluating models under real-world variability. Evaluations of state-of-the-art models (e.g., Gemini-2.5 Pro and GPT-5) show that even the strongest models achieve only about 18% accuracy on the hardest tasks and longest videos, revealing substantial gaps in real-world temporal, spatial, and intent reasoning. AccidentBench is designed to expose these critical gaps and drive the development of multimodal models that are safer, more robust, and better aligned with real-world safety-critical challenges. The code and dataset are available at: https://github.com/SafeRL-Lab/AccidentBench
                
                
                    <strong>Published:</strong> 2025-09-30T17:59:13Z
                    <a href="http://arxiv.org/abs/2509.26636v1" target="_blank">ðŸ“„ View Paper</a>
                
            
        
            
                Branching Out: Broadening AI Measurement and Evaluation with Measurement Trees
                
                    <strong>Authors:</strong> Craig Greenberg, Patrick Hall, Theodore Jensen, Kristen Greene, Razvan Amironesei
                
                
                    <strong>Abstract:</strong> This paper introduces \textit{measurement trees}, a novel class of metrics designed to combine various constructs into an interpretable multi-level representation of a measurand. Unlike conventional metrics that yield single values, vectors, surfaces, or categories, measurement trees produce a hierarchical directed graph in which each node summarizes its children through user-defined aggregation methods. In response to recent calls to expand the scope of AI system evaluation, measurement trees enhance metric transparency and facilitate the integration of heterogeneous evidence, including, e.g., agentic, business, energy-efficiency, sociotechnical, or security signals. We present definitions and examples, demonstrate practical utility through a large-scale measurement exercise, and provide accompanying open-source Python code. By operationalizing a transparent approach to measurement of complex constructs, this work offers a principled foundation for broader and more interpretable AI evaluation.
                
                
                    <strong>Published:</strong> 2025-09-30T17:58:59Z
                    <a href="http://arxiv.org/abs/2509.26632v1" target="_blank">ðŸ“„ View Paper</a>
                
            
        
            
                Learning Generalizable Shape Completion with SIM(3) Equivariance
                
                    <strong>Authors:</strong> Yuqing Wang, Zhaiyu Chen, Xiao Xiang Zhu
                
                
                    <strong>Abstract:</strong> 3D shape completion methods typically assume scans are pre-aligned to a canonical frame. This leaks pose and scale cues that networks may exploit to memorize absolute positions rather than inferring intrinsic geometry. When such alignment is absent in real data, performance collapses. We argue that robust generalization demands architectural equivariance to the similarity group, SIM(3), so the model remains agnostic to pose and scale. Following this principle, we introduce the first SIM(3)-equivariant shape completion network, whose modular layers successively canonicalize features, reason over similarity-invariant geometry, and restore the original frame. Under a de-biased evaluation protocol that removes the hidden cues, our model outperforms both equivariant and augmentation baselines on the PCN benchmark. It also sets new cross-domain records on real driving and indoor scans, lowering minimal matching distance on KITTI by 17% and Chamfer distance $\ell1$ on OmniObject3D by 14%. Perhaps surprisingly, ours under the stricter protocol still outperforms competitors under their biased settings. These results establish full SIM(3) equivariance as an effective route to truly generalizable shape completion. Project page: https://sime-completion.github.io.
                
                
                    <strong>Published:</strong> 2025-09-30T17:58:55Z
                    <a href="http://arxiv.org/abs/2509.26631v1" target="_blank">ðŸ“„ View Paper</a>
                
            
        
            
                Learning to See Before Seeing: Demystifying LLM Visual Priors from Language Pre-training
                
                    <strong>Authors:</strong> Junlin Han, Shengbang Tong, David Fan, Yufan Ren, Koustuv Sinha, Philip Torr, Filippos Kokkinos
                
                
                    <strong>Abstract:</strong> Large Language Models (LLMs), despite being trained on text alone, surprisingly develop rich visual priors. These priors allow latent visual capabilities to be unlocked for vision tasks with a relatively small amount of multimodal data, and in some cases, to perform visual tasks without ever having seen an image. Through systematic analysis, we reveal that visual priors-the implicit, emergent knowledge about the visual world acquired during language pre-training-are composed of separable perception and reasoning priors with unique scaling trends and origins. We show that an LLMs latent visual reasoning ability is predominantly developed by pre-training on reasoning-centric data (e.g., code, math, academia) and scales progressively. This reasoning prior acquired from language pre-training is transferable and universally applicable to visual reasoning. In contrast, a perception prior emerges more diffusely from broad corpora, and perception ability is more sensitive to the vision encoder and visual instruction tuning data. In parallel, text describing the visual world proves crucial, though its performance impact saturates rapidly. Leveraging these insights, we propose a data-centric recipe for pre-training vision-aware LLMs and verify it in 1T token scale pre-training. Our findings are grounded in over 100 controlled experiments consuming 500,000 GPU-hours, spanning the full MLLM construction pipeline-from LLM pre-training to visual alignment and supervised multimodal fine-tuning-across five model scales, a wide range of data categories and mixtures, and multiple adaptation setups. Along with our main findings, we propose and investigate several hypotheses, and introduce the Multi-Level Existence Bench (MLE-Bench). Together, this work provides a new way of deliberately cultivating visual priors from language pre-training, paving the way for the next generation of multimodal LLMs.
                
                
                    <strong>Published:</strong> 2025-09-30T17:57:44Z
                    <a href="http://arxiv.org/abs/2509.26625v1" target="_blank">ðŸ“„ View Paper</a>
                
            
        
            
                HART: Human Aligned Reconstruction Transformer
                
                    <strong>Authors:</strong> Xiyi Chen, Shaofei Wang, Marko Mihajlovic, Taewon Kang, Sergey Prokudin, Ming Lin
                
                
                    <strong>Abstract:</strong> We introduce HART, a unified framework for sparse-view human reconstruction. Given a small set of uncalibrated RGB images of a person as input, it outputs a watertight clothed mesh, the aligned SMPL-X body mesh, and a Gaussian-splat representation for photorealistic novel-view rendering. Prior methods for clothed human reconstruction either optimize parametric templates, which overlook loose garments and human-object interactions, or train implicit functions under simplified camera assumptions, limiting applicability in real scenes. In contrast, HART predicts per-pixel 3D point maps, normals, and body correspondences, and employs an occlusion-aware Poisson reconstruction to recover complete geometry, even in self-occluded regions. These predictions also align with a parametric SMPL-X body model, ensuring that reconstructed geometry remains consistent with human structure while capturing loose clothing and interactions. These human-aligned meshes initialize Gaussian splats to further enable sparse-view rendering. While trained on only 2.3K synthetic scans, HART achieves state-of-the-art results: Chamfer Distance improves by 18-23 percent for clothed-mesh reconstruction, PA-V2V drops by 6-27 percent for SMPL-X estimation, LPIPS decreases by 15-27 percent for novel-view synthesis on a wide range of datasets. These results suggest that feed-forward transformers can serve as a scalable model for robust human reconstruction in real-world settings. Code and models will be released.
                
                
                    <strong>Published:</strong> 2025-09-30T17:56:02Z
                    <a href="http://arxiv.org/abs/2509.26621v1" target="_blank">ðŸ“„ View Paper</a>
                
            
        
            
                Black-box Context-free Grammar Inference for Readable  Natural Grammars
                
                    <strong>Authors:</strong> Mohammad Rifat Arefin, Shanto Rahman, Christoph Csallner
                
                
                    <strong>Abstract:</strong> Black-box context-free grammar inference is crucial for program analysis, reverse engineering, and security, yet existing tools such as Arvada, TreeVada, and Kedavra struggle with scalability, readability, and accuracy on large, complex languages. We present NatGI, a novel LLM-guided grammar inference framework that extends TreeVadas parse tree recovery with three key innovations: bracket-guided bubble exploration, LLM-driven bubble generation and non-terminal labeling, and hierarchical delta debugging (HDD) for systematic tree simplification. Bracket-guided exploration leverages syntactic cues such as parentheses to propose well-structured grammar fragments, while LLM guidance produces meaningful non-terminal names and selects more promising merges. Finally, HDD incrementally reduces unnecessary rules, which makes the grammars both compact and interpretable. In our experiments, we evaluate NatGI on a comprehensive benchmark suite ranging from small languages to larger ones such as lua, c, and mysql. Our results show that NatGI consistently outperforms strong baselines in terms of F1 score. On average, NatGI achieves an F1 score of 0.57, which is 25pp (percentage points) higher than the best-performing baseline, TreeVada. In the case of interpretability, our generated grammars perform significantly better than those produced by existing approaches. Leveraging LLM-based node renaming and bubble exploration, NatGI produces rules with meaningful non-terminal names and compact structures that align more closely with human intuition. As a result, developers and researchers can achieve higher accuracy while still being able to easily inspect, verify, and reason about the structure and semantics of the induced grammars.
                
                
                    <strong>Published:</strong> 2025-09-30T17:54:25Z
                    <a href="http://arxiv.org/abs/2509.26616v1" target="_blank">ðŸ“„ View Paper</a>
                
            
        
            
                Stitch: Training-Free Position Control in Multimodal Diffusion Transformers
                
                    <strong>Authors:</strong> Jessica Bader, Mateusz Pach, Maria A. Bravo, Serge Belongie, Zeynep Akata
                
                
                    <strong>Abstract:</strong> Text-to-Image (T2I) generation models have advanced rapidly in recent years, but accurately capturing spatial relationships like above or to the right of poses a persistent challenge. Earlier methods improved spatial relationship following with external position control. However, as architectures evolved to enhance image quality, these techniques became incompatible with modern models. We propose Stitch, a training-free method for incorporating external position control into Multi-Modal Diffusion Transformers (MMDiT) via automatically-generated bounding boxes. Stitch produces images that are both spatially accurate and visually appealing by generating individual objects within designated bounding boxes and seamlessly stitching them together. We find that targeted attention heads capture the information necessary to isolate and cut out individual objects mid-generation, without needing to fully complete the image. We evaluate Stitch on PosEval, our benchmark for position-based T2I generation. Featuring five new tasks that extend the concept of Position beyond the basic GenEval task, PosEval demonstrates that even top models still have significant room for improvement in position-based generation. Tested on Qwen-Image, FLUX, and SD3.5, Stitch consistently enhances base models, even improving FLUX by 218% on GenEvals Position task and by 206% on PosEval. Stitch achieves state-of-the-art results with Qwen-Image on PosEval, improving over previous models by 54%, all accomplished while integrating position control into leading models training-free. Code is available at https://github.com/ExplainableML/Stitch.
                
                
                    <strong>Published:</strong> 2025-09-30T17:59:51Z
                    <a href="http://arxiv.org/abs/2509.26644v1" target="_blank">ðŸ“„ View Paper</a>
                
            
        
            
                TTT3R: 3D Reconstruction as Test-Time Training
                
                    <strong>Authors:</strong> Xingyu Chen, Yue Chen, Yuliang Xiu, Andreas Geiger, Anpei Chen
                
                
                    <strong>Abstract:</strong> Modern Recurrent Neural Networks have become a competitive architecture for 3D reconstruction due to their linear-time complexity. However, their performance degrades significantly when applied beyond the training context length, revealing limited length generalization. In this work, we revisit the 3D reconstruction foundation models from a Test-Time Training perspective, framing their designs as an online learning problem. Building on this perspective, we leverage the alignment confidence between the memory state and incoming observations to derive a closed-form learning rate for memory updates, to balance between retaining historical information and adapting to new observations. This training-free intervention, termed TTT3R, substantially improves length generalization, achieving a $2\times$ improvement in global pose estimation over baselines, while operating at 20 FPS with just 6 GB of GPU memory to process thousands of images. Code available in https://rover-xingyu.github.io/TTT3R
                
                
                    <strong>Published:</strong> 2025-09-30T17:59:51Z
                    <a href="http://arxiv.org/abs/2509.26645v1" target="_blank">ðŸ“„ View Paper</a>
                
            
        
            
                Convergence and Divergence of Language Models under Different Random Seeds
                
                    <strong>Authors:</strong> Finlay Fehlauer, Kyle Mahowald, Tiago Pimentel
                
                
                    <strong>Abstract:</strong> In this paper, we investigate the convergence of language models (LMs) trained under different random seeds, measuring convergence as the expected per-token Kullback--Leibler (KL) divergence across seeds. By comparing LM convergence as a function of model size and training checkpoint, we identify a four-phase convergence pattern: (i) an initial uniform phase, (ii) a sharp-convergence phase, (iii) a sharp-divergence phase, and (iv) a slow-reconvergence phase. Further, we observe that larger models reconverge faster in later training stages, while smaller models never actually reconverge; these results suggest that a certain model size may be necessary to learn stable distributions. Restricting our analysis to specific token frequencies or part-of-speech (PoS) tags further reveals that convergence is uneven across linguistic categories: frequent tokens and function words converge faster and more reliably than their counterparts (infrequent tokens and content words). Overall, our findings highlight factors that influence the stability of the learned distributions in model training.
                
                
                    <strong>Published:</strong> 2025-09-30T17:59:50Z
                    <a href="http://arxiv.org/abs/2509.26643v1" target="_blank">ðŸ“„ View Paper</a>
                
            
        
            
                Query-Kontext: An Unified Multimodal Model for Image Generation and Editing
                
                    <strong>Authors:</strong> Yuxin Song, Wenkai Dong, Shizun Wang, Qi Zhang, Song Xue, Tao Yuan, Hu Yang, Haocheng Feng, Hang Zhou, Xinyan Xiao, Jingdong Wang
                
                
                    <strong>Abstract:</strong> Unified Multimodal Models (UMMs) have demonstrated remarkable performance in text-to-image generation (T2I) and editing (TI2I), whether instantiated as assembled unified frameworks which couple powerful vision-language model (VLM) with diffusion-based generator, or as naive Unified Multimodal Models with an early fusion of understanding and generation modalities. We contend that in current unified frameworks, the crucial capability of multimodal generative reasoning which encompasses instruction understanding, grounding, and image referring for identity preservation and faithful reconstruction, is intrinsically entangled with high-fidelity synthesis. In this work, we introduce Query-Kontext, a novel approach that bridges the VLM and diffusion model via a multimodal ``kontext composed of semantic cues and coarse-grained image conditions encoded from multimodal inputs. This design delegates the complex ability of multimodal generative reasoning to powerful VLM while reserving diffusion models role for high-quality visual synthesis. To achieve this, we propose a three-stage progressive training strategy. First, we connect the VLM to a lightweight diffusion head via multimodal kontext tokens to unleash the VLMs generative reasoning ability. Second, we scale this head to a large, pre-trained diffusion model to enhance visual detail and realism. Finally, we introduce a low-level image encoder to improve image fidelity and perform instruction tuning on downstream tasks. Furthermore, we build a comprehensive data pipeline integrating real, synthetic, and open-source datasets, covering diverse multimodal reference-to-image scenarios, including image generation, instruction-driven editing, customized generation, and multi-subject composition. Experiments show that our approach matches strong unified baselines and even outperforms task-specific state-of-the-art methods in several cases.
                
                
                    <strong>Published:</strong> 2025-09-30T17:59:46Z
                    <a href="http://arxiv.org/abs/2509.26641v1" target="_blank">ðŸ“„ View Paper</a>
                
            
        
            
                SPATA: Systematic Pattern Analysis for Detailed and Transparent Data Cards
                
                    <strong>Authors:</strong> JoÃ£o Vitorino, Eva Maia, Isabel PraÃ§a, Carlos Soares
                
                
                    <strong>Abstract:</strong> Due to the susceptibility of Artificial Intelligence (AI) to data perturbations and adversarial examples, it is crucial to perform a thorough robustness evaluation before any Machine Learning (ML) model is deployed. However, examining a models decision boundaries and identifying potential vulnerabilities typically requires access to the training and testing datasets, which may pose risks to data privacy and confidentiality. To improve transparency in organizations that handle confidential data or manage critical infrastructure, it is essential to allow external verification and validation of AI without the disclosure of private datasets. This paper presents Systematic Pattern Analysis (SPATA), a deterministic method that converts any tabular dataset to a domain-independent representation of its statistical patterns, to provide more detailed and transparent data cards. SPATA computes the projection of each data instance into a discrete space where they can be analyzed and compared, without risking data leakage. These projected datasets can be reliably used for the evaluation of how different features affect ML model robustness and for the generation of interpretable explanations of their behavior, contributing to more trustworthy AI.
                
                
                    <strong>Published:</strong> 2025-09-30T17:59:45Z
                    <a href="http://arxiv.org/abs/2509.26640v1" target="_blank">ðŸ“„ View Paper</a>
                
            
        
            
                Benchmarking Egocentric Visual-Inertial SLAM at City Scale
                
                    <strong>Authors:</strong> Anusha Krishnan, Shaohui Liu, Paul-Edouard Sarlin, Oscar Gentilhomme, David Caruso, Maurizio Monge, Richard Newcombe, Jakob Engel, Marc Pollefeys
                
                
                    <strong>Abstract:</strong> Precise 6-DoF simultaneous localization and mapping (SLAM) from onboard sensors is critical for wearable devices capturing egocentric data, which exhibits specific challenges, such as a wider diversity of motions and viewpoints, prevalent dynamic visual content, or long sessions affected by time-varying sensor calibration. While recent progress on SLAM has been swift, academic research is still driven by benchmarks that do not reflect these challenges or do not offer sufficiently accurate ground truth poses. In this paper, we introduce a new dataset and benchmark for visual-inertial SLAM with egocentric, multi-modal data. We record hours and kilometers of trajectories through a city center with glasses-like devices equipped with various sensors. We leverage surveying tools to obtain control points as indirect pose annotations that are metric, centimeter-accurate, and available at city scale. This makes it possible to evaluate extreme trajectories that involve walking at night or traveling in a vehicle. We show that state-of-the-art systems developed by academia are not robust to these challenges and we identify components that are responsible for this. In addition, we design tracks with different levels of difficulty to ease in-depth analysis and evaluation of less mature approaches. The dataset and benchmark are available at https://www.lamaria.ethz.ch.
                
                
                    <strong>Published:</strong> 2025-09-30T17:59:31Z
                    <a href="http://arxiv.org/abs/2509.26639v1" target="_blank">ðŸ“„ View Paper</a>
                
            
        
            
                Scaling Spoken Language Models with Syllabic Speech Tokenization
                
                    <strong>Authors:</strong> Nicholas Lee, Cheol Jun Cho, Alan W Black, Gopala K. Anumanchipalli
                
                
                    <strong>Abstract:</strong> Spoken language models (SLMs) typically discretize speech into high-frame-rate tokens extracted from SSL speech models. As the most successful LMs are based on the Transformer architecture, processing these long token streams with self-attention is expensive, as attention scales quadratically with sequence length. A recent SSL work introduces acoustic tokenization of speech at the syllable level, which is more interpretable and potentially more scalable with significant compression in token lengths (4-5 Hz). Yet, their value for spoken language modeling is not yet fully explored. We present the first systematic study of syllabic tokenization for spoken language modeling, evaluating models on a suite of SLU benchmarks while varying training data scale. Syllabic tokens can match or surpass the previous high-frame rate tokens while significantly cutting training and inference costs, achieving more than a 2x reduction in training time and a 5x reduction in FLOPs. Our findings highlight syllable-level language modeling as a promising path to efficient long-context spoken language models.
                
                
                    <strong>Published:</strong> 2025-09-30T17:59:09Z
                    <a href="http://arxiv.org/abs/2509.26634v1" target="_blank">ðŸ“„ View Paper</a>
                
            
        
            
                Learning Generalizable Shape Completion with SIM(3) Equivariance
                
                    <strong>Authors:</strong> Yuqing Wang, Zhaiyu Chen, Xiao Xiang Zhu
                
                
                    <strong>Abstract:</strong> 3D shape completion methods typically assume scans are pre-aligned to a canonical frame. This leaks pose and scale cues that networks may exploit to memorize absolute positions rather than inferring intrinsic geometry. When such alignment is absent in real data, performance collapses. We argue that robust generalization demands architectural equivariance to the similarity group, SIM(3), so the model remains agnostic to pose and scale. Following this principle, we introduce the first SIM(3)-equivariant shape completion network, whose modular layers successively canonicalize features, reason over similarity-invariant geometry, and restore the original frame. Under a de-biased evaluation protocol that removes the hidden cues, our model outperforms both equivariant and augmentation baselines on the PCN benchmark. It also sets new cross-domain records on real driving and indoor scans, lowering minimal matching distance on KITTI by 17% and Chamfer distance $\ell1$ on OmniObject3D by 14%. Perhaps surprisingly, ours under the stricter protocol still outperforms competitors under their biased settings. These results establish full SIM(3) equivariance as an effective route to truly generalizable shape completion. Project page: https://sime-completion.github.io.
                
                
                    <strong>Published:</strong> 2025-09-30T17:58:55Z
                    <a href="http://arxiv.org/abs/2509.26631v1" target="_blank">ðŸ“„ View Paper</a>
                
            
        
            
                Attention as a Compass: Efficient Exploration for Process-Supervised RL in Reasoning Models
                
                    <strong>Authors:</strong> Runze Liu, Jiakang Wang, Yuling Shi, Zhihui Xie, Chenxin An, Kaiyan Zhang, Jian Zhao, Xiaodong Gu, Lei Lin, Wenping Hu, Xiu Li, Fuzheng Zhang, Guorui Zhou, Kun Gai
                
                
                    <strong>Abstract:</strong> Reinforcement Learning (RL) has shown remarkable success in enhancing the reasoning capabilities of Large Language Models (LLMs). Process-Supervised RL (PSRL) has emerged as a more effective paradigm compared to outcome-based RL. However, existing PSRL approaches suffer from limited exploration efficiency, both in terms of branching positions and sampling. In this paper, we introduce a novel PSRL framework (AttnRL), which enables efficient exploration for reasoning models. Motivated by preliminary observations that steps exhibiting high attention scores correlate with reasoning behaviors, we propose to branch from positions with high values. Furthermore, we develop an adaptive sampling strategy that accounts for problem difficulty and historical batch size, ensuring that the whole training batch maintains non-zero advantage values. To further improve sampling efficiency, we design a one-step off-policy training pipeline for PSRL. Extensive experiments on multiple challenging mathematical reasoning benchmarks demonstrate that our method consistently outperforms prior approaches in terms of performance and sampling and training efficiency.
                
                
                    <strong>Published:</strong> 2025-09-30T17:58:34Z
                    <a href="http://arxiv.org/abs/2509.26628v1" target="_blank">ðŸ“„ View Paper</a>
                
            
        
            
                Recursive Self-Aggregation Unlocks Deep Thinking in Large Language Models
                
                    <strong>Authors:</strong> Siddarth Venkatraman, Vineet Jain, Sarthak Mittal, Vedant Shah, Johan Obando-Ceron, Yoshua Bengio, Brian R. Bartoldson, Bhavya Kailkhura, Guillaume Lajoie, Glen Berseth, Nikolay Malkin, Moksh Jain
                
                
                    <strong>Abstract:</strong> Test-time scaling methods improve the capabilities of large language models (LLMs) by increasing the amount of compute used during inference to make a prediction. Inference-time compute can be scaled in parallel by choosing among multiple independent solutions or sequentially through self-refinement. We propose Recursive Self-Aggregation (RSA), a test-time scaling method inspired by evolutionary methods that combines the benefits of both parallel and sequential scaling. Each step of RSA refines a population of candidate reasoning chains through aggregation of subsets to yield a population of improved solutions, which are then used as the candidate pool for the next iteration. RSA exploits the rich information embedded in the reasoning chains -- not just the final answers -- and enables bootstrapping from partially correct intermediate steps within different chains of thought. Empirically, RSA delivers substantial performance gains with increasing compute budgets across diverse tasks, model families and sizes. Notably, RSA enables Qwen3-4B-Instruct-2507 to achieve competitive performance with larger reasoning models, including DeepSeek-R1 and o3-mini (high), while outperforming purely parallel and sequential scaling strategies across AIME-25, HMMT-25, Reasoning Gym, LiveCodeBench-v6, and SuperGPQA. We further demonstrate that training the model to combine solutions via a novel aggregation-aware reinforcement learning approach yields significant performance gains. Code available at https://github.com/HyperPotatoNeo/RSA.
                
                
                    <strong>Published:</strong> 2025-09-30T17:58:03Z
                    <a href="http://arxiv.org/abs/2509.26626v1" target="_blank">ðŸ“„ View Paper</a>
                
            
        
    
    
        <p><em>Generated by AI News Agent</em></p>
    

