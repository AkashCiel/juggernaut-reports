
    
        <h1>ðŸ¤– AI Research Report</h1>
        
            <strong>Date:</strong> 2025-09-01<br>
            <strong>Topics:</strong> ai alignment research, quantum computing<br>
            <strong>Papers Found:</strong> 100
        
        
        
            
                <h2>ðŸ¤– AI Summary</h2>
                <h2>ai alignment research</h2>
<h3>Most Important Trends</h3>
<ol>
<li><p><strong>Multi-Label and Multi-Task Learning:</strong> The shift from single-label to multi-label learning in context recognition demonstrates a broader trend towards handling more complex, real-world data that exhibit ambiguity, as seen in the study on situation recognition.</p>
</li>
<li><p><strong>Privacy and Fairness in AI:</strong> Theres an increasing emphasis on privacy-preserving and fairness-aware AI systems, especially in sensitive domains like healthcare, as highlighted by the development of FLIP for generating fair and private synthetic data.</p>
</li>
<li><p><strong>Integration of Innovative Methodologies:</strong> The combination of Agile methodologies with DevOps practices represents a growing trend in software development, aiming for more efficient and flexible production processes.</p>
</li>
<li><p><strong>High-Quality Data Management:</strong> The importance of high-quality training data is underscored by efforts to filter and index problematic content, as seen in the study using ElasticSearch to manage large language model datasets.</p>
</li>
<li><p><strong>Advanced Simulation Techniques:</strong> The use of LLMs in social simulations and the focus on realistic agent-based modeling reflect a trend towards more sophisticated tools for understanding and predicting social dynamics.</p>
</li>
</ol>
<h3>Breakthroughs</h3>
<ol>
<li><p><strong>Single Positive Multi-Label Learning (SPMLL):</strong> Reformulating verb classification in context recognition as a SPMLL problem allows for more nuanced understanding and handling of data with inherent semantic overlap.</p>
</li>
<li><p><strong>Fair Latent Intervention under Privacy Guarantees (FLIP):</strong> Introducing a method that addresses both privacy and fairness in synthetic data generation, providing a task-agnostic approach to fair data modeling.</p>
</li>
<li><p><strong>ElasticSearch-Based Dataset Indexing:</strong> Implementing real-time dataset analysis with ElasticSearch to improve the accountability and safety of AI systems.</p>
</li>
<li><p><strong>Model Stealing Attack Framework:</strong> The development of a standardized framework for evaluating model stealing attacks highlights a significant advancement in understanding and securing AI intellectual property.</p>
</li>
<li><p><strong>Insight Distillation for Tabular Classification:</strong> The introduction of InsightTab, which distills data into insights for LLMs, represents a novel approach to enhancing few-shot learning capabilities.</p>
</li>
</ol>
<h3>Implications</h3>
<ol>
<li><p><strong>Enhanced AI Alignment with Real-World Complexity:</strong> The shift towards multi-label learning and the integration of fairness and privacy in AI models indicate a move towards AI systems better aligned with the complexities and nuances of real-world applications.</p>
</li>
<li><p><strong>Improved Data Management and Quality:</strong> As AI systems become more reliant on large datasets, methods like SPMLL and ElasticSearch indexing will be crucial in ensuring high-quality, ethical, and safe AI deployments.</p>
</li>
<li><p><strong>Broader Applicability of AI Technologies:</strong> The integration of innovative methodologies, such as those combining Agile and DevOps practices, suggests a growing potential for AI to be applied across diverse and dynamic environments, improving efficiency and adaptability.</p>
</li>
<li><p><strong>Robust Security and Intellectual Property Protection:</strong> The focus on detecting and mitigating model stealing and other security threats highlights the need for robust measures in protecting AI systems against misuse and exploitation.</p>
</li>
<li><p><strong>Advancements in AI-Driven Social Simulations:</strong> The use of LLMs in simulating social interactions opens new avenues for studying and understanding social dynamics, potentially influencing policy and decision-making in social and political contexts.</p>
</li>
</ol>
<p><em>Based on 50 research papers</em></p>

<h2>quantum computing</h2>
<p>The request involves summarizing a diverse set of research papers across multiple domains, only a few of which are directly related to quantum computing. Here, I will focus on summarizing the key findings and significance from the papers that are relevant to quantum computing.</p>
<p><strong>Most important trends:</strong></p>
<ul>
<li><strong>Quantum Concurrency and Entanglement Utilization:</strong> The paper 1D Cluster State Generation On Superconducting Hardware highlights the ongoing trend in quantum computing to leverage entanglement as a resource for quantum computation, particularly through the generation of cluster states. This approach is central to Measurement-based Quantum Computation (MBQC), which is gaining traction as an alternative computational paradigm.</li>
<li><strong>Non-Markovian Dynamics in Quantum Systems:</strong> The research Non-Markovian dynamics of giant emitters beyond the Weisskopf-Wigner approximation explores non-Markovian interactions, a growing area of interest as researchers seek to understand quantum systems behavior outside traditional approximations, which could lead to more accurate quantum system modeling.</li>
<li><strong>Quantum Phase Sensitivity:</strong> The paper Quantum Phase Sensitivity with Generalized Coherent States Based on Deformed su(1,1) and Heisenberg Algebras discusses advancements in quantum metrology, specifically improving phase sensitivity using generalized coherent states. This reflects a broader trend toward enhancing precision in quantum measurements, which is crucial for quantum computing and sensing technologies.</li>
</ul>
<p><strong>Breakthroughs:</strong></p>
<ul>
<li><strong>Cluster State Generation:</strong> The work on generating 1D cluster states on superconducting hardware demonstrates significant progress in the practical implementation of MBQC, addressing challenges related to fidelity and error mitigation in entangled state preparation.</li>
<li><strong>Non-Markovian Interaction Models:</strong> By solving the dynamics of giant emitters in symmetric configurations, the research provides new insights into bound states and coherent superpositions, potentially improving quantum communication and information processing.</li>
<li><strong>Enhanced Phase Sensitivity:</strong> The development of new coherent states for phase sensitivity in quantum interferometers represents a breakthrough in achieving near quantum-limited phase sensitivities, which could enhance the performance of quantum sensors and other applications.</li>
</ul>
<p><strong>Implications:</strong></p>
<ul>
<li><strong>Advancements in Quantum Computation:</strong> The improved understanding and generation of cluster states could accelerate the development of quantum computers by providing more efficient ways to perform computations based on entanglement.</li>
<li><strong>More Accurate Quantum Models:</strong> The insights into non-Markovian dynamics and the ability to model interactions beyond traditional approximations could lead to more accurate quantum simulations, improving the reliability of quantum devices.</li>
<li><strong>Quantum Metrology Applications:</strong> Enhanced phase sensitivity in quantum systems could lead to significant improvements in quantum metrology, impacting fields such as navigation and communication by providing more precise measurement tools.</li>
</ul>
<p>These research efforts collectively push the boundaries of quantum computing by improving state preparation, modeling, and measurement, which are foundational to advancing both the theoretical and practical capabilities of quantum technologies.</p>
<p><em>Based on 50 research papers</em></p>

            
        
        
        <h2>ðŸ“š Research Papers</h2>
        
                
                    <h3><a href="http://arxiv.org/abs/2508.21816v1" target="_blank">The Demon is in Ambiguity: Revisiting Situation Recognition with Single Positive Multi-Label Learning</a></h3>
                    <p><strong>Authors:</strong> Yiming Lin, Yuchen Niu, Shang Wang, Kaizhu Huang, Qiufeng Wang, Xiao-Bo Jin</p>
                    <p><strong>Published:</strong> 8/29/2025</p>
                    <p><strong>Categories:</strong> cs.CV, cs.AI</p>
                    <p><strong>Summary:</strong> Context recognition (SR) is a fundamental task in computer vision that aims to extract structured semantic summaries from images by identifying key events and their associated entities. Specifically, given an input image, the model must first classify the main visual events (verb classification), then identify the participating entities and their semantic roles (semantic role labeling), and finally localize these entities in the image (semantic role localization). Existing methods treat verb classification as a single-label problem, but we show through a comprehensive analysis that this formulation fails to address the inherent ambiguity in visual event recognition, as multiple verb categories may reasonably describe the same image. This paper makes three key contributions: First, we reveal through empirical analysis that verb classification is inherently a multi-label problem due to the ubiquitous semantic overlap between verb categories. Second, given the impracticality of fully annotating large-scale datasets with multiple labels, we propose to reformulate verb classification as a single positive multi-label learning (SPMLL) problem - a novel perspective in SR research. Third, we design a comprehensive multi-label evaluation benchmark for SR that is carefully designed to fairly evaluate model performance in a multi-label setting. To address the challenges of SPMLL, we futher develop the Graph Enhanced Verb Multilayer Perceptron (GE-VerbMLP), which combines graph neural networks to capture label correlations and adversarial training to optimize decision boundaries. Extensive experiments on real-world datasets show that our approach achieves more than 3\% MAP improvement while remaining competitive on traditional top-1 and top-5 accuracy metrics.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.21815v1" target="_blank">Achieving Hilbert-Schmidt Independence Under RÃ©nyi Differential Privacy for Fair and Private Data Generation</a></h3>
                    <p><strong>Authors:</strong> Tobias Hyrup, Emmanouil Panagiotou, Arjun Roy, Arthur Zimek, Eirini Ntoutsi, Peter Schneider-Kamp</p>
                    <p><strong>Published:</strong> 8/29/2025</p>
                    <p><strong>Categories:</strong> cs.LG</p>
                    <p><strong>Summary:</strong> As privacy regulations such as the GDPR and HIPAA and responsibility frameworks for artificial intelligence such as the AI Act gain traction, the ethical and responsible use of real-world data faces increasing constraints. Synthetic data generation has emerged as a promising solution to risk-aware data sharing and model development, particularly for tabular datasets that are foundational to sensitive domains such as healthcare. To address both privacy and fairness concerns in this setting, we propose FLIP (Fair Latent Intervention under Privacy guarantees), a transformer-based variational autoencoder augmented with latent diffusion to generate heterogeneous tabular data. Unlike the typical setup in fairness-aware data generation, we assume a task-agnostic setup, not reliant on a fixed, defined downstream task, thus offering broader applicability. To ensure privacy, FLIP employs R\enyi differential privacy (RDP) constraints during training and addresses fairness in the input space with RDP-compatible balanced sampling that accounts for group-specific noise levels across multiple sampling rates. In the latent space, we promote fairness by aligning neuron activation patterns across protected groups using Centered Kernel Alignment (CKA), a similarity measure extending the Hilbert-Schmidt Independence Criterion (HSIC). This alignment encourages statistical independence between latent representations and the protected feature. Empirical results demonstrate that FLIP effectively provides significant fairness improvements for task-agnostic fairness and across diverse downstream tasks under differential privacy constraints.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.21811v1" target="_blank">The Integration of Agile Methodologies in DevOps Practices within the Information Technology Industry</a></h3>
                    <p><strong>Authors:</strong> Ashley Hourigan, Ridewaan Hanslo</p>
                    <p><strong>Published:</strong> 8/29/2025</p>
                    <p><strong>Categories:</strong> cs.SE, 68, D.2.9</p>
                    <p><strong>Summary:</strong> The demand for rapid software delivery in the Information Technology (IT) industry has significantly intensified, emphasising the need for faster software products and service releases with enhanced features to meet customer expectations. Agile methodologies are replacing traditional approaches such as Waterfall, where flexibility, iterative development and adaptation to change are favoured over rigid planning and execution. DevOps, a subsequent evolution from Agile, emphasises collaborative efforts in development and operations teams, focusing on continuous integration and deployment to deliver resilient and high-quality software products and services. This study aims to critically assess both Agile and DevOps practices in the IT industry to identify the feasibility and applicability of Agile methods in DevOps practices. Eleven semi-structured interviews were conducted with Agile and DevOps practitioners in varying capacities across several sectors within the IT industry. Through thematic analysis, 51 unique codes were extracted and synthesised into 19 themes that reported on each phase of the DevOps lifecycle, specifically regarding the integration and implementation of Agile methods into DevOps practices. Based on the findings, a new understanding detailing the interrelationship of Agile methods in DevOps practices was discussed that met the research objectives.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.21788v1" target="_blank">Going over Fine Web with a Fine-Tooth Comb: Technical Report of Indexing Fine Web for Problematic Content Search and Retrieval</a></h3>
                    <p><strong>Authors:</strong> InÃ©s Altemir Marinas, Anastasiia Kucherenko, Andrei Kucharavy</p>
                    <p><strong>Published:</strong> 8/29/2025</p>
                    <p><strong>Categories:</strong> cs.CL, cs.AI, cs.IR</p>
                    <p><strong>Summary:</strong> Large language models (LLMs) rely heavily on web-scale datasets like Common Crawl, which provides over 80\% of training data for some modern models. However, the indiscriminate nature of web crawling raises challenges in data quality, safety, and ethics. Despite the critical importance of training data quality, prior research on harmful content has been limited to small samples due to computational constraints. This project presents a framework for indexing and analyzing LLM training datasets using an ElasticSearch-based pipeline. We apply it to SwissAIs FineWeb-2 corpus (1.5TB, four languages), achieving fast query performance--most searches in milliseconds, all under 2 seconds. Our work demonstrates real-time dataset analysis, offering practical tools for safer, more accountable AI systems.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.21786v1" target="_blank">The Spontaneous Cascading Mechanism Behind Critical Phenomena in Self-Coupled Lasers</a></h3>
                    <p><strong>Authors:</strong> Jiaoqing Wang, Yael Kfir-Cohen, Chenni Xu, Bnaya Gross, Aswathy Sundaresan, Shlomo Havlin, Patrick Sebbah</p>
                    <p><strong>Published:</strong> 8/29/2025</p>
                    <p><strong>Categories:</strong> physics.optics, physics.soc-ph</p>
                    <p><strong>Summary:</strong> The basic physics of lasers is characterized by a second-order continuous phase transition at the critical lasing threshold. Nevertheless, laser bistability with abrupt transitions has been reported in some laser systems, but its underlying mechanism has never been explored. Here we study experimentally and theoretically a novel nonlinearly self-coupled laser system. We show both experimentally and theoretically that this system experiences spontaneous cascading that yields an abrupt mixed-order transition. At the critical point, a long-lived cascading plateau is observed, characterized by a critical branching factor equal to one. When deviating from criticality, the branching factor departs monotonically from one. The critical scaling close to and at the critical point resembles similar phenomena observed recently in other interdependent systems, suggesting a common universal cascading origin for abrupt transitions. Our results shed light on the cascading mechanism of abrupt transitions in laser systems, which can be utilized for future research and applications.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.21783v1" target="_blank">QoS-Aware Proportional Fairness Scheduling for Multi-Flow 5G UEs: A Smart Factory Perspective</a></h3>
                    <p><strong>Authors:</strong> Mohamed Seliem, Utz Roedig, Cormac Sreenan, Dirk Pesch</p>
                    <p><strong>Published:</strong> 8/29/2025</p>
                    <p><strong>Categories:</strong> cs.NI</p>
                    <p><strong>Summary:</strong> Private 5G networks are emerging as key enablers for smart factories, where a single device often handles multiple concurrent traffic flows with distinct Quality of Service (QoS) requirements. Existing simulation frameworks, however, lack the fidelity to model such multi-flow behavior at the QoS Flow Identifier (QFI) level. This paper addresses this gap by extending Simu5G to support per-QFI modeling and by introducing a novel QoS-aware Proportional Fairness (QoS-PF) scheduler. The scheduler dynamically balances delay, Guaranteed Bit Rate (GBR), and priority metrics to optimize resource allocation across heterogeneous flows. We evaluate the proposed approach in a realistic smart factory scenario featuring edge-hosted machine vision, real-time control loops, and bulk data transfer. Results show that QoS-PF improves deadline adherence and fairness without compromising throughput. All extensions are implemented in a modular and open-source manner to support future research. Our work provides both a methodological and architectural foundation for simulating and analyzing advanced QoS policies in industrial 5G deployments.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.21774v1" target="_blank">Improving Biomedical Knowledge Graph Quality: A Community Approach</a></h3>
                    <p><strong>Authors:</strong> Katherina G Cortes, Shilpa Sundar, Sarah Gehrke, Keenan Manpearl, Junxia Lin, Daniel Robert Korn, Harry Caufield, Kevin Schaper, Justin Reese, Kushal Koirala, Lawrence E Hunter, E. Kathleen Carter, Marcello DeLuca, Arjun Krishnan, Chris Mungall, Melissa Haendel</p>
                    <p><strong>Published:</strong> 8/29/2025</p>
                    <p><strong>Categories:</strong> q-bio.OT</p>
                    <p><strong>Summary:</strong> Biomedical knowledge graphs (KGs) are widely used across research and translational settings, yet their design decisions and implementation are often opaque. Unlike ontologies that more frequently adhere to established creation principles, biomedical KGs lack consistent practices for construction, documentation, and dissemination. To address this gap, we introduce a set of evaluation criteria grounded in widely accepted data standards and principles from related fields. We apply these criteria to 16 biomedical KGs, revealing that even those that appear to align with best practices often obscure essential information required for external reuse. Moreover, biomedical KGs, despite pursuing similar goals and ingesting the same sources in some cases, display substantial variation in models, source integration, and terminology for node types. Reaping the potential benefits of knowledge graphs for biomedical research while reducing wasted effort requires community-wide adoption of shared criteria and maturation of standards such as BioLink and KGX. Such improvements in transparency and standardization are essential for creating long-term reusability, improving comparability across resources, and enhancing the overall utility of KGs within biomedicine.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.21762v1" target="_blank">Reasoning-Intensive Regression</a></h3>
                    <p><strong>Authors:</strong> Diane Tchuindjo, Omar Khattab</p>
                    <p><strong>Published:</strong> 8/29/2025</p>
                    <p><strong>Categories:</strong> cs.CL, cs.AI</p>
                    <p><strong>Summary:</strong> AI researchers and practitioners increasingly apply large language models (LLMs) to what we call reasoning-intensive regression (RiR), i.e. deducing subtle numerical properties from text. Unlike standard language regression tasks, e.g. for sentiment or similarity, RiR often appears instead in ad-hoc problems like rubric-based scoring or domain-specific retrieval, where much deeper analysis of text is required while only limited task-specific training data and computation are available. We cast three realistic problems as RiR tasks to establish an initial benchmark, and use that to test our hypothesis that prompting frozen LLMs and finetuning Transformer encoders via gradient descent will both often struggle in RiR. We then propose MENTAT, a simple and lightweight method that combines batch-reflective prompt optimization with neural ensemble learning. MENTAT achieves up to 65% improvement over both baselines, though substantial room remains for future advances in RiR.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.21761v1" target="_blank">Learning from Silence and Noise for Visual Sound Source Localization</a></h3>
                    <p><strong>Authors:</strong> Xavier Juanola, Giovana Morais, Magdalena Fuentes, Gloria Haro</p>
                    <p><strong>Published:</strong> 8/29/2025</p>
                    <p><strong>Categories:</strong> cs.CV, cs.MM</p>
                    <p><strong>Summary:</strong> Visual sound source localization is a fundamental perception task that aims to detect the location of sounding sources in a video given its audio. Despite recent progress, we identify two shortcomings in current methods: 1) most approaches perform poorly in cases with low audio-visual semantic correspondence such as silence, noise, and offscreen sounds, i.e. in the presence of negative audio; and 2) most prior evaluations are limited to positive cases, where both datasets and metrics convey scenarios with a single visible sound source in the scene. To address this, we introduce three key contributions. First, we propose a new training strategy that incorporates silence and noise, which improves performance in positive cases, while being more robust against negative sounds. Our resulting self-supervised model, SSL-SaN, achieves state-of-the-art performance compared to other self-supervised models, both in sound localization and cross-modal retrieval. Second, we propose a new metric that quantifies the trade-off between alignment and separability of auditory and visual features across positive and negative audio-visual pairs. Third, we present IS3+, an extended and improved version of the IS3 synthetic dataset with negative audio. Our data, metrics and code are available on the https://xavijuanola.github.io/SSL-SaN/.</p>
                
            
                
                    <h3><a href="http://dx.doi.org/10.3847/1538-4357/adff7f" target="_blank">The Impact of Enhanced EUV Flux on the Upper Atmosphere of Earth-like Exoplanets</a></h3>
                    <p><strong>Authors:</strong> Lukas Hanson, Ofer Cohen, Aaron Ridley, Alex Glocer</p>
                    <p><strong>Published:</strong> 8/29/2025</p>
                    <p><strong>Categories:</strong> astro-ph.EP</p>
                    <p><strong>Summary:</strong> Identifying Earth-like planets outside out solar system is a leading research goal in astronomy, but determining if candidate planets have atmospheres, and more importantly if they can retain atmospheres, is still out of reach. In this paper, we present our study on the impact of enhanced EUV flux on the stability and escape of the upper atmosphere of an Earth-like exoplanet using the Global Ionosphere and Thermosphere Model (GITM). We also investigate the differences between one- and three-dimensional solutions. We use a baseline case of EUV flux experienced at the Earth, and multiplying this flux by a constant factor going up to 50. Our results show a clear evidence of an inflated and elevated ionosphere due to enhanced EUV flux, and they provide a detailed picture of how different heating and cooling rates, as well as the conductivity are changing at each EUV flux level. Our results also demonstrate that one-dimensional solutions are limited in their ability to capture a global atmosphere that are not uniform. We find that a threshold EUV flux level for a stable atmosphere occurs around a factor of 10 times the baseline level, where EUV fluxes above this level indicate a rapidly escaping atmosphere. This threshold EUV flux translates to about 0.3AU for a planet orbiting the Sun. Thus, our findings indicate that an Earth-like exoplanet orbiting its host star in a close-in orbit is likely to lose its atmosphere quickly.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.21740v1" target="_blank">Operational Validation of Large-Language-Model Agent Social Simulation: Evidence from Voat v/technology</a></h3>
                    <p><strong>Authors:</strong> Aleksandar TomaÅ¡eviÄ‡, Darja CvetkoviÄ‡, Sara Major, Slobodan MaletiÄ‡, Miroslav AnÄ‘elkoviÄ‡, Ana VraniÄ‡, Boris Stupovski, DuÅ¡an VudragoviÄ‡, Aleksandar BogojeviÄ‡, Marija MitroviÄ‡ Dankulov</p>
                    <p><strong>Published:</strong> 8/29/2025</p>
                    <p><strong>Categories:</strong> cs.CY, cs.SI, physics.soc-ph</p>
                    <p><strong>Summary:</strong> Large Language Models (LLMs) enable generative social simulations that can capture culturally informed, norm-guided interaction on online social platforms. We build a technology community simulation modeled on Voat, a Reddit-like alt-right news aggregator and discussion platform active from 2014 to 2020. Using the YSocial framework, we seed the simulation with a fixed catalog of technology links sampled from Voats shared URLs (covering 30+ domains) and calibrate parameters to Voats v/technology using samples from the MADOC dataset. Agents use a base, uncensored model (Dolphin 3.0, based on Llama 3.1 8B) and concise personas (demographics, political leaning, interests, education, toxicity propensity) to generate posts, replies, and reactions under platform rules for link and text submissions, threaded replies and daily activity cycles. We run a 30-day simulation and evaluate operational validity by comparing distributions and structures with matched Voat data: activity patterns, interaction networks, toxicity, and topic coverage. Results indicate familiar online regularities: similar activity rhythms, heavy-tailed participation, sparse low-clustering interaction networks, core-periphery structure, topical alignment with Voat, and elevated toxicity. Limitations of the current study include the stateless agent design and evaluation based on a single 30-day run, which constrains external validity and variance estimates. The simulation generates realistic discussions, often featuring toxic language, primarily centered on technology topics such as Big Tech and AI. This approach offers a valuable method for examining toxicity dynamics and testing moderation strategies within a controlled environment.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.21739v1" target="_blank">Neural Network Acceleration on MPSoC board: Integrating SLACs SNL, Rogue Software and Auto-SNL</a></h3>
                    <p><strong>Authors:</strong> Hamza Ezzaoui Rahali, Abhilasha Dave, Larry Ruckman, Mohammad Mehdi Rahimifar, Audrey C. Therrien, James J. Russel, Ryan T. Herbst</p>
                    <p><strong>Published:</strong> 8/29/2025</p>
                    <p><strong>Categories:</strong> cs.LG, cs.AI, cs.AR</p>
                    <p><strong>Summary:</strong> The LCLS-II Free Electron Laser (FEL) will generate X-ray pulses for beamline experiments at rates of up to 1~MHz, with detectors producing data throughputs exceeding 1 TB/s. Managing such massive data streams presents significant challenges, as transmission and storage infrastructures become prohibitively expensive. Machine learning (ML) offers a promising solution for real-time data reduction, but conventional implementations introduce excessive latency, making them unsuitable for high-speed experimental environments. To address these challenges, SLAC developed the SLAC Neural Network Library (SNL), a specialized framework designed to deploy real-time ML inference models on Field-Programmable Gate Arrays (FPGA). SNLs key feature is the ability to dynamically update model weights without requiring FPGA resynthesis, enhancing flexibility for adaptive learning applications. To further enhance usability and accessibility, we introduce Auto-SNL, a Python extension that streamlines the process of converting Python-based neural network models into SNL-compatible high-level synthesis code. This paper presents a benchmark comparison against hls4ml, the current state-of-the-art tool, across multiple neural network architectures, fixed-point precisions, and synthesis configurations targeting a Xilinx ZCU102 FPGA. The results showed that SNL achieves competitive or superior latency in most tested architectures, while in some cases also offering FPGA resource savings. This adaptation demonstrates SNLs versatility, opening new opportunities for researchers and academics in fields such as high-energy physics, medical imaging, robotics, and many more.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.21738v1" target="_blank">From Drone Imagery to Livability Mapping: AI-powered Environment Perception in Rural China</a></h3>
                    <p><strong>Authors:</strong> Weihuan Deng, Yaofu Huang, Luan Chen, Xun Li, Yao Yao</p>
                    <p><strong>Published:</strong> 8/29/2025</p>
                    <p><strong>Categories:</strong> cs.CY, cs.CV</p>
                    <p><strong>Summary:</strong> With the deepening of poverty alleviation and rural revitalization strategies, improving the rural living environment and enhancing the quality of life have become key priorities. Rural livability is a key indicator for measuring the effectiveness of these efforts. Current measurement approaches face significant limitations, as questionnaire-based methods are difficult to scale, while urban-oriented visual perception methods are poorly suited for rural contexts. In this paper, a rural-specific livability assessment framework was proposed based on drone imagery and multimodal large language models (MLLMs). To comprehensively assess village livability, this study first used a top-down approach to collect large-scale drone imagery of 1,766 villages in 146 counties across China. In terms of the model framework, an efficient image comparison mechanism was developed, incorporating binary search interpolation to determine effective image pairs while reducing comparison iterations. Building on expert knowledge, a chain-of-thought prompting suitable for nationwide rural livability measurement was constructed, considering both living quality and ecological habitability dimensions. This approach enhanced the rationality and reliability of the livability assessment. Finally, this study characterized the spatial heterogeneity of rural livability across China and thoroughly analyzed its influential factors. The results show that: (1) The rural livability in China demonstrates a dual-core-periphery spatial pattern, radiating outward from Sichuan and Zhejiang provinces with declining gradients; (2) Among various influential factors, government fiscal expenditure emerged as the core determinant, with each unit increase corresponding to a 3.9 - 4.9 unit enhancement in livability. The findings provide valuable insights for rural construction policy-making.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.21736v1" target="_blank">MicroLabVR: Interactive 3D Visualization of Simulated Spatiotemporal Microbiome Data in Virtual Reality</a></h3>
                    <p><strong>Authors:</strong> Simon Burbach, Maria Maleshkova, Florian Centler, Tanja Joan Schmidt</p>
                    <p><strong>Published:</strong> 8/29/2025</p>
                    <p><strong>Categories:</strong> cs.HC, cs.CE, cs.GR, q-bio.CB, q-bio.MN</p>
                    <p><strong>Summary:</strong> Microbiomes are a vital part of the human body, engaging in tasks like food digestion and immune defense. Their structure and function must be understood in order to promote host health and facilitate swift recovery during disease. Due to the difficulties in experimentally studying these systems in situ, more research is being conducted in the field of mathematical modeling. Visualizing spatiotemporal data is challenging, and current tools that simulate microbial communities spatial and temporal development often only provide limited functionalities, often requiring expert knowledge to generate useful results. To overcome these limitations, we provide a user-friendly tool to interactively explore spatiotemporal simulation data, called MicroLabVR, which transfers spatial data into virtual reality (VR) while following guidelines to enhance user experience (UX). With MicroLabVR, users can import CSV datasets containing population growth, substance concentration development, and metabolic flux distribution data. The implemented visualization methods allow users to evaluate the dataset in a VR environment interactively. MicroLabVR aims to improve data analysis for the user by allowing the exploration of microbiome data in their spatial context.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.21733v1" target="_blank">Developer Insights into Designing AI-Based Computer Perception Tools</a></h3>
                    <p><strong>Authors:</strong> Maya Guhan, Meghan E. Hurley, Eric A. Storch, John Herrington, Casey Zampella, Julia Parish-Morris, Gabriel LÃ¡zaro-MuÃ±oz, Kristin Kostick-Quenet</p>
                    <p><strong>Published:</strong> 8/29/2025</p>
                    <p><strong>Categories:</strong> cs.HC, cs.AI, cs.CY</p>
                    <p><strong>Summary:</strong> Artificial intelligence (AI)-based computer perception (CP) technologies use mobile sensors to collect behavioral and physiological data for clinical decision-making. These tools can reshape how clinical knowledge is generated and interpreted. However, effective integration of these tools into clinical workflows depends on how developers balance clinical utility with user acceptability and trustworthiness. Our study presents findings from 20 in-depth interviews with developers of AI-based CP tools. Interviews were transcribed and inductive, thematic analysis was performed to identify 4 key design priorities: 1) to account for context and ensure explainability for both patients and clinicians; 2) align tools with existing clinical workflows; 3) appropriately customize to relevant stakeholders for usability and acceptability; and 4) push the boundaries of innovation while aligning with established paradigms. Our findings highlight that developers view themselves as not merely technical architects but also ethical stewards, designing tools that are both acceptable by users and epistemically responsible (prioritizing objectivity and pushing clinical knowledge forward). We offer the following suggestions to help achieve this balance: documenting how design choices around customization are made, defining limits for customization choices, transparently conveying information about outputs, and investing in user training. Achieving these goals will require interdisciplinary collaboration between developers, clinicians, and ethicists.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.21730v1" target="_blank">Freeze and Conquer: Reusable Ansatz for Solving the Traveling Salesman Problem</a></h3>
                    <p><strong>Authors:</strong> Fabrizio Fagiolo, Nicolo Vescera</p>
                    <p><strong>Published:</strong> 8/29/2025</p>
                    <p><strong>Categories:</strong> cs.AI</p>
                    <p><strong>Summary:</strong> In this paper we present a variational algorithm for the Traveling Salesman Problem (TSP) that combines (i) a compact encoding of permutations, which reduces the qubit requirement too, (ii) an optimize-freeze-reuse strategy: where the circuit topology (``Ansatz) is first optimized on a training instance by Simulated Annealing (SA), then ``frozen and re-used on novel instances, limited to a rapid re-optimization of only the circuit parameters. This pipeline eliminates costly structural research in testing, making the procedure immediately implementable on NISQ hardware. On a set of $40$ randomly generated symmetric instances that span $4 - 7$ cities, the resulting Ansatz achieves an average optimal trip sampling probability of $100\%$ for 4 city cases, $90\%$ for 5 city cases and $80\%$ for 6 city cases. With 7 cities the success rate drops markedly to an average of $\sim 20\%$, revealing the onset of scalability limitations of the proposed method. The results show robust generalization ability for moderate problem sizes and indicate how freezing the Ansatz can dramatically reduce time-to-solution without degrading solution quality. The paper also discusses scalability limitations, the impact of ``warm-start initialization of parameters, and prospects for extension to more complex problems, such as Vehicle Routing and Job-Shop Scheduling.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.21728v1" target="_blank">AI-powered full-data set search for new physics in ultraperipheral and diffractive events</a></h3>
                    <p><strong>Authors:</strong> Simone Ragoni, Brianna Kinkaid, Janet Seger, Christopher Anson, David Tlusty</p>
                    <p><strong>Published:</strong> 8/29/2025</p>
                    <p><strong>Categories:</strong> hep-ph, hep-ex</p>
                    <p><strong>Summary:</strong> We present possible strategies for anomaly detection of rare particle decays and exotic hadrons, such as pentaquarks, in low-background environments such as those characteristic of diffractive events and ultraperipheral \pp, \pA, or \AAcoll collisions at the CERN Large Hadron Collider (LHC). Our models are trained with toy samples representing the UPC processes measured until now by the ALICE Collaboration. When samples containing rare processes such as $\jpsi\rightarrow4\pi$ and pentaquark production, where the number of injected pentaquark events is estimated based on current experimentally available upper limits, and those for $\jpsi\rightarrow4\pi$ are estimated through the branching ratio of the decay channel, are analyzed, the rare processes are flagged as anomalous by the models. This approach demonstrates the applicability of such a technique for searches for new physics in the current and future data sets at collider experiments with high purity, while also allowing for the measurement of upper limits for the production of exotica.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.21722v1" target="_blank">Inferring Effects of Major Events through Discontinuity Forecasting of Population Anxiety</a></h3>
                    <p><strong>Authors:</strong> Siddharth Mangalik, Ojas Deshpande, Adithya V. Ganesan, Sean A. P. Clouston, H. Andrew Schwartz</p>
                    <p><strong>Published:</strong> 8/29/2025</p>
                    <p><strong>Categories:</strong> cs.LG</p>
                    <p><strong>Summary:</strong> Estimating community-specific mental health effects of local events is vital for public health policy. While forecasting mental health scores alone offers limited insights into the impact of events on community well-being, quasi-experimental designs like the Longitudinal Regression Discontinuity Design (LRDD) from econometrics help researchers derive more effects that are more likely to be causal from observational data. LRDDs aim to extrapolate the size of changes in an outcome (e.g. a discontinuity in running scores for anxiety) due to a time-specific event. Here, we propose adapting LRDDs beyond traditional forecasting into a statistical learning framework whereby future discontinuities (i.e. time-specific shifts) and changes in slope (i.e. linear trajectories) are estimated given a locations history of the score, dynamic covariates (other running assessments), and exogenous variables (static representations). Applying our framework to predict discontinuities in the anxiety of US counties from COVID-19 events, we found the task was difficult but more achievable as the sophistication of models was increased, with the best results coming from integrating exogenous and dynamic covariates. Our approach shows strong improvement ($r=+.46$ for discontinuity and $r = +.65$ for slope) over traditional static community representations. Discontinuity forecasting raises new possibilities for estimating the idiosyncratic effects of potential future or hypothetical events on specific communities.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.21714v1" target="_blank">Application of Super-Sampling to Microscopy Images Produces Image Resolution below Optical Diffraction Limit</a></h3>
                    <p><strong>Authors:</strong> James N. Caron</p>
                    <p><strong>Published:</strong> 8/29/2025</p>
                    <p><strong>Categories:</strong> physics.optics</p>
                    <p><strong>Summary:</strong> Image Phase Alignment Super-Sampling (ImPASS) is a computational imaging algorithm for converting a sequence of displaced low-resolution images into a single high-resolution image. The method consists of a unique combination of Phase Correlation image registration and SeDDaRA blind deconvolution. The method has previously been validated in simulations and applied successfully to images captured in a laboratory setting. As discussed here, the performance of ImPASS surpasses similar methods that provide quantitative results. ImPASS is applied for the first time to images taken by a widefield microscope, requiring no customization other than a translation stage, to determine if this approach can subceed the diffraction limit for this application. The 80-frame image sets had as targets a slide with a slice of Porcine Cornea, and a standard US Air Force resolution chart, providing quantitative and quantitative assessments. The sets were up-sampled by a factor of eight, aligned, combined, and processed. The measurement revealed that image resolution improved by a factor of 2.68 and subceeded the diffraction limit by a factor of 1.79.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.21698v1" target="_blank">Unidentified and Confounded? Understanding Two-Tower Models for Unbiased Learning to Rank (Extended Abstract)</a></h3>
                    <p><strong>Authors:</strong> Philipp Hager, Onno Zoeter, Maarten de Rijke</p>
                    <p><strong>Published:</strong> 8/29/2025</p>
                    <p><strong>Categories:</strong> cs.IR</p>
                    <p><strong>Summary:</strong> Additive two-tower models are popular learning-to-rank methods for handling biased user feedback in industry settings. Recent studies, however, report a concerning phenomenon: training two-tower models on clicks collected by well-performing production systems leads to decreased ranking performance. This paper investigates two recent explanations for this observation: confounding effects from logging policies and model identifiability issues. We theoretically analyze the identifiability conditions of two-tower models, showing that either document swaps across positions or overlapping feature distributions are required to recover model parameters from clicks. We also investigate the effect of logging policies on two-tower models, finding that they introduce no bias when models perfectly capture user behavior. However, logging policies can amplify biases when models imperfectly capture user behavior, particularly when prediction errors correlate with document placement across positions. We propose a sample weighting technique to mitigate these effects and provide actionable insights for researchers and practitioners using two-tower models.</p>
                
            
                
                    <h3><a href="http://dx.doi.org/10.1016/j.asr.2024.06.053" target="_blank">The cosmic-ray spectrum in the PeV to EeV energy range</a></h3>
                    <p><strong>Authors:</strong> Donghwa Kang, Andreas Haungs</p>
                    <p><strong>Published:</strong> 8/29/2025</p>
                    <p><strong>Categories:</strong> astro-ph.HE</p>
                    <p><strong>Summary:</strong> Cosmic rays around the so-called knee in the spectrum at around PeV primary energy are generally galactic in origin. Observations on the form of their energy spectrum and their mass composition are fundamental tools to understand the origin, acceleration and propagation mechanism of high-energy cosmic rays. In addition, it is required to find signatures to clarify the transition from galactic to extragalactic sources, which are believed to be responsible for the highest-energy cosmic rays above EeV. This brief review focuses on recent experimental results around the knee of the all-particle energy spectrum and composition in the energy range of the knee up to EeV energies.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.21689v1" target="_blank">Mapping like a Skeptic: Probabilistic BEV Projection for Online HD Mapping</a></h3>
                    <p><strong>Authors:</strong> Fatih ErdoÄŸan, Merve Rabia BarÄ±n, Fatma GÃ¼ney</p>
                    <p><strong>Published:</strong> 8/29/2025</p>
                    <p><strong>Categories:</strong> cs.CV</p>
                    <p><strong>Summary:</strong> Constructing high-definition (HD) maps from sensory input requires accurately mapping the road elements in image space to the Birds Eye View (BEV) space. The precision of this mapping directly impacts the quality of the final vectorized HD map. Existing HD mapping approaches outsource the projection to standard mapping techniques, such as attention-based ones. However, these methods struggle with accuracy due to generalization problems, often hallucinating non-existent road elements. Our key idea is to start with a geometric mapping based on camera parameters and adapt it to the scene to extract relevant map information from camera images. To implement this, we propose a novel probabilistic projection mechanism with confidence scores to (i) refine the mapping to better align with the scene and (ii) filter out irrelevant elements that should not influence HD map generation. In addition, we improve temporal processing by using confidence scores to selectively accumulate reliable information over time. Experiments on new splits of the nuScenes and Argoverse2 datasets demonstrate improved performance over state-of-the-art approaches, indicating better generalization. The improvements are particularly pronounced on nuScenes and in the challenging long perception range. Our code and model checkpoints are available at https://github.com/Fatih-Erdogan/mapping-like-skeptic .</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.21675v1" target="_blank">Is this chart lying to me? Automating the detection of misleading visualizations</a></h3>
                    <p><strong>Authors:</strong> Jonathan Tonglet, Jan Zimny, Tinne Tuytelaars, Iryna Gurevych</p>
                    <p><strong>Published:</strong> 8/29/2025</p>
                    <p><strong>Categories:</strong> cs.CL, cs.CV, cs.GR</p>
                    <p><strong>Summary:</strong> Misleading visualizations are a potent driver of misinformation on social media and the web. By violating chart design principles, they distort data and lead readers to draw inaccurate conclusions. Prior work has shown that both humans and multimodal large language models (MLLMs) are frequently deceived by such visualizations. Automatically detecting misleading visualizations and identifying the specific design rules they violate could help protect readers and reduce the spread of misinformation. However, the training and evaluation of AI models has been limited by the absence of large, diverse, and openly available datasets. In this work, we introduce Misviz, a benchmark of 2,604 real-world visualizations annotated with 12 types of misleaders. To support model training, we also release Misviz-synth, a synthetic dataset of 81,814 visualizations generated using Matplotlib and based on real-world data tables. We perform a comprehensive evaluation on both datasets using state-of-the-art MLLMs, rule-based systems, and fine-tuned classifiers. Our results reveal that the task remains highly challenging. We release Misviz, Misviz-synth, and the accompanying code.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.21669v1" target="_blank">Cybersecurity AI: Hacking the AI Hackers via Prompt Injection</a></h3>
                    <p><strong>Authors:</strong> VÃ­ctor Mayoral-Vilches, Per Mannermaa Rynning</p>
                    <p><strong>Published:</strong> 8/29/2025</p>
                    <p><strong>Categories:</strong> cs.CR</p>
                    <p><strong>Summary:</strong> We demonstrate how AI-powered cybersecurity tools can be turned against themselves through prompt injection attacks. Prompt injection is reminiscent of cross-site scripting (XSS): malicious text is hidden within seemingly trusted content, and when the system processes it, that text is transformed into unintended instructions. When AI agents designed to find and exploit vulnerabilities interact with malicious web servers, carefully crafted reponses can hijack their execution flow, potentially granting attackers system access. We present proof-of-concept exploits against the Cybersecurity AI (CAI) framework and its CLI tool, and detail our mitigations against such attacks in a multi-layered defense implementation. Our findings indicate that prompt injection is a recurring and systemic issue in LLM-based architectures, one that will require dedicated work to address, much as the security community has had to do with XSS in traditional web applications.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.21666v1" target="_blank">Harnessing IoT and Generative AI for Weather-Adaptive Learning in Climate Resilience Education</a></h3>
                    <p><strong>Authors:</strong> Imran S. A. Khan, Emmanuel G. Blanchard, SÃ©bastien George</p>
                    <p><strong>Published:</strong> 8/29/2025</p>
                    <p><strong>Categories:</strong> cs.HC, cs.AI, cs.CY, cs.LG, cs.SE</p>
                    <p><strong>Summary:</strong> This paper introduces the Future Atmospheric Conditions Training System (FACTS), a novel platform that advances climate resilience education through place-based, adaptive learning experiences. FACTS combines real-time atmospheric data collected by IoT sensors with curated resources from a Knowledge Base to dynamically generate localized learning challenges. Learner responses are analyzed by a Generative AI powered server, which delivers personalized feedback and adaptive support. Results from a user evaluation indicate that participants found the system both easy to use and effective for building knowledge related to climate resilience. These findings suggest that integrating IoT and Generative AI into atmospherically adaptive learning technologies holds significant promise for enhancing educational engagement and fostering climate awareness.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.21654v1" target="_blank">I Stolenly Swear That I Am Up to (No) Good: Design and Evaluation of Model Stealing Attacks</a></h3>
                    <p><strong>Authors:</strong> Daryna Oliynyk, Rudolf Mayer, Kathrin Grosse, Andreas Rauber</p>
                    <p><strong>Published:</strong> 8/29/2025</p>
                    <p><strong>Categories:</strong> cs.CR, cs.LG</p>
                    <p><strong>Summary:</strong> Model stealing attacks endanger the confidentiality of machine learning models offered as a service. Although these models are kept secret, a malicious party can query a model to label data samples and train their own substitute model, violating intellectual property. While novel attacks in the field are continually being published, their design and evaluations are not standardised, making it challenging to compare prior works and assess progress in the field. This paper is the first to address this gap by providing recommendations for designing and evaluating model stealing attacks. To this end, we study the largest group of attacks that rely on training a substitute model -- those attacking image classification models. We propose the first comprehensive threat model and develop a framework for attack comparison. Further, we analyse attack setups from related works to understand which tasks and models have been studied the most. Based on our findings, we present best practices for attack development before, during, and beyond experiments and derive an extensive list of open research questions regarding the evaluation of model stealing attacks. Our findings and recommendations also transfer to other problem domains, hence establishing the first generic evaluation methodology for model stealing attacks.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.21648v1" target="_blank">Leveraging Imperfection with MEDLEY A Multi-Model Approach Harnessing Bias in Medical AI</a></h3>
                    <p><strong>Authors:</strong> Farhad Abtahi, Mehdi Astaraki, Fernando Seoane</p>
                    <p><strong>Published:</strong> 8/29/2025</p>
                    <p><strong>Categories:</strong> cs.AI, 68T07, 68T09, 68T20 (Primary) 62P10, 62C20, 62H30 (Secondary)</p>
                    <p><strong>Summary:</strong> Bias in medical artificial intelligence is conventionally viewed as a defect requiring elimination. However, human reasoning inherently incorporates biases shaped by education, culture, and experience, suggesting their presence may be inevitable and potentially valuable. We propose MEDLEY (Medical Ensemble Diagnostic system with Leveraged diversitY), a conceptual framework that orchestrates multiple AI models while preserving their diverse outputs rather than collapsing them into a consensus. Unlike traditional approaches that suppress disagreement, MEDLEY documents model-specific biases as potential strengths and treats hallucinations as provisional hypotheses for clinician verification. A proof-of-concept demonstrator was developed using over 30 large language models, creating a minimum viable product that preserved both consensus and minority views in synthetic cases, making diagnostic uncertainty and latent biases transparent for clinical oversight. While not yet a validated clinical tool, the demonstration illustrates how structured diversity can enhance medical reasoning under clinician supervision. By reframing AI imperfection as a resource, MEDLEY offers a paradigm shift that opens new regulatory, ethical, and innovation pathways for developing trustworthy medical AI systems.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.21636v1" target="_blank">Detecting Stealthy Data Poisoning Attacks in AI Code Generators</a></h3>
                    <p><strong>Authors:</strong> Cristina Improta</p>
                    <p><strong>Published:</strong> 8/29/2025</p>
                    <p><strong>Categories:</strong> cs.CR, cs.SE</p>
                    <p><strong>Summary:</strong> Deep learning (DL) models for natural language-to-code generation have become integral to modern software development pipelines. However, their heavy reliance on large amounts of data, often collected from unsanitized online sources, exposes them to data poisoning attacks, where adversaries inject malicious samples to subtly bias model behavior. Recent targeted attacks silently replace secure code with semantically equivalent but vulnerable implementations without relying on explicit triggers to launch the attack, making it especially hard for detection methods to distinguish clean from poisoned samples. We present a systematic study on the effectiveness of existing poisoning detection methods under this stealthy threat model. Specifically, we perform targeted poisoning on three DL models (CodeBERT, CodeT5+, AST-T5), and evaluate spectral signatures analysis, activation clustering, and static analysis as defenses. Our results show that all methods struggle to detect triggerless poisoning, with representation-based approaches failing to isolate poisoned samples and static analysis suffering false positives and false negatives, highlighting the need for more robust, trigger-independent defenses for AI-assisted code generation.</p>
                
            
                
                    <h3><a href="http://dx.doi.org/10.1177/02783649251368909" target="_blank">The Rosario Dataset v2: Multimodal Dataset for Agricultural Robotics</a></h3>
                    <p><strong>Authors:</strong> Nicolas Soncini, Javier Cremona, Erica Vidal, Maximiliano GarcÃ­a, GastÃ³n Castro, TaihÃº Pire</p>
                    <p><strong>Published:</strong> 8/29/2025</p>
                    <p><strong>Categories:</strong> cs.RO, cs.CV, cs.SY, eess.SY, I.2.9</p>
                    <p><strong>Summary:</strong> We present a multi-modal dataset collected in a soybean crop field, comprising over two hours of recorded data from sensors such as stereo infrared camera, color camera, accelerometer, gyroscope, magnetometer, GNSS (Single Point Positioning, Real-Time Kinematic and Post-Processed Kinematic), and wheel odometry. This dataset captures key challenges inherent to robotics in agricultural environments, including variations in natural lighting, motion blur, rough terrain, and long, perceptually aliased sequences. By addressing these complexities, the dataset aims to support the development and benchmarking of advanced algorithms for localization, mapping, perception, and navigation in agricultural robotics. The platform and data collection system is designed to meet the key requirements for evaluating multi-modal SLAM systems, including hardware synchronization of sensors, 6-DOF ground truth and loops on long trajectories. We run multimodal state-of-the art SLAM methods on the dataset, showcasing the existing limitations in their application on agricultural settings. The dataset and utilities to work with it are released on https://cifasis.github.io/rosariov2/.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.21634v1" target="_blank">Human-Written vs. AI-Generated Code: A Large-Scale Study of Defects, Vulnerabilities, and Complexity</a></h3>
                    <p><strong>Authors:</strong> Domenico Cotroneo, Cristina Improta, Pietro Liguori</p>
                    <p><strong>Published:</strong> 8/29/2025</p>
                    <p><strong>Categories:</strong> cs.SE</p>
                    <p><strong>Summary:</strong> As AI code assistants become increasingly integrated into software development workflows, understanding how their code compares to human-written programs is critical for ensuring reliability, maintainability, and security. In this paper, we present a large-scale comparison of code authored by human developers and three state-of-the-art LLMs, i.e., ChatGPT, DeepSeek-Coder, and Qwen-Coder, on multiple dimensions of software quality: code defects, security vulnerabilities, and structural complexity. Our evaluation spans over 500k code samples in two widely used languages, Python and Java, classifying defects via Orthogonal Defect Classification and security vulnerabilities using the Common Weakness Enumeration. We find that AI-generated code is generally simpler and more repetitive, yet more prone to unused constructs and hardcoded debugging, while human-written code exhibits greater structural complexity and a higher concentration of maintainability issues. Notably, AI-generated code also contains more high-risk security vulnerabilities. These findings highlight the distinct defect profiles of AI- and human-authored code and underscore the need for specialized quality assurance practices in AI-assisted programming.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.21622v1" target="_blank">Integrating Large Language Models with Network Optimization for Interactive and Explainable Supply Chain Planning: A Real-World Case Study</a></h3>
                    <p><strong>Authors:</strong> Saravanan Venkatachalam</p>
                    <p><strong>Published:</strong> 8/29/2025</p>
                    <p><strong>Categories:</strong> cs.AI</p>
                    <p><strong>Summary:</strong> This paper presents an integrated framework that combines traditional network optimization models with large language models (LLMs) to deliver interactive, explainable, and role-aware decision support for supply chain planning. The proposed system bridges the gap between complex operations research outputs and business stakeholder understanding by generating natural language summaries, contextual visualizations, and tailored key performance indicators (KPIs). The core optimization model addresses tactical inventory redistribution across a network of distribution centers for multi-period and multi-item, using a mixed-integer formulation. The technical architecture incorporates AI agents, RESTful APIs, and a dynamic user interface to support real-time interaction, configuration updates, and simulation-based insights. A case study demonstrates how the system improves planning outcomes by preventing stockouts, reducing costs, and maintaining service levels. Future extensions include integrating private LLMs, transfer learning, reinforcement learning, and Bayesian neural networks to enhance explainability, adaptability, and real-time decision-making.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.21614v1" target="_blank">Energy Detection over Composite $Îº-Î¼$ Shadowed Fading Channels with Inverse Gaussian Distribution in Ultra mMTC Networks</a></h3>
                    <p><strong>Authors:</strong> He Huang, Zeping Sui, Zilong Liu, Wei Huang, Md. Noor-A-Rahim, Haishi Wang, Zhiheng Hu</p>
                    <p><strong>Published:</strong> 8/29/2025</p>
                    <p><strong>Categories:</strong> eess.SP</p>
                    <p><strong>Summary:</strong> This paper investigates the characteristics of energy detection (ED) over composite $\kappa$-$\mu$ shadowed fading channels in ultra machine-type communication (mMTC) networks. We have derived the closed-form expressions of the probability density function (PDF) of signal-to-noise ratio (SNR) based on the Inverse Gaussian (\emph{IG}) distribution. By adopting novel integration and mathematical transformation techniques, we derive a truncation-based closed-form expression for the average detection probability for the first time. It can be observed from our simulations that the number of propagation paths has a more pronounced effect on average detection probability compared to average SNR, which is in contrast to earlier studies that focus on device-to-device networks. It suggests that for 6G mMTC network design, we should consider enhancing transmitter-receiver placement and antenna alignment strategies, rather than relying solely on increasing the device-to-device average SNR.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.21597v1" target="_blank">Time-resolved 3D imaging opportunities with XMPI at ForMAX</a></h3>
                    <p><strong>Authors:</strong> Julia Katharina Rogalinski, Zisheng Yao, Yuhe Zhang, Zhe Hu, Korneliya Gordeyeva, Tomas RosÃ©n, Daniel SÃ¶derberg, Andrea Mazzolari, Jackson da Silva, Vahid Haghighat, Samuel A. McDonald, Kim NygÃ¥rd, Eleni Myrto Asimakopoulou, Pablo Villanueva-Perez</p>
                    <p><strong>Published:</strong> 8/29/2025</p>
                    <p><strong>Categories:</strong> physics.app-ph</p>
                    <p><strong>Summary:</strong> X-rays are commonly used in imaging experiments due to their penetration power, which enables non-destructive resolution of internal structures in samples that are opaque to visible light. Time-resolved X-ray tomography is the state-of-the-art method for obtaining volumetric 4D (3D + time) information by rotating the sample and acquiring projections from different angular viewpoints over time. This method enables studies to address a plethora of research questions across various scientific disciplines. However, it faces several limitations, such as incompatibility with single-shot experiments, challenges in rotating complex sample environments that restrict the achievable rotation speed or range, and the introduction of centrifugal forces that can affect the samples dynamics. These limitations can hinder and even preclude the study of certain dynamics. Here, we present an implementation of an alternative approach, X-ray Multi-Projection Imaging (XMPI), which eliminates the need for sample rotation. Instead, the direct incident X-ray beam is split into beamlets using beam splitting X-ray optics. These beamlets intersect at the sample position from different angular viewpoints, allowing multiple projections to be acquired simultaneously. We commissioned this setup at the ForMAX beamline at MAX IV. We present projections acquired from two different sample systems - fibers under mechanical load and particle suspension in multi-phase flow - with distinct spatial and temporal resolution requirements. We demonstrate the capabilities of the ForMAX XMPI setup using the detectors full dynamical range for the relevant sample-driven spatiotemporal resolutions: i) at least 12.5 kHz framerates with 4 micrometer pixel sizes (fibers) and ii) 40 Hz acquisitions with 1.3 micrometer pixel sizes (multi-phase flows), setting the basis for a permanent XMPI endstation at ForMAX.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.21589v1" target="_blank">Middo: Model-Informed Dynamic Data Optimization for Enhanced LLM Fine-Tuning via Closed-Loop Learning</a></h3>
                    <p><strong>Authors:</strong> Zinan Tang, Xin Gao, Qizhi Pei, Zhuoshi Pan, Mengzhang Cai, Jiang Wu, Conghui He, Lijun Wu</p>
                    <p><strong>Published:</strong> 8/29/2025</p>
                    <p><strong>Categories:</strong> cs.CL, cs.AI</p>
                    <p><strong>Summary:</strong> Supervised Fine-Tuning (SFT) Large Language Models (LLM) fundamentally rely on high-quality training data. While data selection and data synthesis are two common strategies to improve data quality, existing approaches often face limitations in static dataset curation that fail to adapt to evolving model capabilities. In this paper, we introduce Middo, a self-evolving Model-informed dynamic data optimization framework that uses model-aware data selection and context-preserving data refinement. Unlike conventional one-off filtering/synthesis methods, our framework establishes a closed-loop optimization system: (1) A self-referential diagnostic module proactively identifies suboptimal samples through tri-axial model signals - loss patterns (complexity), embedding cluster dynamics (diversity), and self-alignment scores (quality); (2) An adaptive optimization engine then transforms suboptimal samples into pedagogically valuable training points while preserving semantic integrity; (3) This optimization process continuously evolves with model capability through dynamic learning principles. Experiments on multiple benchmarks demonstrate that our \method consistently enhances the quality of seed data and boosts LLMs performance with improving accuracy by 7.15% on average while maintaining the original dataset scale. This work establishes a new paradigm for sustainable LLM training through dynamic human-AI co-evolution of data and models. Our datasets, models, and code are coming soon.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.21587v1" target="_blank">A Survey on Current Trends and Recent Advances in Text Anonymization</a></h3>
                    <p><strong>Authors:</strong> Tobias DeuÃŸer, Lorenz Sparrenberg, Armin Berger, Max HahnbÃ¼ck, Christian Bauckhage, Rafet Sifa</p>
                    <p><strong>Published:</strong> 8/29/2025</p>
                    <p><strong>Categories:</strong> cs.CL, cs.AI</p>
                    <p><strong>Summary:</strong> The proliferation of textual data containing sensitive personal information across various domains requires robust anonymization techniques to protect privacy and comply with regulations, while preserving data usability for diverse and crucial downstream tasks. This survey provides a comprehensive overview of current trends and recent advances in text anonymization techniques. We begin by discussing foundational approaches, primarily centered on Named Entity Recognition, before examining the transformative impact of Large Language Models, detailing their dual role as sophisticated anonymizers and potent de-anonymization threats. The survey further explores domain-specific challenges and tailored solutions in critical sectors such as healthcare, law, finance, and education. We investigate advanced methodologies incorporating formal privacy models and risk-aware frameworks, and address the specialized subfield of authorship anonymization. Additionally, we review evaluation frameworks, comprehensive metrics, benchmarks, and practical toolkits for real-world deployment of anonymization solutions. This review consolidates current knowledge, identifies emerging trends and persistent challenges, including the evolving privacy-utility trade-off, the need to address quasi-identifiers, and the implications of LLM capabilities, and aims to guide future research directions for both academics and practitioners in this field.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.21572v1" target="_blank">NewsReX: A More Efficient Approach to News Recommendation with Keras 3 and JAX</a></h3>
                    <p><strong>Authors:</strong> Igor L. R. Azevedo, Toyotaro Suzumura, Yuichiro Yasui</p>
                    <p><strong>Published:</strong> 8/29/2025</p>
                    <p><strong>Categories:</strong> cs.IR</p>
                    <p><strong>Summary:</strong> Reproducing and comparing results in news recommendation research has become increasingly difficult. This is due to a fragmented ecosystem of diverse codebases, varied configurations, and mainly due to resource-intensive models. We introduce NewsReX, an open-source library designed to streamline this process. Our key contribution is a modern implementation built on Keras 3 and JAX, which provides an increase in computational efficiency. Experiments show that NewsReX is faster than current implementations. To support broader research, we provide a straightforward guide and scripts for training models on custom datasets. We validated this functionality using a proprietary Japanese news dataset from Nikkei News, a leading Japanese media corporation renowned for its comprehensive coverage of business, economic, and financial news. NewsReX makes reproducing complex experiments faster and more accessible to a wider range of hardware making sure the speed up it also achieved for less powerful GPUs, like an 8GB RTX 3060 Ti. Beyond the library, this paper offers an analysis of key training parameters often overlooked in the literature, including the effect of different negative sampling strategies, the varying number of epochs, the impact of random batching, and more. This supplementary analysis serves as a valuable reference for future research, aiming to reduce redundant computation when comparing baselines and guide best practices. Code available at https://github.com/igor17400/NewsReX.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.21561v1" target="_blank">Summarize-Exemplify-Reflect: Data-driven Insight Distillation Empowers LLMs for Few-shot Tabular Classification</a></h3>
                    <p><strong>Authors:</strong> Yifei Yuan, Jiatong Li, Weijia Zhang, Mohammad Aliannejadi, Evangelos Kanoulas, Renjun Hu</p>
                    <p><strong>Published:</strong> 8/29/2025</p>
                    <p><strong>Categories:</strong> cs.LG, cs.CL</p>
                    <p><strong>Summary:</strong> Recent studies show the promise of large language models (LLMs) for few-shot tabular classification but highlight challenges due to the variability in structured data. To address this, we propose distilling data into actionable insights to enable robust and effective classification by LLMs. Drawing inspiration from human learning processes, we introduce InsightTab, an insight distillation framework guided by principles of divide-and-conquer, easy-first, and reflective learning. Our approach integrates rule summarization, strategic exemplification, and insight reflection through deep collaboration between LLMs and data modeling techniques. The obtained insights enable LLMs to better align their general knowledge and capabilities with the particular requirements of specific tabular tasks. We extensively evaluate InsightTab on nine datasets. The results demonstrate consistent improvement over state-of-the-art methods. Ablation studies further validate the principle-guided distillation process, while analyses emphasize InsightTabs effectiveness in leveraging labeled data and managing bias.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.21548v1" target="_blank">Quantum Leap in Finance: Economic Advantages, Security, and Post-Quantum Readiness</a></h3>
                    <p><strong>Authors:</strong> Gerhard Hellstern, Esra Yeniaras</p>
                    <p><strong>Published:</strong> 8/29/2025</p>
                    <p><strong>Categories:</strong> quant-ph</p>
                    <p><strong>Summary:</strong> This paper provides an in-depth review of the evolving role of quantum computing in the financial sector, emphasizing both its computational potential and cybersecurity implications. Distinguishing itself from existing surveys, this work integrates classical quantum computing applications - such as portfolio optimization, risk analysis, derivative pricing, and Monte Carlo simulations with a thorough examination of blockchain technologies and post-quantum cryptography (PQC), which are crucial for maintaining secure financial operations in the emerging quantum era. We propose a structured four-step framework to assess the feasibility and expected benefits of implementing quantum solutions in finance, considering factors such as computational scalability, error tolerance, data complexity, and practical implementability. This framework is applied to a series of representative financial scenarios to identify domains where quantum approaches can surpass classical techniques. Furthermore, the paper explores the vulnerabilities quantum computing introduces to digital finance-related applications and blockchain security, including risks to digital signatures, hash functions, and randomness generation, and discusses mitigation strategies through PQC and quantum-resilient alternatives of classical digital finance tools and blockchain architectures. By addressing both quantum blockchain, quantum key distribution (QKD) as well as quantum communication networks, his review presents a more holistic perspective than prior studies, offering actionable insights for researchers, financial practitioners, and policymakers navigating the intersection of quantum computing, blockchain, and secure financial systems.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.21540v1" target="_blank">HealthProcessAI: A Technical Framework and Proof-of-Concept for LLM-Enhanced Healthcare Process Mining</a></h3>
                    <p><strong>Authors:</strong> Eduardo Illueca-Fernandez, Kaile Chen, Fernando Seoane, Farhad Abtahi</p>
                    <p><strong>Published:</strong> 8/29/2025</p>
                    <p><strong>Categories:</strong> cs.AI</p>
                    <p><strong>Summary:</strong> Process mining has emerged as a powerful analytical technique for understanding complex healthcare workflows. However, its application faces significant barriers, including technical complexity, a lack of standardized approaches, and limited access to practical training resources. We introduce HealthProcessAI, a GenAI framework designed to simplify process mining applications in healthcare and epidemiology by providing a comprehensive wrapper around existing Python (PM4PY) and R (bupaR) libraries. To address unfamiliarity and improve accessibility, the framework integrates multiple Large Language Models (LLMs) for automated process map interpretation and report generation, helping translate technical analyses into outputs that diverse users can readily understand. We validated the framework using sepsis progression data as a proof-of-concept example and compared the outputs of five state-of-the-art LLM models through the OpenRouter platform. To test its functionality, the framework successfully processed sepsis data across four proof-of-concept scenarios, demonstrating robust technical performance and its capability to generate reports through automated LLM analysis. LLM evaluation using five independent LLMs as automated evaluators revealed distinct model strengths: Claude Sonnet-4 and Gemini 2.5-Pro achieved the highest consistency scores (3.79/4.0 and 3.65/4.0) when evaluated by automated LLM assessors. By integrating multiple Large Language Models (LLMs) for automated interpretation and report generation, the framework addresses widespread unfamiliarity with process mining outputs, making them more accessible to clinicians, data scientists, and researchers. This structured analytics and AI-driven interpretation combination represents a novel methodological advance in translating complex process mining results into potentially actionable insights for healthcare applications.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.21539v1" target="_blank">HCCM: Hierarchical Cross-Granularity Contrastive and Matching Learning for Natural Language-Guided Drones</a></h3>
                    <p><strong>Authors:</strong> Hao Ruan, Jinliang Lin, Yingxin Lai, Zhiming Luo, Shaozi Li</p>
                    <p><strong>Published:</strong> 8/29/2025</p>
                    <p><strong>Categories:</strong> cs.CV</p>
                    <p><strong>Summary:</strong> Natural Language-Guided Drones (NLGD) provide a novel paradigm for tasks such as target matching and navigation. However, the wide field of view and complex compositional semantics in drone scenarios pose challenges for vision-language understanding. Mainstream Vision-Language Models (VLMs) emphasize global alignment while lacking fine-grained semantics, and existing hierarchical methods depend on precise entity partitioning and strict containment, limiting effectiveness in dynamic environments. To address this, we propose the Hierarchical Cross-Granularity Contrastive and Matching learning (HCCM) framework with two components: (1) Region-Global Image-Text Contrastive Learning (RG-ITC), which avoids precise scene partitioning and captures hierarchical local-to-global semantics by contrasting local visual regions with global text and vice versa; (2) Region-Global Image-Text Matching (RG-ITM), which dispenses with rigid constraints and instead evaluates local semantic consistency within global cross-modal representations, enhancing compositional reasoning. Moreover, drone text descriptions are often incomplete or ambiguous, destabilizing alignment. HCCM introduces a Momentum Contrast and Distillation (MCD) mechanism to improve robustness. Experiments on GeoText-1652 show HCCM achieves state-of-the-art Recall@1 of 28.8% (image retrieval) and 14.7% (text retrieval). On the unseen ERA dataset, HCCM demonstrates strong zero-shot generalization with 39.93% mean recall (mR), outperforming fine-tuned baselines.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.21538v1" target="_blank">Adaptive extended Kalman filter and laser link acquisition in the detection of gravitational waves in space</a></h3>
                    <p><strong>Authors:</strong> Jinke Yang, Yong Xie, Yidi Fan, Pengcheng Wang, Xindong Liang, Haojie Li, Xue Wang, Zhao Cui, Jianjun Jia, Yucheng Tang, Yun Kau Lau</p>
                    <p><strong>Published:</strong> 8/29/2025</p>
                    <p><strong>Categories:</strong> astro-ph.IM, gr-qc</p>
                    <p><strong>Summary:</strong> An alternative, new laser link acquisition scheme for the triangular constellation of spacecraft (SCs) in deep space in the detection of gravitational waves is considered. In place of a wide field CCD camera in the initial stage of laser link acquisition adopted in the conventional scheme, an extended Kalman filter based on precision orbit determination is incorporated in the point ahead angle mechanism (PAAM) to steer the laser beam in such a way to narrow the uncertainty cone and at the same time avoids the heating problem generated by the CCD camera.A quadrant photodetector (QPD) based on the Differential Power Sensing (DPS) technique, which offers a higher dynamic range than differential wavefront sensing (DWS), is employed as the readout of the laser beam spot. The conventional two stages (coarse acquisition and fine acquisition) are integrated into a single control loop. The payload structure of the ATP control loop is simplified and numerical simulations, based on a colored measurement noise model that closely mimics the prospective on-orbit conditions, demonstrate that the AEKF significantly reduces the initial uncertainty region by predicting the point ahead angle (PAA) even when the worst case scenario in SC position (navigation) error is considered.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.21536v1" target="_blank">Triply Robust Panel Estimators</a></h3>
                    <p><strong>Authors:</strong> Susan Athey, Guido Imbens, Zhaonan Qu, Davide Viviano</p>
                    <p><strong>Published:</strong> 8/29/2025</p>
                    <p><strong>Categories:</strong> stat.ME</p>
                    <p><strong>Summary:</strong> This paper studies estimation of causal effects in a panel data setting. We introduce a new estimator, the Triply RObust Panel (TROP) estimator, that combines $(i)$ a flexible model for the potential outcomes based on a low-rank factor structure on top of a two-way-fixed effect specification, with $(ii)$ unit weights intended to upweight units similar to the treated units and $(iii)$ time weights intended to upweight time periods close to the treated time periods. We study the performance of the estimator in a set of simulations designed to closely match several commonly studied real data sets. We find that there is substantial variation in the performance of the estimators across the settings considered. The proposed estimator outperforms two-way-fixed-effect/difference-in-differences, synthetic control, matrix completion and synthetic-difference-in-differences estimators. We investigate what features of the data generating process lead to this performance, and assess the relative importance of the three components of the proposed estimator. We have two recommendations. Our preferred strategy is that researchers use simulations closely matched to the data they are interested in, along the lines discussed in this paper, to investigate which estimators work well in their particular setting. A simpler approach is to use more robust estimators such as synthetic difference-in-differences or the new triply robust panel estimator which we find to substantially outperform two-way fixed effect estimators in many empirically relevant settings.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.21535v1" target="_blank">Non-Take-Up of Unemployment Benefit II in Germany: A Longitudinal Perspective Using Administrative Data</a></h3>
                    <p><strong>Authors:</strong> JÃ¼rgen Wiemers</p>
                    <p><strong>Published:</strong> 8/29/2025</p>
                    <p><strong>Categories:</strong> econ.GN, q-fin.EC</p>
                    <p><strong>Summary:</strong> Extensive research demonstrates that many households eligible for means-tested benefits do not claim them, a phenomenon known as non-take-up. Empirical studies frequently conceptualise non-take-up as a rational decision, occurring when the perceived net utility of claiming is negative. Theoretically, long-term factors can substantially impact this decision. Despite the potential relevance of longitudinal aspects, evidence on their influence remains limited. This study addresses this gap by incorporating long-term factors in the analysis of non-take-up behaviour relating to Unemployment Benefit II (UB II), Germanys basic means-tested welfare programme. Using data from the German Panel Study Labour Market and Social Security (PASS) from 2008 to 2020, linked with administrative data from Germanys Federal Employment Agency (PASS-ADIAB), this study reconstructs households benefit receipt and income histories, even during non-survey periods. This allows modelling benefit non-take-up for eligible households using the duration and frequency of past benefit receipt. In addition, the use of administrative data mitigates bias from self-reported benefit receipt. Household eligibility for UB II is simulated using GETTSIM, an open-source microsimulation model, applied to the PASS dataset for the first time. Findings indicate that long-term factors significantly influence the probability of claiming UB II. Specifically, a longer history of benefit receipt increases this probability, whereas higher income potential and positive income shocks reduce it. Including long-term factors substantially affects the estimated impact of traditionally used determinants of non-take-up, indicating a potential misspecification in existing models that neglect them.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.21526v1" target="_blank">Chemical Control of Mechanical Anisotropy and Band Alignment in Perylene-based Two-dimensional MoS$_2$-Organic Hybrids</a></h3>
                    <p><strong>Authors:</strong> Mohammed El Amine Miloudi, Oliver KÃ¼hn</p>
                    <p><strong>Published:</strong> 8/29/2025</p>
                    <p><strong>Categories:</strong> cond-mat.mtrl-sci, cond-mat.mes-hall</p>
                    <p><strong>Summary:</strong> This study presents a comprehensive investigation of hybrid interfaces formed by monolayer MoS$_2$ coupled with the organic molecules perylene (P), perylene diimide (PDI), and perylene orange (PO). Using density functional theory, we demonstrate the extent to which the mechanical and electronic properties of a hybrid system can be altered by the chemical modification of a given chromophore. The three systems exhibit distinct differences due to their chemical composition and van der Waals contact enabled by their geometry. All systems are structurally stable. The binding energies follow the order PD$$P$$PO due to the large $\pi$-system (PD) and strong structural distortion (PO). Youngs modulus and Poissons ratio exhibit pronounced anisotropy in all cases. PO exhibits the greatest anisotropy due to steric effects and a permanent dipole, which introduce directionality to the molecule-surface interaction. Physisorption is accompanied by net charge transfer in the same order as the binding energies. The associated interfacial polarization results in a change in the work function compared to pristine MoS$_2$ in the order P$$PO$$PD. Finally, the presence of organic molecules introduces states into the MoS$_2$ energy gap, with the band alignment being either type II (P, PO) or type I (PD).</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.21517v1" target="_blank">Modeling Wise Decision Making: A Z-Number Fuzzy Framework Inspired by Phronesis</a></h3>
                    <p><strong>Authors:</strong> Sweta Kaman, Ankita Sharma, Romi Banerjee</p>
                    <p><strong>Published:</strong> 8/29/2025</p>
                    <p><strong>Categories:</strong> cs.AI</p>
                    <p><strong>Summary:</strong> Background: Wisdom is a superordinate construct that embraces perspective taking, reflectiveness, prosocial orientation, reflective empathetic action, and intellectual humility. Unlike conventional models of reasoning that are rigidly bound by binary thinking, wisdom unfolds in shades of ambiguity, requiring both graded evaluation and self-reflective humility. Current measures depend on self-reports and seldom reflect the humility and uncertainty inherent in wise reasoning. A computational framework that takes into account both multidimensionality and confidence has the potential to improve psychological science and allow humane AI. Method: We present a fuzzy inference system with Z numbers, each of the decisions being expressed in terms of a wisdom score (restriction) and confidence score (certainty). As part of this study, participants (N = 100) were exposed to culturally neutral pictorial moral dilemma tasks to which they generated think-aloud linguistic responses, which were mapped into five theoretically based components of wisdom. The scores of each individual component were combined using a base of 21 rules, with membership functions tuned via Gaussian kernel density estimation. Results: In a proof of concept study, the system produced dual attribute wisdom representations that correlated modestly but significantly with established scales while showing negligible relations with unrelated traits, supporting convergent and divergent validity. Contribution: The contribution is to formalize wisdom as a multidimensional, uncertainty-conscious construct, operationalized in the form of Z-numbers. In addition to progressing measurement in psychology, it calculates how fuzzy Z numbers can provide AI systems with interpretable, confidence-sensitive reasoning that affords a safe, middle ground between rigorous computation and human-like judgment.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.21516v1" target="_blank">A Combined Push-Pull Access Framework for Digital Twin Alignment and Anomaly Reporting</a></h3>
                    <p><strong>Authors:</strong> Federico Chiariotti, Fabio Saggese, Andrea Munari, Leonardo Badia, Petar Popovski</p>
                    <p><strong>Published:</strong> 8/29/2025</p>
                    <p><strong>Categories:</strong> cs.NI, 94A05, C.2.1; C.2.5</p>
                    <p><strong>Summary:</strong> A digital twin (DT) contains a set of virtual models of real systems and processes that are synchronized to their physical counterparts. This enables experimentation and examination of counterfactuals, simulating the consequences of decisions in real time. However, the DT accuracy relies on timely updates that maintain alignment with the real system. We can distinguish between: (i) pull-updates, which follow a request from the DT to the sensors, to decrease its drift from the physical state; (ii) push-updates, which are sent directly by the sensors since they represent urgent information, such as anomalies. In this work, we devise a push-pull scheduler (PPS) medium access framework, which dynamically allocates the communication resources used for these two types of updates. Our scheme strikes a balance in the trade-off between DT alignment in normal conditions and anomaly reporting, optimizing resource usage and reducing the drift age of incorrect information (AoII) by over 20% with respect to state-of-the-art solutions, while maintaining the same anomaly detection guarantees, as well as reducing the worst-case anomaly detection AoII from 70 ms to 20 ms when considering a 1 ms average drift AoII constraint.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.21499v1" target="_blank">Gromov hyperbolicity II: Dimension-free inner uniform estimates for quasigeodesics</a></h3>
                    <p><strong>Authors:</strong> Chang-Yu Guo, Manzi Huang, Yaxiang Li, Xiantao Wang</p>
                    <p><strong>Published:</strong> 8/29/2025</p>
                    <p><strong>Categories:</strong> math.CV, math.MG</p>
                    <p><strong>Summary:</strong> This is the second article of a series of our recent works, addressing an open question of Bonk-Heinonen-Koskela [3], to study the relationship between (inner) uniformality and Gromov hyperbolicity in infinite dimensional spaces. Our main focus of this paper is to establish a dimension-free inner uniform estimate for quasigeodesics. More precisely, we prove that a $c_0$-quasigeodesic in a $\delta$-Gromov hyperbolic $c$-John domain in $\mathbb{R}^n$ is $b$-inner uniform, for some constant $b$ depending only on $c_0$, $\delta$ and $c$, but not on the dimension $n$. The proof relies crucially on the techniques introduced by Guo-Huang-Wang in their recent work [arXiv:2502.02930, 2025]. In particular, we actually show that the above result holds in general Banach spaces, which answers affirmatively an open question of J. V\ais\al\a in [Analysis, 2004] and partially addresses the open question of Bonk-Heinonen-Koskela in [Asterisque, 2001]. As a byproduct of our main result, we obtain that a $c_0$-quasigeodesic in a $\delta$-Gromov hyperbolic $c$-John domain in $\mathbb{R}^n$ is a $b$-cone arc with a dimension-free constant $b=b(c_0,\delta,c)$. This resolves an open problem of J. Heinonen in [Rev. Math. Iberoam., 1989].</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.21488v1" target="_blank">Priors Matter: Addressing Misspecification in Bayesian Deep Q-Learning</a></h3>
                    <p><strong>Authors:</strong> Pascal R. van der Vaart, Neil Yorke-Smith, Matthijs T. J. Spaan</p>
                    <p><strong>Published:</strong> 8/29/2025</p>
                    <p><strong>Categories:</strong> cs.LG, cs.AI</p>
                    <p><strong>Summary:</strong> Uncertainty quantification in reinforcement learning can greatly improve exploration and robustness. Approximate Bayesian approaches have recently been popularized to quantify uncertainty in model-free algorithms. However, so far the focus has been on improving the accuracy of the posterior approximation, instead of studying the accuracy of the prior and likelihood assumptions underlying the posterior. In this work, we demonstrate that there is a cold posterior effect in Bayesian deep Q-learning, where contrary to theory, performance increases when reducing the temperature of the posterior. To identify and overcome likely causes, we challenge common assumptions made on the likelihood and priors in Bayesian model-free algorithms. We empirically study prior distributions and show through statistical tests that the common Gaussian likelihood assumption is frequently violated. We argue that developing more suitable likelihoods and priors should be a key focus in future Bayesian reinforcement learning research and we offer simple, implementable solutions for better priors in deep Q-learning that lead to more performant Bayesian algorithms.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.21484v1" target="_blank">Data-driven Discovery of Digital Twins in Biomedical Research</a></h3>
                    <p><strong>Authors:</strong> ClÃ©mence MÃ©tayer, Annabelle Ballesta, Julien Martinelli</p>
                    <p><strong>Published:</strong> 8/29/2025</p>
                    <p><strong>Categories:</strong> q-bio.QM, cs.LG, stat.ML</p>
                    <p><strong>Summary:</strong> Recent technological advances have expanded the availability of high-throughput biological datasets, enabling the reliable design of digital twins of biomedical systems or patients. Such computational tools represent key reaction networks driving perturbation or drug response and can guide drug discovery and personalized therapeutics. Yet, their development still relies on laborious data integration by the human modeler, so that automated approaches are critically needed. The success of data-driven system discovery in Physics, rooted in clean datasets and well-defined governing laws, has fueled interest in applying similar techniques in Biology, which presents unique challenges. Here, we reviewed methodologies for automatically inferring digital twins from biological time series, which mostly involve symbolic or sparse regression. We evaluate algorithms according to eight biological and methodological challenges, associated to noisy/incomplete data, multiple conditions, prior knowledge integration, latent variables, high dimensionality, unobserved variable derivatives, candidate library design, and uncertainty quantification. Upon these criteria, sparse regression generally outperformed symbolic regression, particularly when using Bayesian frameworks. We further highlight the emerging role of deep learning and large language models, which enable innovative prior knowledge integration, though the reliability and consistency of such approaches must be improved. While no single method addresses all challenges, we argue that progress in learning digital twins will come from hybrid and modular frameworks combining chemical reaction network-based mechanistic grounding, Bayesian uncertainty quantification, and the generative and knowledge integration capacities of deep learning. To support their development, we further propose a benchmarking framework to evaluate methods across all challenges.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.21476v1" target="_blank">Igniting Creative Writing in Small Language Models: LLM-as-a-Judge versus Multi-Agent Refined Rewards</a></h3>
                    <p><strong>Authors:</strong> Xiaolong Wei, Bo Lu, Xingyu Zhang, Zhejun Zhao, Dongdong Shen, Long Xia, Dawei Yin</p>
                    <p><strong>Published:</strong> 8/29/2025</p>
                    <p><strong>Categories:</strong> cs.CL, cs.AI</p>
                    <p><strong>Summary:</strong> Large Language Models (LLMs) have demonstrated remarkable creative writing capabilities, yet their substantial computational demands hinder widespread use. Enhancing Small Language Models (SLMs) offers a promising alternative, but current methods like Supervised Fine-Tuning (SFT) struggle with novelty, and Reinforcement Learning from Human Feedback (RLHF) is costly. This paper explores two distinct AI-driven reward strategies within a Reinforcement Learning from AI Feedback (RLAIF) framework to ignite the creative writing of a 7B-parameter SLM, specifically for generating Chinese greetings. The first strategy employs a RM trained on high-quality preference data curated by a novel multi-agent rejection sampling framework designed for creative tasks. The second, more novel strategy utilizes a principle-guided LLM-as-a-Judge, whose reward function is optimized via an adversarial training scheme with a reflection mechanism, to directly provide reward signals. Comprehensive experiments reveal that while both approaches significantly enhance creative output over baselines, the principle-guided LLM-as-a-Judge demonstrably yields superior generation quality. Furthermore, it offers notable advantages in training efficiency and reduced dependency on human-annotated data, presenting a more scalable and effective path towards creative SLMs. Our automated evaluation methods also exhibit strong alignment with human judgments. Our code and data are publicly available at https://github.com/weixiaolong94-hub/Igniting-Creative-Writing-in-Small-Language-Models.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.21825v1" target="_blank">Standard Model Baryon Number Violation at Zero Temperature from Higgs Bubble Collisions</a></h3>
                    <p><strong>Authors:</strong> Nabeen Bhusal, Simone Blasi, Martina Cataldi, Aleksandr Chatrchyan, Marco Gorghetto, Geraldine Servant</p>
                    <p><strong>Published:</strong> 8/29/2025</p>
                    <p><strong>Categories:</strong> hep-ph</p>
                    <p><strong>Summary:</strong> We compute for the first time baryon number violation at zero temperature from Higgs bubble collisions and find that it can be of the same order as that from thermal sphalerons in the symmetric phase at electroweak temperatures. We study the dependence of the rate of Chern--Simons number transitions on the shape of the scalar potential and on the Lorentz factor of the bubble walls at collision via large-scale (3+1)D lattice simulations of the Higgs doublet and SU(2) gauge fields. We estimate the resulting baryon asymmetry assuming some CP-violating source activated by the Higgs-field variation during the phase transition.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.21824v1" target="_blank">DriveQA: Passing the Driving Knowledge Test</a></h3>
                    <p><strong>Authors:</strong> Maolin Wei, Wanzhou Liu, Eshed Ohn-Bar</p>
                    <p><strong>Published:</strong> 8/29/2025</p>
                    <p><strong>Categories:</strong> cs.CV</p>
                    <p><strong>Summary:</strong> If a Large Language Model (LLM) were to take a driving knowledge test today, would it pass? Beyond standard spatial and visual question-answering (QA) tasks on current autonomous driving benchmarks, driving knowledge tests require a complete understanding of all traffic rules, signage, and right-of-way principles. To pass this test, human drivers must discern various edge cases that rarely appear in real-world datasets. In this work, we present DriveQA, an extensive open-source text and vision-based benchmark that exhaustively covers traffic regulations and scenarios. Through our experiments using DriveQA, we show that (1) state-of-the-art LLMs and Multimodal LLMs (MLLMs) perform well on basic traffic rules but exhibit significant weaknesses in numerical reasoning and complex right-of-way scenarios, traffic sign variations, and spatial layouts, (2) fine-tuning on DriveQA improves accuracy across multiple categories, particularly in regulatory sign recognition and intersection decision-making, (3) controlled variations in DriveQA-V provide insights into model sensitivity to environmental factors such as lighting, perspective, distance, and weather conditions, and (4) pretraining on DriveQA enhances downstream driving task performance, leading to improved results on real-world datasets such as nuScenes and BDD, while also demonstrating that models can internalize text and synthetic traffic knowledge to generalize effectively across downstream QA tasks.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.21823v1" target="_blank">A new characterization of the holographic entropy cone</a></h3>
                    <p><strong>Authors:</strong> Guglielmo Grimaldi, Matthew Headrick, Veronika E. Hubeny</p>
                    <p><strong>Published:</strong> 8/29/2025</p>
                    <p><strong>Categories:</strong> hep-th</p>
                    <p><strong>Summary:</strong> Entanglement entropies computed using the holographic Ryu-Takayanagi formula are known to obey an infinite set of linear inequalities, which define the so-called RT entropy cone. The general structure of this cone, or equivalently the set of all valid inequalities, is unknown. It is also unknown whether those same inequalities are also obeyed by entropies computed using the covariant Hubeny-Rangamani-Takayanagi formula, although significant evidence has accumulated that they are. Using Markov states, we develop a test of this conjecture in a heretofore unexplored regime. The test reduces to checking that a given inequality obeys a certain majorization property, which is easy to evaluate. We find that the RT inequalities pass this test and, surprisingly, \emph{only} RT inequalities do so. Our results not only provide strong new evidence that the HRT and RT cones coincide, but also offer a completely new characterization of that cone.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.21817v1" target="_blank">An Introduction to Gravitational Wave Theory</a></h3>
                    <p><strong>Authors:</strong> Simone Speziale, DaniÃ¨le A. Steer</p>
                    <p><strong>Published:</strong> 8/29/2025</p>
                    <p><strong>Categories:</strong> gr-qc, astro-ph.CO, hep-th</p>
                    <p><strong>Summary:</strong> Introduction to the theoretical foundations of gravitational waves: from general relativity to detection and binary system waveforms. Lecture notes prepared for the MaNiTou summer school on gravitational waves. Draft chapter for the CNRS contemporary Encyclopaedia Sciences to be published by ISTE.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.21816v1" target="_blank">The Demon is in Ambiguity: Revisiting Situation Recognition with Single Positive Multi-Label Learning</a></h3>
                    <p><strong>Authors:</strong> Yiming Lin, Yuchen Niu, Shang Wang, Kaizhu Huang, Qiufeng Wang, Xiao-Bo Jin</p>
                    <p><strong>Published:</strong> 8/29/2025</p>
                    <p><strong>Categories:</strong> cs.CV, cs.AI</p>
                    <p><strong>Summary:</strong> Context recognition (SR) is a fundamental task in computer vision that aims to extract structured semantic summaries from images by identifying key events and their associated entities. Specifically, given an input image, the model must first classify the main visual events (verb classification), then identify the participating entities and their semantic roles (semantic role labeling), and finally localize these entities in the image (semantic role localization). Existing methods treat verb classification as a single-label problem, but we show through a comprehensive analysis that this formulation fails to address the inherent ambiguity in visual event recognition, as multiple verb categories may reasonably describe the same image. This paper makes three key contributions: First, we reveal through empirical analysis that verb classification is inherently a multi-label problem due to the ubiquitous semantic overlap between verb categories. Second, given the impracticality of fully annotating large-scale datasets with multiple labels, we propose to reformulate verb classification as a single positive multi-label learning (SPMLL) problem - a novel perspective in SR research. Third, we design a comprehensive multi-label evaluation benchmark for SR that is carefully designed to fairly evaluate model performance in a multi-label setting. To address the challenges of SPMLL, we futher develop the Graph Enhanced Verb Multilayer Perceptron (GE-VerbMLP), which combines graph neural networks to capture label correlations and adversarial training to optimize decision boundaries. Extensive experiments on real-world datasets show that our approach achieves more than 3\% MAP improvement while remaining competitive on traditional top-1 and top-5 accuracy metrics.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.21809v1" target="_blank">VoCap: Video Object Captioning and Segmentation from Any Prompt</a></h3>
                    <p><strong>Authors:</strong> Jasper Uijlings, Xingyi Zhou, Xiuye Gu, Arsha Nagrani, Anurag Arnab, Alireza Fathi, David Ross, Cordelia Schmid</p>
                    <p><strong>Published:</strong> 8/29/2025</p>
                    <p><strong>Categories:</strong> cs.CV</p>
                    <p><strong>Summary:</strong> Understanding objects in videos in terms of fine-grained localization masks and detailed semantic properties is a fundamental task in video understanding. In this paper, we propose VoCap, a flexible video model that consumes a video and a prompt of various modalities (text, box or mask), and produces a spatio-temporal masklet with a corresponding object-centric caption. As such our model addresses simultaneously the tasks of promptable video object segmentation, referring expression segmentation, and object captioning. Since obtaining data for this task is tedious and expensive, we propose to annotate an existing large-scale segmentation dataset (SAV) with pseudo object captions. We do so by preprocessing videos with their ground-truth masks to highlight the object of interest and feed this to a large Vision Language Model (VLM). For an unbiased evaluation, we collect manual annotations on the validation set. We call the resulting dataset SAV-Caption. We train our VoCap model at scale on a SAV-Caption together with a mix of other image and video datasets. Our model yields state-of-the-art results on referring expression video object segmentation, is competitive on semi-supervised video object segmentation, and establishes a benchmark for video object captioning. Our dataset will be made available at https://github.com/google-deepmind/vocap.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.21808v1" target="_blank">Unitary induced channels and Tsirelsons problem</a></h3>
                    <p><strong>Authors:</strong> MichaÅ‚ Banacki, PaweÅ‚ Horodecki</p>
                    <p><strong>Published:</strong> 8/29/2025</p>
                    <p><strong>Categories:</strong> quant-ph, math-ph, math.MP, 81R15</p>
                    <p><strong>Summary:</strong> Motivated by a recent progress concerning quantum commuting and quantum tensor models of composed systems we investigate a notion of (generalized) unitary induced quantum channel. Using properties of Brown algebras we provide an equivalent characterization of discussed families in both tensor and commuting paradigms. In particular, we provide an equivalent formulation of Tsirelsons conjecture (Connes embedding problem) in terms of considered paradigms based on protocols which do not require measurements performed on infinite-dimensional subsystems. As a result we show that there is a difference between quantum commuting and quantum tensor models for generalized unitary induced channels.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.21807v1" target="_blank">Epsilon-saturation for stable graphs and Littlestone classes</a></h3>
                    <p><strong>Authors:</strong> Maryanthe Malliaris, Olga Medrano MartÃ­n del Campo, Shay Moran</p>
                    <p><strong>Published:</strong> 8/29/2025</p>
                    <p><strong>Categories:</strong> math.LO, cs.DM, cs.LO, math.CO</p>
                    <p><strong>Summary:</strong> Any Littlestone class, or stable graph, has finite sets which function as ``virtual elements: these can be seen from the learning side as representing hypotheses which are expressible as weighted majority opinions of hypotheses in the class, and from the model-theoretic side as an approximate finitary version of realizing types. We introduce and study the epsilon-saturation of a Littlestone class, or stable graph, which is essentially the closure of the class under inductively adding all such virtual elements. We characterize this closure and prove that under reasonable choices of parameters, it remains Littlestone (or stable), though not always of the same Littlestone dimension. This highlights some surprising phenomena having to do with regimes of epsilon and the relation between Littlestone/stability and VC dimension.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.21803v1" target="_blank">Automated Clinical Problem Detection from SOAP Notes using a Collaborative Multi-Agent LLM Architecture</a></h3>
                    <p><strong>Authors:</strong> Yeawon Lee, Xiaoyang Wang, Christopher C. Yang</p>
                    <p><strong>Published:</strong> 8/29/2025</p>
                    <p><strong>Categories:</strong> cs.AI, cs.MA</p>
                    <p><strong>Summary:</strong> Accurate interpretation of clinical narratives is critical for patient care, but the complexity of these notes makes automation challenging. While Large Language Models (LLMs) show promise, single-model approaches can lack the robustness required for high-stakes clinical tasks. We introduce a collaborative multi-agent system (MAS) that models a clinical consultation team to address this gap. The system is tasked with identifying clinical problems by analyzing only the Subjective (S) and Objective (O) sections of SOAP notes, simulating the diagnostic reasoning process of synthesizing raw data into an assessment. A Manager agent orchestrates a dynamically assigned team of specialist agents who engage in a hierarchical, iterative debate to reach a consensus. We evaluated our MAS against a single-agent baseline on a curated dataset of 420 MIMIC-III notes. The dynamic multi-agent configuration demonstrated consistently improved performance in identifying congestive heart failure, acute kidney injury, and sepsis. Qualitative analysis of the agent debates reveals that this structure effectively surfaces and weighs conflicting evidence, though it can occasionally be susceptible to groupthink. By modeling a clinical teams reasoning process, our system offers a promising path toward more accurate, robust, and interpretable clinical decision support tools.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.21802v1" target="_blank">An Adaptive Real-Time Forecasting Framework for Cryogenic Fluid Management in Space Systems</a></h3>
                    <p><strong>Authors:</strong> Qiyun Cheng, Huihua Yang, Wei Ji</p>
                    <p><strong>Published:</strong> 8/29/2025</p>
                    <p><strong>Categories:</strong> physics.flu-dyn</p>
                    <p><strong>Summary:</strong> Accurate real-time forecasting of cryogenic tank behavior is essential for the safe and efficient operation of propulsion and storage systems in future deep-space missions. While cryogenic fluid management (CFM) systems increasingly require autonomous capabilities, conventional simulation methods remain hindered by high computational cost, model imperfections, and sensitivity to unanticipated boundary condition changes. To address these limitations, this study proposes an Adaptive Real-Time Forecasting Framework for Cryogenic Propellant Management in Space Systems, featuring a lightweight, non-intrusive method named ARCTIC (Adaptive Real-time Cryogenic Tank Inference and Correction). ARCTIC integrates real-time sensor data with precomputed nodal simulations through a data-driven correction layer that dynamically refines forecast accuracy without modifying the underlying model. Two updating mechanisms, auto-calibration and observation and correction, enable continuous adaptation to evolving system states and transient disturbances. The method is first assessed through synthetic scenarios representing self-pressurization, sloshing, and periodic operations, then validated using experimental data from NASAs Multipurpose Hydrogen Test Bed and K-Site facilities. Results demonstrate that ARCTIC significantly improves forecast accuracy under model imperfections, data noise, and boundary fluctuations, offering a robust real-time forecasting capability to support autonomous CFM operations. The frameworks compatibility with existing simulation tools and its low computational overhead make it especially suited for onboard implementation in space systems requiring predictive autonomy.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.21801v1" target="_blank">DMGIN: How Multimodal LLMs Enhance Large Recommendation Models for Lifelong User Post-click Behaviors</a></h3>
                    <p><strong>Authors:</strong> Zhuoxing Wei, Qingchen Xie, Qi Liu</p>
                    <p><strong>Published:</strong> 8/29/2025</p>
                    <p><strong>Categories:</strong> cs.IR</p>
                    <p><strong>Summary:</strong> Modeling user interest based on lifelong user behavior sequences is crucial for enhancing Click-Through Rate (CTR) prediction. However, long post-click behavior sequences themselves pose severe performance issues: the sheer volume of data leads to high computational costs and inefficiencies in model training and inference. Traditional methods address this by introducing two-stage approaches, but this compromises model effectiveness due to incomplete utilization of the full sequence context. More importantly, integrating multimodal embeddings into existing large recommendation models (LRM) presents significant challenges: These embeddings often exacerbate computational burdens and mismatch with LRM architectures. To address these issues and enhance the models efficiency and accuracy, we introduce Deep Multimodal Group Interest Network (DMGIN). Given the observation that user post-click behavior sequences contain a large number of repeated items with varying behaviors and timestamps, DMGIN employs Multimodal LLMs(MLLM) for grouping to reorganize complete lifelong post-click behavior sequences more effectively, with almost no additional computational overhead, as opposed to directly introducing multimodal embeddings. To mitigate the potential information loss from grouping, we have implemented two key strategies. First, we analyze behaviors within each group using both interest statistics and intra-group transformers to capture group traits. Second, apply inter-group transformers to temporally ordered groups to capture the evolution of user group interests. Our extensive experiments on both industrial and public datasets confirm the effectiveness and efficiency of DMGIN. The A/B test in our LBS advertising system shows that DMGIN improves CTR by 4.7% and Revenue per Mile by 2.3%.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.21798v1" target="_blank">1D Cluster State Generation On Superconducting Hardware</a></h3>
                    <p><strong>Authors:</strong> Rahul Dev Sharma, Md Sakibul Islam</p>
                    <p><strong>Published:</strong> 8/29/2025</p>
                    <p><strong>Categories:</strong> quant-ph, physics.app-ph</p>
                    <p><strong>Summary:</strong> Measurement-based Quantum Computation(MBQC) utilize entanglement as resource for performing quantum computation. Generating cluster state using entanglement as resource is a key bottleneck for the adoption of MBQC. To generate cluster state with charge-qubit arrrays, we provide analytical derivations and numerical validations for 4-qubit cluster state. We compare our fidelities under ideal (noise-free) Hamiltonian evolution and due to effect of decoherence. We show incorporating energy relaxation ($T_1$) yields $$90\% fidelity while pure dephasing $T_2$ show $70\%$ decays at fourth harmonics. We further show under noise $T_2$ decays to 50\% within 15 time units, versus $$70\% under relaxation time units ($T_1$)--only. This decay quantify degradation effect of $T_2$ on preparing cluster--state preparation is more than $T_1$. We highlight the critical need for targeted error-mitigation strategies in near-term MBQC implementations.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.21796v1" target="_blank">Road map for the tuning of hadronic interaction models with accelerator-based and astroparticle data</a></h3>
                    <p><strong>Authors:</strong> Johannes Albrecht, Julia Becker Tjus, Noah Behling, JiÅ™Ã­ BlaÅ¾ek, Marcus Bleicher, Julian Boelhauve, Lorenzo Cazon, Ruben ConceiÃ§Ã£o, Hans Dembinski, Luca Dietrich, Jan Ebr, Jan Ellbracht, Ralph Engel, Anatoli Fedynitch, Max Fieg, Maria Garzelli, ChloÃ© Gaudu, Giacomo Graziani, Pascal Gutjahr, Andreas Haungs, Tim Huege, Karolin Hymon, Karl-Heinz Kampert, Leonora Kardum, Lars Kolk, Natalia Korneeva, Kevin KrÃ¶ninger, Antonin Maire, Hiroaki Menjo, Leonel Morejon, Sergey Ostapchenko, Petja Paakkinen, Tanguy Pierog, Pavlo Plotko, Anton Prosekin, Lilly Pyras, Thomas PÃ¶schl, Maximilian Reininghaus, Wolfgang Rhode, Felix Riehn, Markus Roth, Alexander Sandrock, Ina Sarcevic, Michael Schmelling, GÃ¼nter Sigl, Torbjorn SjÃ¶strand, Dennis Soldin, Michael Unger, Marius Utheim, Jakub VÃ­cha, Klaus Werner, Michael Windau, Valery Zhukov</p>
                    <p><strong>Published:</strong> 8/29/2025</p>
                    <p><strong>Categories:</strong> astro-ph.HE, hep-ex, hep-ph</p>
                    <p><strong>Summary:</strong> In high-energy and astroparticle physics, event generators play an essential role, even in the simplest data analyses. As analysis techniques become more sophisticated, e.g. based on deep neural networks, their correct description of the observed event characteristics becomes even more important. Physical processes occurring in hadronic collisions are simulated within a Monte Carlo framework. A major challenge is the modeling of hadron dynamics at low momentum transfer, which includes the initial and final phases of every hadronic collision. Phenomenological models inspired by Quantum Chromodynamics used for these phases cannot guarantee completeness or correctness over the full phase space. These models usually include parameters which must be tuned to suitable experimental data. Until now, event generators have primarily been developed and tuned based on data from high-energy physics experiments at accelerators. However, in many cases they have been found to not satisfactorily describe data from astroparticle experiments, which provide sensitivity especially to hadrons produced nearly parallel to the collision axis and cover center-of-mass energies up to several hundred TeV, well beyond those reached at colliders so far. In this report, we address the complementarity of these two sets of data and present a road map for exploiting, for the first time, their complementarity by enabling a unified tuning of event generators with accelerator-based and astroparticle data.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.21795v1" target="_blank">TMUAD: Enhancing Logical Capabilities in Unified Anomaly Detection Models with a Text Memory Bank</a></h3>
                    <p><strong>Authors:</strong> Jiawei Liu, Jiahe Hou, Wei Wang, Jinsong Du, Yang Cong, Huijie Fan</p>
                    <p><strong>Published:</strong> 8/29/2025</p>
                    <p><strong>Categories:</strong> cs.CV, cs.AI</p>
                    <p><strong>Summary:</strong> Anomaly detection, which aims to identify anomalies deviating from normal patterns, is challenging due to the limited amount of normal data available. Unlike most existing unified methods that rely on carefully designed image feature extractors and memory banks to capture logical relationships between objects, we introduce a text memory bank to enhance the detection of logical anomalies. Specifically, we propose a Three-Memory framework for Unified structural and logical Anomaly Detection (TMUAD). First, we build a class-level text memory bank for logical anomaly detection by the proposed logic-aware text extractor, which can capture rich logical descriptions of objects from input images. Second, we construct an object-level image memory bank that preserves complete object contours by extracting features from segmented objects. Third, we employ visual encoders to extract patch-level image features for constructing a patch-level memory bank for structural anomaly detection. These three complementary memory banks are used to retrieve and compare normal images that are most similar to the query image, compute anomaly scores at multiple levels, and fuse them into a final anomaly score. By unifying structural and logical anomaly detection through collaborative memory banks, TMUAD achieves state-of-the-art performance across seven publicly available datasets involving industrial and medical domains. The model and code are available at https://github.com/SIA-IDE/TMUAD.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.21793v1" target="_blank">MoE-Health: A Mixture of Experts Framework for Robust Multimodal Healthcare Prediction</a></h3>
                    <p><strong>Authors:</strong> Xiaoyang Wang, Christopher C. Yang</p>
                    <p><strong>Published:</strong> 8/29/2025</p>
                    <p><strong>Categories:</strong> cs.LG, cs.AI</p>
                    <p><strong>Summary:</strong> Healthcare systems generate diverse multimodal data, including Electronic Health Records (EHR), clinical notes, and medical images. Effectively leveraging this data for clinical prediction is challenging, particularly as real-world samples often present with varied or incomplete modalities. Existing approaches typically require complete modality data or rely on manual selection strategies, limiting their applicability in real-world clinical settings where data availability varies across patients and institutions. To address these limitations, we propose MoE-Health, a novel Mixture of Experts framework designed for robust multimodal fusion in healthcare prediction. MoE-Health architecture is specifically developed to handle samples with differing modalities and improve performance on critical clinical tasks. By leveraging specialized expert networks and a dynamic gating mechanism, our approach dynamically selects and combines relevant experts based on available data modalities, enabling flexible adaptation to varying data availability scenarios. We evaluate MoE-Health on the MIMIC-IV dataset across three critical clinical prediction tasks: in-hospital mortality prediction, long length of stay, and hospital readmission prediction. Experimental results demonstrate that MoE-Health achieves superior performance compared to existing multimodal fusion methods while maintaining robustness across different modality availability patterns. The framework effectively integrates multimodal information, offering improved predictive performance and robustness in handling heterogeneous and incomplete healthcare data, making it particularly suitable for deployment in diverse healthcare environments with heterogeneous data availability.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.21792v1" target="_blank">Toward real-time optimization through model reduction and model discrepancy sensitivities</a></h3>
                    <p><strong>Authors:</strong> Joseph Hart, Shane A. McQuarrie, Zachary Morrow, Bart van Bloemen Waanders</p>
                    <p><strong>Published:</strong> 8/29/2025</p>
                    <p><strong>Categories:</strong> math.OC</p>
                    <p><strong>Summary:</strong> Optimization problems arise in a range of scenarios, from optimal control to model parameter estimation. In many applications, such as the development of digital twins, it is essential to solve these optimization problems within wall-clock-time limitations. However, this is often unattainable for complex systems, such as those modeled by nonlinear partial differential equations. One strategy for mitigating this issue is to construct a reduced-order model (ROM) that enables more rapid optimization. In particular, the use of nonintrusive ROMs -- those that do not require access to the full-order model at evaluation time -- is popular because they facilitate optimization solutions can be computed within the wall-clock-time requirements. However, the optimization solution will be unreliable if the iterates move outside the ROM training data. This article proposes the use of hyper-differential sensitivity analysis with respect to model discrepancy (HDSA-MD) as a computationally efficient tool to augment ROM-constrained optimization and improve its reliability. The proposed approach consists of two phases: (i) an offline phase where several full-order model evaluations are computed to train the ROM, and (ii) an online phase where a ROM-constrained optimization problem is solved, $N=\mathcal{O}(1)$ full-order model evaluations are computed, and HDSA-MD is used to enhance the optimization solution using the full-order model data. Numerical results are demonstrated for two examples, atmospheric contaminant control and wildfire ignition location estimation, in which a ROM is trained offline using inaccurate atmospheric data. The HDSA-MD update yields a significant improvement in the ROM-constrained optimization solution using only one full-order model evaluation online with corrected atmospheric data.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.21791v1" target="_blank">Quantum Geometry Induced KekulÃ© Superconductivity in Haldane phases</a></h3>
                    <p><strong>Authors:</strong> Yafis Barlas, Fan Zhang, Enrico Rossi</p>
                    <p><strong>Published:</strong> 8/29/2025</p>
                    <p><strong>Categories:</strong> cond-mat.supr-con, cond-mat.mes-hall</p>
                    <p><strong>Summary:</strong> Chiral two-dimensional electron gases, which capture the electronic properties of graphene and rhombohedral graphene systems, exhibit singular momentum-space vortices and are susceptible to interaction-induced topological Haldane phases. Here, we investigate pairing interactions in these inversion-symmetric Haldane phases of chiral two-dimensional electron gases. We demonstrate that the nontrivial band topology of the Haldane phases enhances intra-valley (${\bf Q} = \pm 2 {\bf K_D}$) pair susceptibility relative to inter-valley (${\bf Q} = 0$) pair susceptibility, favoring the emergence of a lattice-scale pair-density wave order. When longitudinal acoustic phonons mediate the pairing interaction, the system supports a chiral Kekul\`{e} superconducting order. Our findings are relevant to superconductivity in rhombohedral graphene and Kagome metals.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.21790v1" target="_blank">Experimental measurement of quantum-first-passage-time distributions</a></h3>
                    <p><strong>Authors:</strong> Joseph M. Ryan, Simon Gorbaty, Thomas J. Kessler, Mitchell G. Peaks, Stephen W. Teitsworth, Crystal Noel</p>
                    <p><strong>Published:</strong> 8/29/2025</p>
                    <p><strong>Categories:</strong> quant-ph, physics.data-an</p>
                    <p><strong>Summary:</strong> Classical First-Passage-Time Distributions (FPTDs) have been extensively studied both theoretically and experimentally. Their quantum counterparts, Quantum First-Passage-Time Distributions (QFPTDs), remain largely unexplored and have deep implications for both fundamental physics and the development of emerging quantum technologies. We measure the first QFPTDs using a motional mode of a single trapped ion. We develop a novel composite-phase laser pulse sequence to perform tunable stroboscopic projective measurements of the motional state of a trapped ion. We measure QFPTDs of the ion energy when coupled to electric-field noise and establish a clear connection with its classical counterpart. The measurement protocol developed here is broadly applicable to other quantum systems and provides a powerful method for exploring a broad range of QFPTD phenomena. With these results we open a new field of experimental investigations of QFPT processes with potential future relevance to quantum search algorithms, unraveling connections between classical and quantum dynamics, and study of the quantum measurement problem.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.21788v1" target="_blank">Going over Fine Web with a Fine-Tooth Comb: Technical Report of Indexing Fine Web for Problematic Content Search and Retrieval</a></h3>
                    <p><strong>Authors:</strong> InÃ©s Altemir Marinas, Anastasiia Kucherenko, Andrei Kucharavy</p>
                    <p><strong>Published:</strong> 8/29/2025</p>
                    <p><strong>Categories:</strong> cs.CL, cs.AI, cs.IR</p>
                    <p><strong>Summary:</strong> Large language models (LLMs) rely heavily on web-scale datasets like Common Crawl, which provides over 80\% of training data for some modern models. However, the indiscriminate nature of web crawling raises challenges in data quality, safety, and ethics. Despite the critical importance of training data quality, prior research on harmful content has been limited to small samples due to computational constraints. This project presents a framework for indexing and analyzing LLM training datasets using an ElasticSearch-based pipeline. We apply it to SwissAIs FineWeb-2 corpus (1.5TB, four languages), achieving fast query performance--most searches in milliseconds, all under 2 seconds. Our work demonstrates real-time dataset analysis, offering practical tools for safer, more accountable AI systems.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.21787v1" target="_blank">PiCSAR: Probabilistic Confidence Selection And Ranking</a></h3>
                    <p><strong>Authors:</strong> Joshua Ong Jun Leang, Zheng Zhao, Aryo Pradipta Gema, Sohee Yang, Wai-Chung Kwan, Xuanli He, Wenda Li, Pasquale Minervini, Eleonora Giunchiglia, Shay B. Cohen</p>
                    <p><strong>Published:</strong> 8/29/2025</p>
                    <p><strong>Categories:</strong> cs.CL, cs.AI</p>
                    <p><strong>Summary:</strong> Best-of-n sampling improves the accuracy of large language models (LLMs) and large reasoning models (LRMs) by generating multiple candidate solutions and selecting the one with the highest reward. The key challenge for reasoning tasks is designing a scoring function that can identify correct reasoning chains without access to ground-truth answers. We propose Probabilistic Confidence Selection And Ranking (PiCSAR): a simple, training-free method that scores each candidate generation using the joint log-likelihood of the reasoning and final answer. The joint log-likelihood of the reasoning and final answer naturally decomposes into reasoning confidence and answer confidence. PiCSAR achieves substantial gains across diverse benchmarks (+10.18 on MATH500, +9.81 on AIME2025), outperforming baselines with at least 2x fewer samples in 16 out of 20 comparisons. Our analysis reveals that correct reasoning chains exhibit significantly higher reasoning and answer confidence, justifying the effectiveness of PiCSAR.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.21785v1" target="_blank">Learning Unified Representations from Heterogeneous Data for Robust Heart Rate Modeling</a></h3>
                    <p><strong>Authors:</strong> Peng Yang, Zhengdong Huang, Zicheng Xie, Wentao Tian, Jingyu Liu, Lunhong Dong</p>
                    <p><strong>Published:</strong> 8/29/2025</p>
                    <p><strong>Categories:</strong> cs.LG, cs.CV</p>
                    <p><strong>Summary:</strong> Heart rate prediction is vital for personalized health monitoring and fitness, while it frequently faces a critical challenge when deploying in real-world: data heterogeneity. We classify it in two key dimensions: source heterogeneity from fragmented device markets with varying feature sets, and user heterogeneity reflecting distinct physiological patterns across individuals and activities. Existing methods either discard device-specific information, or fail to model user-specific differences, limiting their real-world performance. To address this, we propose a framework that learns latent representations agnostic to both heterogeneity, enabling downstream predictors to work consistently under heterogeneous data patterns. Specifically, we introduce a random feature dropout strategy to handle source heterogeneity, making the model robust to various feature sets. To manage user heterogeneity, we employ a time-aware attention module to capture long-term physiological traits and use a contrastive learning objective to build a discriminative representation space. To reflect the heterogeneous nature of real-world data, we created and publicly released a new benchmark dataset, ParroTao. Evaluations on both ParroTao and the public FitRec dataset show that our model significantly outperforms existing baselines by 17% and 15%, respectively. Furthermore, analysis of the learned representations demonstrates their strong discriminative power, and one downstream application task confirm the practical value of our model.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.21784v1" target="_blank">Non-Markovian dynamics of giant emitters beyond the Weisskopf-Wigner approximation</a></h3>
                    <p><strong>Authors:</strong> Carlos A. GonzÃ¡lez-GutiÃ©rrez</p>
                    <p><strong>Published:</strong> 8/29/2025</p>
                    <p><strong>Categories:</strong> quant-ph</p>
                    <p><strong>Summary:</strong> Giant quantum emitters, whose effective size is comparable to the wavelength of the radiation they couple to, give rise to interference effects and non-Markovian dynamics that lie beyond the scope of the standard Weisskopf-Wigner approximation. Here we study minimal models of giant emitters coupled to cavit$-array waveguides and identify symmetric configurations where the dynamics can be solved exactly. This allows us to capture the emergence of bound states both inside and outside the photonic continuum, and to demonstrate the possibility of engineering their coherent superpositions. Our results provide analytical insight into non-Markovian light-matter interaction and suggest a feasible implementation using superconducting circuit platforms.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.21779v1" target="_blank">Quantum Phase Sensitivity with Generalized Coherent States Based on Deformed su(1,1) and Heisenberg Algebras</a></h3>
                    <p><strong>Authors:</strong> N. E. Abouelkhir, A. Slaoui, R. Ahl Laamara</p>
                    <p><strong>Published:</strong> 8/29/2025</p>
                    <p><strong>Categories:</strong> quant-ph, math-ph, math.MP</p>
                    <p><strong>Summary:</strong> We investigate the phase sensitivity of a Mach Zehnder interferometer using a special class of generalized coherent states constructed from generalized Heisenberg and deformed su(1,1) algebras. These states, derived from a perturbed harmonic oscillator with a four parameter deformed spectrum, provide enhanced tunability and nonclassical features. The quantum Fisher information and its associated quantum Cram\er-Rao bound are used to define the fundamental precision limits in phase estimation. We analyze the phase sensitivity under three realistic detection methods: difference intensity detection, single mode intensity detection, and balanced homodyne detection. The performance of each method is compared with the quantum Cram\er Rao bound to evaluate their optimality. Our results demonstrate that, for suitable parameter regimes, these generalized coherent states enable phase sensitivities approaching the quantum limit, offering a flexible framework for precision quantum metrology.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.21778v1" target="_blank">Magnetic soliton molecules in binary condensates</a></h3>
                    <p><strong>Authors:</strong> R. M. V. RÃ¶hrs, Chunlei Qu, R. N. Bisset</p>
                    <p><strong>Published:</strong> 8/29/2025</p>
                    <p><strong>Categories:</strong> cond-mat.quant-gas, nlin.PS</p>
                    <p><strong>Summary:</strong> Two-component Bose-Einstein condensates in the miscible phase can support polarization solitary waves, known as magnetic solitons. By calculating the interaction potential between two magnetic solitons, we elucidate the mechanisms and conditions for the formation of bound states -- or molecules -- and support these predictions with dynamical simulations. We analytically determine the dissociation energy of bound states consisting of two oppositely polarized solitons and find good agreement with full numerical simulations. Collisions between bound states -- either with other bound states or with individual solitons -- produce intriguing dynamics. Notably, collisions between a pair of bound states exhibit a dipole-like behavior. We anticipate that such bound states, along with their rich collision dynamics, are within reach of current experimental capabilities.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.21777v1" target="_blank">Benchmarking GPT-5 in Radiation Oncology: Measurable Gains, but Persistent Need for Expert Oversight</a></h3>
                    <p><strong>Authors:</strong> Ugur Dinc, Jibak Sarkar, Philipp Schubert, Sabine Semrau, Thomas Weissmann, Andre Karius, Johann Brand, Bernd-Niklas Axer, Ahmed Gomaa, Pluvio Stephan, Ishita Sheth, Sogand Beirami, Annette Schwarz, Udo Gaipl, Benjamin Frey, Christoph Bert, Stefanie Corradini, Rainer Fietkau, Florian Putz</p>
                    <p><strong>Published:</strong> 8/29/2025</p>
                    <p><strong>Categories:</strong> cs.CV, cs.AI, cs.LG</p>
                    <p><strong>Summary:</strong> Introduction: Large language models (LLM) have shown great potential in clinical decision support. GPT-5 is a novel LLM system that has been specifically marketed towards oncology use. Methods: Performance was assessed using two complementary benchmarks: (i) the ACR Radiation Oncology In-Training Examination (TXIT, 2021), comprising 300 multiple-choice items, and (ii) a curated set of 60 authentic radiation oncologic vignettes representing diverse disease sites and treatment indications. For the vignette evaluation, GPT-5 was instructed to generate concise therapeutic plans. Four board-certified radiation oncologists rated correctness, comprehensiveness, and hallucinations. Inter-rater reliability was quantified using Fleiss \k{appa}. Results: On the TXIT benchmark, GPT-5 achieved a mean accuracy of 92.8%, outperforming GPT-4 (78.8%) and GPT-3.5 (62.1%). Domain-specific gains were most pronounced in Dose and Diagnosis. In the vignette evaluation, GPT-5s treatment recommendations were rated highly for correctness (mean 3.24/4, 95% CI: 3.11-3.38) and comprehensiveness (3.59/4, 95% CI: 3.49-3.69). Hallucinations were rare with no case reaching majority consensus for their presence. Inter-rater agreement was low (Fleiss \k{appa} 0.083 for correctness), reflecting inherent variability in clinical judgment. Errors clustered in complex scenarios requiring precise trial knowledge or detailed clinical adaptation. Discussion: GPT-5 clearly outperformed prior model variants on the radiation oncology multiple-choice benchmark. Although GPT-5 exhibited favorable performance in generating real-world radiation oncology treatment recommendations, correctness ratings indicate room for further improvement. While hallucinations were infrequent, the presence of substantive errors underscores that GPT-5-generated recommendations require rigorous expert oversight before clinical implementation.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.21775v1" target="_blank">A Multi-Stage Fine-Tuning and Ensembling Strategy for Pancreatic Tumor Segmentation in Diagnostic and Therapeutic MRI</a></h3>
                    <p><strong>Authors:</strong> Omer Faruk Durugol, Maximilian Rokuss, Yannick Kirchhoff, Klaus H. Maier-Hein</p>
                    <p><strong>Published:</strong> 8/29/2025</p>
                    <p><strong>Categories:</strong> cs.CV</p>
                    <p><strong>Summary:</strong> Automated segmentation of Pancreatic Ductal Adenocarcinoma (PDAC) from MRI is critical for clinical workflows but is hindered by poor tumor-tissue contrast and a scarcity of annotated data. This paper details our submission to the PANTHER challenge, addressing both diagnostic T1-weighted (Task 1) and therapeutic T2-weighted (Task 2) segmentation. Our approach is built upon the nnU-Net framework and leverages a deep, multi-stage cascaded pre-training strategy, starting from a general anatomical foundation model and sequentially fine-tuning on CT pancreatic lesion datasets and the target MRI modalities. Through extensive five-fold cross-validation, we systematically evaluated data augmentation schemes and training schedules. Our analysis revealed a critical trade-off, where aggressive data augmentation produced the highest volumetric accuracy, while default augmentations yielded superior boundary precision (achieving a state-of-the-art MASD of 5.46 mm and HD95 of 17.33 mm for Task 1). For our final submission, we exploited this finding by constructing custom, heterogeneous ensembles of specialist models, essentially creating a mix of experts. This metric-aware ensembling strategy proved highly effective, achieving a top cross-validation Tumor Dice score of 0.661 for Task 1 and 0.523 for Task 2. Our work presents a robust methodology for developing specialized, high-performance models in the context of limited data and complex medical imaging tasks (Team MIC-DKFZ).</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.21773v1" target="_blank">Unsupervised Video Continual Learning via Non-Parametric Deep Embedded Clustering</a></h3>
                    <p><strong>Authors:</strong> Nattapong Kurpukdee, Adrian G. Bors</p>
                    <p><strong>Published:</strong> 8/29/2025</p>
                    <p><strong>Categories:</strong> cs.CV, cs.AI, cs.LG</p>
                    <p><strong>Summary:</strong> We propose a realistic scenario for the unsupervised video learning where neither task boundaries nor labels are provided when learning a succession of tasks. We also provide a non-parametric learning solution for the under-explored problem of unsupervised video continual learning. Videos represent a complex and rich spatio-temporal media information, widely used in many applications, but which have not been sufficiently explored in unsupervised continual learning. Prior studies have only focused on supervised continual learning, relying on the knowledge of labels and task boundaries, while having labeled data is costly and not practical. To address this gap, we study the unsupervised video continual learning (uVCL). uVCL raises more challenges due to the additional computational and memory requirements of processing videos when compared to images. We introduce a general benchmark experimental protocol for uVCL by considering the learning of unstructured video data categories during each task. We propose to use the Kernel Density Estimation (KDE) of deep embedded video features extracted by unsupervised video transformer networks as a non-parametric probabilistic representation of the data. We introduce a novelty detection criterion for the incoming new task data, dynamically enabling the expansion of memory clusters, aiming to capture new knowledge when learning a succession of tasks. We leverage the use of transfer learning from the previous tasks as an initial state for the knowledge transfer to the current learning task. We found that the proposed methodology substantially enhances the performance of the model when successively learning many tasks. We perform in-depth evaluations on three standard video action recognition datasets, including UCF101, HMDB51, and Something-to-Something V2, without using any labels or class boundaries.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.21770v1" target="_blank">What Can We Learn from Harry Potter? An Exploratory Study of Visual Representation Learning from Atypical Videos</a></h3>
                    <p><strong>Authors:</strong> Qiyue Sun, Qiming Huang, Yang Yang, Hongjun Wang, Jianbo Jiao</p>
                    <p><strong>Published:</strong> 8/29/2025</p>
                    <p><strong>Categories:</strong> cs.CV</p>
                    <p><strong>Summary:</strong> Humans usually show exceptional generalisation and discovery ability in the open world, when being shown uncommon new concepts. Whereas most existing studies in the literature focus on common typical data from closed sets, open-world novel discovery is under-explored in videos. In this paper, we are interested in asking: \textit{What if atypical unusual videos are exposed in the learning process?} To this end, we collect a new video dataset consisting of various types of unusual atypical data (\eg sci-fi, animation, \etc). To study how such atypical data may benefit open-world learning, we feed them into the model training process for representation learning. Focusing on three key tasks in open-world learning: out-of-distribution (OOD) detection, novel category discovery (NCD), and zero-shot action recognition (ZSAR), we found that even straightforward learning approaches with atypical data consistently improve performance across various settings. Furthermore, we found that increasing the categorical diversity of the atypical samples further boosts OOD detection performance. Additionally, in the NCD task, using a smaller yet more semantically diverse set of atypical samples leads to better performance compared to using a larger but more typical dataset. In the ZSAR setting, the semantic diversity of atypical videos helps the model generalise better to unseen action classes. These observations in our extensive experimental evaluations reveal the benefits of atypical videos for visual representation learning in the open world, together with the newly proposed dataset, encouraging further studies in this direction.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.21769v1" target="_blank">Domain Generalization in-the-Wild: Disentangling Classification from Domain-Aware Representations</a></h3>
                    <p><strong>Authors:</strong> Ha Min Son, Zhe Zhao, Shahbaz Rezaei, Xin Liu</p>
                    <p><strong>Published:</strong> 8/29/2025</p>
                    <p><strong>Categories:</strong> cs.CV, cs.LG</p>
                    <p><strong>Summary:</strong> Evaluating domain generalization (DG) for foundational models like CLIP is challenging, as web-scale pretraining data potentially covers many existing benchmarks. Consequently, current DG evaluation may neither be sufficiently challenging nor adequately test genuinely unseen data scenarios. To better assess the performance of CLIP on DG in-the-wild, a scenario where CLIP encounters challenging unseen data, we consider two approaches: (1) evaluating on 33 diverse datasets with quantified out-of-distribution (OOD) scores after fine-tuning CLIP on ImageNet, and (2) using unlearning to make CLIP `forget some domains as an approximation. We observe that CLIPs performance deteriorates significantly on more OOD datasets. To address this, we present CLIP-DCA (Disentangling Classification from enhanced domain Aware representations). Our approach is motivated by the observation that while standard domain invariance losses aim to make representations domain-invariant, this can be harmful to foundation models by forcing the discarding of domain-aware representations beneficial for generalization. We instead hypothesize that enhancing domain awareness is a prerequisite for effective domain-invariant classification in foundation models. CLIP-DCA identifies and enhances domain awareness within CLIPs encoders using a separate domain head and synthetically generated diverse domain data. Simultaneously, it encourages domain-invariant classification through disentanglement from the domain features. CLIP-DCA shows significant improvements within this challenging evaluation compared to existing methods, particularly on datasets that are more OOD.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.21768v1" target="_blank">A Hybrid Anyon-Otto thermal machine</a></h3>
                    <p><strong>Authors:</strong> Mohit Lal Bera, Armando PÃ©rez, Miguel A. GarcÃ­a-March, Ravindra Chhajlany, Tobias Grass, Maciej Lewenstein, Utso Bhattacharya, Sourav Bhattacharjee</p>
                    <p><strong>Published:</strong> 8/29/2025</p>
                    <p><strong>Categories:</strong> cond-mat.str-el, cond-mat.stat-mech, quant-ph</p>
                    <p><strong>Summary:</strong> We propose a four-stroke quantum thermal machine based on the 1D anyon Hubbard model, which is capable of extracting the excess energy arising from anyon exclusion statistics at low temperature into finite work. Defining a hybrid anyon-Otto (HAO) cycle, we find that the low temperature work, in the absence of any interactions, is maximised in the pseudo fermionic limit, where the anyons most closely resemble free fermions. However, when weak interactions are introduced, the work output is no longer maximized at the bosonic or pseudo-fermionic extremes, but instead peaks at intermediate statistical angles. This clearly demonstrates that interactions and anyonic statistics conspire non-trivially to enhance performance, with interacting anyons offering greater quantum thermodynamic advantage than either bosons or pseudo-fermions, in this regime. Furthermore, we also identify different modes of operation of the HAO cycle, one of which emerges as a direct consequence of the finite anyon energy at low temperature.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.21767v1" target="_blank">UItron: Foundational GUI Agent with Advanced Perception and Planning</a></h3>
                    <p><strong>Authors:</strong> Zhixiong Zeng, Jing Huang, Liming Zheng, Wenkang Han, Yufeng Zhong, Lei Chen, Longrong Yang, Yingjie Chu, Yuzhi He, Lin Ma</p>
                    <p><strong>Published:</strong> 8/29/2025</p>
                    <p><strong>Categories:</strong> cs.CV</p>
                    <p><strong>Summary:</strong> GUI agent aims to enable automated operations on Mobile/PC devices, which is an important task toward achieving artificial general intelligence. The rapid advancement of VLMs accelerates the development of GUI agents, owing to their powerful capabilities in visual understanding and task planning. However, building a GUI agent remains a challenging task due to the scarcity of operation trajectories, the availability of interactive infrastructure, and the limitation of initial capabilities in foundation models. In this work, we introduce UItron, an open-source foundational model for automatic GUI agents, featuring advanced GUI perception, grounding, and planning capabilities. UItron highlights the necessity of systemic data engineering and interactive infrastructure as foundational components for advancing GUI agent development. It not only systematically studies a series of data engineering strategies to enhance training effects, but also establishes an interactive environment connecting both Mobile and PC devices. In training, UItron adopts supervised finetuning over perception and planning tasks in various GUI scenarios, and then develop a curriculum reinforcement learning framework to enable complex reasoning and exploration for online environments. As a result, UItron achieves superior performance in benchmarks of GUI perception, grounding, and planning. In particular, UItron highlights the interaction proficiency with top-tier Chinese mobile APPs, as we identified a general lack of Chinese capabilities even in state-of-the-art solutions. To this end, we manually collect over one million steps of operation trajectories across the top 100 most popular apps, and build the offline and online agent evaluation environments. Experimental results demonstrate that UItron achieves significant progress in Chinese app scenarios, propelling GUI agents one step closer to real-world application.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.21763v1" target="_blank">On the Implementation Security of Twin-Field Quantum Key Distribution using Optical Injection Locking</a></h3>
                    <p><strong>Authors:</strong> Sergio JuÃ¡rez, Alessandro Marcomini, Mikhail Petrov, Robert I. Woodward, Toby J. Dowling, R. Mark Stevenson, Marcos Curty, Davide Rusca</p>
                    <p><strong>Published:</strong> 8/29/2025</p>
                    <p><strong>Categories:</strong> quant-ph</p>
                    <p><strong>Summary:</strong> Twin-Field Quantum Key Distribution (TF-QKD) has emerged as a leading quantum communication protocol, enabling secure key distribution over unprecedented distances by utilising coherent interference of quantum states. Optical Injection Locking (OIL) architectures have been used to simplify the precise phase and frequency stabilisation required by TF-QKD. In this work, we systematically analyse potential side-channels in OIL-based TF-QKD that can be introduced through the various optical degrees of freedom of the externally injected reference laser. We experimentally demonstrate two realistic attack scenarios: rapid intensity modulation of the reference laser and Trojan-horse signals exploiting wavelengths undetectable by conventional monitoring techniques. To counter these vulnerabilities, we propose straightforward and highly effective countermeasures including high-speed photodiodes for real-time power monitoring and targeted spectral filtering to detect and suppress out-of-band signals. Our experimental results confirm that these practical solutions substantially reinforce the security of TF-QKD systems without significant additional complexity or performance degradation. More broadly, our analysis highlights the critical importance of comprehensive optical monitoring to ensure the robust practical deployment of TF-QKD in real-world quantum communication networks.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.21762v1" target="_blank">Reasoning-Intensive Regression</a></h3>
                    <p><strong>Authors:</strong> Diane Tchuindjo, Omar Khattab</p>
                    <p><strong>Published:</strong> 8/29/2025</p>
                    <p><strong>Categories:</strong> cs.CL, cs.AI</p>
                    <p><strong>Summary:</strong> AI researchers and practitioners increasingly apply large language models (LLMs) to what we call reasoning-intensive regression (RiR), i.e. deducing subtle numerical properties from text. Unlike standard language regression tasks, e.g. for sentiment or similarity, RiR often appears instead in ad-hoc problems like rubric-based scoring or domain-specific retrieval, where much deeper analysis of text is required while only limited task-specific training data and computation are available. We cast three realistic problems as RiR tasks to establish an initial benchmark, and use that to test our hypothesis that prompting frozen LLMs and finetuning Transformer encoders via gradient descent will both often struggle in RiR. We then propose MENTAT, a simple and lightweight method that combines batch-reflective prompt optimization with neural ensemble learning. MENTAT achieves up to 65% improvement over both baselines, though substantial room remains for future advances in RiR.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.21761v1" target="_blank">Learning from Silence and Noise for Visual Sound Source Localization</a></h3>
                    <p><strong>Authors:</strong> Xavier Juanola, Giovana Morais, Magdalena Fuentes, Gloria Haro</p>
                    <p><strong>Published:</strong> 8/29/2025</p>
                    <p><strong>Categories:</strong> cs.CV, cs.MM</p>
                    <p><strong>Summary:</strong> Visual sound source localization is a fundamental perception task that aims to detect the location of sounding sources in a video given its audio. Despite recent progress, we identify two shortcomings in current methods: 1) most approaches perform poorly in cases with low audio-visual semantic correspondence such as silence, noise, and offscreen sounds, i.e. in the presence of negative audio; and 2) most prior evaluations are limited to positive cases, where both datasets and metrics convey scenarios with a single visible sound source in the scene. To address this, we introduce three key contributions. First, we propose a new training strategy that incorporates silence and noise, which improves performance in positive cases, while being more robust against negative sounds. Our resulting self-supervised model, SSL-SaN, achieves state-of-the-art performance compared to other self-supervised models, both in sound localization and cross-modal retrieval. Second, we propose a new metric that quantifies the trade-off between alignment and separability of auditory and visual features across positive and negative audio-visual pairs. Third, we present IS3+, an extended and improved version of the IS3 synthetic dataset with negative audio. Our data, metrics and code are available on the https://xavijuanola.github.io/SSL-SaN/.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.21759v1" target="_blank">Universal relation between residual resistivity and A coefficient in correlated metals</a></h3>
                    <p><strong>Authors:</strong> Anna Yu. Efimova, Yohei Saito, Atsushi Kawamoto, Martin Dressel, Louk Rademaker, Andrej Pustogow</p>
                    <p><strong>Published:</strong> 8/29/2025</p>
                    <p><strong>Categories:</strong> cond-mat.str-el, cond-mat.mtrl-sci</p>
                    <p><strong>Summary:</strong> The effects of strong electronic correlations and disorder are crucial for emergent phenomena such as unconventional superconductivity, metal-insulator transitions, and quantum criticality. While both are omnipresent in real materials, their individual impacts on charge transport remain elusive. To disentangle their respective roles, we have independently varied the degree of randomness and the strength of electronic correlations -- by chemical substitution and physical pressure, respectively -- within the metallic phase nearby a Mott-insulating state. We find a distinct correlation dependence of the disorder-dependent residual resistivity $\rho_0$ in the Fermi-liquid regime $\rho(T)=\rho_0 + A T^2$, where $A\propto (m^{\star}/m)^2$ quantifies the electronic mass enhancement. Contrary to conventional expectations, we observe that at fixed disorder level $\rho_0$ grows linearly with $A$. This scaling can be understood in terms of chemical-potential fluctuations with variance $\sigma_\mu^2$, yielding $\rho_0 \propto A\,\sigma_\mu^2$. By comparing our findings to transport data on other organic Mott systems, oxides, heavy-fermion compounds, and moir\e materials, we demonstrate that this new relation between residual resistivity and mass enhancement is a universal feature of correlated metals.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.21756v1" target="_blank">Diagrammatic Reasoning with Control as a Constructor, Applications to Quantum Circuits</a></h3>
                    <p><strong>Authors:</strong> NoÃ© Delorme, Simon Perdrix</p>
                    <p><strong>Published:</strong> 8/29/2025</p>
                    <p><strong>Categories:</strong> quant-ph</p>
                    <p><strong>Summary:</strong> Control is a predominant concept in quantum and reversible computational models. It allows to apply or not a transformation on a system, depending on the state of another system. We introduce a general framework for diagrammatic reasoning featuring control as a constructor. To do so, we provide an elementary axiomatisation of control functors, extending the standard formalism of props (symmetric monoidal categories with the natural numbers as objects) to controlled props. As an application, we show that controlled props ease diagrammatic reasoning for quantum circuits by allowing a simple complete set of relations that only involves relations acting on at most three qubits, whereas it is known that in the standard prop setting any complete axiomatisation requires relations acting on arbitrarily many qubits.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.21753v1" target="_blank">Sequential Fair Allocation With Replenishments: A Little Envy Goes An Exponentially Long Way</a></h3>
                    <p><strong>Authors:</strong> Chido Onyeze, Sean R. Sinclair, Chamsi Hssaine, Siddhartha Banerjee</p>
                    <p><strong>Published:</strong> 8/29/2025</p>
                    <p><strong>Categories:</strong> math.OC, cs.GT, math.PR</p>
                    <p><strong>Summary:</strong> We study the trade-off between envy and inefficiency in repeated resource allocation settings with stochastic replenishments, motivated by real-world systems such as food banks and medical supply chains. Specifically, we consider a model in which a decision-maker faced with stochastic demand and resource donations must trade off between an equitable and efficient allocation of resources over an infinite horizon. The decision-maker has access to storage with fixed capacity $M$, and incurs efficiency losses when storage is empty (stockouts) or full (overflows). We provide a nearly tight (up to constant factors) characterization of achievable envy-inefficiency pairs. Namely, we introduce a class of Bang-Bang control policies whose inefficiency exhibits a sharp phase transition, dropping from $\Theta(1/M)$ when $\Delta = 0$ to $e^{-\Omega(\Delta M)}$ when $\Delta  0$, where $\Delta$ is used to denote the target envy of the policy. We complement this with matching lower bounds, demonstrating that the trade-off is driven by supply, as opposed to demand uncertainty. Our results demonstrate that envy-inefficiency trade-offs not only persist in settings with dynamic replenishment, but are shaped by the decision-makers available capacity, and are therefore qualitatively different compared to previously studied settings with fixed supply.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.21746v1" target="_blank">The effect of the gravitational constant variation on the phase of gravitational waves</a></h3>
                    <p><strong>Authors:</strong> Jiachen An, Bing Sun, Zhoujian Cao</p>
                    <p><strong>Published:</strong> 8/29/2025</p>
                    <p><strong>Categories:</strong> gr-qc</p>
                    <p><strong>Summary:</strong> We have previously investigated the effect of the gravitational constant variation on the gravitational wave propagation. Pure theoretical analysis indicates that the leading order effect of the gravitational constant variation corrects the amplitude of gravitational waves, and the second order effect corrects the phase of gravitational waves. As the matched filtering technique is used by gravitational wave data analysis, the phase is more important than the amplitude. In the current paper we use LIGO-VIRGO-KAGRA data to constrain the gravitational constant variation. Our findings indicate that we need to wait until the distance of the detected gravitational wave events and/or the signal-to-noise ratio increases by $2n$ orders compared to the current detection state, and then we can use phase correction to get constraint $|\frac{G}{G}|10^{-n}$/yr for gravitational wave events without electromagnetic counterparts. For gravitational wave events with electromagnetic counterparts which provide the information of sources luminosity distance, the phase correction can almost always be neglected to constrain $\frac{G}{G}$.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.21743v1" target="_blank">Topological Magnon Frequency Combs</a></h3>
                    <p><strong>Authors:</strong> Zhixiong Li, Xuejuan Liu, Zhejunyu Jin, Guanghua Guo, Xingen Zheng, Peng Yan</p>
                    <p><strong>Published:</strong> 8/29/2025</p>
                    <p><strong>Categories:</strong> cond-mat.mes-hall</p>
                    <p><strong>Summary:</strong> Exploring the synergy between topological physics and nonlinear dynamics unveils profound insights into emergent states of matter. Inspired by recent experimental demonstrations of topological frequency combs in photonics, we theoretically introduce topological magnon frequency combs (MFCs) in a two-dimensional triangular skyrmion lattice. Computing the Chern numbers of magnon bands reveals robust chiral edge states. Strikingly, these topological MFCs originate from nonlinear four-magnon scattering among the chiral edge modes, activated by dual-frequency driving without an amplitude threshold. Comb spacings are readily tunable through excitation frequency detuning. Micromagnetic simulations validate our predictions with good concordance. This work paves the way for defect-immune magnonic devices exploiting MFCs and sparks investigations into topological-nonlinear phenomena in magnetic systems.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.21741v1" target="_blank">Not All Parameters Are Created Equal: Smart Isolation Boosts Fine-Tuning Performance</a></h3>
                    <p><strong>Authors:</strong> Yao Wang, Di Liang, Minlong Peng</p>
                    <p><strong>Published:</strong> 8/29/2025</p>
                    <p><strong>Categories:</strong> cs.CL</p>
                    <p><strong>Summary:</strong> Supervised fine-tuning (SFT) is a pivotal approach to adapting large language models (LLMs) for downstream tasks; however, performance often suffers from the ``seesaw phenomenon, where indiscriminate parameter updates yield progress on certain tasks at the expense of others. To address this challenge, we propose a novel \emph{Core Parameter Isolation Fine-Tuning} (CPI-FT) framework. Specifically, we first independently fine-tune the LLM on each task to identify its core parameter regions by quantifying parameter update magnitudes. Tasks with similar core regions are then grouped based on region overlap, forming clusters for joint modeling. We further introduce a parameter fusion technique: for each task, core parameters from its individually fine-tuned model are directly transplanted into a unified backbone, while non-core parameters from different tasks are smoothly integrated via Spherical Linear Interpolation (SLERP), mitigating destructive interference. A lightweight, pipelined SFT training phase using mixed-task data is subsequently employed, while freezing core regions from prior tasks to prevent catastrophic forgetting. Extensive experiments on multiple public benchmarks demonstrate that our approach significantly alleviates task interference and forgetting, consistently outperforming vanilla multi-task and multi-stage fine-tuning baselines.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.21740v1" target="_blank">Operational Validation of Large-Language-Model Agent Social Simulation: Evidence from Voat v/technology</a></h3>
                    <p><strong>Authors:</strong> Aleksandar TomaÅ¡eviÄ‡, Darja CvetkoviÄ‡, Sara Major, Slobodan MaletiÄ‡, Miroslav AnÄ‘elkoviÄ‡, Ana VraniÄ‡, Boris Stupovski, DuÅ¡an VudragoviÄ‡, Aleksandar BogojeviÄ‡, Marija MitroviÄ‡ Dankulov</p>
                    <p><strong>Published:</strong> 8/29/2025</p>
                    <p><strong>Categories:</strong> cs.CY, cs.SI, physics.soc-ph</p>
                    <p><strong>Summary:</strong> Large Language Models (LLMs) enable generative social simulations that can capture culturally informed, norm-guided interaction on online social platforms. We build a technology community simulation modeled on Voat, a Reddit-like alt-right news aggregator and discussion platform active from 2014 to 2020. Using the YSocial framework, we seed the simulation with a fixed catalog of technology links sampled from Voats shared URLs (covering 30+ domains) and calibrate parameters to Voats v/technology using samples from the MADOC dataset. Agents use a base, uncensored model (Dolphin 3.0, based on Llama 3.1 8B) and concise personas (demographics, political leaning, interests, education, toxicity propensity) to generate posts, replies, and reactions under platform rules for link and text submissions, threaded replies and daily activity cycles. We run a 30-day simulation and evaluate operational validity by comparing distributions and structures with matched Voat data: activity patterns, interaction networks, toxicity, and topic coverage. Results indicate familiar online regularities: similar activity rhythms, heavy-tailed participation, sparse low-clustering interaction networks, core-periphery structure, topical alignment with Voat, and elevated toxicity. Limitations of the current study include the stateless agent design and evaluation based on a single 30-day run, which constrains external validity and variance estimates. The simulation generates realistic discussions, often featuring toxic language, primarily centered on technology topics such as Big Tech and AI. This approach offers a valuable method for examining toxicity dynamics and testing moderation strategies within a controlled environment.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.21738v1" target="_blank">From Drone Imagery to Livability Mapping: AI-powered Environment Perception in Rural China</a></h3>
                    <p><strong>Authors:</strong> Weihuan Deng, Yaofu Huang, Luan Chen, Xun Li, Yao Yao</p>
                    <p><strong>Published:</strong> 8/29/2025</p>
                    <p><strong>Categories:</strong> cs.CY, cs.CV</p>
                    <p><strong>Summary:</strong> With the deepening of poverty alleviation and rural revitalization strategies, improving the rural living environment and enhancing the quality of life have become key priorities. Rural livability is a key indicator for measuring the effectiveness of these efforts. Current measurement approaches face significant limitations, as questionnaire-based methods are difficult to scale, while urban-oriented visual perception methods are poorly suited for rural contexts. In this paper, a rural-specific livability assessment framework was proposed based on drone imagery and multimodal large language models (MLLMs). To comprehensively assess village livability, this study first used a top-down approach to collect large-scale drone imagery of 1,766 villages in 146 counties across China. In terms of the model framework, an efficient image comparison mechanism was developed, incorporating binary search interpolation to determine effective image pairs while reducing comparison iterations. Building on expert knowledge, a chain-of-thought prompting suitable for nationwide rural livability measurement was constructed, considering both living quality and ecological habitability dimensions. This approach enhanced the rationality and reliability of the livability assessment. Finally, this study characterized the spatial heterogeneity of rural livability across China and thoroughly analyzed its influential factors. The results show that: (1) The rural livability in China demonstrates a dual-core-periphery spatial pattern, radiating outward from Sichuan and Zhejiang provinces with declining gradients; (2) Among various influential factors, government fiscal expenditure emerged as the core determinant, with each unit increase corresponding to a 3.9 - 4.9 unit enhancement in livability. The findings provide valuable insights for rural construction policy-making.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.21737v1" target="_blank">The NilHecke schober</a></h3>
                    <p><strong>Authors:</strong> Jasper van de Kreeke</p>
                    <p><strong>Published:</strong> 8/29/2025</p>
                    <p><strong>Categories:</strong> math.RT, math.QA, 18G80, 18N25</p>
                    <p><strong>Summary:</strong> Perverse schobers can be used to describe Fukaya categories but are hard to axiomatize and construct. In this paper, we give an explicit construction of a perverse schober intended to accurately describe the Fukaya category of the horizontal Hilbert scheme considered by Aganagic et al. in the frame of the knot invariant categorification program. The formalism is based on the notion of $ A_n $-schobers of Dyckerhoff and Wedrich. The construction is entirely algebraic and we check the schober axioms with the help of diagrammatic calculus.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.21736v1" target="_blank">MicroLabVR: Interactive 3D Visualization of Simulated Spatiotemporal Microbiome Data in Virtual Reality</a></h3>
                    <p><strong>Authors:</strong> Simon Burbach, Maria Maleshkova, Florian Centler, Tanja Joan Schmidt</p>
                    <p><strong>Published:</strong> 8/29/2025</p>
                    <p><strong>Categories:</strong> cs.HC, cs.CE, cs.GR, q-bio.CB, q-bio.MN</p>
                    <p><strong>Summary:</strong> Microbiomes are a vital part of the human body, engaging in tasks like food digestion and immune defense. Their structure and function must be understood in order to promote host health and facilitate swift recovery during disease. Due to the difficulties in experimentally studying these systems in situ, more research is being conducted in the field of mathematical modeling. Visualizing spatiotemporal data is challenging, and current tools that simulate microbial communities spatial and temporal development often only provide limited functionalities, often requiring expert knowledge to generate useful results. To overcome these limitations, we provide a user-friendly tool to interactively explore spatiotemporal simulation data, called MicroLabVR, which transfers spatial data into virtual reality (VR) while following guidelines to enhance user experience (UX). With MicroLabVR, users can import CSV datasets containing population growth, substance concentration development, and metabolic flux distribution data. The implemented visualization methods allow users to evaluate the dataset in a VR environment interactively. MicroLabVR aims to improve data analysis for the user by allowing the exploration of microbiome data in their spatial context.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.21734v1" target="_blank">Computational study of interactions between ionized glyphosate and carbon nanotube: An alternative for mitigating environmental contamination</a></h3>
                    <p><strong>Authors:</strong> H. T. Silva, L. C. S. Faria, T. A. Aversi-Ferreira, I. Camps</p>
                    <p><strong>Published:</strong> 8/29/2025</p>
                    <p><strong>Categories:</strong> cond-mat.mtrl-sci</p>
                    <p><strong>Summary:</strong> The extensive use of glyphosate in agriculture has raised environmental concerns due to its adverse effects on plants, animals, microorganisms, and humans. This study investigates the interactions between ionized glyphosate and single-walled carbon nanotubes (CNT) using computational simulations through semi-empirical tight-binding methods (GFN2-xTB) implemented in the xTB software. The analysis focused on different glyphosate ionization states corresponding to various pH levels: G1 (pH  10.6). Results revealed that glyphosate in G1, G3, G4, and G5 forms exhibited stronger interactions with CNT, demonstrating higher adsorption energies and greater electronic coupling. The neutral state (G2) showed lower affinity, indicating that molecular protonation significantly influences adsorption. Topological analysis and molecular dynamics confirmed the presence of covalent, non-covalent, and partially covalent interactions, while the CNT+G5 system demonstrated moderate interactions suitable for material recycling. These findings suggest that carbon nanotubes, with their extraordinary properties such as nanocapillarity, porosity, and extensive surface area, show promise for environmental monitoring and remediation of glyphosate contamination.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.21733v1" target="_blank">Developer Insights into Designing AI-Based Computer Perception Tools</a></h3>
                    <p><strong>Authors:</strong> Maya Guhan, Meghan E. Hurley, Eric A. Storch, John Herrington, Casey Zampella, Julia Parish-Morris, Gabriel LÃ¡zaro-MuÃ±oz, Kristin Kostick-Quenet</p>
                    <p><strong>Published:</strong> 8/29/2025</p>
                    <p><strong>Categories:</strong> cs.HC, cs.AI, cs.CY</p>
                    <p><strong>Summary:</strong> Artificial intelligence (AI)-based computer perception (CP) technologies use mobile sensors to collect behavioral and physiological data for clinical decision-making. These tools can reshape how clinical knowledge is generated and interpreted. However, effective integration of these tools into clinical workflows depends on how developers balance clinical utility with user acceptability and trustworthiness. Our study presents findings from 20 in-depth interviews with developers of AI-based CP tools. Interviews were transcribed and inductive, thematic analysis was performed to identify 4 key design priorities: 1) to account for context and ensure explainability for both patients and clinicians; 2) align tools with existing clinical workflows; 3) appropriately customize to relevant stakeholders for usability and acceptability; and 4) push the boundaries of innovation while aligning with established paradigms. Our findings highlight that developers view themselves as not merely technical architects but also ethical stewards, designing tools that are both acceptable by users and epistemically responsible (prioritizing objectivity and pushing clinical knowledge forward). We offer the following suggestions to help achieve this balance: documenting how design choices around customization are made, defining limits for customization choices, transparently conveying information about outputs, and investing in user training. Achieving these goals will require interdisciplinary collaboration between developers, clinicians, and ethicists.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.21732v1" target="_blank">CAD2DMD-SET: Synthetic Generation Tool of Digital Measurement Device CAD Model Datasets for fine-tuning Large Vision-Language Models</a></h3>
                    <p><strong>Authors:</strong> JoÃ£o Valente, Atabak Dehban, Rodrigo Ventura</p>
                    <p><strong>Published:</strong> 8/29/2025</p>
                    <p><strong>Categories:</strong> cs.CV, cs.AI</p>
                    <p><strong>Summary:</strong> Recent advancements in Large Vision-Language Models (LVLMs) have demonstrated impressive capabilities across various multimodal tasks. They continue, however, to struggle with trivial scenarios such as reading values from Digital Measurement Devices (DMDs), particularly in real-world conditions involving clutter, occlusions, extreme viewpoints, and motion blur; common in head-mounted cameras and Augmented Reality (AR) applications. Motivated by these limitations, this work introduces CAD2DMD-SET, a synthetic data generation tool designed to support visual question answering (VQA) tasks involving DMDs. By leveraging 3D CAD models, advanced rendering, and high-fidelity image composition, our tool produces diverse, VQA-labelled synthetic DMD datasets suitable for fine-tuning LVLMs. Additionally, we present DMDBench, a curated validation set of 1,000 annotated real-world images designed to evaluate model performance under practical constraints. Benchmarking three state-of-the-art LVLMs using Average Normalised Levenshtein Similarity (ANLS) and further fine-tuning LoRAs of these models with CAD2DMD-SETs generated dataset yielded substantial improvements, with InternVL showcasing a score increase of 200% without degrading on other tasks. This demonstrates that the CAD2DMD-SET training dataset substantially improves the robustness and performance of LVLMs when operating under the previously stated challenging conditions. The CAD2DMD-SET tool is expected to be released as open-source once the final version of this manuscript is prepared, allowing the community to add different measurement devices and generate their own datasets.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.21729v1" target="_blank">Bayesian perspectives for quantum states and application to ab initio quantum chemistry</a></h3>
                    <p><strong>Authors:</strong> Yannic Rath, Massimo Bortone, George H. Booth</p>
                    <p><strong>Published:</strong> 8/29/2025</p>
                    <p><strong>Categories:</strong> cond-mat.str-el, physics.chem-ph, physics.comp-ph, quant-ph</p>
                    <p><strong>Summary:</strong> The quantum many-electron problem is not just at the heart of condensed matter phenomena, but also essential for first-principles simulation of chemical phenomena. Strong correlation in chemical systems are prevalent and present a formidable challenge in the simulation of these systems, while predictive phenomena in this domain often also requires a demanding level of accuracy to inform chemical behavior. Efficient representations of the many-electron states of chemical systems are therefore also being inspired by machine learning principles to provide an alternative to established approaches. In this chapter, we review recent progress in this endeavor for quantum chemical problems represented in second quantization, and the particular challenges present in this field. In particular, we focus on the application of Gaussian Process States emerging from efficient representations of the many-body wavefunction with rigorous Bayesian modeling frameworks, allowing for the unification of multiple paradigms under a common umbrella. We show how such models (and other representations derived from machine learning) can be used as novel tools to compute ab initio chemical properties, while in turn also informing the design of machine learning models to extract correlation patterns in classical data.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.21725v1" target="_blank">Braneworlds in Einstein-Scalar-Gauss-Bonnet gravity</a></h3>
                    <p><strong>Authors:</strong> JosÃ© Euclides G. Silva, Leandro A. Lessa, Roberto V. Maluf</p>
                    <p><strong>Published:</strong> 8/29/2025</p>
                    <p><strong>Categories:</strong> gr-qc</p>
                    <p><strong>Summary:</strong> We explore the features of a thick braneworld model in five dimensions governed by a Einstein-Gauss-Bonnet gravity with a non-minimal coupling to a dynamical scalar field. We consider two possible scalar-GB coupling function $\chi(\phi)$, one parity-even and another parity-odd function of the scalar field $\phi$. For both choices, the scalar-Gauss-Bonnet non-minimal coupling produces a warped asymptotically $AdS_5$ spacetime even in the absence of a bulk cosmological constant. Outside the brane core, a negative cosmological constant is bounded by the scalar-GB coupling function. For a thick 3-brane configuration, we found solutions with localized brane energy density and pressure that dynamically produce a bulk cosmological constant. The corresponding scalar field solutions exhibit a non-topological (domain wall) behavior. In order to probe the 3-brane stability solution, we employed a perturbative analysis, by perturbing the thick brane solutions up to first-order. The Kaluza-Klein (KK) tensorial gravitational modes possess a localized massless mode and a tower of non-tachyonic diverging massive modes, what renders the solutions stable at least at the perturbative level.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.21724v1" target="_blank">A Single Subject Machine Learning Based Classification of Motor Imagery EEGs</a></h3>
                    <p><strong>Authors:</strong> Dario Sanalitro, Marco Finocchiaro, Pasquale Memmolo, Emanuela Cutuli, Maide Bucolo</p>
                    <p><strong>Published:</strong> 8/29/2025</p>
                    <p><strong>Categories:</strong> eess.SP, cs.SY, eess.SY</p>
                    <p><strong>Summary:</strong> Motor Imagery-Based Brain-Computer Interfaces (MI-BCIs) are systems that detect and interpret brain activity patterns linked to the mental visualization of movement, and then translate these into instructions for controlling external robotic or domotic devices. Such devices have the potential to be useful in a broad variety of applications. While implementing a system that would help individuals restore some freedom levels, the interpretation of (Electroencephalography) EEG data remains a complex and unsolved problem. In the literature, the classification of left and right imagined movements has been extensively studied. This study introduces a novel pipeline that makes use of machine learning techniques for classifying MI EEG data. The entire framework is capable of accurately categorizing left and imagined motions, as well as rest phases, for a set of 52 subjects who performed a MI task. We trained a within subject model on each individual subject. The methodology has been offline evaluated and compared to four studies that are currently the state-of-the-art regarding the specified dataset. The results show that our proposed framework could be used with MI-BCI systems in light of its failsafe classification performances, i.e. 99.5% in accuracy</p>
                
            
        
        
            <p><em>Generated by AI News Agent</em></p>
        
    

