
    
        <h1>ðŸ¤– AI Research News Report</h1>
        <p>Sunday, September 21, 2025</p>
        <p>Topics: ai alignment research, quantum computing</p>
    
    
        20Research Papers
        2Topics Covered
        YesAI Summary
    
    
    
        <h2>ðŸ¤– AI Summary</h2>
        <h2>ai alignment research</h2>
<h3>Summary of AI Alignment Research Papers</h3>
<h4>Most Important Trends</h4>
<ol>
<li><p><strong>Domain Adaptation and Calibration</strong>: The research highlights trends in improving model performance through domain adaptation without relying on source data and enhancing the reliability of model predictions, particularly in the medical field. This includes frameworks like VocAlign for semantic segmentation and CalibPrompt for Med-VLMs, emphasizing improved alignment and calibration.</p>
</li>
<li><p><strong>Cross-Modal Learning and Data Utilization</strong>: There is a growing trend of leveraging cross-modal learning techniques, as seen in Depth AnyEvent, which uses event data for better depth estimation, and ScaleCUA, which scales computer use agents through diverse data sources across platforms.</p>
</li>
<li><p><strong>Data Contamination and Fair Evaluation</strong>: Addressing data contamination in LLMs, as demonstrated by LNE-Blocking, underscores the need for fair evaluation methods that account for inadvertent data overlap, ensuring more reliable benchmarking.</p>
</li>
<li><p><strong>Trajectory Prediction and Unseen Object Tracking</strong>: Out-of-Sight Trajectories research highlights the focus on predicting trajectories in scenarios with limited visibility and noisy data, an essential aspect for autonomous systems and safety.</p>
</li>
<li><p><strong>Vulnerabilities and Security in Integrated Systems</strong>: The Evil Vizier paper points to the increasing concern over the vulnerabilities in XR systems integrated with LLMs, demonstrating the need for robust security measures.</p>
</li>
<li><p><strong>Oppression Measurement with LLMs</strong>: Leveraging LLMs to assess historical oppression showcases a broader application of AI in social sciences, aiming to provide nuanced insights into systemic issues.</p>
</li>
</ol>
<h4>Breakthroughs</h4>
<ol>
<li><p><strong>VocAlign and CalibPrompt</strong>: These frameworks improve model adaptation and calibration, setting new performance standards while maintaining efficiency and accuracy.</p>
</li>
<li><p><strong>Cross-Modal Distillation in Depth Estimation</strong>: The novel approach in Depth AnyEvent to use cross-modal distillation significantly enhances depth estimation without large labeled datasets.</p>
</li>
<li><p><strong>ScaleCUAs Cross-Platform Data Integration</strong>: This development in computer use agents demonstrates substantial improvements by integrating data from various platforms, leading to state-of-the-art performance.</p>
</li>
<li><p><strong>Vision-Positioning Denoising Module</strong>: The advancement in trajectory prediction for out-of-sight objects through vision-positioning mapping represents a significant leap in dealing with real-world noisy data.</p>
</li>
<li><p><strong>Rule-Guided Prompting for Oppression Measurement</strong>: This framework allows LLMs to generate context-sensitive scores of historical disadvantage, offering a new tool for cross-cultural analysis.</p>
</li>
</ol>
<h4>Implications</h4>
<ol>
<li><p><strong>Enhanced AI Alignment</strong>: The trends and breakthroughs in domain adaptation, cross-modal learning, and calibration improve AI alignment by making models more adaptable, reliable, and efficient in diverse environments.</p>
</li>
<li><p><strong>Broader Applications in Social Sciences and Security</strong>: The use of AI for assessing historical oppression and identifying vulnerabilities in XR systems suggests that AI alignment research is expanding beyond traditional domains, impacting social sciences and security.</p>
</li>
<li><p><strong>Improved Data Utilization and Fair Evaluation</strong>: Techniques like LNE-Blocking and ScaleCUA demonstrate how better data utilization and fair evaluation can lead to more reliable AI systems, addressing data leakage issues and enhancing performance across platforms.</p>
</li>
<li><p><strong>Safety and Trust in Autonomous Systems</strong>: The focus on trajectory prediction and unseen object tracking emphasizes the importance of safety and trust in autonomous systems, crucial for their deployment in real-world scenarios.</p>
</li>
<li><p><strong>Need for Robust Security Measures</strong>: The vulnerabilities identified in LLM-integrated systems highlight the urgent need for developing robust security strategies to protect against potential attacks, ensuring user safety and privacy.</p>
</li>
</ol>
<p>Overall, these papers illustrate significant advancements in AI alignment, emphasizing adaptability, reliability, and the broadening scope of AI applications.</p>
<p><em>Based on 10 research papers</em></p>

<h2>quantum computing</h2>
<p>It seems there has been a misunderstanding. The research papers listed do not directly pertain to quantum computing. Instead, they cover a variety of topics in computer science and related fields, such as semantic segmentation, medical imaging, depth estimation, cosmology, piano performance, computer use agents, stereo vision, language model contamination, trajectory prediction, and image captioning. Despite the lack of direct relevance to quantum computing, I can still summarize the key trends, breakthroughs, and implications of these papers in their respective areas.</p>
<h3>Most Important Trends</h3>
<ul>
<li><strong>Domain Adaptation and Calibration:</strong> There is a growing focus on improving the adaptability and reliability of models across domains, especially in semantic segmentation and medical imaging, which require robust performance despite data scarcity or variability.</li>
<li><strong>Cross-Modal Learning:</strong> The integration of multiple data modalities, such as in event-based monocular depth estimation and multimodal piano performance, is becoming more prevalent. This trend reflects the need for richer data representations and more comprehensive model training.</li>
<li><strong>Data Scalability and Efficiency:</strong> Several studies emphasize scaling datasets and models efficiently, as seen in the development of large-scale datasets and frameworks for computer use agents, and efficient frameworks for multi-view stereo reconstruction.</li>
</ul>
<h3>Breakthroughs</h3>
<ul>
<li><strong>Vocabulary Alignment in Semantic Segmentation:</strong> The introduction of VocAlign demonstrates a significant improvement in open-vocabulary segmentation tasks by refining pseudo-labels and reducing computational overhead.</li>
<li><strong>Calibration in Medical Imaging Models:</strong> CalibPrompt emerges as a novel method to enhance the calibration of medical vision-language models, ensuring more reliable confidence estimates without sacrificing accuracy.</li>
<li><strong>Cross-Modal Depth Estimation:</strong> The development of a cross-modal distillation paradigm for event-based depth estimation shows promise in achieving high accuracy without the need for dense annotations.</li>
<li><strong>Out-of-Sight Trajectory Prediction:</strong> Advancements in predicting trajectories of unseen objects using noisy sensor data illustrate significant progress in applications like autonomous driving and robotics.</li>
</ul>
<h3>Implications</h3>
<ul>
<li><strong>Enhanced Model Reliability:</strong> Improved calibration and domain adaptation techniques are likely to increase model trustworthiness in critical applications, such as healthcare and autonomous systems, where reliability is paramount.</li>
<li><strong>Broader Application Scope:</strong> The ability to integrate multiple data modalities and scale datasets effectively opens new avenues for research and application in diverse fields, including surveillance, virtual reality, and human-computer interaction.</li>
<li><strong>Efficiency and Accessibility:</strong> Methods that reduce computational costs while maintaining or improving performance make advanced technologies more accessible and feasible for widespread adoption, particularly in resource-constrained environments.</li>
<li><strong>Future Research Directions:</strong> These studies set the stage for further exploration into robust model training, efficient data handling, and cross-modal learning, driving innovation in both academic and practical domains.</li>
</ul>
<p>While these papers do not directly contribute to quantum computing, their findings and methodologies may inspire analogous approaches in quantum research, particularly in areas like data processing and model training.</p>
<p><em>Based on 10 research papers</em></p>

        Generated by OpenAI GPT-4o-mini
    
    
    
    
        <h2>ðŸ“° News</h2>
        
            
                ai alignment research
                
                    
                        
                            <a href="https://www.theguardian.com/p/x38jaz" target="_blank">Chatbot site depicting child sexual abuse images raises fears over misuse of AI</a> â€” Dan Milmo Global technology editor
                        
                        2025-09-21T11:00:43Z
                        <br>
                        A chatbot site featuring explicit scenarios with preteen characters and illegal abuse images has sparked concerns over the misuse of artificial intelligence. The Internet Watch Foundation (IWF) reported the site, which presents scenarios like child prostitute in a hotel and expands chatbot icons into full-screen depictions of child sexual abuse imagery. The IWF identified 17 AI-generated, photo-realistic images that qualify as child sexual abuse material under UK law. This has prompted calls for the UK government to enforce safety guidelines on AI companies, as the government plans an AI bill focusing on future model development and criminalizing the distribution of models generating such illegal content. The IWF advocates for mandatory child-protecting guidelines to be integrated into AI models from their inception.
                        <a href="https://www.theguardian.com/p/x38jaz" target="_blank">Read full article</a>
                    
                    
                    
                        
                            <a href="https://www.theguardian.com/p/x38nnm" target="_blank">British AI startup beats humans in international forecasting competition</a> â€” Robert Booth UK technology editor
                        
                        2025-09-20T08:00:08Z
                        <br>
                        A British AI startup, ManticAI, has outperformed many human competitors in an international forecasting competition, ranking eighth in the Metaculus Cup. The competition entailed predicting the likelihood of 60 events, with topics ranging from political outcomes to environmental occurrences. ManticAIs achievement, co-founded by a former Google DeepMind researcher, demonstrates significant progress in AI forecasting capabilities, though it still trails behind the best human forecasters. This development has sparked discussions about the potential for AI to surpass human forecasting abilities sooner than anticipated. Metaculus CEO Deger Turan noted the impressive performance of ManticAI and projected that AI could match or exceed top human forecasters by 2029.
                        <a href="https://www.theguardian.com/p/x38nnm" target="_blank">Read full article</a>
                    
                    
                    
                        
                            <a href="https://www.theguardian.com/p/x38tkh" target="_blank">Meta announces first Ray-Ban smart glasses with in-built augmented reality display</a> â€” Samuel Gibbs Consumer technology editor
                        
                        2025-09-18T01:06:06Z
                        <br>
                        Meta has unveiled its first Ray-Ban smart glasses equipped with an augmented reality display, marking a significant development since Google Glass. These glasses, designed to resemble classic Wayfarers, feature an internal display projected onto the right lens, capable of showing text, images, and live video calls without appearing externally. They include a camera, speakers, and microphone, with an external LED to indicate camera activity. The glasses were presented by CEO Mark Zuckerberg at the Meta Connect event, highlighting their potential in AI applications by allowing users to see and hear what the AI perceives. Additionally, they come with a water-resistant Neural Band bracelet that detects forearm impulses for gesture-based controls.
                        <a href="https://www.theguardian.com/p/x38tkh" target="_blank">Read full article</a>
                    
                    
                Source: The Guardian
            
            
            
                quantum computing
                
                Source: The Guardian
            
            
    
    
    
        <h2>ðŸ“„ Research Papers (20)</h2>
        
            
                Lost in Translation? Vocabulary Alignment for Source-Free Domain Adaptation in Open-Vocabulary Semantic Segmentation
                
                    <strong>Authors:</strong> Silvio Mazzucco, Carl Persson, Mattia Segu, Pier Luigi Dovesi, Federico Tombari, Luc Van Gool, Matteo Poggi
                
                
                    <strong>Abstract:</strong> We introduce VocAlign, a novel source-free domain adaptation framework specifically designed for VLMs in open-vocabulary semantic segmentation. Our method adopts a student-teacher paradigm enhanced with a vocabulary alignment strategy, which improves pseudo-label generation by incorporating additional class concepts. To ensure efficiency, we use Low-Rank Adaptation (LoRA) to fine-tune the model, preserving its original capabilities while minimizing computational overhead. In addition, we propose a Top-K class selection mechanism for the student model, which significantly reduces memory requirements while further improving adaptation performance. Our approach achieves a notable 6.11 mIoU improvement on the CityScapes dataset and demonstrates superior performance on zero-shot segmentation benchmarks, setting a new standard for source-free adaptation in the open-vocabulary setting.
                
                
                    <strong>Published:</strong> 2025-09-18T17:59:58Z
                    <a href="http://arxiv.org/abs/2509.15225v1" target="_blank">ðŸ“„ View Paper</a>
                
            
        
            
                Calibration-Aware Prompt Learning for Medical Vision-Language Models
                
                    <strong>Authors:</strong> Abhishek Basu, Fahad Shamshad, Ashshak Sharifdeen, Karthik Nandakumar, Muhammad Haris Khan
                
                
                    <strong>Abstract:</strong> Medical Vision-Language Models (Med-VLMs) have demonstrated remarkable performance across diverse medical imaging tasks by leveraging large-scale image-text pretraining. However, their confidence calibration is largely unexplored, and so remains a significant challenge. As such, miscalibrated predictions can lead to overconfident errors, undermining clinical trust and decision-making reliability. To address this, we introduce CalibPrompt, the first framework to calibrate Med-VLMs during prompt tuning. CalibPrompt optimizes a small set of learnable prompts with carefully designed calibration objectives under scarce labeled data regime. First, we study a regularizer that attempts to align the smoothed accuracy with the predicted model confidences. Second, we introduce an angular separation loss to maximize textual feature proximity toward improving the reliability in confidence estimates of multimodal Med-VLMs. Extensive experiments on four publicly available Med-VLMs and five diverse medical imaging datasets reveal that CalibPrompt consistently improves calibration without drastically affecting clean accuracy. Our code is available at https://github.com/iabh1shekbasu/CalibPrompt.
                
                
                    <strong>Published:</strong> 2025-09-18T17:59:58Z
                    <a href="http://arxiv.org/abs/2509.15226v1" target="_blank">ðŸ“„ View Paper</a>
                
            
        
            
                Depth AnyEvent: A Cross-Modal Distillation Paradigm for Event-Based Monocular Depth Estimation
                
                    <strong>Authors:</strong> Luca Bartolomei, Enrico Mannocci, Fabio Tosi, Matteo Poggi, Stefano Mattoccia
                
                
                    <strong>Abstract:</strong> Event cameras capture sparse, high-temporal-resolution visual information, making them particularly suitable for challenging environments with high-speed motion and strongly varying lighting conditions. However, the lack of large datasets with dense ground-truth depth annotations hinders learning-based monocular depth estimation from event data. To address this limitation, we propose a cross-modal distillation paradigm to generate dense proxy labels leveraging a Vision Foundation Model (VFM). Our strategy requires an event stream spatially aligned with RGB frames, a simple setup even available off-the-shelf, and exploits the robustness of large-scale VFMs. Additionally, we propose to adapt VFMs, either a vanilla one like Depth Anything v2 (DAv2), or deriving from it a novel recurrent architecture to infer depth from monocular event cameras. We evaluate our approach with synthetic and real-world datasets, demonstrating that i) our cross-modal paradigm achieves competitive performance compared to fully supervised methods without requiring expensive depth annotations, and ii) our VFM-based models achieve state-of-the-art performance.
                
                
                    <strong>Published:</strong> 2025-09-18T17:59:51Z
                    <a href="http://arxiv.org/abs/2509.15224v1" target="_blank">ðŸ“„ View Paper</a>
                
            
        
            
                Two Web Toolkits for Multimodal Piano Performance Dataset Acquisition and Fingering Annotation
                
                    <strong>Authors:</strong> Junhyung Park, Yonghyun Kim, Joonhyung Bae, Kirak Kim, Taegyun Kwon, Alexander Lerch, Juhan Nam
                
                
                    <strong>Abstract:</strong> Piano performance is a multimodal activity that intrinsically combines physical actions with the acoustic rendition. Despite growing research interest in analyzing the multimodal nature of piano performance, the laborious process of acquiring large-scale multimodal data remains a significant bottleneck, hindering further progress in this field. To overcome this barrier, we present an integrated web toolkit comprising two graphical user interfaces (GUIs): (i) PiaRec, which supports the synchronized acquisition of audio, video, MIDI, and performance metadata. (ii) ASDF, which enables the efficient annotation of performer fingering from the visual data. Collectively, this system can streamline the acquisition of multimodal piano performance datasets.
                
                
                    <strong>Published:</strong> 2025-09-18T17:59:24Z
                    <a href="http://arxiv.org/abs/2509.15222v1" target="_blank">ðŸ“„ View Paper</a>
                
            
        
            
                ScaleCUA: Scaling Open-Source Computer Use Agents with Cross-Platform Data
                
                    <strong>Authors:</strong> Zhaoyang Liu, JingJing Xie, Zichen Ding, Zehao Li, Bowen Yang, Zhenyu Wu, Xuehui Wang, Qiushi Sun, Shi Liu, Weiyun Wang, Shenglong Ye, Qingyun Li, Zeyue Tian, Gen Luo, Xiangyu Yue, Biqing Qi, Kai Chen, Bowen Zhou, Yu Qiao, Qifeng Chen, Wenhai Wang
                
                
                    <strong>Abstract:</strong> Vision-Language Models (VLMs) have enabled computer use agents (CUAs) that operate GUIs autonomously, showing great potential, yet progress is limited by the lack of large-scale, open-source computer use data and foundation models. In this work, we introduce ScaleCUA, a step toward scaling open-source CUAs. It offers a large-scale dataset spanning 6 operating systems and 3 task domains, built via a closed-loop pipeline uniting automated agents with human experts. Trained on this scaled-up data, ScaleCUA can operate seamlessly across platforms. Specifically, it delivers strong gains over baselines (+26.6 on WebArena-Lite-v2, +10.7 on ScreenSpot-Pro) and sets new state-of-the-art results (94.4% on MMBench-GUI L1-Hard, 60.6% on OSWorld-G, 47.4% on WebArena-Lite-v2). These findings underscore the power of data-driven scaling for general-purpose computer use agents. We will release data, models, and code to advance future research: https://github.com/OpenGVLab/ScaleCUA.
                
                
                    <strong>Published:</strong> 2025-09-18T17:59:22Z
                    <a href="http://arxiv.org/abs/2509.15221v1" target="_blank">ðŸ“„ View Paper</a>
                
            
        
            
                LNE-Blocking: An Efficient Framework for Contamination Mitigation Evaluation on Large Language Models
                
                    <strong>Authors:</strong> Ruijie Hou, Yueyang Jiao, Hanxu Hu, Yingming Li, Wai Lam, Huajian Zhang, Hongyuan Lu
                
                
                    <strong>Abstract:</strong> The problem of data contamination is now almost inevitable during the development of large language models (LLMs), with the training data commonly integrating those evaluation benchmarks even unintentionally. This problem subsequently makes it hard to benchmark LLMs fairly. Instead of constructing contamination-free datasets (quite hard), we propose a novel framework, \textbf{LNE-Blocking}, to restore model performance prior to contamination on potentially leaked datasets. Our framework consists of two components: contamination detection and disruption operation. For the prompt, the framework first uses the contamination detection method, \textbf{LNE}, to assess the extent of contamination in the model. Based on this, it adjusts the intensity of the disruption operation, \textbf{Blocking}, to elicit non-memorized responses from the model. Our framework is the first to efficiently restore the models greedy decoding performance. This comes with a strong performance on multiple datasets with potential leakage risks, and it consistently achieves stable recovery results across different models and varying levels of data contamination. We release the code at https://github.com/RuijieH/LNE-Blocking to facilitate research.
                
                
                    <strong>Published:</strong> 2025-09-18T17:59:16Z
                    <a href="http://arxiv.org/abs/2509.15218v1" target="_blank">ðŸ“„ View Paper</a>
                
            
        
            
                Out-of-Sight Trajectories: Tracking, Fusion, and Prediction
                
                    <strong>Authors:</strong> Haichao Zhang, Yi Xu, Yun Fu
                
                
                    <strong>Abstract:</strong> Trajectory prediction is a critical task in computer vision and autonomous systems, playing a key role in autonomous driving, robotics, surveillance, and virtual reality. Existing methods often rely on complete and noise-free observational data, overlooking the challenges associated with out-of-sight objects and the inherent noise in sensor data caused by limited camera coverage, obstructions, and the absence of ground truth for denoised trajectories. These limitations pose safety risks and hinder reliable prediction in real-world scenarios. In this extended work, we present advancements in Out-of-Sight Trajectory (OST), a novel task that predicts the noise-free visual trajectories of out-of-sight objects using noisy sensor data. Building on our previous research, we broaden the scope of Out-of-Sight Trajectory Prediction (OOSTraj) to include pedestrians and vehicles, extending its applicability to autonomous driving, robotics, surveillance, and virtual reality. Our enhanced Vision-Positioning Denoising Module leverages camera calibration to establish a vision-positioning mapping, addressing the lack of visual references, while effectively denoising noisy sensor data in an unsupervised manner. Through extensive evaluations on the Vi-Fi and JRDB datasets, our approach achieves state-of-the-art performance in both trajectory denoising and prediction, significantly surpassing previous baselines. Additionally, we introduce comparisons with traditional denoising methods, such as Kalman filtering, and adapt recent trajectory prediction models to our task, providing a comprehensive benchmark. This work represents the first initiative to integrate vision-positioning projection for denoising noisy sensor trajectories of out-of-sight agents, paving the way for future advances. The code and preprocessed datasets are available at github.com/Hai-chao-Zhang/OST
                
                
                    <strong>Published:</strong> 2025-09-18T17:59:16Z
                    <a href="http://arxiv.org/abs/2509.15219v1" target="_blank">ðŸ“„ View Paper</a>
                
            
        
            
                Assessing Historical Structural Oppression Worldwide via Rule-Guided Prompting of Large Language Models
                
                    <strong>Authors:</strong> Sreejato Chatterjee, Linh Tran, Quoc Duy Nguyen, Roni Kirson, Drue Hamlin, Harvest Aquino, Hanjia Lyu, Jiebo Luo, Timothy Dye
                
                
                    <strong>Abstract:</strong> Traditional efforts to measure historical structural oppression struggle with cross-national validity due to the unique, locally specified histories of exclusion, colonization, and social status in each country, and often have relied on structured indices that privilege material resources while overlooking lived, identity-based exclusion. We introduce a novel framework for oppression measurement that leverages Large Language Models (LLMs) to generate context-sensitive scores of lived historical disadvantage across diverse geopolitical settings. Using unstructured self-identified ethnicity utterances from a multilingual COVID-19 global study, we design rule-guided prompting strategies that encourage models to produce interpretable, theoretically grounded estimations of oppression. We systematically evaluate these strategies across multiple state-of-the-art LLMs. Our results demonstrate that LLMs, when guided by explicit rules, can capture nuanced forms of identity-based historical oppression within nations. This approach provides a complementary measurement tool that highlights dimensions of systemic exclusion, offering a scalable, cross-cultural lens for understanding how oppression manifests in data-driven research and public health contexts. To support reproducible evaluation, we release an open-sourced benchmark dataset for assessing LLMs on oppression measurement (https://github.com/chattergpt/llm-oppression-benchmark).
                
                
                    <strong>Published:</strong> 2025-09-18T17:59:05Z
                    <a href="http://arxiv.org/abs/2509.15216v1" target="_blank">ðŸ“„ View Paper</a>
                
            
        
            
                Evil Vizier: Vulnerabilities of LLM-Integrated XR Systems
                
                    <strong>Authors:</strong> Yicheng Zhang, Zijian Huang, Sophie Chen, Erfan Shayegani, Jiasi Chen, Nael Abu-Ghazaleh
                
                
                    <strong>Abstract:</strong> Extended reality (XR) applications increasingly integrate Large Language Models (LLMs) to enhance user experience, scene understanding, and even generate executable XR content, and are often called AI glasses. Despite these potential benefits, the integrated XR-LLM pipeline makes XR applications vulnerable to new forms of attacks. In this paper, we analyze LLM-Integated XR systems in the literature and in practice and categorize them along different dimensions from a systems perspective. Building on this categorization, we identify a common threat model and demonstrate a series of proof-of-concept attacks on multiple XR platforms that employ various LLM models (Meta Quest 3, Meta Ray-Ban, Android, and Microsoft HoloLens 2 running Llama and GPT models). Although these platforms each implement LLM integration differently, they share vulnerabilities where an attacker can modify the public context surrounding a legitimate LLM query, resulting in erroneous visual or auditory feedback to users, thus compromising their safety or privacy, sowing confusion, or other harmful effects. To defend against these threats, we discuss mitigation strategies and best practices for developers, including an initial defense prototype, and call on the community to develop new protection mechanisms to mitigate these risks.
                
                
                    <strong>Published:</strong> 2025-09-18T17:58:15Z
                    <a href="http://arxiv.org/abs/2509.15213v1" target="_blank">ðŸ“„ View Paper</a>
                
            
        
            
                Localized In-Plane Cavity Optomechanics in MEMS
                
                    <strong>Authors:</strong> Sasan Rahmanian
                
                
                    <strong>Abstract:</strong> This study demonstrates the realization of localized in-plane optomechanical microcavities embedded within an electrostatic MEMS architecture. The system consists of a curved, clamped-clamped microbeam, fabricated on a silicon-on-insulator (SOI) wafer. A green laser emitted from a Laser Doppler Vibrometer (LDV), is directed perpendicularly onto the device under a vacuum pressure of 7 mTorr, with the beam aligned to fill the gap between the movable microbeam and its adjacent side fixed mirror. This configuration forms localized cavity optomechanical resonators that enable the generation of optomechanical soliton frequency combs through phonon lasing without electrical excitation. The optomechanical resonators dynamics are examined through experiments and numerical simulations. First, the experimental findings unveil that in electrostatic MEMS structures, the two reflective electrodes positioned to form a capacitive gap can inadvertently form localized cavities. These cavities significantly affect optical readouts, as the photodetected signal encodes contributions from both Doppler-shifted electromagnetic waves and light scattered from the intracavity optical field. This dual contributions can distort mechanical response interpretation unless appropriately filtered. Second, experiments show that optical pumping at various positions along the microbeam induces periodic pulse trains with distinct free spectral ranges (FSRs), each corresponding to different mechanical modes. Our results present the generation of solitary optical wavepackets using in-plane localized Fabry-P\erot microcavities formed within a MEMS device. The results suggest a path toward chip-scale, soliton frequency combs generators featuring frequency spacing on the order of kilohertz, without relying on integrated fiber optics.
                
                
                    <strong>Published:</strong> 2025-09-18T17:54:45Z
                    <a href="http://arxiv.org/abs/2509.15203v1" target="_blank">ðŸ“„ View Paper</a>
                
            
        
            
                Lost in Translation? Vocabulary Alignment for Source-Free Domain Adaptation in Open-Vocabulary Semantic Segmentation
                
                    <strong>Authors:</strong> Silvio Mazzucco, Carl Persson, Mattia Segu, Pier Luigi Dovesi, Federico Tombari, Luc Van Gool, Matteo Poggi
                
                
                    <strong>Abstract:</strong> We introduce VocAlign, a novel source-free domain adaptation framework specifically designed for VLMs in open-vocabulary semantic segmentation. Our method adopts a student-teacher paradigm enhanced with a vocabulary alignment strategy, which improves pseudo-label generation by incorporating additional class concepts. To ensure efficiency, we use Low-Rank Adaptation (LoRA) to fine-tune the model, preserving its original capabilities while minimizing computational overhead. In addition, we propose a Top-K class selection mechanism for the student model, which significantly reduces memory requirements while further improving adaptation performance. Our approach achieves a notable 6.11 mIoU improvement on the CityScapes dataset and demonstrates superior performance on zero-shot segmentation benchmarks, setting a new standard for source-free adaptation in the open-vocabulary setting.
                
                
                    <strong>Published:</strong> 2025-09-18T17:59:58Z
                    <a href="http://arxiv.org/abs/2509.15225v1" target="_blank">ðŸ“„ View Paper</a>
                
            
        
            
                Calibration-Aware Prompt Learning for Medical Vision-Language Models
                
                    <strong>Authors:</strong> Abhishek Basu, Fahad Shamshad, Ashshak Sharifdeen, Karthik Nandakumar, Muhammad Haris Khan
                
                
                    <strong>Abstract:</strong> Medical Vision-Language Models (Med-VLMs) have demonstrated remarkable performance across diverse medical imaging tasks by leveraging large-scale image-text pretraining. However, their confidence calibration is largely unexplored, and so remains a significant challenge. As such, miscalibrated predictions can lead to overconfident errors, undermining clinical trust and decision-making reliability. To address this, we introduce CalibPrompt, the first framework to calibrate Med-VLMs during prompt tuning. CalibPrompt optimizes a small set of learnable prompts with carefully designed calibration objectives under scarce labeled data regime. First, we study a regularizer that attempts to align the smoothed accuracy with the predicted model confidences. Second, we introduce an angular separation loss to maximize textual feature proximity toward improving the reliability in confidence estimates of multimodal Med-VLMs. Extensive experiments on four publicly available Med-VLMs and five diverse medical imaging datasets reveal that CalibPrompt consistently improves calibration without drastically affecting clean accuracy. Our code is available at https://github.com/iabh1shekbasu/CalibPrompt.
                
                
                    <strong>Published:</strong> 2025-09-18T17:59:58Z
                    <a href="http://arxiv.org/abs/2509.15226v1" target="_blank">ðŸ“„ View Paper</a>
                
            
        
            
                Depth AnyEvent: A Cross-Modal Distillation Paradigm for Event-Based Monocular Depth Estimation
                
                    <strong>Authors:</strong> Luca Bartolomei, Enrico Mannocci, Fabio Tosi, Matteo Poggi, Stefano Mattoccia
                
                
                    <strong>Abstract:</strong> Event cameras capture sparse, high-temporal-resolution visual information, making them particularly suitable for challenging environments with high-speed motion and strongly varying lighting conditions. However, the lack of large datasets with dense ground-truth depth annotations hinders learning-based monocular depth estimation from event data. To address this limitation, we propose a cross-modal distillation paradigm to generate dense proxy labels leveraging a Vision Foundation Model (VFM). Our strategy requires an event stream spatially aligned with RGB frames, a simple setup even available off-the-shelf, and exploits the robustness of large-scale VFMs. Additionally, we propose to adapt VFMs, either a vanilla one like Depth Anything v2 (DAv2), or deriving from it a novel recurrent architecture to infer depth from monocular event cameras. We evaluate our approach with synthetic and real-world datasets, demonstrating that i) our cross-modal paradigm achieves competitive performance compared to fully supervised methods without requiring expensive depth annotations, and ii) our VFM-based models achieve state-of-the-art performance.
                
                
                    <strong>Published:</strong> 2025-09-18T17:59:51Z
                    <a href="http://arxiv.org/abs/2509.15224v1" target="_blank">ðŸ“„ View Paper</a>
                
            
        
            
                Parameter sensitivity of cosmic pairwise velocities in the non-linear regime of structure formation
                
                    <strong>Authors:</strong> Jorge Enrique GarcÃ­a-Farieta, HÃ©ctor J. HortÃºa
                
                
                    <strong>Abstract:</strong> The peculiar velocities of dark matter tracers drive the growth of cosmic structures, providing a sensitive test of cosmological models and strengthening constraints on the nature of dark energy. In this work, we investigate the mean pairwise velocities, $v_{12}$, of dark matter tracers as a cosmological probe in the non-linear regime of cosmic structure formation. Using N-body dark matter-only simulations, we measure $v_{12}$ for pair separations up to 50 $h^{-1}$Mpc and model it by solving the pair conservation equation for a self-gravitating particle system, along with various prescriptions of the nonlinear matter power spectrum. We quantified the sensitivity of $v_{12}$ to variations in key cosmological parameters such as $\Omega_{\mathrm{m}}$, $\sigma_8$, $h$, $M_\nu$, and $w$. Our parameter inference analysis using MCMC shows sub-11% agreement with simulation data, with notable degeneracies, particularly between $\Omega_\mathrm{m}$ and $\sigma_8$. We further compute the stable clustering crossing scale across redshifts $z=0$, $0.5$, and $1$, assessing its dependence on cosmology. Among the tested power spectrum modeling approaches, we find that the CSSTEmu emulator provides the most accurate predictions, with deviations below 5% for $r  10$ $h^{-1}$Mpc at $z=0.5$. Our results are validated using independent simulation suites, demonstrating that our framework offers a robust method for extracting cosmological constraints from upcoming peculiar velocity data.
                
                
                    <strong>Published:</strong> 2025-09-18T17:59:30Z
                    <a href="http://arxiv.org/abs/2509.15223v1" target="_blank">ðŸ“„ View Paper</a>
                
            
        
            
                Two Web Toolkits for Multimodal Piano Performance Dataset Acquisition and Fingering Annotation
                
                    <strong>Authors:</strong> Junhyung Park, Yonghyun Kim, Joonhyung Bae, Kirak Kim, Taegyun Kwon, Alexander Lerch, Juhan Nam
                
                
                    <strong>Abstract:</strong> Piano performance is a multimodal activity that intrinsically combines physical actions with the acoustic rendition. Despite growing research interest in analyzing the multimodal nature of piano performance, the laborious process of acquiring large-scale multimodal data remains a significant bottleneck, hindering further progress in this field. To overcome this barrier, we present an integrated web toolkit comprising two graphical user interfaces (GUIs): (i) PiaRec, which supports the synchronized acquisition of audio, video, MIDI, and performance metadata. (ii) ASDF, which enables the efficient annotation of performer fingering from the visual data. Collectively, this system can streamline the acquisition of multimodal piano performance datasets.
                
                
                    <strong>Published:</strong> 2025-09-18T17:59:24Z
                    <a href="http://arxiv.org/abs/2509.15222v1" target="_blank">ðŸ“„ View Paper</a>
                
            
        
            
                ScaleCUA: Scaling Open-Source Computer Use Agents with Cross-Platform Data
                
                    <strong>Authors:</strong> Zhaoyang Liu, JingJing Xie, Zichen Ding, Zehao Li, Bowen Yang, Zhenyu Wu, Xuehui Wang, Qiushi Sun, Shi Liu, Weiyun Wang, Shenglong Ye, Qingyun Li, Zeyue Tian, Gen Luo, Xiangyu Yue, Biqing Qi, Kai Chen, Bowen Zhou, Yu Qiao, Qifeng Chen, Wenhai Wang
                
                
                    <strong>Abstract:</strong> Vision-Language Models (VLMs) have enabled computer use agents (CUAs) that operate GUIs autonomously, showing great potential, yet progress is limited by the lack of large-scale, open-source computer use data and foundation models. In this work, we introduce ScaleCUA, a step toward scaling open-source CUAs. It offers a large-scale dataset spanning 6 operating systems and 3 task domains, built via a closed-loop pipeline uniting automated agents with human experts. Trained on this scaled-up data, ScaleCUA can operate seamlessly across platforms. Specifically, it delivers strong gains over baselines (+26.6 on WebArena-Lite-v2, +10.7 on ScreenSpot-Pro) and sets new state-of-the-art results (94.4% on MMBench-GUI L1-Hard, 60.6% on OSWorld-G, 47.4% on WebArena-Lite-v2). These findings underscore the power of data-driven scaling for general-purpose computer use agents. We will release data, models, and code to advance future research: https://github.com/OpenGVLab/ScaleCUA.
                
                
                    <strong>Published:</strong> 2025-09-18T17:59:22Z
                    <a href="http://arxiv.org/abs/2509.15221v1" target="_blank">ðŸ“„ View Paper</a>
                
            
        
            
                Lightweight and Accurate Multi-View Stereo with Confidence-Aware Diffusion Model
                
                    <strong>Authors:</strong> Fangjinhua Wang, Qingshan Xu, Yew-Soon Ong, Marc Pollefeys
                
                
                    <strong>Abstract:</strong> To reconstruct the 3D geometry from calibrated images, learning-based multi-view stereo (MVS) methods typically perform multi-view depth estimation and then fuse depth maps into a mesh or point cloud. To improve the computational efficiency, many methods initialize a coarse depth map and then gradually refine it in higher resolutions. Recently, diffusion models achieve great success in generation tasks. Starting from a random noise, diffusion models gradually recover the sample with an iterative denoising process. In this paper, we propose a novel MVS framework, which introduces diffusion models in MVS. Specifically, we formulate depth refinement as a conditional diffusion process. Considering the discriminative characteristic of depth estimation, we design a condition encoder to guide the diffusion process. To improve efficiency, we propose a novel diffusion network combining lightweight 2D U-Net and convolutional GRU. Moreover, we propose a novel confidence-based sampling strategy to adaptively sample depth hypotheses based on the confidence estimated by diffusion model. Based on our novel MVS framework, we propose two novel MVS methods, DiffMVS and CasDiffMVS. DiffMVS achieves competitive performance with state-of-the-art efficiency in run-time and GPU memory. CasDiffMVS achieves state-of-the-art performance on DTU, Tanks  Temples and ETH3D. Code is available at: https://github.com/cvg/diffmvs.
                
                
                    <strong>Published:</strong> 2025-09-18T17:59:19Z
                    <a href="http://dx.doi.org/10.1109/TPAMI.2025.3597148" target="_blank">ðŸ“„ View Paper</a>
                
            
        
            
                LNE-Blocking: An Efficient Framework for Contamination Mitigation Evaluation on Large Language Models
                
                    <strong>Authors:</strong> Ruijie Hou, Yueyang Jiao, Hanxu Hu, Yingming Li, Wai Lam, Huajian Zhang, Hongyuan Lu
                
                
                    <strong>Abstract:</strong> The problem of data contamination is now almost inevitable during the development of large language models (LLMs), with the training data commonly integrating those evaluation benchmarks even unintentionally. This problem subsequently makes it hard to benchmark LLMs fairly. Instead of constructing contamination-free datasets (quite hard), we propose a novel framework, \textbf{LNE-Blocking}, to restore model performance prior to contamination on potentially leaked datasets. Our framework consists of two components: contamination detection and disruption operation. For the prompt, the framework first uses the contamination detection method, \textbf{LNE}, to assess the extent of contamination in the model. Based on this, it adjusts the intensity of the disruption operation, \textbf{Blocking}, to elicit non-memorized responses from the model. Our framework is the first to efficiently restore the models greedy decoding performance. This comes with a strong performance on multiple datasets with potential leakage risks, and it consistently achieves stable recovery results across different models and varying levels of data contamination. We release the code at https://github.com/RuijieH/LNE-Blocking to facilitate research.
                
                
                    <strong>Published:</strong> 2025-09-18T17:59:16Z
                    <a href="http://arxiv.org/abs/2509.15218v1" target="_blank">ðŸ“„ View Paper</a>
                
            
        
            
                Out-of-Sight Trajectories: Tracking, Fusion, and Prediction
                
                    <strong>Authors:</strong> Haichao Zhang, Yi Xu, Yun Fu
                
                
                    <strong>Abstract:</strong> Trajectory prediction is a critical task in computer vision and autonomous systems, playing a key role in autonomous driving, robotics, surveillance, and virtual reality. Existing methods often rely on complete and noise-free observational data, overlooking the challenges associated with out-of-sight objects and the inherent noise in sensor data caused by limited camera coverage, obstructions, and the absence of ground truth for denoised trajectories. These limitations pose safety risks and hinder reliable prediction in real-world scenarios. In this extended work, we present advancements in Out-of-Sight Trajectory (OST), a novel task that predicts the noise-free visual trajectories of out-of-sight objects using noisy sensor data. Building on our previous research, we broaden the scope of Out-of-Sight Trajectory Prediction (OOSTraj) to include pedestrians and vehicles, extending its applicability to autonomous driving, robotics, surveillance, and virtual reality. Our enhanced Vision-Positioning Denoising Module leverages camera calibration to establish a vision-positioning mapping, addressing the lack of visual references, while effectively denoising noisy sensor data in an unsupervised manner. Through extensive evaluations on the Vi-Fi and JRDB datasets, our approach achieves state-of-the-art performance in both trajectory denoising and prediction, significantly surpassing previous baselines. Additionally, we introduce comparisons with traditional denoising methods, such as Kalman filtering, and adapt recent trajectory prediction models to our task, providing a comprehensive benchmark. This work represents the first initiative to integrate vision-positioning projection for denoising noisy sensor trajectories of out-of-sight agents, paving the way for future advances. The code and preprocessed datasets are available at github.com/Hai-chao-Zhang/OST
                
                
                    <strong>Published:</strong> 2025-09-18T17:59:16Z
                    <a href="http://arxiv.org/abs/2509.15219v1" target="_blank">ðŸ“„ View Paper</a>
                
            
        
            
                Generalizable Geometric Image Caption Synthesis
                
                    <strong>Authors:</strong> Yue Xin, Wenyuan Wang, Rui Pan, Ruida Wang, Howard Meng, Renjie Pi, Shizhe Diao, Tong Zhang
                
                
                    <strong>Abstract:</strong> Multimodal large language models have various practical applications that demand strong reasoning abilities. Despite recent advancements, these models still struggle to solve complex geometric problems. A key challenge stems from the lack of high-quality image-text pair datasets for understanding geometric images. Furthermore, most template-based data synthesis pipelines typically fail to generalize to questions beyond their predefined templates. In this paper, we bridge this gap by introducing a complementary process of Reinforcement Learning with Verifiable Rewards (RLVR) into the data generation pipeline. By adopting RLVR to refine captions for geometric images synthesized from 50 basic geometric relations and using reward signals derived from mathematical problem-solving tasks, our pipeline successfully captures the key features of geometry problem-solving. This enables better task generalization and yields non-trivial improvements. Furthermore, even in out-of-distribution scenarios, the generated dataset enhances the general reasoning capabilities of multimodal large language models, yielding accuracy improvements of $2.8\%\text{-}4.8\%$ in statistics, arithmetic, algebraic, and numerical tasks with non-geometric input images of MathVista and MathVerse, along with $2.4\%\text{-}3.9\%$ improvements in Art, Design, Tech, and Engineering tasks in MMMU.
                
                
                    <strong>Published:</strong> 2025-09-18T17:59:11Z
                    <a href="http://arxiv.org/abs/2509.15217v1" target="_blank">ðŸ“„ View Paper</a>
                
            
        
    
    
        <p><em>Generated by AI News Agent</em></p>
    

