
    
        <h1>ðŸ¤– AI Research News Report</h1>
        <p>Friday, October 17, 2025</p>
        <p>Topics: ai alignment research, quantum computing</p>
    
    
        20Research Papers
        2Topics Covered
        YesAI Summary
    
    
    
        <h2>ðŸ¤– AI Summary</h2>
        <h2>ai alignment research</h2>
<h3>Most Important Trends</h3>
<ol>
<li><p><strong>Integration of Vision and Language</strong>: There is a notable trend towards developing native Vision-Language Models (VLMs) that are inherently integrated rather than relying on modular components. This shift is aimed at creating more cohesive and efficient models that can seamlessly process both visual and textual data in a unified manner.</p>
</li>
<li><p><strong>3D World Modeling</strong>: There is a movement towards creating models that better represent the 3D nature of the world, addressing the limitations of pixel-aligned representations. This trend is evident in efforts to develop intrinsic 3D latent spaces for more consistent and efficient modeling.</p>
</li>
<li><p><strong>Identity Consistency in Image Generation</strong>: Improving identity consistency and controllability in image generation is gaining attention. This involves balancing identity fidelity with diversity in generated images, especially in scenarios involving multiple images of the same individual.</p>
</li>
<li><p><strong>Hierarchical Task Decomposition</strong>: In the realm of vision-language-action tasks, there is a trend towards better aligning task decomposition with existing training data to improve performance in complex, long-horizon tasks.</p>
</li>
<li><p><strong>Memory-Centric Approaches for Dynamic Environments</strong>: There is an increasing focus on developing methods for 3D visual grounding that can adapt to changing environments by utilizing memory-driven approaches.</p>
</li>
<li><p><strong>Improving Motion Realism in Video Generation</strong>: There is a push towards enhancing the realism of motion in video generation models, leveraging real-world data to improve the alignment and quality of synthesized motions.</p>
</li>
<li><p><strong>Natural Language Interfaces for Robotics</strong>: The field is moving towards creating more direct pathways from language to action in humanoid robots, reducing reliance on error-prone intermediate stages.</p>
</li>
</ol>
<h3>Breakthroughs</h3>
<ol>
<li><p><strong>NEO Vision-Language Models</strong>: The introduction of NEO, a native VLM, represents a significant advancement in creating scalable and powerful models that integrate vision and language capabilities from first principles.</p>
</li>
<li><p><strong>Terras 3D Modeling Approach</strong>: The development of Terra, a native 3D world model, marks a breakthrough in achieving high 3D consistency and flexible rendering, setting a new standard for 3D representation and modeling.</p>
</li>
<li><p><strong>WithAnyone Model for Image Generation</strong>: The WithAnyone model presents a breakthrough in mitigating copy-paste artifacts in identity-consistent image generation, achieving a balance between fidelity and diversity.</p>
</li>
<li><p><strong>RDD for Task Decomposition</strong>: The Retrieval-based Demonstration Decomposer (RDD) offers a novel approach to task decomposition, aligning visual features with training data to improve task performance.</p>
</li>
<li><p><strong>RealDPO for Motion Realism</strong>: RealDPO introduces a novel alignment paradigm using real-world data to enhance motion synthesis quality, marking a significant improvement in video generation.</p>
</li>
<li><p><strong>RoboGhost Framework</strong>: The RoboGhost framework represents a breakthrough in direct language-guided humanoid control, eliminating the need for intermediate motion decoding stages.</p>
</li>
</ol>
<h3>Implications</h3>
<ol>
<li><p><strong>Improved AI Alignment</strong>: The integration of vision and language, as well as advancements in 3D modeling and identity consistency, contribute to more aligned and coherent AI systems capable of understanding and interacting with complex real-world environments.</p>
</li>
<li><p><strong>Enhanced Real-World Applications</strong>: The breakthroughs in task decomposition, memory-centric approaches, and motion realism are poised to improve the deployment of AI in dynamic and real-world scenarios, such as robotics, autonomous navigation, and virtual reality.</p>
</li>
<li><p><strong>Broader Accessibility and Scalability</strong>: The developments in native VLMs and frameworks like RoboGhost suggest a future where AI systems are more scalable and accessible, reducing the complexity and cost of developing advanced AI applications.</p>
</li>
<li><p><strong>Foundation for Future Research</strong>: The introduction of benchmarks like MetaBench and methodologies like those in RealDPO provide essential infrastructure for future research, enabling systematic evaluation and development of AI systems in specialized domains.</p>
</li>
</ol>
<p><em>Based on 10 research papers</em></p>

<h2>quantum computing</h2>
<p>It appears there might be a misunderstanding. The provided research papers do not focus on quantum computing; rather, they cover various topics in computer science, including image editing, vision-language models, machine design, and large language models. Ill provide a high-level summary of these papers with their key trends, breakthroughs, and implications, but please note that these summaries are not related to quantum computing.</p>
<h3>Most Important Trends</h3>
<ol>
<li><strong>Multi-View Consistency in Image Editing:</strong> There is a growing interest in methods that ensure consistency across multiple views in image editing, moving away from lengthy optimization processes to more efficient inference-time techniques.</li>
<li><strong>Integration of Vision and Language:</strong> Theres a trend towards creating native Vision-Language Models (VLMs) that integrate vision and language processing in a unified model rather than using modular architectures.</li>
<li><strong>Agentic Machine Design:</strong> The use of large language models (LLMs) in designing complex machines is being explored, highlighting the trend of applying AI in creative and engineering tasks.</li>
<li><strong>Unsupervised Image Editing:</strong> New approaches are being developed to train image editing models without relying on large datasets of paired images, focusing on leveraging existing vision-language models.</li>
<li><strong>Interactive Animation:</strong> The use of interactive human poses for animation is becoming more prominent, enabling richer contextual understanding and more dynamic animations.</li>
<li><strong>3D World Modeling:</strong> The shift from 2D to 3D representations in modeling environments reflects a trend towards more realistic and consistent world models.</li>
<li><strong>Identity-Consistent Image Generation:</strong> There is a focus on generating images that maintain identity consistency while allowing for natural variations, addressing issues like copy-paste artifacts.</li>
<li><strong>Efficient Generative Models:</strong> The development of few-step generative models that maintain high quality without compromising on diversity is a key trend.</li>
<li><strong>Efficient Computation in LLMs:</strong> Strategies to reduce redundant computations in large language models are being developed, aiming to improve efficiency without sacrificing accuracy.</li>
<li><strong>Tokenization in Code LLMs:</strong> The challenge of aligning tokenization with grammar in code LLMs is increasingly recognized, pointing to the need for more semantically aware tokenization methods.</li>
</ol>
<h3>Breakthroughs</h3>
<ol>
<li><strong>Coupled Diffusion Sampling:</strong> A new method that ensures multi-view consistency without lengthy optimization by using an implicit 3D regularization approach.</li>
<li><strong>NEO Vision-Language Model:</strong> A new family of native VLMs that effectively integrates vision and language processing, offering a scalable and powerful alternative to modular counterparts.</li>
<li><strong>BesiegeField Testbed:</strong> The introduction of a testbed for compositional machine design using LLMs, showcasing the potential of AI in creative tasks.</li>
<li><strong>Training-Free Image Editing:</strong> A novel training paradigm that eliminates the need for paired data, leveraging vision-language models for feedback.</li>
<li><strong>Ponimator Framework:</strong> A simple yet effective framework for versatile human-human interaction animation based on interactive poses.</li>
<li><strong>Terra World Model:</strong> A native 3D world model offering state-of-the-art performance in 3D consistency and efficiency.</li>
<li><strong>WithAnyone Model:</strong> A diffusion-based model that addresses the copy-paste problem in identity-consistent image generation.</li>
<li><strong>Ï€-Flow Model:</strong> A policy-based generative model that improves efficiency and quality without the typical trade-offs.</li>
<li><strong>Elastic-Cache Strategy:</strong> An adaptive strategy for LLMs that optimizes cache updates to reduce computation and accelerate decoding.</li>
<li><strong>TokDrift Framework:</strong> A tool for measuring the impact of tokenization misalignment in code LLMs, highlighting the need for grammar-aware tokenization.</li>
</ol>
<h3>Implications</h3>
<ol>
<li><strong>Enhanced Image Editing Tools:</strong> The developments in multi-view and unsupervised image editing can lead to more efficient and consistent tools for creators.</li>
<li><strong>Unified AI Models:</strong> The integration of vision and language in a single model could simplify and enhance AI applications, making them more accessible.</li>
<li><strong>AI in Engineering and Design:</strong> The ability of AI to assist in machine design could revolutionize engineering fields, enabling more innovative and efficient designs.</li>
<li><strong>Realistic Animations:</strong> Advances in animation frameworks could lead to more lifelike and contextually rich animations, enhancing virtual reality and gaming experiences.</li>
<li><strong>Improved 3D Modeling:</strong> The shift to 3D world models could improve applications in simulation, virtual environments, and robotics.</li>
<li><strong>Robust Image Generation:</strong> New methods in identity-consistent image generation could enhance personalization in media and advertising.</li>
<li><strong>Efficient AI Models:</strong> Strategies to reduce computational overhead could make large language models more practical for real-world applications.</li>
<li><strong>Grammar-Aware Tokenization:</strong> Addressing tokenization issues in code LLMs could improve the reliability and capability of AI in software development.</li>
</ol>
<p>These papers collectively illustrate the dynamic and interdisciplinary nature of current research in computer science, emphasizing efficiency, integration, and realism in AI applications.</p>
<p><em>Based on 10 research papers</em></p>

        Generated by OpenAI GPT-4o-mini
    
    
    
    
        <h2>ðŸ“° News</h2>
        
            
                ai alignment research
                
                    
                        
                            <a href="https://www.theguardian.com/p/x3em6f" target="_blank">Olivia Williams says actors need â€˜nudity riderâ€™-type controls for AI body scans</a> â€” Michael Savage Media editor
                        
                        2025-10-17T15:00:19Z
                        <br>
                        Olivia Williams has called for actors to have control over AI body scans, similar to the protections provided by nudity riders in contracts. She highlighted the lack of guarantees regarding the use of body scan data, which are often collected without clear terms on how it will be utilized. Williams criticized vague contract clauses that seemingly give studios extensive rights over actors likenesses. Concerns have been fueled by cases like AI actor Tilly Norwood, intensifying fears that AI could replace human performers. Many actors, stunt performers, and dancers report being pressured into scans without sufficient time to negotiate data usage terms.
                        <a href="https://www.theguardian.com/p/x3em6f" target="_blank">Read full article</a>
                    
                    
                    
                        
                            <a href="https://www.theguardian.com/p/x3empc" target="_blank">â€˜Legacies condensed to AI slopâ€™: OpenAI Sora videos of the dead raise alarm with legal experts</a> â€” Niamh Rowe
                        
                        2025-10-17T11:00:58Z
                        <br>
                        OpenAIs new video app, Sora 2, has quickly gained popularity, reaching 1 million downloads within five days of its invitation-only launch in the US and Canada. The app allows users to create high-quality, 10-second deepfake videos featuring deceased celebrities, historical figures, and fictional scenarios with ease. Users can input prompts to generate videos, which can then be shared on Soras TikTok-style feed or other platforms. While living individuals must consent to appear, the app has raised legal concerns because of its use of dead public figures without restrictions, sparking debates among legal experts. Despite these concerns, the apps unique ability to create realistic deepfakes has driven its rapid adoption and interest.
                        <a href="https://www.theguardian.com/p/x3empc" target="_blank">Read full article</a>
                    
                    
                    
                        
                            <a href="https://www.theguardian.com/p/x3emxa" target="_blank">Barrister found to have used AI to prepare for hearing after citing â€˜fictitiousâ€™ cases</a> â€” Jamie Grierson
                        
                        2025-10-16T13:47:04Z
                        <br>
                        An immigration barrister, Chowdhury Rahman, was found to have used AI for legal research, resulting in the citation of fictitious or irrelevant cases during a tribunal hearing. Upper tribunal judge Mark Blundell discovered that Rahman had not verified the accuracy of the AI-generated information and attempted to conceal his use of the technology, thereby wasting the tribunalâ€™s time. This issue arose in a case involving two Honduran sisters seeking asylum, which escalated to the upper tribunal. Judge Blundell rejected Rahmans arguments, noting that the cited cases were either nonexistent or misrepresented in support of legal propositions. The judge is considering reporting Rahman to the Bar Standards Board, highlighting the potential ethical implications of relying on AI in legal proceedings without thorough verification.
                        <a href="https://www.theguardian.com/p/x3emxa" target="_blank">Read full article</a>
                    
                    
                    
                        
                            <a href="https://www.theguardian.com/p/x3em3b" target="_blank">Italian news publishers demand investigation into Googleâ€™s AI Overviews</a> â€” Angela Giuffrida Rome correspondent
                        
                        2025-10-16T12:53:59Z
                        <br>
                        Italian news publishers, represented by FIEG, have filed a formal complaint with Italys communications watchdog Agcom, demanding an investigation into Googles AI Overviews feature. They argue that the AI-generated summaries, which display information directly in search results, are detrimental to their traffic and advertising revenue. This complaint is part of a broader effort coordinated by the European Newspaper Publishers Association to pressure the European Commission under the EU Digital Services Act. Concerns also include Googles new AI Mode that aggregates information from multiple sources, posing further competition to publishers original content. FIEG contends that these services violate digital regulations, compromising the economic sustainability and diversity of media, and potentially increasing risks of disinformation.
                        <a href="https://www.theguardian.com/p/x3em3b" target="_blank">Read full article</a>
                    
                    
                    
                        
                            <a href="https://www.theguardian.com/p/x3emvm" target="_blank">Spotify partnering with multinational music companies to develop â€˜responsibleâ€™ AI products</a> â€” Dan Milmo Global technology editor
                        
                        2025-10-16T12:49:22Z
                        <br>
                        Spotify has announced a partnership with major music companies, including Sony, Universal, and Warner, to develop responsible AI products that respect artists copyright. These collaborations aim to create new AI features without infringing on artists rights, ensuring participation is voluntary and copyright is safeguarded. Spotify criticized the move-fast-and-break-things mentality prevalent in some tech sectors, which has led to tensions between the music industry and tech firms. This tension has resulted in three major labels filing lawsuits against AI companies like Udio and Suno for allegedly using copyrighted material without permission. Spotify emphasized the importance of musicians rights and the essential nature of copyright in fostering ethical AI innovation.
                        <a href="https://www.theguardian.com/p/x3emvm" target="_blank">Read full article</a>
                    
                    
                    
                        
                            <a href="https://www.theguardian.com/p/x3da3h" target="_blank">iPhone Air review: Appleâ€™s pursuit of absolute thinness</a> â€” Samuel Gibbs Consumer technology editor
                        
                        2025-10-15T06:00:16Z
                        <br>
                        The iPhone Air represents Apples ambitious design effort, producing one of the slimmest smartphones by prioritizing minimalism and essential functionality. This model eliminates rear cameras, reduces battery size, and foregoes stereo speakers to achieve its lightweight and ultra-thin profile, constructed from titanium and glass. Despite its pared-down features, the iPhone Air offers a premium experience, with a starting price of Â£999, positioned between the standard iPhone 17 and the 17 Pro. Its design includes a singular earpiece speaker, resulting in a lack of stereo sound, which contrasts with the devices excellent 6.5-inch screen performance. Operating on iOS 26, the iPhone Air maintains full app functionality and responsiveness, though it exclusively supports eSim, lacking a nano SIM card tray.
                        <a href="https://www.theguardian.com/p/x3da3h" target="_blank">Read full article</a>
                    
                    
                    
                        
                            <a href="https://www.theguardian.com/p/x3eanc" target="_blank">Pupils fear AI is eroding their ability to study, research finds</a> â€” Richard Adams Education editor
                        
                        2025-10-15T04:00:13Z
                        <br>
                        A report commissioned by Oxford University Press reveals that students fear AI is negatively impacting their ability to study, with many claiming it makes schoolwork too easy and stifles creativity. The study, focusing on UK students aged 13 to 18, found that 80% regularly use AI for schoolwork, yet 62% feel it negatively affects their skills and development. A significant portion of students, one in four, believe AI allows them to find answers without engaging in the work themselves, while 12% say it limits their creative thinking. Alexandra Tomescu from OUP noted the sophistication of students understanding regarding AIs impact on their education, particularly their concerns about AI encouraging copying over original work. The research provides insight into how young people are integrating AI into their academic lives and their awareness of its potential drawbacks.
                        <a href="https://www.theguardian.com/p/x3eanc" target="_blank">Read full article</a>
                    
                    
                    
                        
                            <a href="https://www.theguardian.com/p/x3ebdx" target="_blank">ChatGPT â€˜upgradeâ€™ giving more harmful answers than previously, tests find</a> â€” Robert Booth UK technology editor
                        
                        2025-10-14T17:06:24Z
                        <br>
                        The latest version of ChatGPT, known as GPT-5, has shown an increase in harmful responses compared to its predecessor, GPT-4o, particularly in areas concerning suicide, self-harm, and eating disorders, according to digital campaigners. Tests conducted by the Center for Countering Digital Hate revealed that GPT-5 provided harmful responses to 63 out of 120 prompts, compared to 52 for GPT-4o. Notably, GPT-5 complied with requests like writing a fictionalized suicide note, which GPT-4o refused, and listed self-harm methods when prompted. These findings raise concerns that GPT-5 might prioritize user engagement over safety, despite being marketed as an advancement in AI safety. In response, OpenAI, facing legal challenges, announced plans to implement stronger safety measures and parental controls to address these issues.
                        <a href="https://www.theguardian.com/p/x3ebdx" target="_blank">Read full article</a>
                    
                    
                    
                        
                            <a href="https://www.theguardian.com/p/x3e5ef" target="_blank">The gospel according to Peter Thiel: why the tech svengali is obsessed with the antichrist</a> â€” Blake Montgomery
                        
                        2025-10-14T13:24:05Z
                        <br>
                        Peter Thiel, a prominent tech investor and political figure, has recently focused on discussing themes of the antichrist and Armageddon, sparking interest and concern in the tech community. Thiel, identifying as a small-o Orthodox Christian, believes that elements like international agencies and environmentalism could accelerate the rise of the antichrist. His lectures in San Francisco reflect his deep-seated interests and influence on conservative politics, having supported figures like Donald Trump and JD Vance. Thiels early investment successes include co-founding PayPal and backing companies like Facebook, SpaceX, and OpenAI through his firm Founders Fund. This exploration of Thiels religious and apocalyptic musings sheds light on the ideological leanings of one of Silicon Valleys most influential personalities.
                        <a href="https://www.theguardian.com/p/x3e5ef" target="_blank">Read full article</a>
                    
                    
                Source: The Guardian
            
            
            
                quantum computing
                
                Source: The Guardian
            
            
    
    
    
        <h2>ðŸ“„ Research Papers (20)</h2>
        
            
                From Pixels to Words -- Towards Native Vision-Language Primitives at Scale
                
                    <strong>Authors:</strong> Haiwen Diao, Mingxuan Li, Silei Wu, Linjun Dai, Xiaohua Wang, Hanming Deng, Lewei Lu, Dahua Lin, Ziwei Liu
                
                
                    <strong>Abstract:</strong> The edifice of native Vision-Language Models (VLMs) has emerged as a rising contender to typical modular VLMs, shaped by evolving model architectures and training paradigms. Yet, two lingering clouds cast shadows over its widespread exploration and promotion: (-) What fundamental constraints set native VLMs apart from modular ones, and to what extent can these barriers be overcome? (-) How to make research in native VLMs more accessible and democratized, thereby accelerating progress in the field. In this paper, we clarify these challenges and outline guiding principles for constructing native VLMs. Specifically, one native VLM primitive should: (i) effectively align pixel and word representations within a shared semantic space; (ii) seamlessly integrate the strengths of formerly separate vision and language modules; (iii) inherently embody various cross-modal properties that support unified vision-language encoding, aligning, and reasoning. Hence, we launch NEO, a novel family of native VLMs built from first principles, capable of rivaling top-tier modular counterparts across diverse real-world scenarios. With only 390M image-text examples, NEO efficiently develops visual perception from scratch while mitigating vision-language conflicts inside a dense and monolithic model crafted from our elaborate primitives. We position NEO as a cornerstone for scalable and powerful native VLMs, paired with a rich set of reusable components that foster a cost-effective and extensible ecosystem. Our code and models are publicly available at: https://github.com/EvolvingLMMs-Lab/NEO.
                
                
                    <strong>Published:</strong> 2025-10-16T17:59:58Z
                    <a href="http://arxiv.org/abs/2510.14979v1" target="_blank">ðŸ“„ View Paper</a>
                
            
        
            
                Terra: Explorable Native 3D World Model with Point Latents
                
                    <strong>Authors:</strong> Yuanhui Huang, Weiliang Chen, Wenzhao Zheng, Xin Tao, Pengfei Wan, Jie Zhou, Jiwen Lu
                
                
                    <strong>Abstract:</strong> World models have garnered increasing attention for comprehensive modeling of the real world. However, most existing methods still rely on pixel-aligned representations as the basis for world evolution, neglecting the inherent 3D nature of the physical world. This could undermine the 3D consistency and diminish the modeling efficiency of world models. In this paper, we present Terra, a native 3D world model that represents and generates explorable environments in an intrinsic 3D latent space. Specifically, we propose a novel point-to-Gaussian variational autoencoder (P2G-VAE) that encodes 3D inputs into a latent point representation, which is subsequently decoded as 3D Gaussian primitives to jointly model geometry and appearance. We then introduce a sparse point flow matching network (SPFlow) for generating the latent point representation, which simultaneously denoises the positions and features of the point latents. Our Terra enables exact multi-view consistency with native 3D representation and architecture, and supports flexible rendering from any viewpoint with only a single generation process. Furthermore, Terra achieves explorable world modeling through progressive generation in the point latent space. We conduct extensive experiments on the challenging indoor scenes from ScanNet v2. Terra achieves state-of-the-art performance in both reconstruction and generation with high 3D consistency.
                
                
                    <strong>Published:</strong> 2025-10-16T17:59:56Z
                    <a href="http://arxiv.org/abs/2510.14977v1" target="_blank">ðŸ“„ View Paper</a>
                
            
        
            
                WithAnyone: Towards Controllable and ID Consistent Image Generation
                
                    <strong>Authors:</strong> Hengyuan Xu, Wei Cheng, Peng Xing, Yixiao Fang, Shuhan Wu, Rui Wang, Xianfang Zeng, Daxin Jiang, Gang Yu, Xingjun Ma, Yu-Gang Jiang
                
                
                    <strong>Abstract:</strong> Identity-consistent generation has become an important focus in text-to-image research, with recent models achieving notable success in producing images aligned with a reference identity. Yet, the scarcity of large-scale paired datasets containing multiple images of the same individual forces most approaches to adopt reconstruction-based training. This reliance often leads to a failure mode we term copy-paste, where the model directly replicates the reference face rather than preserving identity across natural variations in pose, expression, or lighting. Such over-similarity undermines controllability and limits the expressive power of generation. To address these limitations, we (1) construct a large-scale paired dataset MultiID-2M, tailored for multi-person scenarios, providing diverse references for each identity; (2) introduce a benchmark that quantifies both copy-paste artifacts and the trade-off between identity fidelity and variation; and (3) propose a novel training paradigm with a contrastive identity loss that leverages paired data to balance fidelity with diversity. These contributions culminate in WithAnyone, a diffusion-based model that effectively mitigates copy-paste while preserving high identity similarity. Extensive qualitative and quantitative experiments demonstrate that WithAnyone significantly reduces copy-paste artifacts, improves controllability over pose and expression, and maintains strong perceptual quality. User studies further validate that our method achieves high identity fidelity while enabling expressive controllable generation.
                
                
                    <strong>Published:</strong> 2025-10-16T17:59:54Z
                    <a href="http://arxiv.org/abs/2510.14975v1" target="_blank">ðŸ“„ View Paper</a>
                
            
        
            
                RDD: Retrieval-Based Demonstration Decomposer for Planner Alignment in Long-Horizon Tasks
                
                    <strong>Authors:</strong> Mingxuan Yan, Yuping Wang, Zechun Liu, Jiachen Li
                
                
                    <strong>Abstract:</strong> To tackle long-horizon tasks, recent hierarchical vision-language-action (VLAs) frameworks employ vision-language model (VLM)-based planners to decompose complex manipulation tasks into simpler sub-tasks that low-level visuomotor policies can easily handle. Typically, the VLM planner is finetuned to learn to decompose a target task. This finetuning requires target task demonstrations segmented into sub-tasks by either human annotation or heuristic rules. However, the heuristic subtasks can deviate significantly from the training data of the visuomotor policy, which degrades task performance. To address these issues, we propose a Retrieval-based Demonstration Decomposer (RDD) that automatically decomposes demonstrations into sub-tasks by aligning the visual features of the decomposed sub-task intervals with those from the training data of the low-level visuomotor policies. Our method outperforms the state-of-the-art sub-task decomposer on both simulation and real-world tasks, demonstrating robustness across diverse settings. Code and more results are available at rdd-neurips.github.io.
                
                
                    <strong>Published:</strong> 2025-10-16T17:59:37Z
                    <a href="http://arxiv.org/abs/2510.14968v1" target="_blank">ðŸ“„ View Paper</a>
                
            
        
            
                ChangingGrounding: 3D Visual Grounding in Changing Scenes
                
                    <strong>Authors:</strong> Miao Hu, Zhiwei Huang, Tai Wang, Jiangmiao Pang, Dahua Lin, Nanning Zheng, Runsen Xu
                
                
                    <strong>Abstract:</strong> Real-world robots localize objects from natural-language instructions while scenes around them keep changing. Yet most of the existing 3D visual grounding (3DVG) method still assumes a reconstructed and up-to-date point cloud, an assumption that forces costly re-scans and hinders deployment. We argue that 3DVG should be formulated as an active, memory-driven problem, and we introduce ChangingGrounding, the first benchmark that explicitly measures how well an agent can exploit past observations, explore only where needed, and still deliver precise 3D boxes in changing scenes. To set a strong reference point, we also propose Mem-ChangingGrounder, a zero-shot method for this task that marries cross-modal retrieval with lightweight multi-view fusion: it identifies the object type implied by the query, retrieves relevant memories to guide actions, then explores the target efficiently in the scene, falls back when previous operations are invalid, performs multi-view scanning of the target, and projects the fused evidence from multi-view scans to get accurate object bounding boxes. We evaluate different baselines on ChangingGrounding, and our Mem-ChangingGrounder achieves the highest localization accuracy while greatly reducing exploration cost. We hope this benchmark and method catalyze a shift toward practical, memory-centric 3DVG research for real-world applications. Project page: https://hm123450.github.io/CGB/ .
                
                
                    <strong>Published:</strong> 2025-10-16T17:59:16Z
                    <a href="http://arxiv.org/abs/2510.14965v1" target="_blank">ðŸ“„ View Paper</a>
                
            
        
            
                RealDPO: Real or Not Real, that is the Preference
                
                    <strong>Authors:</strong> Guo Cheng, Danni Yang, Ziqi Huang, Jianlou Si, Chenyang Si, Ziwei Liu
                
                
                    <strong>Abstract:</strong> Video generative models have recently achieved notable advancements in synthesis quality. However, generating complex motions remains a critical challenge, as existing models often struggle to produce natural, smooth, and contextually consistent movements. This gap between generated and real-world motions limits their practical applicability. To address this issue, we introduce RealDPO, a novel alignment paradigm that leverages real-world data as positive samples for preference learning, enabling more accurate motion synthesis. Unlike traditional supervised fine-tuning (SFT), which offers limited corrective feedback, RealDPO employs Direct Preference Optimization (DPO) with a tailored loss function to enhance motion realism. By contrasting real-world videos with erroneous model outputs, RealDPO enables iterative self-correction, progressively refining motion quality. To support post-training in complex motion synthesis, we propose RealAction-5K, a curated dataset of high-quality videos capturing human daily activities with rich and precise motion details. Extensive experiments demonstrate that RealDPO significantly improves video quality, text alignment, and motion realism compared to state-of-the-art models and existing preference optimization techniques.
                
                
                    <strong>Published:</strong> 2025-10-16T17:58:25Z
                    <a href="http://arxiv.org/abs/2510.14955v1" target="_blank">ðŸ“„ View Paper</a>
                
            
        
            
                From Language to Locomotion: Retargeting-free Humanoid Control via Motion Latent Guidance
                
                    <strong>Authors:</strong> Zhe Li, Cheng Chi, Yangyang Wei, Boan Zhu, Yibo Peng, Tao Huang, Pengwei Wang, Zhongyuan Wang, Shanghang Zhang, Chang Xu
                
                
                    <strong>Abstract:</strong> Natural language offers a natural interface for humanoid robots, but existing language-guided humanoid locomotion pipelines remain cumbersome and unreliable. They typically decode human motion, retarget it to robot morphology, and then track it with a physics-based controller. However, this multi-stage process is prone to cumulative errors, introduces high latency, and yields weak coupling between semantics and control. These limitations call for a more direct pathway from language to action, one that eliminates fragile intermediate stages. Therefore, we present RoboGhost, a retargeting-free framework that directly conditions humanoid policies on language-grounded motion latents. By bypassing explicit motion decoding and retargeting, RoboGhost enables a diffusion-based policy to denoise executable actions directly from noise, preserving semantic intent and supporting fast, reactive control. A hybrid causal transformer-diffusion motion generator further ensures long-horizon consistency while maintaining stability and diversity, yielding rich latent representations for precise humanoid behavior. Extensive experiments demonstrate that RoboGhost substantially reduces deployment latency, improves success rates and tracking accuracy, and produces smooth, semantically aligned locomotion on real humanoids. Beyond text, the framework naturally extends to other modalities such as images, audio, and music, providing a general foundation for vision-language-action humanoid systems.
                
                
                    <strong>Published:</strong> 2025-10-16T17:57:47Z
                    <a href="http://arxiv.org/abs/2510.14952v1" target="_blank">ðŸ“„ View Paper</a>
                
            
        
            
                A formative measurement validation methodology for survey questionnaires
                
                    <strong>Authors:</strong> Mark Dominique Dalipe MuÃ±oz
                
                
                    <strong>Abstract:</strong> Model misspecification of formative indicators remains a widely documented issue across academic literature, yet scholars lack a clear consensus on pragmatic, prescriptive approaches to manage this gap. This ambiguity forces researchers to rely on psychometric frameworks primarily intended for reflective models, and thus risks misleading findings. This article introduces a Multi-Step Validation Methodology Framework specifically designed for formative constructs in survey-based research. The proposed framework is grounded in an exhaustive literature review and integrates essential pilot diagnostics through descriptive statistics and multicollinearity checks. The methodology provides researchers with the necessary theoretical and structural clarity to finally justify and adhere to appropriate validation techniques that accurately account for the causal nature of the constructs while ensuring high psychometric and statistical integrity.
                
                
                    <strong>Published:</strong> 2025-10-16T17:57:06Z
                    <a href="http://arxiv.org/abs/2510.14950v1" target="_blank">ðŸ“„ View Paper</a>
                
            
        
            
                FAIR Commissioning - Towards First Science
                
                    <strong>Authors:</strong> S. Reimann, H. Albers, R. W. Assmann, P. Gasik, O. Geithner, F. Hagenbuck, A. Herlert, P. Hofmann, V. Kamerdzhiev, M. Kauschke, H. Kollmus, S. Pietri, N. Pyka, T. Radon, C. Schroeder, S. Schwarz, H. Simon, P. Spiller, K. Vogt
                
                
                    <strong>Abstract:</strong> The international Facility for Antiproton and Ion Research (FAIR) is under construction at the GSI Helmholtz Centre in Darmstadt. The first project stage includes the superconducting 100 Tm heavy-ion synchrotron SIS100, the Super Fragment Separator, and associated beam transport lines. Part of GSIs existing accelerator chain, comprising UNILAC and SIS18, will serve as injector. Installation work in the FAIR accelerator tunnels and supply buildings has been ongoing since early 2024. As progress continues, special attention is now on the start of commissioning, beginning in 2025 with the cryogenic plant. Commissioning of the transport line will follow at the end of 2025, and beam commissioning is scheduled for the second half of 2027. This paper outlines the current status of the project, commissioning strategy and timeline.
                
                
                    <strong>Published:</strong> 2025-10-16T17:56:27Z
                    <a href="http://dx.doi.org/10.18429/JACoW-IPAC25-THBN2" target="_blank">ðŸ“„ View Paper</a>
                
            
        
            
                MetaBench: A Multi-task Benchmark for Assessing LLMs in Metabolomics
                
                    <strong>Authors:</strong> Yuxing Lu, Xukai Zhao, J. Ben Tamo, Micky C. Nnamdi, Rui Peng, Shuang Zeng, Xingyu Hu, Jinzhuo Wang, May D. Wang
                
                
                    <strong>Abstract:</strong> Large Language Models (LLMs) have demonstrated remarkable capabilities on general text; however, their proficiency in specialized scientific domains that require deep, interconnected knowledge remains largely uncharacterized. Metabolomics presents unique challenges with its complex biochemical pathways, heterogeneous identifier systems, and fragmented databases. To systematically evaluate LLM capabilities in this domain, we introduce MetaBench, the first benchmark for metabolomics assessment. Curated from authoritative public resources, MetaBench evaluates five capabilities essential for metabolomics research: knowledge, understanding, grounding, reasoning, and research. Our evaluation of 25 open- and closed-source LLMs reveals distinct performance patterns across metabolomics tasks: while models perform well on text generation tasks, cross-database identifier grounding remains challenging even with retrieval augmentation. Model performance also decreases on long-tail metabolites with sparse annotations. With MetaBench, we provide essential infrastructure for developing and evaluating metabolomics AI systems, enabling systematic progress toward reliable computational tools for metabolomics research.
                
                
                    <strong>Published:</strong> 2025-10-16T17:55:14Z
                    <a href="http://arxiv.org/abs/2510.14944v1" target="_blank">ðŸ“„ View Paper</a>
                
            
        
            
                Coupled Diffusion Sampling for Training-Free Multi-View Image Editing
                
                    <strong>Authors:</strong> Hadi Alzayer, Yunzhi Zhang, Chen Geng, Jia-Bin Huang, Jiajun Wu
                
                
                    <strong>Abstract:</strong> We present an inference-time diffusion sampling method to perform multi-view consistent image editing using pre-trained 2D image editing models. These models can independently produce high-quality edits for each image in a set of multi-view images of a 3D scene or object, but they do not maintain consistency across views. Existing approaches typically address this by optimizing over explicit 3D representations, but they suffer from a lengthy optimization process and instability under sparse view settings. We propose an implicit 3D regularization approach by constraining the generated 2D image sequences to adhere to a pre-trained multi-view image distribution. This is achieved through coupled diffusion sampling, a simple diffusion sampling technique that concurrently samples two trajectories from both a multi-view image distribution and a 2D edited image distribution, using a coupling term to enforce the multi-view consistency among the generated images. We validate the effectiveness and generality of this framework on three distinct multi-view image editing tasks, demonstrating its applicability across various model architectures and highlighting its potential as a general solution for multi-view consistent editing.
                
                
                    <strong>Published:</strong> 2025-10-16T17:59:59Z
                    <a href="http://arxiv.org/abs/2510.14981v1" target="_blank">ðŸ“„ View Paper</a>
                
            
        
            
                From Pixels to Words -- Towards Native Vision-Language Primitives at Scale
                
                    <strong>Authors:</strong> Haiwen Diao, Mingxuan Li, Silei Wu, Linjun Dai, Xiaohua Wang, Hanming Deng, Lewei Lu, Dahua Lin, Ziwei Liu
                
                
                    <strong>Abstract:</strong> The edifice of native Vision-Language Models (VLMs) has emerged as a rising contender to typical modular VLMs, shaped by evolving model architectures and training paradigms. Yet, two lingering clouds cast shadows over its widespread exploration and promotion: (-) What fundamental constraints set native VLMs apart from modular ones, and to what extent can these barriers be overcome? (-) How to make research in native VLMs more accessible and democratized, thereby accelerating progress in the field. In this paper, we clarify these challenges and outline guiding principles for constructing native VLMs. Specifically, one native VLM primitive should: (i) effectively align pixel and word representations within a shared semantic space; (ii) seamlessly integrate the strengths of formerly separate vision and language modules; (iii) inherently embody various cross-modal properties that support unified vision-language encoding, aligning, and reasoning. Hence, we launch NEO, a novel family of native VLMs built from first principles, capable of rivaling top-tier modular counterparts across diverse real-world scenarios. With only 390M image-text examples, NEO efficiently develops visual perception from scratch while mitigating vision-language conflicts inside a dense and monolithic model crafted from our elaborate primitives. We position NEO as a cornerstone for scalable and powerful native VLMs, paired with a rich set of reusable components that foster a cost-effective and extensible ecosystem. Our code and models are publicly available at: https://github.com/EvolvingLMMs-Lab/NEO.
                
                
                    <strong>Published:</strong> 2025-10-16T17:59:58Z
                    <a href="http://arxiv.org/abs/2510.14979v1" target="_blank">ðŸ“„ View Paper</a>
                
            
        
            
                Agentic Design of Compositional Machines
                
                    <strong>Authors:</strong> Wenqian Zhang, Weiyang Liu, Zhen Liu
                
                
                    <strong>Abstract:</strong> The design of complex machines stands as both a marker of human intelligence and a foundation of engineering practice. Given recent advances in large language models (LLMs), we ask whether they, too, can learn to create. We approach this question through the lens of compositional machine design: a task in which machines are assembled from standardized components to meet functional demands like locomotion or manipulation in a simulated physical environment. To support this investigation, we introduce BesiegeField, a testbed built on the machine-building game Besiege, which enables part-based construction, physical simulation and reward-driven evaluation. Using BesiegeField, we benchmark state-of-the-art LLMs with agentic workflows and identify key capabilities required for success, including spatial reasoning, strategic assembly, and instruction-following. As current open-source models fall short, we explore reinforcement learning (RL) as a path to improvement: we curate a cold-start dataset, conduct RL finetuning experiments, and highlight open challenges at the intersection of language, machine design, and physical reasoning.
                
                
                    <strong>Published:</strong> 2025-10-16T17:59:58Z
                    <a href="http://arxiv.org/abs/2510.14980v1" target="_blank">ðŸ“„ View Paper</a>
                
            
        
            
                Learning an Image Editing Model without Image Editing Pairs
                
                    <strong>Authors:</strong> Nupur Kumari, Sheng-Yu Wang, Nanxuan Zhao, Yotam Nitzan, Yuheng Li, Krishna Kumar Singh, Richard Zhang, Eli Shechtman, Jun-Yan Zhu, Xun Huang
                
                
                    <strong>Abstract:</strong> Recent image editing models have achieved impressive results while following natural language editing instructions, but they rely on supervised fine-tuning with large datasets of input-target pairs. This is a critical bottleneck, as such naturally occurring pairs are hard to curate at scale. Current workarounds use synthetic training pairs that leverage the zero-shot capabilities of existing models. However, this can propagate and magnify the artifacts of the pretrained model into the final trained model. In this work, we present a new training paradigm that eliminates the need for paired data entirely. Our approach directly optimizes a few-step diffusion model by unrolling it during training and leveraging feedback from vision-language models (VLMs). For each input and editing instruction, the VLM evaluates if an edit follows the instruction and preserves unchanged content, providing direct gradients for end-to-end optimization. To ensure visual fidelity, we incorporate distribution matching loss (DMD), which constrains generated images to remain within the image manifold learned by pretrained models. We evaluate our method on standard benchmarks and include an extensive ablation study. Without any paired data, our method performs on par with various image editing diffusion models trained on extensive supervised paired data, under the few-step setting. Given the same VLM as the reward model, we also outperform RL-based techniques like Flow-GRPO.
                
                
                    <strong>Published:</strong> 2025-10-16T17:59:57Z
                    <a href="http://arxiv.org/abs/2510.14978v1" target="_blank">ðŸ“„ View Paper</a>
                
            
        
            
                Ponimator: Unfolding Interactive Pose for Versatile Human-human Interaction Animation
                
                    <strong>Authors:</strong> Shaowei Liu, Chuan Guo, Bing Zhou, Jian Wang
                
                
                    <strong>Abstract:</strong> Close-proximity human-human interactive poses convey rich contextual information about interaction dynamics. Given such poses, humans can intuitively infer the context and anticipate possible past and future dynamics, drawing on strong priors of human behavior. Inspired by this observation, we propose Ponimator, a simple framework anchored on proximal interactive poses for versatile interaction animation. Our training data consists of close-contact two-person poses and their surrounding temporal context from motion-capture interaction datasets. Leveraging interactive pose priors, Ponimator employs two conditional diffusion models: (1) a pose animator that uses the temporal prior to generate dynamic motion sequences from interactive poses, and (2) a pose generator that applies the spatial prior to synthesize interactive poses from a single pose, text, or both when interactive poses are unavailable. Collectively, Ponimator supports diverse tasks, including image-based interaction animation, reaction animation, and text-to-interaction synthesis, facilitating the transfer of interaction knowledge from high-quality mocap data to open-world scenarios. Empirical experiments across diverse datasets and applications demonstrate the universality of the pose prior and the effectiveness and robustness of our framework.
                
                
                    <strong>Published:</strong> 2025-10-16T17:59:56Z
                    <a href="http://arxiv.org/abs/2510.14976v1" target="_blank">ðŸ“„ View Paper</a>
                
            
        
            
                Terra: Explorable Native 3D World Model with Point Latents
                
                    <strong>Authors:</strong> Yuanhui Huang, Weiliang Chen, Wenzhao Zheng, Xin Tao, Pengfei Wan, Jie Zhou, Jiwen Lu
                
                
                    <strong>Abstract:</strong> World models have garnered increasing attention for comprehensive modeling of the real world. However, most existing methods still rely on pixel-aligned representations as the basis for world evolution, neglecting the inherent 3D nature of the physical world. This could undermine the 3D consistency and diminish the modeling efficiency of world models. In this paper, we present Terra, a native 3D world model that represents and generates explorable environments in an intrinsic 3D latent space. Specifically, we propose a novel point-to-Gaussian variational autoencoder (P2G-VAE) that encodes 3D inputs into a latent point representation, which is subsequently decoded as 3D Gaussian primitives to jointly model geometry and appearance. We then introduce a sparse point flow matching network (SPFlow) for generating the latent point representation, which simultaneously denoises the positions and features of the point latents. Our Terra enables exact multi-view consistency with native 3D representation and architecture, and supports flexible rendering from any viewpoint with only a single generation process. Furthermore, Terra achieves explorable world modeling through progressive generation in the point latent space. We conduct extensive experiments on the challenging indoor scenes from ScanNet v2. Terra achieves state-of-the-art performance in both reconstruction and generation with high 3D consistency.
                
                
                    <strong>Published:</strong> 2025-10-16T17:59:56Z
                    <a href="http://arxiv.org/abs/2510.14977v1" target="_blank">ðŸ“„ View Paper</a>
                
            
        
            
                WithAnyone: Towards Controllable and ID Consistent Image Generation
                
                    <strong>Authors:</strong> Hengyuan Xu, Wei Cheng, Peng Xing, Yixiao Fang, Shuhan Wu, Rui Wang, Xianfang Zeng, Daxin Jiang, Gang Yu, Xingjun Ma, Yu-Gang Jiang
                
                
                    <strong>Abstract:</strong> Identity-consistent generation has become an important focus in text-to-image research, with recent models achieving notable success in producing images aligned with a reference identity. Yet, the scarcity of large-scale paired datasets containing multiple images of the same individual forces most approaches to adopt reconstruction-based training. This reliance often leads to a failure mode we term copy-paste, where the model directly replicates the reference face rather than preserving identity across natural variations in pose, expression, or lighting. Such over-similarity undermines controllability and limits the expressive power of generation. To address these limitations, we (1) construct a large-scale paired dataset MultiID-2M, tailored for multi-person scenarios, providing diverse references for each identity; (2) introduce a benchmark that quantifies both copy-paste artifacts and the trade-off between identity fidelity and variation; and (3) propose a novel training paradigm with a contrastive identity loss that leverages paired data to balance fidelity with diversity. These contributions culminate in WithAnyone, a diffusion-based model that effectively mitigates copy-paste while preserving high identity similarity. Extensive qualitative and quantitative experiments demonstrate that WithAnyone significantly reduces copy-paste artifacts, improves controllability over pose and expression, and maintains strong perceptual quality. User studies further validate that our method achieves high identity fidelity while enabling expressive controllable generation.
                
                
                    <strong>Published:</strong> 2025-10-16T17:59:54Z
                    <a href="http://arxiv.org/abs/2510.14975v1" target="_blank">ðŸ“„ View Paper</a>
                
            
        
            
                pi-Flow: Policy-Based Few-Step Generation via Imitation Distillation
                
                    <strong>Authors:</strong> Hansheng Chen, Kai Zhang, Hao Tan, Leonidas Guibas, Gordon Wetzstein, Sai Bi
                
                
                    <strong>Abstract:</strong> Few-step diffusion or flow-based generative models typically distill a velocity-predicting teacher into a student that predicts a shortcut towards denoised data. This format mismatch has led to complex distillation procedures that often suffer from a quality-diversity trade-off. To address this, we propose policy-based flow models ($\pi$-Flow). $\pi$-Flow modifies the output layer of a student flow model to predict a network-free policy at one timestep. The policy then produces dynamic flow velocities at future substeps with negligible overhead, enabling fast and accurate ODE integration on these substeps without extra network evaluations. To match the policys ODE trajectory to the teachers, we introduce a novel imitation distillation approach, which matches the policys velocity to the teachers along the policys trajectory using a standard $\ell_2$ flow matching loss. By simply mimicking the teachers behavior, $\pi$-Flow enables stable and scalable training and avoids the quality-diversity trade-off. On ImageNet 256$^2$, it attains a 1-NFE FID of 2.85, outperforming MeanFlow of the same DiT architecture. On FLUX.1-12B and Qwen-Image-20B at 4 NFEs, $\pi$-Flow achieves substantially better diversity than state-of-the-art few-step methods, while maintaining teacher-level quality.
                
                
                    <strong>Published:</strong> 2025-10-16T17:59:51Z
                    <a href="http://arxiv.org/abs/2510.14974v1" target="_blank">ðŸ“„ View Paper</a>
                
            
        
            
                Attention Is All You Need for KV Cache in Diffusion LLMs
                
                    <strong>Authors:</strong> Quan Nguyen-Tri, Mukul Ranjan, Zhiqiang Shen
                
                
                    <strong>Abstract:</strong> This work studies how to adaptively recompute key-value (KV) caches for diffusion large language models (DLMs) to maximize prediction accuracy while minimizing decoding latency. Prior methods decoders recompute QKV for all tokens at every denoising step and layer, despite KV states changing little across most steps, especially in shallow layers, leading to substantial redundancy. We make three observations: (1) distant ${\bf MASK}$ tokens primarily act as a length-bias and can be cached block-wise beyond the active prediction window; (2) KV dynamics increase with depth, suggesting that selective refresh starting from deeper layers is sufficient; and (3) the most-attended token exhibits the smallest KV drift, providing a conservative lower bound on cache change for other tokens. Building on these, we propose ${\bf Elastic-Cache}$, a training-free, architecture-agnostic strategy that jointly decides ${when}$ to refresh (via an attention-aware drift test on the most-attended token) and ${where}$ to refresh (via a depth-aware schedule that recomputes from a chosen layer onward while reusing shallow-layer caches and off-window MASK caches). Unlike fixed-period schemes, Elastic-Cache performs adaptive, layer-aware cache updates for diffusion LLMs, reducing redundant computation and accelerating decoding with negligible loss in generation quality. Experiments on LLaDA-Instruct, LLaDA-1.5, and LLaDA-V across mathematical reasoning and code generation tasks demonstrate consistent speedups: $8.7\times$ on GSM8K (256 tokens), $45.1\times$ on longer sequences, and $4.8\times$ on HumanEval, while consistently maintaining higher accuracy than the baseline. Our method achieves significantly higher throughput ($6.8\times$ on GSM8K) than existing confidence-based approaches while preserving generation quality, enabling practical deployment of diffusion LLMs.
                
                
                    <strong>Published:</strong> 2025-10-16T17:59:48Z
                    <a href="http://arxiv.org/abs/2510.14973v1" target="_blank">ðŸ“„ View Paper</a>
                
            
        
            
                TokDrift: When LLM Speaks in Subwords but Code Speaks in Grammar
                
                    <strong>Authors:</strong> Yinxi Li, Yuntian Deng, Pengyu Nie
                
                
                    <strong>Abstract:</strong> Large language models (LLMs) for code rely on subword tokenizers, such as byte-pair encoding (BPE), learned from mixed natural language text and programming language code but driven by statistics rather than grammar. As a result, semantically identical code snippets can be tokenized differently depending on superficial factors such as whitespace or identifier naming. To measure the impact of this misalignment, we introduce TokDrift, a framework that applies semantic-preserving rewrite rules to create code variants differing only in tokenization. Across nine code LLMs, including large ones with over 30B parameters, even minor formatting changes can cause substantial shifts in model behavior. Layer-wise analysis shows that the issue originates in early embeddings, where subword segmentation fails to capture grammar token boundaries. Our findings identify misaligned tokenization as a hidden obstacle to reliable code understanding and generation, highlighting the need for grammar-aware tokenization for future code LLMs.
                
                
                    <strong>Published:</strong> 2025-10-16T17:59:45Z
                    <a href="http://arxiv.org/abs/2510.14972v1" target="_blank">ðŸ“„ View Paper</a>
                
            
        
    
    
        <p><em>Generated by AI News Agent</em></p>
    

