
    
        <h1>🤖 AI Research News Report</h1>
        <p>Monday, September 29, 2025</p>
        <p>Topics: ai alignment research, quantum computing</p>
    
    
        20Research Papers
        2Topics Covered
        YesAI Summary
    
    
    
        <h2>🤖 AI Summary</h2>
        <h2>ai alignment research</h2>
<p>Here is a high-level summary of the research papers focused on AI alignment research, broken down into trends, breakthroughs, and implications:</p>
<h3>Most Important Trends</h3>
<ol>
<li><p><strong>Comprehensive Evaluation Frameworks</strong>: There is a growing trend towards developing comprehensive benchmarks and evaluation frameworks for AI systems. Papers like VoiceAssistant-Eval and DeeptraceReward indicate a move towards evaluating AI assistants and video generation models on a wide array of tasks, highlighting the importance of multi-faceted assessment.</p>
</li>
<li><p><strong>Human Perception and AI</strong>: The integration of human perception into AI model training and evaluation is becoming more pronounced. Understanding how humans perceive AI-generated artifacts, as seen in the study on AI-generated videos, is critical for aligning AI outputs with human expectations and trust.</p>
</li>
<li><p><strong>Interactive and Real-time AI Systems</strong>: There is a noticeable trend towards real-time and interactive AI systems, as exemplified by LongLive. This reflects an increasing demand for AI systems that can interact dynamically and responsively with users.</p>
</li>
<li><p><strong>Data Curation and Efficiency</strong>: Efficient data curation for training AI models is becoming crucial, as discussed in LABELING COPILOT, emphasizing the need for optimized and scalable tools to handle large datasets.</p>
</li>
<li><p><strong>Enhanced Learning Mechanisms</strong>: Papers like IA2 and Language Models Can Learn from Verbal Feedback Without Scalar Rewards highlight a trend towards improving learning mechanisms in AI, moving beyond traditional supervised fine-tuning by incorporating more nuanced feedback and internal computations.</p>
</li>
</ol>
<h3>Breakthroughs</h3>
<ol>
<li><p><strong>VoiceAssistant-Eval</strong>: This benchmark provides a detailed evaluation framework for AI assistants, revealing that smaller, well-designed models can compete with larger ones, and highlighting persistent challenges in multimodal input handling.</p>
</li>
<li><p><strong>DeeptraceReward</strong>: This benchmark advances the ability to detect deepfake traces by training models that mimic human judgment, significantly outperforming existing models in identifying and explaining fake video content.</p>
</li>
<li><p><strong>WoW</strong>: By grounding AI models in real-world interactions, WoW demonstrates state-of-the-art performance in physical consistency and causal reasoning, showing the potential of embodied interaction for improving AIs understanding of physical dynamics.</p>
</li>
<li><p><strong>Feedback-Conditional Policy (FCP)</strong>: This approach reframes feedback-driven learning, allowing language models to learn directly from verbal feedback without reducing it to scalar rewards, offering a more expressive learning method.</p>
</li>
<li><p><strong>Voting-Bloc Entropy (VBE)</strong>: This new metric for measuring DAO decentralization provides a rigorous framework to evaluate voter alignment and suggests practical ways to enhance decentralization in DAOs.</p>
</li>
</ol>
<h3>Implications</h3>
<ol>
<li><p><strong>Improved AI Evaluation Standards</strong>: The development of comprehensive benchmarks like VoiceAssistant-Eval and DeeptraceReward will lead to higher standards in evaluating AI systems, ensuring they meet complex, multifaceted human expectations.</p>
</li>
<li><p><strong>AI Trust and Safety</strong>: Understanding human perception of AI outputs, particularly in detecting deepfakes, is crucial for developing trustworthy AI systems and mitigating risks associated with AI-generated content.</p>
</li>
<li><p><strong>Real-Time AI Interactions</strong>: Advances in frameworks like LongLive suggest that AI systems will increasingly support dynamic, real-time interactions, enhancing user engagement and satisfaction.</p>
</li>
<li><p><strong>Efficient Data Management</strong>: Tools like LABELING COPILOT will streamline data curation processes, enabling more efficient and cost-effective training of robust AI models, which is essential for scaling AI applications.</p>
</li>
<li><p><strong>Enhanced Learning Dynamics</strong>: Innovations in learning methods, such as IA2 and FCP, will improve the adaptability and effectiveness of AI models, allowing them to better align with human preferences and produce more nuanced outputs.</p>
</li>
<li><p><strong>Decentralization in Governance</strong>: The introduction of VBE offers a more structured approach to evaluating and enhancing decentralization in DAOs, potentially leading to more equitable and diverse participation in decentralized governance.</p>
</li>
</ol>
<p><em>Based on 10 research papers</em></p>

<h2>quantum computing</h2>
<p>While the provided papers are not directly related to quantum computing, they showcase advancements in AI, machine learning, and robotics, which may indirectly influence the development of quantum computing technologies. Here is a high-level summary of the trends, breakthroughs, and implications found in these papers:</p>
<h3>Most Important Trends</h3>
<ol>
<li><p><strong>Multimodal and Multitask AI Systems</strong>: The development of benchmarks like VoiceAssistant-Eval highlights the trend towards evaluating AI systems on multiple modalities (audio, visual, textual) and tasks, suggesting a push for more holistic AI capabilities.</p>
</li>
<li><p><strong>Training-Free Frameworks</strong>: The emergence of training-free frameworks such as See, Point, Fly (SPF) for navigation and RefAM for video segmentation indicates a shift towards leveraging existing models without additional training for specific tasks, emphasizing efficiency and versatility.</p>
</li>
<li><p><strong>Language-Conditioned Robotics</strong>: DAWNs diffusion-based framework for robotic control reflects an increasing focus on using language-driven models to enhance robotic interactions, bridging high-level intents with low-level actions through innovative representations.</p>
</li>
</ol>
<h3>Breakthroughs</h3>
<ol>
<li><p><strong>Comprehensive Evaluation Frameworks</strong>: VoiceAssistant-Eval provides a robust framework for assessing AI assistants, revealing that proprietary models do not always outperform open-source counterparts and highlighting the importance of specialized evaluation metrics.</p>
</li>
<li><p><strong>Diffusion Models for Robot Control</strong>: DAWNs success in robotic manipulation tasks using diffusion models represents a significant advancement, showcasing how these models can manage complex motion tasks and generalize well to real-world environments.</p>
</li>
<li><p><strong>Learning-Free Aerial Navigation</strong>: SPFs ability to outperform existing methods without training underscores the potential of vision-language models in autonomous navigation, setting new standards in efficiency and adaptability.</p>
</li>
</ol>
<h3>Implications</h3>
<ol>
<li><p><strong>AI and Quantum Computing Synergy</strong>: The advancements in AI capabilities, such as multitask processing and efficient model utilization, could inform quantum computing development, particularly in optimizing quantum algorithms and simulations.</p>
</li>
<li><p><strong>Enhanced Human-Machine Interaction</strong>: Tools like VoiceAssistant-Eval and DAWN improve how machines understand and act upon human inputs, which may translate into more intuitive quantum computing interfaces.</p>
</li>
<li><p><strong>Scalable AI Solutions</strong>: The trend towards scalable and adaptable AI solutions, as seen in SPF and RefAM, suggests pathways for quantum computing to adopt similar strategies, potentially leading to more widespread adoption and application of quantum technologies.</p>
</li>
</ol>
<p>In summary, while these papers do not directly address quantum computing, the innovations in AI and machine learning they describe may influence quantum computing through improvements in computational techniques, interface design, and algorithmic efficiency.</p>
<p><em>Based on 10 research papers</em></p>

        Generated by OpenAI GPT-4o-mini
    
    
    
    
        <h2>📰 News</h2>
        
            
                ai alignment research
                
                    
                        
                            <a href="https://www.theguardian.com/p/x37gz3" target="_blank">Why I gave the world wide web away for free | Tim Berners-Lee</a> — Tim Berners-Lee
                        
                        2025-09-28T11:00:18Z
                        <br>
                        Tim Berners-Lee, at 34, conceived the idea for the world wide web by integrating the internet with hypertext to facilitate global creativity and collaboration. Despite initial skepticism from his Cern bosses, he was eventually allowed to develop the concept, resulting in a platform where anything could eventually be available. To ensure universal access and participation, Berners-Lee advocated for the web to be free, successfully convincing Cern to release its intellectual property into the public domain in 1993. This decision made the web accessible to everyone, fostering its widespread adoption. However, Berners-Lee reflects on the current state of the web, noting that parts of it are no longer truly free due to data exploitation by major platforms.
                        <a href="https://www.theguardian.com/p/x37gz3" target="_blank">Read full article</a>
                    
                    
                    
                        
                            <a href="https://www.theguardian.com/p/x3af9c" target="_blank">Cute fluffy characters and Egyptian selfies: Meta launches AI feed Vibes</a> — Dan Milmo Global technology editor
                        
                        2025-09-26T17:00:18Z
                        <br>
                        Meta, led by Mark Zuckerberg, has launched Vibes, a new feed on the Meta AI app featuring videos entirely generated by artificial intelligence. This development highlights the growing presence of AI-generated content, often criticized as slop, in social media feeds. Notably, nearly 10% of the fastest-growing YouTube channels focus exclusively on AI content. The Vibes feed, showcasing expressive videos from AI artists and creators, will be available in over 40 countries, excluding the UK. Zuckerberg shared an AI-generated video as an example, sparking mixed reactions, with some users criticizing it as contributing to the AI content clutter.
                        <a href="https://www.theguardian.com/p/x3af9c" target="_blank">Read full article</a>
                    
                    
                Source: The Guardian
            
            
            
                quantum computing
                
                    
                        
                            <a href="https://www.theguardian.com/p/x39ydp" target="_blank">Murdoch, Ellison and China: what we know about the US’s TikTok deal</a> — Dara Kerr and agencies
                        
                        2025-09-26T00:03:28Z
                        <br>
                        The White House has confirmed a deal for US investors to take over ownership of TikToks operations in the United States. An executive order signed by Donald Trump has created a framework for this transition, with Oracle set to license TikToks recommendation algorithm, leveraging its existing management of US user data. Trump discussed the deal with Chinese President Xi Jinping, who reportedly approved it following a meeting between leaders from both governments in Madrid. This move comes after a year of uncertainty surrounding TikToks future in the US, sparked by legislation that would ban the app unless sold to a US entity. The involvement of notable figures such as the Murdoch family and Oracles Larry Ellison highlights the deals significance in maintaining TikToks presence in the US market.
                        <a href="https://www.theguardian.com/p/x39ydp" target="_blank">Read full article</a>
                    
                    
                Source: The Guardian
            
            
    
    
    
        <h2>📄 Research Papers (20)</h2>
        
            
                VoiceAssistant-Eval: Benchmarking AI Assistants across Listening, Speaking, and Viewing
                
                    <strong>Authors:</strong> Ke Wang, Houxing Ren, Zimu Lu, Mingjie Zhan, Hongsheng Li
                
                
                    <strong>Abstract:</strong> The growing capabilities of large language models and multimodal systems have spurred interest in voice-first AI assistants, yet existing benchmarks are inadequate for evaluating the full range of these systems capabilities. We introduce VoiceAssistant-Eval, a comprehensive benchmark designed to assess AI assistants across listening, speaking, and viewing. VoiceAssistant-Eval comprises 10,497 curated examples spanning 13 task categories. These tasks include natural sounds, music, and spoken dialogue for listening; multi-turn dialogue, role-play imitation, and various scenarios for speaking; and highly heterogeneous images for viewing. To demonstrate its utility, we evaluate 21 open-source models and GPT-4o-Audio, measuring the quality of the response content and speech, as well as their consistency. The results reveal three key findings: (1) proprietary models do not universally outperform open-source models; (2) most models excel at speaking tasks but lag in audio understanding; and (3) well-designed smaller models can rival much larger ones. Notably, the mid-sized Step-Audio-2-mini (7B) achieves more than double the listening accuracy of LLaMA-Omni2-32B-Bilingual. However, challenges remain: multimodal (audio plus visual) input and role-play voice imitation tasks are difficult for current models, and significant gaps persist in robustness and safety alignment. VoiceAssistant-Eval identifies these gaps and establishes a rigorous framework for evaluating and guiding the development of next-generation AI assistants. Code and data will be released at https://mathllm.github.io/VoiceAssistantEval/ .
                
                
                    <strong>Published:</strong> 2025-09-26T17:59:59Z
                    <a href="http://arxiv.org/abs/2509.22651v1" target="_blank">📄 View Paper</a>
                
            
        
            
                Learning Human-Perceived Fakeness in AI-Generated Videos via Multimodal LLMs
                
                    <strong>Authors:</strong> Xingyu Fu, Siyi Liu, Yinuo Xu, Pan Lu, Guangqiuse Hu, Tianbo Yang, Taran Anantasagar, Christopher Shen, Yikai Mao, Yuanzhe Liu, Keyush Shah, Chung Un Lee, Yejin Choi, James Zou, Dan Roth, Chris Callison-Burch
                
                
                    <strong>Abstract:</strong> Can humans identify AI-generated (fake) videos and provide grounded reasons? While video generation models have advanced rapidly, a critical dimension -- whether humans can detect deepfake traces within a generated video, i.e., spatiotemporal grounded visual artifacts that reveal a video as machine generated -- has been largely overlooked. We introduce DeeptraceReward, the first fine-grained, spatially- and temporally- aware benchmark that annotates human-perceived fake traces for video generation reward. The dataset comprises 4.3K detailed annotations across 3.3K high-quality generated videos. Each annotation provides a natural-language explanation, pinpoints a bounding-box region containing the perceived trace, and marks precise onset and offset timestamps. We consolidate these annotations into 9 major categories of deepfake traces that lead humans to identify a video as AI-generated, and train multimodal language models (LMs) as reward models to mimic human judgments and localizations. On DeeptraceReward, our 7B reward model outperforms GPT-5 by 34.7% on average across fake clue identification, grounding, and explanation. Interestingly, we observe a consistent difficulty gradient: binary fake v.s. real classification is substantially easier than fine-grained deepfake trace detection; within the latter, performance degrades from natural language explanations (easiest), to spatial grounding, to temporal labeling (hardest). By foregrounding human-perceived deepfake traces, DeeptraceReward provides a rigorous testbed and training signal for socially aware and trustworthy video generation.
                
                
                    <strong>Published:</strong> 2025-09-26T17:59:54Z
                    <a href="http://arxiv.org/abs/2509.22646v1" target="_blank">📄 View Paper</a>
                
            
        
            
                WoW: Towards a World omniscient World model Through Embodied Interaction
                
                    <strong>Authors:</strong> Xiaowei Chi, Peidong Jia, Chun-Kai Fan, Xiaozhu Ju, Weishi Mi, Kevin Zhang, Zhiyuan Qin, Wanxin Tian, Kuangzhi Ge, Hao Li, Zezhong Qian, Anthony Chen, Qiang Zhou, Yueru Jia, Jiaming Liu, Yong Dai, Qingpo Wuwu, Chengyu Bai, Yu-Kai Wang, Ying Li, Lizhang Chen, Yong Bao, Zhiyuan Jiang, Jiacheng Zhu, Kai Tang, Ruichuan An, Yulin Luo, Qiuxuan Feng, Siyuan Zhou, Chi-min Chan, Chengkai Hou, Wei Xue, Sirui Han, Yike Guo, Shanghang Zhang, Jian Tang
                
                
                    <strong>Abstract:</strong> Humans develop an understanding of intuitive physics through active interaction with the world. This approach is in stark contrast to current video models, such as Sora, which rely on passive observation and therefore struggle with grasping physical causality. This observation leads to our central hypothesis: authentic physical intuition of the world model must be grounded in extensive, causally rich interactions with the real world. To test this hypothesis, we present WoW, a 14-billion-parameter generative world model trained on 2 million robot interaction trajectories. Our findings reveal that the models understanding of physics is a probabilistic distribution of plausible outcomes, leading to stochastic instabilities and physical hallucinations. Furthermore, we demonstrate that this emergent capability can be actively constrained toward physical realism by SOPHIA, where vision-language model agents evaluate the DiT-generated output and guide its refinement by iteratively evolving the language instructions. In addition, a co-trained Inverse Dynamics Model translates these refined plans into executable robotic actions, thus closing the imagination-to-action loop. We establish WoWBench, a new benchmark focused on physical consistency and causal reasoning in video, where WoW achieves state-of-the-art performance in both human and autonomous evaluation, demonstrating strong ability in physical causality, collision dynamics, and object permanence. Our work provides systematic evidence that large-scale, real-world interaction is a cornerstone for developing physical intuition in AI. Models, data, and benchmarks will be open-sourced.
                
                
                    <strong>Published:</strong> 2025-09-26T17:59:07Z
                    <a href="http://arxiv.org/abs/2509.22642v1" target="_blank">📄 View Paper</a>
                
            
        
            
                Death of the Novel(ty): Beyond n-Gram Novelty as a Metric for Textual Creativity
                
                    <strong>Authors:</strong> Arkadiy Saakyan, Najoung Kim, Smaranda Muresan, Tuhin Chakrabarty
                
                
                    <strong>Abstract:</strong> N-gram novelty is widely used to evaluate language models ability to generate text outside of their training data. More recently, it has also been adopted as a metric for measuring textual creativity. However, theoretical work on creativity suggests that this approach may be inadequate, as it does not account for creativitys dual nature: novelty (how original the text is) and appropriateness (how sensical and pragmatic it is). We investigate the relationship between this notion of creativity and n-gram novelty through 7542 expert writer annotations (n=26) of novelty, pragmaticality, and sensicality via close reading of human and AI-generated text. We find that while n-gram novelty is positively associated with expert writer-judged creativity, ~91% of top-quartile expressions by n-gram novelty are not judged as creative, cautioning against relying on n-gram novelty alone. Furthermore, unlike human-written text, higher n-gram novelty in open-source LLMs correlates with lower pragmaticality. In an exploratory study with frontier close-source models, we additionally confirm that they are less likely to produce creative expressions than humans. Using our dataset, we test whether zero-shot, few-shot, and finetuned models are able to identify creative expressions (a positive aspect of writing) and non-pragmatic ones (a negative aspect). Overall, frontier LLMs exhibit performance much higher than random but leave room for improvement, especially struggling to identify non-pragmatic expressions. We further find that LLM-as-a-Judge novelty scores from the best-performing model were predictive of expert writer preferences.
                
                
                    <strong>Published:</strong> 2025-09-26T17:59:05Z
                    <a href="http://arxiv.org/abs/2509.22641v1" target="_blank">📄 View Paper</a>
                
            
        
            
                Language Models Can Learn from Verbal Feedback Without Scalar Rewards
                
                    <strong>Authors:</strong> Renjie Luo, Zichen Liu, Xiangyan Liu, Chao Du, Min Lin, Wenhu Chen, Wei Lu, Tianyu Pang
                
                
                    <strong>Abstract:</strong> LLMs are often trained with RL from human or AI feedback, yet such methods typically compress nuanced feedback into scalar rewards, discarding much of their richness and inducing scale imbalance. We propose treating verbal feedback as a conditioning signal. Inspired by language priors in text-to-image generation, which enable novel outputs from unseen prompts, we introduce the feedback-conditional policy (FCP). FCP learns directly from response-feedback pairs, approximating the feedback-conditional posterior through maximum likelihood training on offline data. We further develop an online bootstrapping stage where the policy generates under positive conditions and receives fresh feedback to refine itself. This reframes feedback-driven learning as conditional generation rather than reward optimization, offering a more expressive way for LLMs to directly learn from verbal feedback. Our code is available at https://github.com/sail-sg/feedback-conditional-policy.
                
                
                    <strong>Published:</strong> 2025-09-26T17:58:27Z
                    <a href="http://arxiv.org/abs/2509.22638v1" target="_blank">📄 View Paper</a>
                
            
        
            
                Towards Efficient Online Exploration for Reinforcement Learning with Human Feedback
                
                    <strong>Authors:</strong> Gen Li, Yuling Yan
                
                
                    <strong>Abstract:</strong> Reinforcement learning with human feedback (RLHF), which learns a reward model from human preference data and then optimizes a policy to favor preferred responses, has emerged as a central paradigm for aligning large language models (LLMs) with human preferences. In this paper, we investigate exploration principles for online RLHF, where one seeks to adaptively collect new preference data to refine both the reward model and the policy in a data-efficient manner. By examining existing optimism-based exploration algorithms, we identify a drawback in their sampling protocol: they tend to gather comparisons that fail to reduce the most informative uncertainties in reward differences, and we prove lower bounds showing that such methods can incur linear regret over exponentially long horizons. Motivated by this insight, we propose a new exploration scheme that directs preference queries toward reducing uncertainty in reward differences most relevant to policy improvement. Under a multi-armed bandit model of RLHF, we establish regret bounds of order $T^{(\beta+1)/(\beta+2)}$, where $\beta0$ is a hyperparameter that balances reward maximization against mitigating distribution shift. To our knowledge, this is the first online RLHF algorithm with regret scaling polynomially in all model parameters.
                
                
                    <strong>Published:</strong> 2025-09-26T17:57:17Z
                    <a href="http://arxiv.org/abs/2509.22633v1" target="_blank">📄 View Paper</a>
                
            
        
            
                LABELING COPILOT: A Deep Research Agent for Automated Data Curation in Computer Vision
                
                    <strong>Authors:</strong> Debargha Ganguly, Sumit Kumar, Ishwar Balappanawar, Weicong Chen, Shashank Kambhatla, Srinivasan Iyengar, Shivkumar Kalyanaraman, Ponnurangam Kumaraguru, Vipin Chaudhary
                
                
                    <strong>Abstract:</strong> Curating high-quality, domain-specific datasets is a major bottleneck for deploying robust vision systems, requiring complex trade-offs between data quality, diversity, and cost when researching vast, unlabeled data lakes. We introduce Labeling Copilot, the first data curation deep research agent for computer vision. A central orchestrator agent, powered by a large multimodal language model, uses multi-step reasoning to execute specialized tools across three core capabilities: (1) Calibrated Discovery sources relevant, in-distribution data from large repositories; (2) Controllable Synthesis generates novel data for rare scenarios with robust filtering; and (3) Consensus Annotation produces accurate labels by orchestrating multiple foundation models via a novel consensus mechanism incorporating non-maximum suppression and voting. Our large-scale validation proves the effectiveness of Labeling Copilots components. The Consensus Annotation module excels at object discovery: on the dense COCO dataset, it averages 14.2 candidate proposals per image-nearly double the 7.4 ground-truth objects-achieving a final annotation mAP of 37.1%. On the web-scale Open Images dataset, it navigated extreme class imbalance to discover 903 new bounding box categories, expanding its capability to over 1500 total. Concurrently, our Calibrated Discovery tool, tested at a 10-million sample scale, features an active learning strategy that is up to 40x more computationally efficient than alternatives with equivalent sample efficiency. These experiments validate that an agentic workflow with optimized, scalable tools provides a robust foundation for curating industrial-scale datasets.
                
                
                    <strong>Published:</strong> 2025-09-26T17:55:26Z
                    <a href="http://arxiv.org/abs/2509.22631v1" target="_blank">📄 View Paper</a>
                
            
        
            
                LongLive: Real-time Interactive Long Video Generation
                
                    <strong>Authors:</strong> Shuai Yang, Wei Huang, Ruihang Chu, Yicheng Xiao, Yuyang Zhao, Xianbang Wang, Muyang Li, Enze Xie, Yingcong Chen, Yao Lu, Song Han, Yukang Chen
                
                
                    <strong>Abstract:</strong> We present LongLive, a frame-level autoregressive (AR) framework for real-time and interactive long video generation. Long video generation presents challenges in both efficiency and quality. Diffusion and Diffusion-Forcing models can produce high-quality videos but suffer from low efficiency due to bidirectional attention. Causal attention AR models support KV caching for faster inference, but often degrade in quality on long videos due to memory challenges during long-video training. In addition, beyond static prompt-based generation, interactive capabilities, such as streaming prompt inputs, are critical for dynamic content creation, enabling users to guide narratives in real time. This interactive requirement significantly increases complexity, especially in ensuring visual consistency and semantic coherence during prompt transitions. To address these challenges, LongLive adopts a causal, frame-level AR design that integrates a KV-recache mechanism that refreshes cached states with new prompts for smooth, adherent switches; streaming long tuning to enable long video training and to align training and inference (train-long-test-long); and short window attention paired with a frame-level attention sink, shorten as frame sink, preserving long-range consistency while enabling faster generation. With these key designs, LongLive fine-tunes a 1.3B-parameter short-clip model to minute-long generation in just 32 GPU-days. At inference, LongLive sustains 20.7 FPS on a single NVIDIA H100, achieves strong performance on VBench in both short and long videos. LongLive supports up to 240-second videos on a single H100 GPU. LongLive further supports INT8-quantized inference with only marginal quality loss.
                
                
                    <strong>Published:</strong> 2025-09-26T17:48:24Z
                    <a href="http://arxiv.org/abs/2509.22622v1" target="_blank">📄 View Paper</a>
                
            
        
            
                IA2: Alignment with ICL Activations Improves Supervised Fine-Tuning
                
                    <strong>Authors:</strong> Aayush Mishra, Daniel Khashabi, Anqi Liu
                
                
                    <strong>Abstract:</strong> Supervised Fine-Tuning (SFT) is used to specialize model behavior by training weights to produce intended target responses for queries. In contrast, In-Context Learning (ICL) adapts models during inference with instructions or demonstrations in the prompt. ICL can offer better generalizability and more calibrated responses compared to SFT in data scarce settings, at the cost of more inference compute. In this work, we ask the question: Can ICLs internal computations be used to improve the qualities of SFT? We first show that ICL and SFT produce distinct activation patterns, indicating that the two methods achieve adaptation through different functional mechanisms. Motivated by this observation and to use ICLs rich functionality, we introduce ICL Activation Alignment (IA2), a self-distillation technique which aims to replicate ICLs activation patterns in SFT models and incentivizes ICL-like internal reasoning. Performing IA2 as a priming step before SFT significantly improves the accuracy and calibration of model outputs, as shown by our extensive empirical results on 12 popular benchmarks and 2 model families. This finding is not only practically useful, but also offers a conceptual window into the inner mechanics of model adaptation.
                
                
                    <strong>Published:</strong> 2025-09-26T17:46:32Z
                    <a href="http://arxiv.org/abs/2509.22621v1" target="_blank">📄 View Paper</a>
                
            
        
            
                Voting-Bloc Entropy: A New Metric for DAO Decentralization
                
                    <strong>Authors:</strong> Andrés Fábrega, Amy Zhao, Jay Yu, James Austgen, Sarah Allen, Kushal Babel, Mahimna Kelkar, Ari Juels
                
                
                    <strong>Abstract:</strong> Decentralized Autonomous Organizations (DAOs) use smart contracts to foster communities working toward common goals. Existing definitions of decentralization, however -- the D in DAO -- fall short of capturing the key properties characteristic of diverse and equitable participation. This work proposes a new framework for measuring DAO decentralization called Voting-Bloc Entropy (VBE, pronounced vibe). VBE is based on the idea that voters with closely aligned interests act as a centralizing force and should be modeled as such. VBE formalizes this notion by measuring the similarity of participants utility functions across a set of voting rounds. Unlike prior, ad hoc definitions of decentralization, VBE derives from first principles: We introduce a simple (yet powerful) reinforcement learning-based conceptual model for voting, that in turn implies VBE. We first show VBEs utility as a theoretical tool. We prove a number of results about the (de)centralizing effects of vote delegation, proposal bundling, bribery, etc. that are overlooked in previous notions of DAO decentralization. Our results lead to practical suggestions for enhancing DAO decentralization. We also show how VBE can be used empirically by presenting measurement studies and VBE-based governance experiments. We make the tools we developed for these results available to the community in the form of open-source artifacts in order to facilitate future study of DAO decentralization.
                
                
                    <strong>Published:</strong> 2025-09-26T17:46:07Z
                    <a href="http://arxiv.org/abs/2509.22620v1" target="_blank">📄 View Paper</a>
                
            
        
            
                VoiceAssistant-Eval: Benchmarking AI Assistants across Listening, Speaking, and Viewing
                
                    <strong>Authors:</strong> Ke Wang, Houxing Ren, Zimu Lu, Mingjie Zhan, Hongsheng Li
                
                
                    <strong>Abstract:</strong> The growing capabilities of large language models and multimodal systems have spurred interest in voice-first AI assistants, yet existing benchmarks are inadequate for evaluating the full range of these systems capabilities. We introduce VoiceAssistant-Eval, a comprehensive benchmark designed to assess AI assistants across listening, speaking, and viewing. VoiceAssistant-Eval comprises 10,497 curated examples spanning 13 task categories. These tasks include natural sounds, music, and spoken dialogue for listening; multi-turn dialogue, role-play imitation, and various scenarios for speaking; and highly heterogeneous images for viewing. To demonstrate its utility, we evaluate 21 open-source models and GPT-4o-Audio, measuring the quality of the response content and speech, as well as their consistency. The results reveal three key findings: (1) proprietary models do not universally outperform open-source models; (2) most models excel at speaking tasks but lag in audio understanding; and (3) well-designed smaller models can rival much larger ones. Notably, the mid-sized Step-Audio-2-mini (7B) achieves more than double the listening accuracy of LLaMA-Omni2-32B-Bilingual. However, challenges remain: multimodal (audio plus visual) input and role-play voice imitation tasks are difficult for current models, and significant gaps persist in robustness and safety alignment. VoiceAssistant-Eval identifies these gaps and establishes a rigorous framework for evaluating and guiding the development of next-generation AI assistants. Code and data will be released at https://mathllm.github.io/VoiceAssistantEval/ .
                
                
                    <strong>Published:</strong> 2025-09-26T17:59:59Z
                    <a href="http://arxiv.org/abs/2509.22651v1" target="_blank">📄 View Paper</a>
                
            
        
            
                Pixel Motion Diffusion is What We Need for Robot Control
                
                    <strong>Authors:</strong> E-Ro Nguyen, Yichi Zhang, Kanchana Ranasinghe, Xiang Li, Michael S. Ryoo
                
                
                    <strong>Abstract:</strong> We present DAWN (Diffusion is All We Need for robot control), a unified diffusion-based framework for language-conditioned robotic manipulation that bridges high-level motion intent and low-level robot action via structured pixel motion representation. In DAWN, both the high-level and low-level controllers are modeled as diffusion processes, yielding a fully trainable, end-to-end system with interpretable intermediate motion abstractions. DAWN achieves state-of-the-art results on the challenging CALVIN benchmark, demonstrating strong multi-task performance, and further validates its effectiveness on MetaWorld. Despite the substantial domain gap between simulation and reality and limited real-world data, we demonstrate reliable real-world transfer with only minimal finetuning, illustrating the practical viability of diffusion-based motion abstractions for robotic control. Our results show the effectiveness of combining diffusion modeling with motion-centric representations as a strong baseline for scalable and robust robot learning. Project page: https://nero1342.github.io/DAWN/
                
                
                    <strong>Published:</strong> 2025-09-26T17:59:59Z
                    <a href="http://arxiv.org/abs/2509.22652v1" target="_blank">📄 View Paper</a>
                
            
        
            
                See, Point, Fly: A Learning-Free VLM Framework for Universal Unmanned Aerial Navigation
                
                    <strong>Authors:</strong> Chih Yao Hu, Yang-Sen Lin, Yuna Lee, Chih-Hai Su, Jie-Ying Lee, Shr-Ruei Tsai, Chin-Yang Lin, Kuan-Wen Chen, Tsung-Wei Ke, Yu-Lun Liu
                
                
                    <strong>Abstract:</strong> We present See, Point, Fly (SPF), a training-free aerial vision-and-language navigation (AVLN) framework built atop vision-language models (VLMs). SPF is capable of navigating to any goal based on any type of free-form instructions in any kind of environment. In contrast to existing VLM-based approaches that treat action prediction as a text generation task, our key insight is to consider action prediction for AVLN as a 2D spatial grounding task. SPF harnesses VLMs to decompose vague language instructions into iterative annotation of 2D waypoints on the input image. Along with the predicted traveling distance, SPF transforms predicted 2D waypoints into 3D displacement vectors as action commands for UAVs. Moreover, SPF also adaptively adjusts the traveling distance to facilitate more efficient navigation. Notably, SPF performs navigation in a closed-loop control manner, enabling UAVs to follow dynamic targets in dynamic environments. SPF sets a new state of the art in DRL simulation benchmark, outperforming the previous best method by an absolute margin of 63%. In extensive real-world evaluations, SPF outperforms strong baselines by a large margin. We also conduct comprehensive ablation studies to highlight the effectiveness of our design choice. Lastly, SPF shows remarkable generalization to different VLMs. Project page: https://spf-web.pages.dev
                
                
                    <strong>Published:</strong> 2025-09-26T17:59:59Z
                    <a href="http://arxiv.org/abs/2509.22653v1" target="_blank">📄 View Paper</a>
                
            
        
            
                Schur log-concavity and the quantum Pascal triangle
                
                    <strong>Authors:</strong> Álvaro Gutiérrez, Christian Krattenthaler
                
                
                    <strong>Abstract:</strong> We say a sequence $f_0, f_1, f_2, \ldots$ of symmetric functions is Schur log-concave if $f_n^2 - f_{n-1}f_{n+1}$ is Schur positive for all $n\ge1$. We conjecture that a very general class of sequences of Schur functions satisfies this property, and show it for sequences of Schur functions indexed by partitions with growing first part and column. Our findings are related to work of Lam, Postnikov and Pylyavskyy on Schur positivity, and of Butler, Sagan, and the second author on $q$-log-concavity.
                
                
                    <strong>Published:</strong> 2025-09-26T17:59:57Z
                    <a href="http://arxiv.org/abs/2509.22648v1" target="_blank">📄 View Paper</a>
                
            
        
            
                RefAM: Attention Magnets for Zero-Shot Referral Segmentation
                
                    <strong>Authors:</strong> Anna Kukleva, Enis Simsar, Alessio Tonioni, Muhammad Ferjad Naeem, Federico Tombari, Jan Eric Lenssen, Bernt Schiele
                
                
                    <strong>Abstract:</strong> Most existing approaches to referring segmentation achieve strong performance only through fine-tuning or by composing multiple pre-trained models, often at the cost of additional training and architectural modifications. Meanwhile, large-scale generative diffusion models encode rich semantic information, making them attractive as general-purpose feature extractors. In this work, we introduce a new method that directly exploits features, attention scores, from diffusion transformers for downstream tasks, requiring neither architectural modifications nor additional training. To systematically evaluate these features, we extend benchmarks with vision-language grounding tasks spanning both images and videos. Our key insight is that stop words act as attention magnets: they accumulate surplus attention and can be filtered to reduce noise. Moreover, we identify global attention sinks (GAS) emerging in deeper layers and show that they can be safely suppressed or redirected onto auxiliary tokens, leading to sharper and more accurate grounding maps. We further propose an attention redistribution strategy, where appended stop words partition background activations into smaller clusters, yielding sharper and more localized heatmaps. Building on these findings, we develop RefAM, a simple training-free grounding framework that combines cross-attention maps, GAS handling, and redistribution. Across zero-shot referring image and video segmentation benchmarks, our approach consistently outperforms prior methods, establishing a new state of the art without fine-tuning or additional components.
                
                
                    <strong>Published:</strong> 2025-09-26T17:59:57Z
                    <a href="http://arxiv.org/abs/2509.22650v1" target="_blank">📄 View Paper</a>
                
            
        
            
                CapRL: Stimulating Dense Image Caption Capabilities via Reinforcement Learning
                
                    <strong>Authors:</strong> Long Xing, Xiaoyi Dong, Yuhang Zang, Yuhang Cao, Jianze Liang, Qidong Huang, Jiaqi Wang, Feng Wu, Dahua Lin
                
                
                    <strong>Abstract:</strong> Image captioning is a fundamental task that bridges the visual and linguistic domains, playing a critical role in pre-training Large Vision-Language Models (LVLMs). Current state-of-the-art captioning models are typically trained with Supervised Fine-Tuning (SFT), a paradigm that relies on expensive, non-scalable data annotated by humans or proprietary models. This approach often leads to models that memorize specific ground-truth answers, limiting their generality and ability to generate diverse, creative descriptions. To overcome the limitation of SFT, we propose applying the Reinforcement Learning with Verifiable Rewards (RLVR) paradigm to the open-ended task of image captioning. A primary challenge, however, is designing an objective reward function for the inherently subjective nature of what constitutes a good caption. We introduce Captioning Reinforcement Learning (CapRL), a novel training framework that redefines caption quality through its utility: a high-quality caption should enable a non-visual language model to accurately answer questions about the corresponding image. CapRL employs a decoupled two-stage pipeline where an LVLM generates a caption, and the objective reward is derived from the accuracy of a separate, vision-free LLM answering Multiple-Choice Questions based solely on that caption. As the first study to apply RLVR to the subjective image captioning task, we demonstrate that CapRL significantly enhances multiple settings. Pretraining on the CapRL-5M caption dataset annotated by CapRL-3B results in substantial gains across 12 benchmarks. Moreover, within the Prism Framework for caption quality evaluation, CapRL achieves performance comparable to Qwen2.5-VL-72B, while exceeding the baseline by an average margin of 8.4%. Code is available here: https://github.com/InternLM/CapRL.
                
                
                    <strong>Published:</strong> 2025-09-26T17:59:55Z
                    <a href="http://arxiv.org/abs/2509.22647v1" target="_blank">📄 View Paper</a>
                
            
        
            
                Learning Human-Perceived Fakeness in AI-Generated Videos via Multimodal LLMs
                
                    <strong>Authors:</strong> Xingyu Fu, Siyi Liu, Yinuo Xu, Pan Lu, Guangqiuse Hu, Tianbo Yang, Taran Anantasagar, Christopher Shen, Yikai Mao, Yuanzhe Liu, Keyush Shah, Chung Un Lee, Yejin Choi, James Zou, Dan Roth, Chris Callison-Burch
                
                
                    <strong>Abstract:</strong> Can humans identify AI-generated (fake) videos and provide grounded reasons? While video generation models have advanced rapidly, a critical dimension -- whether humans can detect deepfake traces within a generated video, i.e., spatiotemporal grounded visual artifacts that reveal a video as machine generated -- has been largely overlooked. We introduce DeeptraceReward, the first fine-grained, spatially- and temporally- aware benchmark that annotates human-perceived fake traces for video generation reward. The dataset comprises 4.3K detailed annotations across 3.3K high-quality generated videos. Each annotation provides a natural-language explanation, pinpoints a bounding-box region containing the perceived trace, and marks precise onset and offset timestamps. We consolidate these annotations into 9 major categories of deepfake traces that lead humans to identify a video as AI-generated, and train multimodal language models (LMs) as reward models to mimic human judgments and localizations. On DeeptraceReward, our 7B reward model outperforms GPT-5 by 34.7% on average across fake clue identification, grounding, and explanation. Interestingly, we observe a consistent difficulty gradient: binary fake v.s. real classification is substantially easier than fine-grained deepfake trace detection; within the latter, performance degrades from natural language explanations (easiest), to spatial grounding, to temporal labeling (hardest). By foregrounding human-perceived deepfake traces, DeeptraceReward provides a rigorous testbed and training signal for socially aware and trustworthy video generation.
                
                
                    <strong>Published:</strong> 2025-09-26T17:59:54Z
                    <a href="http://arxiv.org/abs/2509.22646v1" target="_blank">📄 View Paper</a>
                
            
        
            
                WebGen-Agent: Enhancing Interactive Website Generation with Multi-Level Feedback and Step-Level Reinforcement Learning
                
                    <strong>Authors:</strong> Zimu Lu, Houxing Ren, Yunqiao Yang, Ke Wang, Zhuofan Zong, Junting Pan, Mingjie Zhan, Hongsheng Li
                
                
                    <strong>Abstract:</strong> Agent systems powered by large language models (LLMs) have demonstrated impressive performance on repository-level code-generation tasks. However, for tasks such as website codebase generation, which depend heavily on visual effects and user-interaction feedback, current code agents rely only on simple code execution for feedback and verification. This approach fails to capture the actual quality of the generated code. In this paper, we propose WebGen-Agent, a novel website-generation agent that leverages comprehensive and multi-level visual feedback to iteratively generate and refine the website codebase. Detailed and expressive text descriptions and suggestions regarding the screenshots and GUI-agent testing of the websites are generated by a visual language model (VLM), together with scores that quantify their quality. The screenshot and GUI-agent scores are further integrated with a backtracking and select-best mechanism, enhancing the performance of the agent. Utilizing the accurate visual scores inherent in the WebGen-Agent workflow, we further introduce \textit{Step-GRPO with Screenshot and GUI-agent Feedback} to improve the ability of LLMs to act as the reasoning engine of WebGen-Agent. By using the screenshot and GUI-agent scores at each step as the reward in Step-GRPO, we provide a dense and reliable process supervision signal, which effectively improves the models website-generation ability. On the WebGen-Bench dataset, WebGen-Agent increases the accuracy of Claude-3.5-Sonnet from 26.4% to 51.9% and its appearance score from 3.0 to 3.9, outperforming the previous state-of-the-art agent system. Additionally, our Step-GRPO training approach increases the accuracy of Qwen2.5-Coder-7B-Instruct from 38.9% to 45.4% and raises the appearance score from 3.4 to 3.7.
                
                
                    <strong>Published:</strong> 2025-09-26T17:59:51Z
                    <a href="http://arxiv.org/abs/2509.22644v1" target="_blank">📄 View Paper</a>
                
            
        
            
                Hierarchical Representation Matching for CLIP-based Class-Incremental Learning
                
                    <strong>Authors:</strong> Zhen-Hao Wen, Yan Wang, Ji Feng, Han-Jia Ye, De-Chuan Zhan, Da-Wei Zhou
                
                
                    <strong>Abstract:</strong> Class-Incremental Learning (CIL) aims to endow models with the ability to continuously adapt to evolving data streams. Recent advances in pre-trained vision-language models (e.g., CLIP) provide a powerful foundation for this task. However, existing approaches often rely on simplistic templates, such as a photo of a [CLASS], which overlook the hierarchical nature of visual concepts. For example, recognizing cat versus car depends on coarse-grained cues, while distinguishing cat from lion requires fine-grained details. Similarly, the current feature mapping in CLIP relies solely on the representation from the last layer, neglecting the hierarchical information contained in earlier layers. In this work, we introduce HiErarchical Representation MAtchiNg (HERMAN) for CLIP-based CIL. Our approach leverages LLMs to recursively generate discriminative textual descriptors, thereby augmenting the semantic space with explicit hierarchical cues. These descriptors are matched to different levels of the semantic hierarchy and adaptively routed based on task-specific requirements, enabling precise discrimination while alleviating catastrophic forgetting in incremental tasks. Extensive experiments on multiple benchmarks demonstrate that our method consistently achieves state-of-the-art performance.
                
                
                    <strong>Published:</strong> 2025-09-26T17:59:51Z
                    <a href="http://arxiv.org/abs/2509.22645v1" target="_blank">📄 View Paper</a>
                
            
        
            
                VLA-Reasoner: Empowering Vision-Language-Action Models with Reasoning via Online Monte Carlo Tree Search
                
                    <strong>Authors:</strong> Wenkai Guo, Guanxing Lu, Haoyuan Deng, Zhenyu Wu, Yansong Tang, Ziwei Wang
                
                
                    <strong>Abstract:</strong> Vision-Language-Action models (VLAs) achieve strong performance in general robotic manipulation tasks by scaling imitation learning. However, existing VLAs are limited to predicting short-sighted next-action, which struggle with long-horizon trajectory tasks due to incremental deviations. To address this problem, we propose a plug-in framework named VLA-Reasoner that effectively empowers off-the-shelf VLAs with the capability of foreseeing future states via test-time scaling. Specifically, VLA-Reasoner samples and rolls out possible action trajectories where involved actions are rationales to generate future states via a world model, which enables VLA-Reasoner to foresee and reason potential outcomes and search for the optimal actions. We further leverage Monte Carlo Tree Search (MCTS) to improve search efficiency in large action spaces, where stepwise VLA predictions seed the root. Meanwhile, we introduce a confidence sampling mechanism based on Kernel Density Estimation (KDE), to enable efficient exploration in MCTS without redundant VLA queries. We evaluate intermediate states in MCTS via an offline reward shaping strategy, to score predicted futures and correct deviations with long-term feedback. We conducted extensive experiments in both simulators and the real world, demonstrating that our proposed VLA-Reasoner achieves significant improvements over the state-of-the-art VLAs. Our method highlights a potential pathway toward scalable test-time computation of robotic manipulation.
                
                
                    <strong>Published:</strong> 2025-09-26T17:59:40Z
                    <a href="http://arxiv.org/abs/2509.22643v1" target="_blank">📄 View Paper</a>
                
            
        
    
    
        <p><em>Generated by AI News Agent</em></p>
    

