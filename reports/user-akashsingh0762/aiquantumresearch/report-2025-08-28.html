
    
        <h1>🤖 AI Research Report</h1>
        
            <strong>Date:</strong> 2025-08-28<br>
            <strong>Topics:</strong> ai alignment research, quantum computing<br>
            <strong>Papers Found:</strong> 75
        
        
        
            
                <h2>🤖 AI Summary</h2>
                <h2>ai alignment research</h2>
<p>Certainly! Here is a structured summary highlighting the key findings and significance of the provided research papers in the context of AI alignment research:</p>
<h3>Most Important Trends</h3>
<ol>
<li><p><strong>Integration of AI with Other Technologies</strong>: Papers like AudioStory and FlyMeThrough illustrate the trend of integrating AI with other technological domains (e.g., text-to-audio generation, drone-based indoor mapping) to enhance capabilities and address specific challenges like coherence in long-form audio narratives and efficient 3D mapping.</p>
</li>
<li><p><strong>AI for Enhanced Human-AI Collaboration</strong>: Research such as Socially Interactive Agents for Preserving and Transferring Tacit Knowledge in Organizations and Your AI Bosses Are Still Prejudiced focuses on using AI to facilitate collaboration and learning in human-centric environments, highlighting both opportunities and ethical challenges.</p>
</li>
<li><p><strong>AI in Domain-Specific Applications</strong>: Papers like CHEMSMART and CataractSurg-80K demonstrate AIs growing role in specialized fields, such as chemistry and medical surgery, where domain-specific models are being developed to improve efficiency and decision-making.</p>
</li>
<li><p><strong>AI Safety and Robustness</strong>: Several papers, such as Forewarned is Forearmed and Model Science, emphasize the importance of developing AI systems that are safe, explainable, and aligned with human values. This includes creating frameworks to predict and mitigate potential risks.</p>
</li>
<li><p><strong>Emergence of Large Language Models (LLMs)</strong>: The use of LLMs in various contexts, from Large Language Models (LLMs) for Electronic Design Automation (EDA) to Using item recommendations and LLMs in marketing email titles, highlights the trend of leveraging these models for generating and processing information across different domains.</p>
</li>
</ol>
<h3>Breakthroughs</h3>
<ol>
<li><p><strong>Novel AI Architectures</strong>: Papers such as HERMES and FlowletFormer introduce new AI frameworks that enhance the ability of robots and network systems to learn and operate efficiently in dynamic environments, showcasing advancements in AI architecture design.</p>
</li>
<li><p><strong>Generative AI for Complex Tasks</strong>: DeepScholar-Bench and GS: Generative Segmentation via Label Diffusion illustrate breakthroughs in using generative AI for tasks such as research synthesis and image segmentation, highlighting AIs potential in creative and analytical processes.</p>
</li>
<li><p><strong>AI for Enhanced Safety and Security</strong>: The development of systems like IMAGINE for LLM safety and CASE for scam detection demonstrates significant progress in using AI to enhance safety and security in digital interactions and transactions.</p>
</li>
<li><p><strong>AI in Healthcare</strong>: CataractSurg-80K and AI-Powered Detection of Inappropriate Language in Medical School Curricula represent significant advancements in applying AI to improve healthcare outcomes, from surgical planning to educational content evaluation.</p>
</li>
</ol>
<h3>Implications</h3>
<ol>
<li><p><strong>Improved Human-AI Interaction</strong>: The research indicates a potential for AI to significantly enhance how humans interact with technology, whether through improved communication systems or better integration of tacit knowledge in organizations.</p>
</li>
<li><p><strong>Ethical and Safety Considerations</strong>: The emphasis on AI safety, as seen in works like Model Science, highlights the need for careful consideration of ethical implications and the development of systems that align with human values.</p>
</li>
<li><p><strong>Domain-Specific AI Applications</strong>: The application of AI in specialized domains such as medicine, chemistry, and autonomous systems suggests a future where AI tools are deeply embedded in domain-specific workflows, potentially transforming industries.</p>
</li>
<li><p><strong>Scalability and Efficiency</strong>: Many papers discuss AIs role in enhancing scalability and efficiency, whether in processing large datasets or automating complex tasks, underscoring AIs potential to revolutionize operational models across sectors.</p>
</li>
<li><p><strong>AI as a Creative and Analytical Tool</strong>: The use of AI for generative tasks, from synthesizing research to creating narrative audio, points to a future where AI not only supports but also actively contributes to creative and analytical endeavors.</p>
</li>
</ol>
<p>These research insights collectively underscore the evolving role of AI in society and its potential to align more closely with human values and requirements, highlighting both opportunities and challenges in ensuring responsible AI development and deployment.</p>
<p><em>Based on 50 research papers</em></p>

<h2>quantum computing</h2>
<p>The provided research papers encompass a range of topics, primarily outside the direct scope of quantum computing. However, I can still summarize the key trends, breakthroughs, and implications from these papers as they relate to broader technological advancements and methodologies, which may indirectly intersect with quantum computing or similar advanced computational fields.</p>
<h3>Most Important Trends</h3>
<ol>
<li><p><strong>Integration of AI and Domain-Specific Applications</strong>: Several papers highlight integrating AI frameworks with specialized applications, such as scientific computing (CODA framework), language models for audio generation (AudioStory), and semantic communication (ProMSC-MIS). This trend suggests an increasing move towards using AI to handle complex, domain-specific tasks that require precise execution and planning.</p>
</li>
<li><p><strong>Emergence of Multimodal and Multispectral Data Processing</strong>: Papers like the one on multimodal semantic communication for image segmentation (ProMSC-MIS) and OpenM3D for 3D object detection underline the growing trend of leveraging multimodal data (e.g., combining visual and thermal data) to enhance the capability of models in complex environments.</p>
</li>
<li><p><strong>Focus on Robustness and Adaptability</strong>: Research in robust optimization (Robust Paths), uncertainty-guided approaches (PAUL for geo-localization), and reinforcement learning for scheduling (Reinforcement Learning for Search Tree Size Minimization) reflects a trend towards designing systems that can adapt to real-world uncertainties and improve decision-making under varying conditions.</p>
</li>
</ol>
<h3>Breakthroughs</h3>
<ol>
<li><p><strong>Novel Frameworks for Enhanced Model Performance</strong>: CODA introduces a dual-brain architecture for better planning and execution in scientific domains, setting a new state-of-the-art for open-source models in its benchmark. Similarly, Discrete Diffusion VLA offers an innovative approach to action decoding that outperforms traditional methods.</p>
</li>
<li><p><strong>Advanced Techniques for Data Synthesis and Management</strong>: The IMAGINE framework for synthesizing jailbreak-like instructions showcases a novel method to enhance the safety of language models against potential attacks by bridging distributional gaps in training data.</p>
</li>
<li><p><strong>Efficient Computational Methods</strong>: The development of CHEMSMART for automating chemistry workflows and MCOT for efficient control of thermostatically controlled loads introduces new efficiencies in computational and control processes, potentially applicable to quantum computing environments where efficiency and automation are crucial.</p>
</li>
</ol>
<h3>Implications</h3>
<ol>
<li><p><strong>Enhanced Domain-Specific AI Applications</strong>: The advancements in integrating AI with specialized domains suggest potential applications in quantum computing, where domain-specific challenges could benefit from similar AI-driven frameworks for improved problem-solving capabilities.</p>
</li>
<li><p><strong>Improved Data Handling and Security</strong>: Techniques like those in ProMSC-MIS and IMAGINE have implications for quantum computing in terms of robust data processing and security, which are critical in quantum environments where data integrity and security are paramount.</p>
</li>
<li><p><strong>Potential for Cross-Disciplinary Innovations</strong>: The methodologies and frameworks demonstrated in these papers, such as those for robust optimization and adaptive learning, could inspire cross-disciplinary innovations, including quantum computing, where similar challenges of optimization and adaptability are prevalent.</p>
</li>
</ol>
<p>Overall, while the papers themselves do not directly address quantum computing, the trends and breakthroughs they represent could influence future developments in quantum technologies, particularly in areas requiring high levels of precision, adaptability, and robust data processing.</p>
<p><em>Based on 25 research papers</em></p>

            
        
        
        <h2>📚 Research Papers</h2>
        
                
                    <h3><a href="http://arxiv.org/abs/2508.20088v1" target="_blank">AudioStory: Generating Long-Form Narrative Audio with Large Language Models</a></h3>
                    <p><strong>Authors:</strong> Yuxin Guo, Teng Wang, Yuying Ge, Shijie Ma, Yixiao Ge, Wei Zou, Ying Shan</p>
                    <p><strong>Published:</strong> 8/27/2025</p>
                    <p><strong>Categories:</strong> cs.CV, cs.MM, cs.SD</p>
                    <p><strong>Summary:</strong> Recent advances in text-to-audio (TTA) generation excel at synthesizing short audio clips but struggle with long-form narrative audio, which requires temporal coherence and compositional reasoning. To address this gap, we propose AudioStory, a unified framework that integrates large language models (LLMs) with TTA systems to generate structured, long-form audio narratives. AudioStory possesses strong instruction-following reasoning generation capabilities. It employs LLMs to decompose complex narrative queries into temporally ordered sub-tasks with contextual cues, enabling coherent scene transitions and emotional tone consistency. AudioStory has two appealing features: (1) Decoupled bridging mechanism: AudioStory disentangles LLM-diffuser collaboration into two specialized components, i.e., a bridging query for intra-event semantic alignment and a residual query for cross-event coherence preservation. (2) End-to-end training: By unifying instruction comprehension and audio generation within a single end-to-end framework, AudioStory eliminates the need for modular training pipelines while enhancing synergy between components. Furthermore, we establish a benchmark AudioStory-10K, encompassing diverse domains such as animated soundscapes and natural sound narratives. Extensive experiments show the superiority of AudioStory on both single-audio generation and narrative audio generation, surpassing prior TTA baselines in both instruction-following ability and audio fidelity. Our code is available at https://github.com/TencentARC/AudioStory</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.20085v1" target="_blank">HERMES: Human-to-Robot Embodied Learning from Multi-Source Motion Data for Mobile Dexterous Manipulation</a></h3>
                    <p><strong>Authors:</strong> Zhecheng Yuan, Tianming Wei, Langzhe Gu, Pu Hua, Tianhai Liang, Yuanpei Chen, Huazhe Xu</p>
                    <p><strong>Published:</strong> 8/27/2025</p>
                    <p><strong>Categories:</strong> cs.RO</p>
                    <p><strong>Summary:</strong> Leveraging human motion data to impart robots with versatile manipulation skills has emerged as a promising paradigm in robotic manipulation. Nevertheless, translating multi-source human hand motions into feasible robot behaviors remains challenging, particularly for robots equipped with multi-fingered dexterous hands characterized by complex, high-dimensional action spaces. Moreover, existing approaches often struggle to produce policies capable of adapting to diverse environmental conditions. In this paper, we introduce HERMES, a human-to-robot learning framework for mobile bimanual dexterous manipulation. First, HERMES formulates a unified reinforcement learning approach capable of seamlessly transforming heterogeneous human hand motions from multiple sources into physically plausible robotic behaviors. Subsequently, to mitigate the sim2real gap, we devise an end-to-end, depth image-based sim2real transfer method for improved generalization to real-world scenarios. Furthermore, to enable autonomous operation in varied and unstructured environments, we augment the navigation foundation model with a closed-loop Perspective-n-Point (PnP) localization mechanism, ensuring precise alignment of visual goals and effectively bridging autonomous navigation and dexterous manipulation. Extensive experimental results demonstrate that HERMES consistently exhibits generalizable behaviors across diverse, in-the-wild scenarios, successfully performing numerous complex mobile bimanual dexterous manipulation tasks. Project Page:https:/gemcollector.github.io/HERMES/.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.20069v1" target="_blank">There must be an error here! Experimental evidence on coding errors biases</a></h3>
                    <p><strong>Authors:</strong> Bruno Ferman, Lucas Finamor</p>
                    <p><strong>Published:</strong> 8/27/2025</p>
                    <p><strong>Categories:</strong> econ.GN, q-fin.EC</p>
                    <p><strong>Summary:</strong> Quantitative research relies heavily on coding, and coding errors are relatively common even in published research. In this paper, we examine whether individuals are more or less likely to check their code depending on the results they obtain. We test this hypothesis in a randomized experiment embedded in the recruitment process for research positions at a large international economic organization. In a coding task designed to assess candidates programming abilities, we randomize whether participants obtain an expected or unexpected result if they commit a simple coding error. We find that individuals are 20% more likely to detect coding errors when they lead to unexpected results. This asymmetry in error detection depending on the results they generate suggests that coding errors may lead to biased findings in scientific research.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.20066v1" target="_blank">PAUL: Uncertainty-Guided Partition and Augmentation for Robust Cross-View Geo-Localization under Noisy Correspondence</a></h3>
                    <p><strong>Authors:</strong> Zheng Li, Yanming Guo, WenZhe Liu, Xueyi Zhang, Zhaoyun Ding, Long Xu, Mingrui Lao</p>
                    <p><strong>Published:</strong> 8/27/2025</p>
                    <p><strong>Categories:</strong> cs.CV</p>
                    <p><strong>Summary:</strong> Cross-view geo-localization is a critical task for UAV navigation, event detection, and aerial surveying, as it enables matching between drone-captured and satellite imagery. Most existing approaches embed multi-modal data into a joint feature space to maximize the similarity of paired images. However, these methods typically assume perfect alignment of image pairs during training, which rarely holds true in real-world scenarios. In practice, factors such as urban canyon effects, electromagnetic interference, and adverse weather frequently induce GPS drift, resulting in systematic alignment shifts where only partial correspondences exist between pairs. Despite its prevalence, this source of noisy correspondence has received limited attention in current research. In this paper, we formally introduce and address the Noisy Correspondence on Cross-View Geo-Localization (NC-CVGL) problem, aiming to bridge the gap between idealized benchmarks and practical applications. To this end, we propose PAUL (Partition and Augmentation by Uncertainty Learning), a novel framework that partitions and augments training data based on estimated data uncertainty through uncertainty-aware co-augmentation and evidential co-training. Specifically, PAUL selectively augments regions with high correspondence confidence and utilizes uncertainty estimation to refine feature learning, effectively suppressing noise from misaligned pairs. Distinct from traditional filtering or label correction, PAUL leverages both data uncertainty and loss discrepancy for targeted partitioning and augmentation, thus providing robust supervision for noisy samples. Comprehensive experiments validate the effectiveness of individual components in PAUL,which consistently achieves superior performance over other competitive noisy-correspondence-driven methods in various noise ratios.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.20063v1" target="_blank">OpenM3D: Open Vocabulary Multi-view Indoor 3D Object Detection without Human Annotations</a></h3>
                    <p><strong>Authors:</strong> Peng-Hao Hsu, Ke Zhang, Fu-En Wang, Tao Tu, Ming-Feng Li, Yu-Lun Liu, Albert Y. C. Chen, Min Sun, Cheng-Hao Kuo</p>
                    <p><strong>Published:</strong> 8/27/2025</p>
                    <p><strong>Categories:</strong> cs.CV</p>
                    <p><strong>Summary:</strong> Open-vocabulary (OV) 3D object detection is an emerging field, yet its exploration through image-based methods remains limited compared to 3D point cloud-based methods. We introduce OpenM3D, a novel open-vocabulary multi-view indoor 3D object detector trained without human annotations. In particular, OpenM3D is a single-stage detector adapting the 2D-induced voxel features from the ImGeoNet model. To support OV, it is jointly trained with a class-agnostic 3D localization loss requiring high-quality 3D pseudo boxes and a voxel-semantic alignment loss requiring diverse pre-trained CLIP features. We follow the training setting of OV-3DET where posed RGB-D images are given but no human annotations of 3D boxes or classes are available. We propose a 3D Pseudo Box Generation method using a graph embedding technique that combines 2D segments into coherent 3D structures. Our pseudo-boxes achieve higher precision and recall than other methods, including the method proposed in OV-3DET. We further sample diverse CLIP features from 2D segments associated with each coherent 3D structure to align with the corresponding voxel feature. The key to training a highly accurate single-stage detector requires both losses to be learned toward high-quality targets. At inference, OpenM3D, a highly efficient detector, requires only multi-view images for input and demonstrates superior accuracy and speed (0.3 sec. per scene) on ScanNet200 and ARKitScenes indoor benchmarks compared to existing methods. We outperform a strong two-stage method that leverages our class-agnostic detector with a ViT CLIP-based OV classifier and a baseline incorporating multi-view depth estimator on both accuracy and speed.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.20057v1" target="_blank">ProMSC-MIS: Prompt-based Multimodal Semantic Communication for Multi-Spectral Image Segmentation</a></h3>
                    <p><strong>Authors:</strong> Haoshuo Zhang, Yufei Bo, Meixia Tao</p>
                    <p><strong>Published:</strong> 8/27/2025</p>
                    <p><strong>Categories:</strong> cs.MM</p>
                    <p><strong>Summary:</strong> Multimodal semantic communication has great potential to enhance downstream task performance by integrating complementary information across modalities. This paper introduces ProMSC-MIS, a novel Prompt-based Multimodal Semantic Communication framework for Multi-Spectral Image Segmentation. It enables efficient task-oriented transmission of spatially aligned RGB and thermal images over band-limited channels. Our framework has two main design novelties. First, by leveraging prompt learning and contrastive learning, unimodal semantic encoders are pre-trained to learn diverse and complementary semantic representations by using features from one modality as prompts for another. Second, a semantic fusion module that combines cross-attention mechanism and squeeze-and-excitation (SE) networks is designed to effectively fuse cross-modal features. Experimental results demonstrate that ProMSC-MIS substantially outperforms conventional image transmission combined with state-of-the-art segmentation methods. Notably, it reduces the required channel bandwidth by 50%--70% at the same segmentation performance, while also decreasing the storage overhead and computational complexity by 26% and 37%, respectively. Ablation studies also validate the effectiveness of the proposed pre-training and semantic fusion strategies. Our scheme is highly suitable for applications such as autonomous driving and nighttime surveillance.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.20042v1" target="_blank">CHEMSMART: Chemistry Simulation and Modeling Automation Toolkit for High-Efficiency Computational Chemistry Workflows</a></h3>
                    <p><strong>Authors:</strong> Xinglong Zhang, Huiwen Tan, Jingyi Liu, Zihan Li, Lewen Wang, Benjamin W. J. Chen</p>
                    <p><strong>Published:</strong> 8/27/2025</p>
                    <p><strong>Categories:</strong> physics.chem-ph, physics.comp-ph</p>
                    <p><strong>Summary:</strong> CHEMSMART (Chemistry Simulation and Modeling Automation Toolkit) is an open-source, Python-based framework designed to streamline quantum chemistry workflows for homogeneous catalysis and molecular modeling. By integrating job preparation, submission, execution, results analysis, and visualization, CHEMSMART addresses the inefficiencies of manual workflow management in computational chemistry by ensuring seamless interoperability with quantum chemistry packages and cheminformatics platforms. Its modular architecture supports automated job submission and execution tasks for geometry optimization, transition state searches, thermochemical analysis, and non-covalent interaction plotting, while auxiliary scripts facilitate file conversion, data organization, and electronic structure analysis. Future developments aim to expand compatibility with additional software, incorporate QM/MM and classical MD, and align with FAIR data principles for enhanced reproducibility and data reuse. Available on GitHub, CHEMSMART empowers researchers with a robust, user-friendly platform for efficient and reproducible computational chemistry.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.20040v1" target="_blank">Model Science: getting serious about verification, explanation and control of AI systems</a></h3>
                    <p><strong>Authors:</strong> Przemyslaw Biecek, Wojciech Samek</p>
                    <p><strong>Published:</strong> 8/27/2025</p>
                    <p><strong>Categories:</strong> cs.AI, cs.LG</p>
                    <p><strong>Summary:</strong> The growing adoption of foundation models calls for a paradigm shift from Data Science to Model Science. Unlike data-centric approaches, Model Science places the trained model at the core of analysis, aiming to interact, verify, explain, and control its behavior across diverse operational contexts. This paper introduces a conceptual framework for a new discipline called Model Science, along with the proposal for its four key pillars: Verification, which requires strict, context-aware evaluation protocols; Explanation, which is understood as various approaches to explore of internal model operations; Control, which integrates alignment techniques to steer model behavior; and Interface, which develops interactive and visual explanation tools to improve human calibration and decision-making. The proposed framework aims to guide the development of credible, safe, and human-aligned AI systems.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.20038v1" target="_blank">Forewarned is Forearmed: Pre-Synthesizing Jailbreak-like Instructions to Enhance LLM Safety Guardrail to Potential Attacks</a></h3>
                    <p><strong>Authors:</strong> Sheng Liu, Qiang Sheng, Danding Wang, Yang Li, Guang Yang, Juan Cao</p>
                    <p><strong>Published:</strong> 8/27/2025</p>
                    <p><strong>Categories:</strong> cs.CL</p>
                    <p><strong>Summary:</strong> Despite advances in improving large language model(LLM) to refuse to answer malicious instructions, widely used LLMs remain vulnerable to jailbreak attacks where attackers generate instructions with distributions differing from safety alignment corpora. New attacks expose LLMs inability to recognize unseen malicious instructions, highlighting a critical distributional mismatch between training data and real-world attacks that forces developers into reactive patching cycles. To tackle this challenge, we propose IMAGINE, a synthesis framework that leverages embedding space distribution analysis to generate jailbreak-like instructions. This approach effectively fills the distributional gap between authentic jailbreak patterns and safety alignment corpora. IMAGINE follows an iterative optimization process that dynamically evolves text generation distributions across iterations, thereby augmenting the coverage of safety alignment data distributions through synthesized data examples. Based on the safety-aligned corpus enhanced through IMAGINE, our framework demonstrates significant decreases in attack success rate on Qwen2.5, Llama3.1, and Llama3.2 without compromising their utility.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.20034v1" target="_blank">FlyMeThrough: Human-AI Collaborative 3D Indoor Mapping with Commodity Drones</a></h3>
                    <p><strong>Authors:</strong> Xia Su, Ruiqi Chen, Jingwei Ma, Chu Li, Jon E. Froehlich</p>
                    <p><strong>Published:</strong> 8/27/2025</p>
                    <p><strong>Categories:</strong> cs.HC, H.5.2; I.2.10</p>
                    <p><strong>Summary:</strong> Indoor mapping data is crucial for routing, navigation, and building management, yet such data are widely lacking due to the manual labor and expense of data collection, especially for larger indoor spaces. Leveraging recent advancements in commodity drones and photogrammetry, we introduce FlyMeThrough -- a drone-based indoor scanning system that efficiently produces 3D reconstructions of indoor spaces with human-AI collaborative annotations for key indoor points-of-interest (POI) such as entrances, restrooms, stairs, and elevators. We evaluated FlyMeThrough in 12 indoor spaces with varying sizes and functionality. To investigate use cases and solicit feedback from target stakeholders, we also conducted a qualitative user study with five building managers and five occupants. Our findings indicate that FlyMeThrough can efficiently and precisely create indoor 3D maps for strategic space planning, resource management, and navigation.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.20033v1" target="_blank">DeepScholar-Bench: A Live Benchmark and Automated Evaluation for Generative Research Synthesis</a></h3>
                    <p><strong>Authors:</strong> Liana Patel, Negar Arabzadeh, Harshit Gupta, Ankita Sundar, Ion Stoica, Matei Zaharia, Carlos Guestrin</p>
                    <p><strong>Published:</strong> 8/27/2025</p>
                    <p><strong>Categories:</strong> cs.CL, cs.AI</p>
                    <p><strong>Summary:</strong> The ability to research and synthesize knowledge is central to human expertise and progress. An emerging class of systems promises these exciting capabilities through generative research synthesis, performing retrieval over the live web and synthesizing discovered sources into long-form, cited summaries. However, evaluating such systems remains an open challenge: existing question-answering benchmarks focus on short-form factual responses, while expert-curated datasets risk staleness and data contamination. Both fail to capture the complexity and evolving nature of real research synthesis tasks. In this work, we introduce DeepScholar-bench, a live benchmark and holistic, automated evaluation framework designed to evaluate generative research synthesis. DeepScholar-bench draws queries from recent, high-quality ArXiv papers and focuses on a real research synthesis task: generating the related work sections of a paper by retrieving, synthesizing, and citing prior research. Our evaluation framework holistically assesses performance across three key dimensions, knowledge synthesis, retrieval quality, and verifiability. We also develop DeepScholar-base, a reference pipeline implemented efficiently using the LOTUS API. Using the DeepScholar-bench framework, we perform a systematic evaluation of prior open-source systems, search AIs, OpenAIs DeepResearch, and DeepScholar-base. We find that DeepScholar-base establishes a strong baseline, attaining competitive or higher performance than each other method. We also find that DeepScholar-bench remains far from saturated, with no system exceeding a score of $19\%$ across all metrics. These results underscore the difficulty of DeepScholar-bench, as well as its importance for progress towards AI systems capable of generative research synthesis. We make our code available at https://github.com/guestrin-lab/deepscholar-bench.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.20031v1" target="_blank">Bridging the Regulatory Divide: Ensuring Safety and Equity in Wearable Health Technologies</a></h3>
                    <p><strong>Authors:</strong> Akshay Kelshiker, Susan Cheng, Jivan Achar, Jane Bambauer, Leo Anthony Celi, Divya Jain, Thinh Nguyen, Harsh Patel, Nina Prakash, Alice Wong, Barbara Evans</p>
                    <p><strong>Published:</strong> 8/27/2025</p>
                    <p><strong>Categories:</strong> cs.CY</p>
                    <p><strong>Summary:</strong> As wearable health technologies have grown more sophisticated, the distinction between wellness and medical devices has become increasingly blurred. While some features undergo formal U.S. Food and Drug Administration (FDA) review, many over-the-counter tools operate in a regulatory grey zone, leveraging health-related data and outputs without clinical validation. Further complicating the issue is the widespread repurposing of wellness devices for medical uses, which can introduce safety risks beyond the reach of current oversight. Drawing on legal analysis, case studies, and ethical considerations, we propose an approach emphasizing distributed risk, patient-centered outcomes, and iterative reform. Without a more pluralistic and evolving framework, the promise of wearable health technology risks being undermined by growing inequities, misuse, and eroded public trust.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.20030v1" target="_blank">Large Language Models (LLMs) for Electronic Design Automation (EDA)</a></h3>
                    <p><strong>Authors:</strong> Kangwei Xu, Denis Schwachhofer, Jason Blocklove, Ilia Polian, Peter Domanski, Dirk Pflüger, Siddharth Garg, Ramesh Karri, Ozgur Sinanoglu, Johann Knechtel, Zhuorui Zhao, Ulf Schlichtmann, Bing Li</p>
                    <p><strong>Published:</strong> 8/27/2025</p>
                    <p><strong>Categories:</strong> eess.SY, cs.AI, cs.AR, cs.LG, cs.SY</p>
                    <p><strong>Summary:</strong> With the growing complexity of modern integrated circuits, hardware engineers are required to devote more effort to the full design-to-manufacturing workflow. This workflow involves numerous iterations, making it both labor-intensive and error-prone. Therefore, there is an urgent demand for more efficient Electronic Design Automation (EDA) solutions to accelerate hardware development. Recently, large language models (LLMs) have shown remarkable advancements in contextual comprehension, logical reasoning, and generative capabilities. Since hardware designs and intermediate scripts can be represented as text, integrating LLM for EDA offers a promising opportunity to simplify and even automate the entire workflow. Accordingly, this paper provides a comprehensive overview of incorporating LLMs into EDA, with emphasis on their capabilities, limitations, and future opportunities. Three case studies, along with their outlook, are introduced to demonstrate the capabilities of LLMs in hardware design, testing, and optimization. Finally, future directions and challenges are highlighted to further explore the potential of LLMs in shaping the next-generation EDA, providing valuable insights for researchers interested in leveraging advanced AI technologies for EDA.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.20024v1" target="_blank">Using item recommendations and LLMs in marketing email titles</a></h3>
                    <p><strong>Authors:</strong> Deddy Jobson, Muktti Shukla, Phuong Dinh, Julio Christian Young, Nick Pitton, Nina Chen, Ryan Ginstrom</p>
                    <p><strong>Published:</strong> 8/27/2025</p>
                    <p><strong>Categories:</strong> cs.LG</p>
                    <p><strong>Summary:</strong> E-commerce marketplaces make use of a number of marketing channels like emails, push notifications, etc. to reach their users and stimulate purchases. Personalized emails especially are a popular touch point for marketers to inform users of latest items in stock, especially for those who stopped visiting the marketplace. Such emails contain personalized recommendations tailored to each users interests, enticing users to buy relevant items. A common limitation of these emails is that the primary entry point, the title of the email, tends to follow fixed templates, failing to inspire enough interest in the contents. In this work, we explore the potential of large language models (LLMs) for generating thematic titles that reflect the personalized content of the emails. We perform offline simulations and conduct online experiments on the order of millions of users, finding our techniques useful in improving the engagement between customers and our emails. We highlight key findings and learnings as we productionize the safe and automated generation of email titles for millions of users.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.20020v1" target="_blank">GS: Generative Segmentation via Label Diffusion</a></h3>
                    <p><strong>Authors:</strong> Yuhao Chen, Shubin Chen, Liang Lin, Guangrun Wang</p>
                    <p><strong>Published:</strong> 8/27/2025</p>
                    <p><strong>Categories:</strong> cs.CV</p>
                    <p><strong>Summary:</strong> Language-driven image segmentation is a fundamental task in vision-language understanding, requiring models to segment regions of an image corresponding to natural language expressions. Traditional methods approach this as a discriminative problem, assigning each pixel to foreground or background based on semantic alignment. Recently, diffusion models have been introduced to this domain, but existing approaches remain image-centric: they either (i) use image diffusion models as visual feature extractors, (ii) synthesize segmentation data via image generation to train discriminative models, or (iii) perform diffusion inversion to extract attention cues from pre-trained image diffusion models-thereby treating segmentation as an auxiliary process. In this paper, we propose GS (Generative Segmentation), a novel framework that formulates segmentation itself as a generative task via label diffusion. Instead of generating images conditioned on label maps and text, GS reverses the generative process: it directly generates segmentation masks from noise, conditioned on both the input image and the accompanying language description. This paradigm makes label generation the primary modeling target, enabling end-to-end training with explicit control over spatial and semantic fidelity. To demonstrate the effectiveness of our approach, we evaluate GS on Panoptic Narrative Grounding (PNG), a representative and challenging benchmark for multimodal segmentation that requires panoptic-level reasoning guided by narrative captions. Experimental results show that GS significantly outperforms existing discriminative and diffusion-based methods, setting a new state-of-the-art for language-driven segmentation.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.20019v1" target="_blank">Symphony: A Decentralized Multi-Agent Framework for Scalable Collective Intelligence</a></h3>
                    <p><strong>Authors:</strong> Ji Wang, Kashing Chen, Xinyuan Song, Ke Zhang, Lynn Ai, Eric Yang, Bill Shi</p>
                    <p><strong>Published:</strong> 8/27/2025</p>
                    <p><strong>Categories:</strong> cs.LG, cs.AI, cs.CL, cs.MA</p>
                    <p><strong>Summary:</strong> Most existing Large Language Model (LLM)-based agent frameworks rely on centralized orchestration, incurring high deployment costs, rigid communication topologies, and limited adaptability. To address these challenges, we introduce Symphony, a decentralized multi-agent system which enables lightweight LLMs on consumer-grade GPUs to coordinate. Symphony introduces three key mechanisms: (1) a decentralized ledger that records capabilities, (2) a Beacon-selection protocol for dynamic task allocation, and (3) weighted result voting based on CoTs. This design forms a privacy-saving, scalable, and fault-tolerant orchestration with low overhead. Empirically, Symphony outperforms existing baselines on reasoning benchmarks, achieving substantial accuracy gains and demonstrating robustness across models of varying capacities.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.20015v1" target="_blank">Decomposing Behavioral Phase Transitions in LLMs: Order Parameters for Emergent Misalignment</a></h3>
                    <p><strong>Authors:</strong> Julian Arnold, Niels Lörch</p>
                    <p><strong>Published:</strong> 8/27/2025</p>
                    <p><strong>Categories:</strong> cs.LG, cs.AI</p>
                    <p><strong>Summary:</strong> Fine-tuning LLMs on narrowly harmful datasets can lead to behavior that is broadly misaligned with respect to human values. To understand when and how this emergent misalignment occurs, we develop a comprehensive framework for detecting and characterizing rapid transitions during fine-tuning using both distributional change detection methods as well as order parameters that are formulated in plain English and evaluated by an LLM judge. Using an objective statistical dissimilarity measure, we quantify how the phase transition that occurs during fine-tuning affects multiple aspects of the model. In particular, we assess what percentage of the total distributional change in model outputs is captured by different aspects, such as alignment or verbosity, providing a decomposition of the overall transition. We also find that the actual behavioral transition occurs later in training than indicated by the peak in the gradient norm alone. Our framework enables the automated discovery and quantification of language-based order parameters, which we demonstrate on examples ranging from knowledge questions to politics and ethics.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.20014v1" target="_blank">CataractSurg-80K: Knowledge-Driven Benchmarking for Structured Reasoning in Ophthalmic Surgery Planning</a></h3>
                    <p><strong>Authors:</strong> Yang Meng, Zewen Pan, Yandi Lu, Ruobing Huang, Yanfeng Liao, Jiarui Yang</p>
                    <p><strong>Published:</strong> 8/27/2025</p>
                    <p><strong>Categories:</strong> cs.MA</p>
                    <p><strong>Summary:</strong> Cataract surgery remains one of the most widely performed and effective procedures for vision restoration. Effective surgical planning requires integrating diverse clinical examinations for patient assessment, intraocular lens (IOL) selection, and risk evaluation. Large language models (LLMs) have shown promise in supporting clinical decision-making. However, existing LLMs often lack the domain-specific expertise to interpret heterogeneous ophthalmic data and provide actionable surgical plans. To enhance the models ability to interpret heterogeneous ophthalmic reports, we propose a knowledge-driven Multi-Agent System (MAS), where each agent simulates the reasoning process of specialist ophthalmologists, converting raw clinical inputs into structured, actionable summaries in both training and deployment stages. Building on MAS, we introduce CataractSurg-80K, the first large-scale benchmark for cataract surgery planning that incorporates structured clinical reasoning. Each case is annotated with diagnostic questions, expert reasoning chains, and structured surgical recommendations. We further introduce Qwen-CSP, a domain-specialized model built on Qwen-4B, fine-tuned through a multi-stage process tailored for surgical planning. Comprehensive experiments show that Qwen-CSP outperforms strong general-purpose LLMs across multiple metrics. Our work delivers a high-quality dataset, a rigorous benchmark, and a domain-adapted LLM to facilitate future research in medical AI reasoning and decision support.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.19986v1" target="_blank">Energetic proton dropouts during the Juno flyby of Europa strongly depend on magnetic field perturbations</a></h3>
                    <p><strong>Authors:</strong> H. L. F. Huybrighs, S. Cervantes, P. Kollmann, C. Paranicas, C. F. Bowers, X. Cao, M. K. G. Holmberg, C. M. Jackman, S. Brophy Lee, A. Bloecker, E. Marchisio</p>
                    <p><strong>Published:</strong> 8/27/2025</p>
                    <p><strong>Categories:</strong> astro-ph.EP</p>
                    <p><strong>Summary:</strong> During Junos only flyby of Europa, the Jupiter Energetic Particle Detector Instrument (JEDI) measured complex dropouts in the energetic ion flux in Europas wake. We investigate the causes of these dropouts, focusing specifically on energetic protons of ~100 keV and ~1 MeV, using back-tracking particle simulations, a prescribed description of Europas atmosphere and a three-dimensional single fluid magnetohydrodynamics (MHD) model of the plasma-atmosphere interaction. We investigate the role of magnetic field perturbations resulting from the interaction between Jupiters magnetospheric plasma and Europas atmosphere and the presence of field-aligned electron beams in Europas wake. We compare the simulated effect of the perturbed fields on the pitch angle distributions of the ion losses to Juno-JEDI measurements. We find that at ~100 keV, field perturbations are the dominant factor controlling the distribution of the losses along the flyby, while at ~1 MeV a combination of field perturbations and absorption by the surface due to short half bounce periods is required to explain the measured losses. We also find that the effect of charge-exchange with Europas tenuous atmosphere is weak and absorption by dust in Europas environment is negligible. Furthermore, we find that the perturbed magnetic fields which best represent the measurements are those that account for the plasma interaction with a sub-/anti-Jovian asymmetric atmosphere, non-uniform ionization of the atmosphere, and electron beams. This sensitivity to the specific field perturbation demonstrates that combining observations and modeling of proton depletions constitute an important tool to probe the electromagnetic field and atmospheric configurations of Europa.</p>
                
            
                
                    <h3><a href="http://dx.doi.org/10.1371/journal.pone.0328766" target="_blank">Communicating astrobiology and the search for life elsewhere: speculations and promises of a developing scientific field in newspapers, press releases and papers</a></h3>
                    <p><strong>Authors:</strong> Danilo Albergaria, Pedro Russo, Ionica Smeets, Thilina Heenatigala, Dallyce Vetter</p>
                    <p><strong>Published:</strong> 8/27/2025</p>
                    <p><strong>Categories:</strong> astro-ph.IM, physics.soc-ph</p>
                    <p><strong>Summary:</strong> This study examines the communication of astrobiology and the Search for Life Elsewhere (SLE) in academic papers, press releases, and news articles over three decades. Through a quantitative content analysis, it investigates the prevalence of speculations and promises/expectations in these sources, aiming to understand how research results are portrayed and their potential impact on public perception and future research directions. Findings reveal that speculations and promises/expectations are more frequent in news articles and press releases compared to academic papers. Speculations about conditions for life and the existence of life beyond Earth are common, particularly in news articles covering exoplanet research, while promises of life detection are rare. Press releases tend to emphasize the significance of research findings and the progress of the field. Speculations and promises/expectations in news articles often occur without attribution to scientists and in quotes of authors of the studies, and slightly less so in quotes of outside experts. The study highlights the complex dynamics of science communication in astrobiology, where speculations and promises can generate public excitement and influence research funding, but also risk misrepresenting scientific uncertainty and creating unrealistic expectations. It underscores the need for responsible communication practices that acknowledge the speculative dimension of the field while fostering public engagement and informed decision-making.</p>
                
            
                
                    <h3><a href="http://dx.doi.org/10.1145/3663547.3746346" target="_blank">CapTune: Adapting Non-Speech Captions With Anchored Generative Models</a></h3>
                    <p><strong>Authors:</strong> Jeremy Zhengqi Huang, Caluã de Lacerda Pataca, Liang-Yuan Wu, Dhruv Jain</p>
                    <p><strong>Published:</strong> 8/27/2025</p>
                    <p><strong>Categories:</strong> cs.HC, cs.HC, cs.AI</p>
                    <p><strong>Summary:</strong> Non-speech captions are essential to the video experience of deaf and hard of hearing (DHH) viewers, yet conventional approaches often overlook the diversity of their preferences. We present CapTune, a system that enables customization of non-speech captions based on DHH viewers needs while preserving creator intent. CapTune allows caption authors to define safe transformation spaces using concrete examples and empowers viewers to personalize captions across four dimensions: level of detail, expressiveness, sound representation method, and genre alignment. Evaluations with seven caption creators and twelve DHH participants showed that CapTune supported creators creative control while enhancing viewers emotional engagement with content. Our findings also reveal trade-offs between information richness and cognitive load, tensions between interpretive and descriptive representations of sound, and the context-dependent nature of caption preferences.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.19970v1" target="_blank">Hyper-spectral Imaging with Up-Converted Mid-Infrared Single-Photons</a></h3>
                    <p><strong>Authors:</strong> Yijian Meng, Asbjørn Arvad Jørgensen, Andreas Næsby Rasmussen, Lasse Høgstedt, Søren M. M. Friis, Mikael Lassen</p>
                    <p><strong>Published:</strong> 8/27/2025</p>
                    <p><strong>Categories:</strong> quant-ph, physics.optics</p>
                    <p><strong>Summary:</strong> Hyperspectral imaging in the mid-infrared (MIR) spectral range provides unique molecular specificity by probing fundamental vibrational modes of molecular bonds, making it highly valuable for biomedical and biochemical applications. However, conventional MIR imaging techniques often rely on high-intensity illumination that can induce photodamage in sensitive biological tissues. Single-photon MIR imaging offers a label-free, non-invasive alternative, yet its adoption is hindered by the lack of efficient, room-temperature MIR single-photon detectors. We present a single-photon hyperspectral imaging platform that combines cavity-enhanced spontaneous parametric down-conversion (SPDC) with nonlinear frequency up-conversion. This approach enables MIR spectral imaging using cost-effective, visible-wavelength silicon single-photon avalanche diodes (Si-SPADs), supporting room-temperature, low-noise, and high-efficiency operation. Time-correlated photon pairs generated via SPDC suppress classical intensity noise, enabling near shot-noise-limited hyperspectral imaging. We demonstrate chemically specific single-photon imaging across the \SIrange{2.9}{3.6}{\micro\meter} range on biological (egg yolk, yeast) and polymeric (polystyrene, polyethylene) samples. The system delivers high-contrast, label-free imaging at ultralow photon flux, overcoming key limitations of current MIR technologies. This platform paves the way toward scalable, quantum-enabled MIR imaging for applications in molecular diagnostics, environmental sensing, and biomedical research.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.19967v1" target="_blank">Assessing the Geolocation Capabilities, Limitations and Societal Risks of Generative Vision-Language Models</a></h3>
                    <p><strong>Authors:</strong> Oliver Grainge, Sania Waheed, Jack Stilgoe, Michael Milford, Shoaib Ehsan</p>
                    <p><strong>Published:</strong> 8/27/2025</p>
                    <p><strong>Categories:</strong> cs.CV</p>
                    <p><strong>Summary:</strong> Geo-localization is the task of identifying the location of an image using visual cues alone. It has beneficial applications, such as improving disaster response, enhancing navigation, and geography education. Recently, Vision-Language Models (VLMs) are increasingly demonstrating capabilities as accurate image geo-locators. This brings significant privacy risks, including those related to stalking and surveillance, considering the widespread uses of AI models and sharing of photos on social media. The precision of these models is likely to improve in the future. Despite these risks, there is little work on systematically evaluating the geolocation precision of Generative VLMs, their limits and potential for unintended inferences. To bridge this gap, we conduct a comprehensive assessment of the geolocation capabilities of 25 state-of-the-art VLMs on four benchmark image datasets captured in diverse environments. Our results offer insight into the internal reasoning of VLMs and highlight their strengths, limitations, and potential societal risks. Our findings indicate that current VLMs perform poorly on generic street-level images yet achieve notably high accuracy (61\%) on images resembling social media content, raising significant and urgent privacy concerns.</p>
                
            
                
                    <h3><a href="http://dx.doi.org/10.1063/5.0249931" target="_blank">A cryogenic chamber setup for superfluid helium experiments with optical fiber and electrical access</a></h3>
                    <p><strong>Authors:</strong> Alexander R. Korsch, Niccolò Fiaschi, Simon Gröblacher</p>
                    <p><strong>Published:</strong> 8/27/2025</p>
                    <p><strong>Categories:</strong> physics.ins-det, cond-mat.mes-hall, cond-mat.quant-gas, cond-mat.supr-con, physics.app-ph, quant-ph</p>
                    <p><strong>Summary:</strong> Superfluid helium is a prototypical quantum liquid. As such, it has been a prominent platform for the study of quantum many body physics. More recently, the outstanding mechanical and optical properties of superfluid helium, such as low mechanical dissipation and low optical absorption, have positioned superfluid helium as a promising material platform in applications ranging from dark matter and gravitational wave detection to quantum computation. However, experiments with superfluid helium incur a high barrier to entry as they require incorporation of complex optical and electrical setups within a hermetically sealed cryogenic chamber to confine the superfluid. Here, we report on the design and construction of a helium chamber setup for operation inside a dilution refrigerator at Millikelvin temperatures, featuring electrical and optical fiber access. By incorporating an automated gas handling system, we can precisely control the amount of helium gas inserted into the chamber, rendering our setup particularly promising for experiments with superfluid helium thin films, such as superfluid thin film optomechanics. Using silicon nanophotonic resonators, we demonstrate precise control and in-situ tuning of the thickness of a superfluid helium film on the sub-nanometer level. By making use of the exceptional tunability of the superfluid film thickness, we demonstrate optomechanically induced phonon lasing of phononic crystal cavity third sound modes in the superfluid film and show that the lasing threshold crucially depends on the film thickness. The large internal volume of our chamber (V_chamber = 1l) is adaptable for integration of various optical and electrical measurement and control techniques. Therefore, our setup provides a versatile platform for a variety of experiments in fundamental and applied superfluid helium research.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.19944v1" target="_blank">KRETA: A Benchmark for Korean Reading and Reasoning in Text-Rich VQA Attuned to Diverse Visual Contexts</a></h3>
                    <p><strong>Authors:</strong> Taebaek Hwang, Minseo Kim, Gisang Lee, Seonuk Kim, Hyunjun Eun</p>
                    <p><strong>Published:</strong> 8/27/2025</p>
                    <p><strong>Categories:</strong> cs.CV, cs.CL</p>
                    <p><strong>Summary:</strong> Understanding and reasoning over text within visual contexts poses a significant challenge for Vision-Language Models (VLMs), given the complexity and diversity of real-world scenarios. To address this challenge, text-rich Visual Question Answering (VQA) datasets and benchmarks have emerged for high-resource languages like English. However, a critical gap persists for low-resource languages such as Korean, where the lack of comprehensive benchmarks hinders robust model evaluation and comparison. To bridge this gap, we introduce KRETA, a benchmark for Korean Reading and rEasoning in Text-rich VQA Attuned to diverse visual contexts. KRETA facilitates an in-depth evaluation of both visual text understanding and reasoning capabilities, while also supporting a multifaceted assessment across 15 domains and 26 image types. Additionally, we introduce a semi-automated VQA generation pipeline specifically optimized for text-rich settings, leveraging refined stepwise image decomposition and a rigorous seven-metric evaluation protocol to ensure data quality. While KRETA is tailored for Korean, we hope our adaptable and extensible pipeline will facilitate the development of similar benchmarks in other languages, thereby accelerating multilingual VLM research. The code and dataset for KRETA are available at https://github.com/tabtoyou/KRETA.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.19942v1" target="_blank">Socially Interactive Agents for Preserving and Transferring Tacit Knowledge in Organizations</a></h3>
                    <p><strong>Authors:</strong> Martin Benderoth, Patrick Gebhard, Christian Keller, C. Benjamin Nakhosteen, Stefan Schaffer, Tanja Schneeberger</p>
                    <p><strong>Published:</strong> 8/27/2025</p>
                    <p><strong>Categories:</strong> cs.HC</p>
                    <p><strong>Summary:</strong> This paper introduces a novel approach to tackle the challenges of preserving and transferring tacit knowledge--deep, experience-based insights that are hard to articulate but vital for decision-making, innovation, and problem-solving. Traditional methods rely heavily on human facilitators, which, while effective, are resource-intensive and lack scalability. A promising alternative is the use of Socially Interactive Agents (SIAs) as AI-driven knowledge transfer facilitators. These agents interact autonomously and socially intelligently with users through multimodal behaviors (verbal, paraverbal, nonverbal), simulating expert roles in various organizational contexts. SIAs engage employees in empathic, natural-language dialogues, helping them externalize insights that might otherwise remain unspoken. Their success hinges on building trust, as employees are often hesitant to share tacit knowledge without assurance of confidentiality and appreciation. Key technologies include Large Language Models (LLMs) for generating context-relevant dialogue, Retrieval-Augmented Generation (RAG) to integrate organizational knowledge, and Chain-of-Thought (CoT) prompting to guide structured reflection. These enable SIAs to actively elicit knowledge, uncover implicit assumptions, and connect insights to broader organizational contexts. Potential applications span onboarding, where SIAs support personalized guidance and introductions, and knowledge retention, where they conduct structured interviews with retiring experts to capture heuristics behind decisions. Success depends on addressing ethical and operational challenges such as data privacy, algorithmic bias, and resistance to AI. Transparency, robust validation, and a culture of trust are essential to mitigate these risks.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.19932v1" target="_blank">CASE: An Agentic AI Framework for Enhancing Scam Intelligence in Digital Payments</a></h3>
                    <p><strong>Authors:</strong> Nitish Jaipuria, Lorenzo Gatto, Zijun Kan, Shankey Poddar, Bill Cheung, Diksha Bansal, Ramanan Balakrishnan, Aviral Suri, Jose Estevez</p>
                    <p><strong>Published:</strong> 8/27/2025</p>
                    <p><strong>Categories:</strong> cs.AI</p>
                    <p><strong>Summary:</strong> The proliferation of digital payment platforms has transformed commerce, offering unmatched convenience and accessibility globally. However, this growth has also attracted malicious actors, leading to a corresponding increase in sophisticated social engineering scams. These scams are often initiated and orchestrated on multiple surfaces outside the payment platform, making user and transaction-based signals insufficient for a complete understanding of the scams methodology and underlying patterns, without which it is very difficult to prevent it in a timely manner. This paper presents CASE (Conversational Agent for Scam Elucidation), a novel Agentic AI framework that addresses this problem by collecting and managing user scam feedback in a safe and scalable manner. A conversational agent is uniquely designed to proactively interview potential victims to elicit intelligence in the form of a detailed conversation. The conversation transcripts are then consumed by another AI system that extracts information and converts it into structured data for downstream usage in automated and manual enforcement mechanisms. Using Googles Gemini family of LLMs, we implemented this framework on Google Pay (GPay) India. By augmenting our existing features with this new intelligence, we have observed a 21% uplift in the volume of scam enforcements. The architecture and its robust evaluation framework are highly generalizable, offering a blueprint for building similar AI-driven systems to collect and manage scam intelligence in other sensitive domains.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.19924v1" target="_blank">FlowletFormer: Network Behavioral Semantic Aware Pre-training Model for Traffic Classification</a></h3>
                    <p><strong>Authors:</strong> Liming Liu, Ruoyu Li, Qing Li, Meijia Hou, Yong Jiang, Mingwei Xu</p>
                    <p><strong>Published:</strong> 8/27/2025</p>
                    <p><strong>Categories:</strong> cs.LG</p>
                    <p><strong>Summary:</strong> Network traffic classification using pre-training models has shown promising results, but existing methods struggle to capture packet structural characteristics, flow-level behaviors, hierarchical protocol semantics, and inter-packet contextual relationships. To address these challenges, we propose FlowletFormer, a BERT-based pre-training model specifically designed for network traffic analysis. FlowletFormer introduces a Coherent Behavior-Aware Traffic Representation Model for segmenting traffic into semantically meaningful units, a Protocol Stack Alignment-Based Embedding Layer to capture multilayer protocol semantics, and Field-Specific and Context-Aware Pretraining Tasks to enhance both inter-packet and inter-flow learning. Experimental results demonstrate that FlowletFormer significantly outperforms existing methods in the effectiveness of traffic representation, classification accuracy, and few-shot learning capability. Moreover, by effectively integrating domain-specific network knowledge, FlowletFormer shows better comprehension of the principles of network transmission (e.g., stateful connections of TCP), providing a more robust and trustworthy framework for traffic analysis.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.19922v1" target="_blank">HEAL: A Hypothesis-Based Preference-Aware Analysis Framework</a></h3>
                    <p><strong>Authors:</strong> Yifu Huo, Chenglong Wang, Qiren Zhu, Shunjie Xing, Tong Xiao, Chunliang Zhang, Tongran Liu, Jinbo Zhu</p>
                    <p><strong>Published:</strong> 8/27/2025</p>
                    <p><strong>Categories:</strong> cs.CL</p>
                    <p><strong>Summary:</strong> Preference optimization methods like DPO have achieved remarkable performance in LLM alignment. However, the evaluation for these methods relies on a single response and overlooks other potential outputs, which could also be generated in real-world applications within this hypothetical space. To address this issue, this paper presents a \textbf{H}ypothesis-based Pr\textbf{E}ference-aware \textbf{A}na\textbf{L}ysis Framework (HEAL), a novel evaluation paradigm that formulates preference alignment as a re-ranking process within hypothesis spaces. The framework incorporates two complementary metrics: ranking accuracy for evaluating ordinal consistency and preference strength correlation for assessing continuous alignment. To facilitate this framework, we develop UniHypoBench, a unified hypothesis benchmark constructed from diverse instruction-response pairs. Through extensive experiments based on HEAL, with a particular focus on the intrinsic mechanisms of preference learning, we demonstrate that current preference learning methods can effectively capture preferences provided by proxy models while simultaneously suppressing negative samples. These findings contribute to preference learning research through two significant avenues. Theoretically, we introduce hypothesis space analysis as an innovative paradigm for understanding preference alignment. Practically, HEAL offers researchers robust diagnostic tools for refining preference optimization methods, while our empirical results identify promising directions for developing more advanced alignment algorithms capable of comprehensive preference capture.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.19920v1" target="_blank">Walk the Robot: Exploring Soft Robotic Morphological Communication driven by Spiking Neural Networks</a></h3>
                    <p><strong>Authors:</strong> Matthew Meek, Guy Tallent, Thomas Breimer, James Gaskell, Abhay Kashyap, Atharv Tekurkar, Jonathan Fischman, Luodi Wang, Viet-Dung Nguyen, John Rieffel</p>
                    <p><strong>Published:</strong> 8/27/2025</p>
                    <p><strong>Categories:</strong> cs.NE</p>
                    <p><strong>Summary:</strong> Recently, researchers have explored control methods that embrace nonlinear dynamic coupling instead of suppressing it. Such designs leverage dynamical coupling for communication between different parts of the robot. Morphological communication refers to when those dynamics can be used as an emergent data bus to facilitate coordination among independent controller modules within the same robot. Previous research with tensegrity-based robot designs has shown that evolutionary learning models that evolve spiking neural networks (SNN) as robot control mechanisms are effective for controlling non-rigid robots. Our own research explores the emergence of morphological communication in an SNN-based simulated soft robot in theEvoGym environment.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.19919v1" target="_blank">Your AI Bosses Are Still Prejudiced: The Emergence of Stereotypes in LLM-Based Multi-Agent Systems</a></h3>
                    <p><strong>Authors:</strong> Jingyu Guo, Yingying Xu</p>
                    <p><strong>Published:</strong> 8/27/2025</p>
                    <p><strong>Categories:</strong> cs.CL</p>
                    <p><strong>Summary:</strong> While stereotypes are well-documented in human social interactions, AI systems are often presumed to be less susceptible to such biases. Previous studies have focused on biases inherited from training data, but whether stereotypes can emerge spontaneously in AI agent interactions merits further exploration. Through a novel experimental framework simulating workplace interactions with neutral initial conditions, we investigate the emergence and evolution of stereotypes in LLM-based multi-agent systems. Our findings reveal that (1) LLM-Based AI agents develop stereotype-driven biases in their interactions despite beginning without predefined biases; (2) stereotype effects intensify with increased interaction rounds and decision-making power, particularly after introducing hierarchical structures; (3) these systems exhibit group effects analogous to human social behavior, including halo effects, confirmation bias, and role congruity; and (4) these stereotype patterns manifest consistently across different LLM architectures. Through comprehensive quantitative analysis, these findings suggest that stereotype formation in AI systems may arise as an emergent property of multi-agent interactions, rather than merely from training data biases. Our work underscores the need for future research to explore the underlying mechanisms of this phenomenon and develop strategies to mitigate its ethical impacts.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.19915v1" target="_blank">Ontology-Based Concept Distillation for Radiology Report Retrieval and Labeling</a></h3>
                    <p><strong>Authors:</strong> Felix Nützel, Mischa Dombrowski, Bernhard Kainz</p>
                    <p><strong>Published:</strong> 8/27/2025</p>
                    <p><strong>Categories:</strong> cs.LG</p>
                    <p><strong>Summary:</strong> Retrieval-augmented learning based on radiology reports has emerged as a promising direction to improve performance on long-tail medical imaging tasks, such as rare disease detection in chest X-rays. Most existing methods rely on comparing high-dimensional text embeddings from models like CLIP or CXR-BERT, which are often difficult to interpret, computationally expensive, and not well-aligned with the structured nature of medical knowledge. We propose a novel, ontology-driven alternative for comparing radiology report texts based on clinically grounded concepts from the Unified Medical Language System (UMLS). Our method extracts standardised medical entities from free-text reports using an enhanced pipeline built on RadGraph-XL and SapBERT. These entities are linked to UMLS concepts (CUIs), enabling a transparent, interpretable set-based representation of each report. We then define a task-adaptive similarity measure based on a modified and weighted version of the Tversky Index that accounts for synonymy, negation, and hierarchical relationships between medical entities. This allows efficient and semantically meaningful similarity comparisons between reports. We demonstrate that our approach outperforms state-of-the-art embedding-based retrieval methods in a radiograph classification task on MIMIC-CXR, particularly in long-tail settings. Additionally, we use our pipeline to generate ontology-backed disease labels for MIMIC-CXR, offering a valuable new resource for downstream learning tasks. Our work provides more explainable, reliable, and task-specific retrieval strategies in clinical AI systems, especially when interpretability and domain knowledge integration are essential. Our code is available at https://github.com/Felix-012/ontology-concept-distillation</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.19914v1" target="_blank">The Next Layer: Augmenting Foundation Models with Structure-Preserving and Attention-Guided Learning for Local Patches to Global Context Awareness in Computational Pathology</a></h3>
                    <p><strong>Authors:</strong> Muhammad Waqas, Rukhmini Bandyopadhyay, Eman Showkatian, Amgad Muneer, Anas Zafar, Frank Rojas Alvarez, Maricel Corredor Marin, Wentao Li, David Jaffray, Cara Haymaker, John Heymach, Natalie I Vokes, Luisa Maren Solis Soto, Jianjun Zhang, Jia Wu</p>
                    <p><strong>Published:</strong> 8/27/2025</p>
                    <p><strong>Categories:</strong> q-bio.QM, cs.AI, stat.ML</p>
                    <p><strong>Summary:</strong> Foundation models have recently emerged as powerful feature extractors in computational pathology, yet they typically omit mechanisms for leveraging the global spatial structure of tissues and the local contextual relationships among diagnostically relevant regions - key elements for understanding the tumor microenvironment. Multiple instance learning (MIL) remains an essential next step following foundation model, designing a framework to aggregate patch-level features into slide-level predictions. We present EAGLE-Net, a structure-preserving, attention-guided MIL architecture designed to augment prediction and interpretability. EAGLE-Net integrates multi-scale absolute spatial encoding to capture global tissue architecture, a top-K neighborhood-aware loss to focus attention on local microenvironments, and background suppression loss to minimize false positives. We benchmarked EAGLE-Net on large pan-cancer datasets, including three cancer types for classification (10,260 slides) and seven cancer types for survival prediction (4,172 slides), using three distinct histology foundation backbones (REMEDIES, Uni-V1, Uni2-h). Across tasks, EAGLE-Net achieved up to 3% higher classification accuracy and the top concordance indices in 6 of 7 cancer types, producing smooth, biologically coherent attention maps that aligned with expert annotations and highlighted invasive fronts, necrosis, and immune infiltration. These results position EAGLE-Net as a generalizable, interpretable framework that complements foundation models, enabling improved biomarker discovery, prognostic modeling, and clinical decision support</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.19905v1" target="_blank">Hyperspectral Sensors and Autonomous Driving: Technologies, Limitations, and Opportunities</a></h3>
                    <p><strong>Authors:</strong> Imad Ali Shah, Jiarong Li, Roshan George, Tim Brophy, Enda Ward, Martin Glavin, Edward Jones, Brian Deegan</p>
                    <p><strong>Published:</strong> 8/27/2025</p>
                    <p><strong>Categories:</strong> cs.CV, cs.ET</p>
                    <p><strong>Summary:</strong> Hyperspectral imaging (HSI) offers a transformative sensing modality for Advanced Driver Assistance Systems (ADAS) and autonomous driving (AD) applications, enabling material-level scene understanding through fine spectral resolution beyond the capabilities of traditional RGB imaging. This paper presents the first comprehensive review of HSI for automotive applications, examining the strengths, limitations, and suitability of current HSI technologies in the context of ADAS/AD. In addition to this qualitative review, we analyze 216 commercially available HSI and multispectral imaging cameras, benchmarking them against key automotive criteria: frame rate, spatial resolution, spectral dimensionality, and compliance with AEC-Q100 temperature standards. Our analysis reveals a significant gap between HSIs demonstrated research potential and its commercial readiness. Only four cameras meet the defined performance thresholds, and none comply with AEC-Q100 requirements. In addition, the paper reviews recent HSI datasets and applications, including semantic segmentation for road surface classification, pedestrian separability, and adverse weather perception. Our review shows that current HSI datasets are limited in terms of scale, spectral consistency, the number of spectral channels, and environmental diversity, posing challenges for the development of perception algorithms and the adequate validation of HSIs true potential in ADAS/AD applications. This review paper establishes the current state of HSI in automotive contexts as of 2025 and outlines key research directions toward practical integration of spectral imaging in ADAS and autonomous systems.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.19896v1" target="_blank">NM-Hebb: Coupling Local Hebbian Plasticity with Metric Learning for More Accurate and Interpretable CNNs</a></h3>
                    <p><strong>Authors:</strong> Davorin Miličević, Ratko Grbić</p>
                    <p><strong>Published:</strong> 8/27/2025</p>
                    <p><strong>Categories:</strong> cs.LG, cs.CV, I.2.6; I.5.4</p>
                    <p><strong>Summary:</strong> Deep Convolutional Neural Networks (CNNs) achieve high accuracy but often rely on purely global, gradient-based optimisation, which can lead to overfitting, redundant filters, and reduced interpretability. To address these limitations, we propose NM-Hebb, a two-phase training framework that integrates neuro-inspired local plasticity with distance-aware supervision. Phase 1 extends standard supervised training by jointly optimising a cross-entropy objective with two biologically inspired mechanisms: (i) a Hebbian regulariser that aligns the spatial mean of activations with the mean of the corresponding convolutional filter weights, encouraging structured, reusable primitives; and (ii) a learnable neuromodulator that gates an elastic-weight-style consolidation loss, preserving beneficial parameters without freezing the network. Phase 2 fine-tunes the backbone with a pairwise metric-learning loss, explicitly compressing intra-class distances and enlarging inter-class margins in the embedding space. Evaluated on CIFAR-10, CIFAR-100, and TinyImageNet across five backbones (ResNet-18, VGG-11, MobileNet-v2, EfficientNet-V2, DenseNet-121), NM-Hebb achieves consistent gains over baseline and other methods: Top-1 accuracy improves by +2.0-10.0 pp (CIFAR-10), +2.0-9.0 pp (CIFAR-100), and up to +4.3-8.9 pp (TinyImageNet), with Normalised Mutual Information (NMI) increased by up to +0.15. Qualitative visualisations and filter-level analyses further confirm that NM-Hebb produces more structured and selective features, yielding tighter and more interpretable class clusters. Overall, coupling local Hebbian plasticity with metric-based fine-tuning yields CNNs that are not only more accurate but also more interpretable, offering practical benefits for resource-constrained and safety-critical AI deployments.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.19894v1" target="_blank">Kullback-Leibler Potential for Non-Ergodic Replication Dynamics:An Information-Theoretic Second Law</a></h3>
                    <p><strong>Authors:</strong> Tatsuaki Tsuruyama</p>
                    <p><strong>Published:</strong> 8/27/2025</p>
                    <p><strong>Categories:</strong> math-ph, math.MP, physics.data-an</p>
                    <p><strong>Summary:</strong> This study aims to quantify and visualize the degradation of fidelity (information degradation) that inevitably accompanies the replication of information within the framework of information thermodynamics and to propose an information-theoretic formulation of the second law based on this phenomenon. While previous research in information thermodynamics has focused on the thermodynamic costs associated with information erasure or measurement through concepts such as Landauers principle and mutual information, little systematic discussion has addressed the inherently irreversible nature of replication itself and the accompanying degradation of information structure. In this study, we construct a mathematical model of information replication using a discrete Markov model and Gaussian convolution, and quantify changes in information at each replication step: Shannon entropy, cross-entropy, and the Kullback--Leibler divergence (KLD). The monotonic decrease of KLD exhibits a Lyapunov-like property, which can be interpreted as a potential analogous to the free energy in the process by which a nonequilibrium system converges to a particular steady state. Furthermore, we extend this framework to the potential applicability to biological information processes such as DNA replication, showing that the free energy required for degradation and repair can be expressed in terms of KLD. This contributes to building a unified information-thermodynamic framework for operations such as replication, transmission, and repair of information.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.19887v1" target="_blank">Bangla-Bayanno: A 52K-Pair Bengali Visual Question Answering Dataset with LLM-Assisted Translation Refinement</a></h3>
                    <p><strong>Authors:</strong> Mohammed Rakibul Hasan, Rafi Majid, Ahanaf Tahmid</p>
                    <p><strong>Published:</strong> 8/27/2025</p>
                    <p><strong>Categories:</strong> cs.CL, cs.CV</p>
                    <p><strong>Summary:</strong> In this paper, we introduce Bangla-Bayanno, an open-ended Visual Question Answering (VQA) Dataset in Bangla, a widely used, low-resource language in multimodal AI research. The majority of existing datasets are either manually annotated with an emphasis on a specific domain, query type, or answer type or are constrained by niche answer formats. In order to mitigate human-induced errors and guarantee lucidity, we implemented a multilingual LLM-assisted translation refinement pipeline. This dataset overcomes the issues of low-quality translations from multilingual sources. The dataset comprises 52,650 question-answer pairs across 4750+ images. Questions are classified into three distinct answer types: nominal (short descriptive), quantitative (numeric), and polar (yes/no). Bangla-Bayanno provides the most comprehensive open-source, high-quality VQA benchmark in Bangla, aiming to advance research in low-resource multimodal learning and facilitate the development of more inclusive AI systems.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.19884v1" target="_blank">Parameter-Free Structural-Diversity Message Passing for Graph Neural Networks</a></h3>
                    <p><strong>Authors:</strong> Mingyue Kong, Yinglong Zhang, Chengda Xu, Xuewen Xia, Xing Xu</p>
                    <p><strong>Published:</strong> 8/27/2025</p>
                    <p><strong>Categories:</strong> cs.LG</p>
                    <p><strong>Summary:</strong> Graph Neural Networks (GNNs) have shown remarkable performance in structured data modeling tasks such as node classification. However, mainstream approaches generally rely on a large number of trainable parameters and fixed aggregation rules, making it difficult to adapt to graph data with strong structural heterogeneity and complex feature distributions. This often leads to over-smoothing of node representations and semantic degradation. To address these issues, this paper proposes a parameter-free graph neural network framework based on structural diversity, namely SDGNN (Structural-Diversity Graph Neural Network). The framework is inspired by structural diversity theory and designs a unified structural-diversity message passing mechanism that simultaneously captures the heterogeneity of neighborhood structures and the stability of feature semantics, without introducing additional trainable parameters. Unlike traditional parameterized methods, SDGNN does not rely on complex model training, but instead leverages complementary modeling from both structure-driven and feature-driven perspectives, thereby effectively improving adaptability across datasets and scenarios. Experimental results show that on eight public benchmark datasets and an interdisciplinary PubMed citation network, SDGNN consistently outperforms mainstream GNNs under challenging conditions such as low supervision, class imbalance, and cross-domain transfer. This work provides a new theoretical perspective and general approach for the design of parameter-free graph neural networks, and further validates the importance of structural diversity as a core signal in graph representation learning. To facilitate reproducibility and further research, the full implementation of SDGNN has been released at: https://github.com/mingyue15694/SGDNN/tree/main</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.19883v1" target="_blank">AI-Powered Detection of Inappropriate Language in Medical School Curricula</a></h3>
                    <p><strong>Authors:</strong> Chiman Salavati, Shannon Song, Scott A. Hale, Roberto E. Montenegro, Shiri Dori-Hacohen, Fabricio Murai</p>
                    <p><strong>Published:</strong> 8/27/2025</p>
                    <p><strong>Categories:</strong> cs.CL, cs.AI, cs.CY, I.2.1; I.2.7</p>
                    <p><strong>Summary:</strong> The use of inappropriate language -- such as outdated, exclusionary, or non-patient-centered terms -- medical instructional materials can significantly influence clinical training, patient interactions, and health outcomes. Despite their reputability, many materials developed over past decades contain examples now considered inappropriate by current medical standards. Given the volume of curricular content, manually identifying instances of inappropriate use of language (IUL) and its subcategories for systematic review is prohibitively costly and impractical. To address this challenge, we conduct a first-in-class evaluation of small language models (SLMs) fine-tuned on labeled data and pre-trained LLMs with in-context learning on a dataset containing approximately 500 documents and over 12,000 pages. For SLMs, we consider: (1) a general IUL classifier, (2) subcategory-specific binary classifiers, (3) a multilabel classifier, and (4) a two-stage hierarchical pipeline for general IUL detection followed by multilabel classification. For LLMs, we consider variations of prompts that include subcategory definitions and/or shots. We found that both LLama-3 8B and 70B, even with carefully curated shots, are largely outperformed by SLMs. While the multilabel classifier performs best on annotated data, supplementing training with unflagged excerpts as negative examples boosts the specific classifiers AUC by up to 25%, making them most effective models for mitigating harmful language in medical curricula.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.19882v1" target="_blank">Generative AI for Testing of Autonomous Driving Systems: A Survey</a></h3>
                    <p><strong>Authors:</strong> Qunying Song, He Ye, Mark Harman, Federica Sarro</p>
                    <p><strong>Published:</strong> 8/27/2025</p>
                    <p><strong>Categories:</strong> cs.SE, cs.AI</p>
                    <p><strong>Summary:</strong> Autonomous driving systems (ADS) have been an active area of research, with the potential to deliver significant benefits to society. However, before large-scale deployment on public roads, extensive testing is necessary to validate their functionality and safety under diverse driving conditions. Therefore, different testing approaches are required, and achieving effective and efficient testing of ADS remains an open challenge. Recently, generative AI has emerged as a powerful tool across many domains, and it is increasingly being applied to ADS testing due to its ability to interpret context, reason about complex tasks, and generate diverse outputs. To gain a deeper understanding of its role in ADS testing, we systematically analyzed 91 relevant studies and synthesized their findings into six major application categories, primarily centered on scenario-based testing of ADS. We also reviewed their effectiveness and compiled a wide range of datasets, simulators, ADS, metrics, and benchmarks used for evaluation, while identifying 27 limitations. This survey provides an overview and practical insights into the use of generative AI for testing ADS, highlights existing challenges, and outlines directions for future research in this rapidly evolving field.</p>
                
            
                
                    <h3><a href="http://dx.doi.org/10.1145/3748336.3748341" target="_blank">The IRMA Dataset: A Structured Audio-MIDI Corpus for Iranian Classical Music</a></h3>
                    <p><strong>Authors:</strong> Sepideh Shafiei, Shapour Hakam</p>
                    <p><strong>Published:</strong> 8/27/2025</p>
                    <p><strong>Categories:</strong> cs.SD, cs.DL</p>
                    <p><strong>Summary:</strong> We present the IRMA Dataset (Iranian Radif MIDI Audio), a multi-level, open-access corpus designed for the computational study of Iranian classical music, with a particular emphasis on the radif, a structured repertoire of modal-melodic units central to pedagogy and performance. The dataset combines symbolic MIDI representations, phrase-level audio-MIDI alignment, musicological transcriptions in PDF format, and comparative tables of theoretical information curated from a range of performers and scholars. We outline the multi-phase construction process, including segment annotation, alignment methods, and a structured system of identifier codes to reference individual musical units. The current release includes the complete radif of Karimi; MIDI files and metadata from Mirza Abdollahs radif; selected segments from the vocal radif of Davami, as transcribed by Payvar and Fereyduni; and a dedicated section featuring audio-MIDI examples of tahrir ornamentation performed by prominent 20th-century vocalists. While the symbolic and analytical components are released under an open-access license (CC BY-NC 4.0), some referenced audio recordings and third-party transcriptions are cited using discographic information to enable users to locate the original materials independently, pending copyright permission. Serving both as a scholarly archive and a resource for computational analysis, this dataset supports applications in ethnomusicology, pedagogy, symbolic audio research, cultural heritage preservation, and AI-driven tasks such as automatic transcription and music generation. We welcome collaboration and feedback to support its ongoing refinement and broader integration into musicological and machine learning workflows.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.19870v1" target="_blank">Secure Multi-LLM Agentic AI and Agentification for Edge General Intelligence by Zero-Trust: A Survey</a></h3>
                    <p><strong>Authors:</strong> Yinqiu Liu, Ruichen Zhang, Haoxiang Luo, Yijing Lin, Geng Sun, Dusit Niyato, Hongyang Du, Zehui Xiong, Yonggang Wen, Abbas Jamalipour, Dong In Kim, Ping Zhang</p>
                    <p><strong>Published:</strong> 8/27/2025</p>
                    <p><strong>Categories:</strong> cs.NI</p>
                    <p><strong>Summary:</strong> Agentification serves as a critical enabler of Edge General Intelligence (EGI), transforming massive edge devices into cognitive agents through integrating Large Language Models (LLMs) and perception, reasoning, and acting modules. These agents collaborate across heterogeneous edge infrastructures, forming multi-LLM agentic AI systems that leverage collective intelligence and specialized capabilities to tackle complex, multi-step tasks. However, the collaborative nature of multi-LLM systems introduces critical security vulnerabilities, including insecure inter-LLM communications, expanded attack surfaces, and cross-domain data leakage that traditional perimeter-based security cannot adequately address. To this end, this survey introduces zero-trust security of multi-LLM in EGI, a paradigmatic shift following the ``never trust, always verify principle. We begin by systematically analyzing the security risks in multi-LLM systems within EGI contexts. Subsequently, we present the vision of a zero-trust multi-LLM framework in EGI. We then survey key technical progress to facilitate zero-trust multi-LLM systems in EGI. Particularly, we categorize zero-trust security mechanisms into model- and system-level approaches. The former and latter include strong identification, context-aware access control, etc., and proactive maintenance, blockchain-based management, etc., respectively. Finally, we identify critical research directions. This survey serves as the first systematic treatment of zero-trust applied to multi-LLM systems, providing both theoretical foundations and practical strategies.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.19869v1" target="_blank">Revisiting Classical Two-phase and Kerner Three-phase Traffic Flow Theories: A Comparison of Pre-crash and Normal Traffic Conditions</a></h3>
                    <p><strong>Authors:</strong> Md Mahmud Hossain, Kazi Tahsin Huda, Moinul Hossain, Yasunori Muromachi</p>
                    <p><strong>Published:</strong> 8/27/2025</p>
                    <p><strong>Categories:</strong> physics.soc-ph</p>
                    <p><strong>Summary:</strong> Extensive research has been conducted to develop statistical and artificial intelligence-based models for predicting short-term crash probabilities using fundamental traffic flow variables and their associated descriptive statistics and mathematical transformations. However, there has been limited exploration into whether and how the fundamental relationships within traffic flow theories vary between pre-crash and normal traffic conditions. This study reevaluates four classical two-phase traffic flow theories and employs two methods from Kerners three-phase traffic flow theory to compare their characteristics in the context of pre-crash and normal traffic conditions. The investigation is centered around the Shibuya 3 and Shinjuku 4 routes within the Tokyo Metropolitan Expressway. Data from both crashes and detectors was collected over a six-month period, spanning from March 2014 to August 2014. The findings reveal that data from the nearest downstream detectors to the crash locations provided a superior fit for pre-crash data compared to the upstream detectors. Notably, a noticeable decrease in goodness-of-fit was observed when compared to normal traffic conditions. In pre-crash scenarios, wide-moving jams exhibited a faster propagation from downstream to upstream, distinct from the patterns observed in normal traffic conditions. Furthermore, pre-crash data displayed a higher standard deviation in the calculated wide-moving jam velocities compared to normal traffic conditions. These insights have the potential to be highly valuable in the development of predictive crash models and in the estimation of traffic volumes on freeways across various timeframes, accounting for both free-flow and congested traffic scenarios.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.19866v1" target="_blank">TrajFusionNet: Pedestrian Crossing Intention Prediction via Fusion of Sequential and Visual Trajectory Representations</a></h3>
                    <p><strong>Authors:</strong> François G. Landry, Moulay A. Akhloufi</p>
                    <p><strong>Published:</strong> 8/27/2025</p>
                    <p><strong>Categories:</strong> cs.CV, cs.LG</p>
                    <p><strong>Summary:</strong> With the introduction of vehicles with autonomous capabilities on public roads, predicting pedestrian crossing intention has emerged as an active area of research. The task of predicting pedestrian crossing intention involves determining whether pedestrians in the scene are likely to cross the road or not. In this work, we propose TrajFusionNet, a novel transformer-based model that combines future pedestrian trajectory and vehicle speed predictions as priors for predicting crossing intention. TrajFusionNet comprises two branches: a Sequence Attention Module (SAM) and a Visual Attention Module (VAM). The SAM branch learns from a sequential representation of the observed and predicted pedestrian trajectory and vehicle speed. Complementarily, the VAM branch enables learning from a visual representation of the predicted pedestrian trajectory by overlaying predicted pedestrian bounding boxes onto scene images. By utilizing a small number of lightweight modalities, TrajFusionNet achieves the lowest total inference time (including model runtime and data preprocessing) among current state-of-the-art approaches. In terms of performance, it achieves state-of-the-art results across the three most commonly used datasets for pedestrian crossing intention prediction.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.19854v1" target="_blank">4D-Printing Assisted Scaffolds to Form Cardiac Bricks</a></h3>
                    <p><strong>Authors:</strong> Hossein Goodarzi Hosseinabadi</p>
                    <p><strong>Published:</strong> 8/27/2025</p>
                    <p><strong>Categories:</strong> physics.med-ph, cond-mat.mtrl-sci, q-bio.TO</p>
                    <p><strong>Summary:</strong> Myocardial infarction causes myocardium thinning, fibrosis, and progressive heart failure. Engineered human myocardium (EHM) is tested clinically as a first-in-class product for sustainable remuscularization in patients with advanced heart failure. Current EHM production procedure from iPSC-derived cardiomyocytes and stromal cells, is time consuming and involves thin constructs. Here, I introduce 4D-DLP-printed foldable scaffolds with potential to create modular cylindrical cardiac bricks. This enables self-assembly into thicker and aligned sarcomeres with synchronous contractility mimicking a native myocardium. When optimized and integrated with cryopreservation protocols, the biomanufacturing and biobanking of these cellular building blocks may overcome current EHM limitations and advance translational regenerative therapies for myocardial infarction. The structure-material properties investigations into these new class of life building blocks paves the way for future medical breakthroughs.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.19851v1" target="_blank">Tracking World States with Language Models: State-Based Evaluation Using Chess</a></h3>
                    <p><strong>Authors:</strong> Romain Harang, Jason Naradowsky, Yaswitha Gujju, Yusuke Miyao</p>
                    <p><strong>Published:</strong> 8/27/2025</p>
                    <p><strong>Categories:</strong> cs.AI</p>
                    <p><strong>Summary:</strong> Large Language Models (LLMs) exhibit emergent capabilities in structured domains, suggesting they may implicitly internalize high-fidelity representations of world models. While probing techniques have shown promising signs of this in scientific and game-based settings, they rely on model-specific internal activations, which limit interpretability and generalizability. In this work, we propose a model-agnostic, state-based evaluation framework using chess as a benchmark to assess whether LLMs preserve the semantics of structured environments. Our method analyzes the downstream legal move distributions (state affordances) to estimate semantic fidelity between predicted and actual game states. This approach offers a more meaningful evaluation than conventional string-based metrics by aligning more closely with the strategic and rule-governed nature of chess. Experimental results demonstrate that our metrics capture deficiencies in state-tracking, highlighting limitations of LLMs in maintaining coherent internal models over long sequences. Our framework provides a robust tool for evaluating structured reasoning in LLMs without requiring internal model access, and generalizes to a wide class of symbolic environments.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.19849v1" target="_blank">Tunable quantum anomalous Hall effect in fullerene monolayers</a></h3>
                    <p><strong>Authors:</strong> Leonard Werner Pingen, Jiaqi Wu, Bo Peng</p>
                    <p><strong>Published:</strong> 8/27/2025</p>
                    <p><strong>Categories:</strong> cond-mat.mes-hall, cond-mat.mtrl-sci, physics.chem-ph, physics.comp-ph, quant-ph</p>
                    <p><strong>Summary:</strong> Nearly four decades after its theoretical prediction, the search for material realizations of quantum anomalous Hall effect (QAHE) remains a highly active field of research. Many materials have been predicted to exhibit quantum anomalous Hall (QAH) physics under feasible conditions but the experimental verification remains widely elusive. In this work, we propose an alternative approach towards QAH materials design by engineering customized molecular building blocks. We demonstrate this ansatz for a two-dimensional (2D) honeycomb lattice of C26 fullerenes, which exhibits a ferromagnetic ground state and thus breaks time-reversal symmetry. The molecular system is found to be highly tunable with respect to its magnetic degrees of freedom and applied strain, giving rise to a rich phase diagram with Chern numbers C= +/-2, +/-1, 0. Our proposal offers a versatile platform to realize tunable QAH physics under accessible conditions and provides an experimentally feasible approach for chemical synthesis of molecular networks with QAHE.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.19843v1" target="_blank">SoK: Large Language Model Copyright Auditing via Fingerprinting</a></h3>
                    <p><strong>Authors:</strong> Shuo Shao, Yiming Li, Yu He, Hongwei Yao, Wenyuan Yang, Dacheng Tao, Zhan Qin</p>
                    <p><strong>Published:</strong> 8/27/2025</p>
                    <p><strong>Categories:</strong> cs.CR, cs.AI, cs.CL</p>
                    <p><strong>Summary:</strong> The broad capabilities and substantial resources required to train Large Language Models (LLMs) make them valuable intellectual property, yet they remain vulnerable to copyright infringement, such as unauthorized use and model theft. LLM fingerprinting, a non-intrusive technique that extracts and compares the distinctive features from LLMs to identify infringements, offers a promising solution to copyright auditing. However, its reliability remains uncertain due to the prevalence of diverse model modifications and the lack of standardized evaluation. In this SoK, we present the first comprehensive study of LLM fingerprinting. We introduce a unified framework and formal taxonomy that categorizes existing methods into white-box and black-box approaches, providing a structured overview of the state of the art. We further propose LeaFBench, the first systematic benchmark for evaluating LLM fingerprinting under realistic deployment scenarios. Built upon mainstream foundation models and comprising 149 distinct model instances, LeaFBench integrates 13 representative post-development techniques, spanning both parameter-altering methods (e.g., fine-tuning, quantization) and parameter-independent mechanisms (e.g., system prompts, RAG). Extensive experiments on LeaFBench reveal the strengths and weaknesses of existing methods, thereby outlining future research directions and critical open problems in this emerging field. The code is available at https://github.com/shaoshuo-ss/LeaFBench.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.19836v1" target="_blank">Scalable and consistent few-shot classification of survey responses using text embeddings</a></h3>
                    <p><strong>Authors:</strong> Jonas Timmann Mjaaland, Markus Fleten Kreutzer, Halvor Tyseng, Rebeckah K. Fussell, Gina Passante, N. G. Holmes, Anders Malthe-Sørenssen, Tor Ole B. Odden</p>
                    <p><strong>Published:</strong> 8/27/2025</p>
                    <p><strong>Categories:</strong> cs.CL, physics.ed-ph</p>
                    <p><strong>Summary:</strong> Qualitative analysis of open-ended survey responses is a commonly-used research method in the social sciences, but traditional coding approaches are often time-consuming and prone to inconsistency. Existing solutions from Natural Language Processing such as supervised classifiers, topic modeling techniques, and generative large language models have limited applicability in qualitative analysis, since they demand extensive labeled data, disrupt established qualitative workflows, and/or yield variable results. In this paper, we introduce a text embedding-based classification framework that requires only a handful of examples per category and fits well with standard qualitative workflows. When benchmarked against human analysis of a conceptual physics survey consisting of 2899 open-ended responses, our framework achieves a Cohens Kappa ranging from 0.74 to 0.83 as compared to expert human coders in an exhaustive coding scheme. We further show how performance of this framework improves with fine-tuning of the text embedding model, and how the method can be used to audit previously-analyzed datasets. These findings demonstrate that text embedding-assisted coding can flexibly scale to thousands of responses without sacrificing interpretability, opening avenues for deductive qualitative analysis at scale.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.19834v1" target="_blank">On the Future of Software Reuse in the Era of AI Native Software Engineering</a></h3>
                    <p><strong>Authors:</strong> Antero Taivalsaari, Tommi Mikkonen, Cesare Pautasso</p>
                    <p><strong>Published:</strong> 8/27/2025</p>
                    <p><strong>Categories:</strong> cs.SE</p>
                    <p><strong>Summary:</strong> Software development is currently under a paradigm shift in which artificial intelligence and generative software reuse are taking the center stage in software creation. Earlier opportunistic software reuse practices and organic software development methods are rapidly being replaced by AI Native approaches in which developers place their trust on code that has been generated by artificial intelligence. This is leading to a new form of software reuse that is conceptually not all that different from cargo cult development. In this paper we discuss the implications of AI-assisted generative software reuse, bring forth relevant questions, and define a research agenda for tackling the central issues associated with this emerging approach.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.20096v1" target="_blank">CODA: Coordinating the Cerebrum and Cerebellum for a Dual-Brain Computer Use Agent with Decoupled Reinforcement Learning</a></h3>
                    <p><strong>Authors:</strong> Zeyi Sun, Yuhang Cao, Jianze Liang, Qiushi Sun, Ziyu Liu, Zhixiong Zhang, Yuhang Zang, Xiaoyi Dong, Kai Chen, Dahua Lin, Jiaqi Wang</p>
                    <p><strong>Published:</strong> 8/27/2025</p>
                    <p><strong>Categories:</strong> cs.CV, cs.AI, cs.LG</p>
                    <p><strong>Summary:</strong> Autonomous agents for Graphical User Interfaces (GUIs) face significant challenges in specialized domains such as scientific computing, where both long-horizon planning and precise execution are required. Existing approaches suffer from a trade-off: generalist agents excel at planning but perform poorly in execution, while specialized agents demonstrate the opposite weakness. Recent compositional frameworks attempt to bridge this gap by combining a planner and an actor, but they are typically static and non-trainable, which prevents adaptation from experience. This is a critical limitation given the scarcity of high-quality data in scientific domains. To address these limitations, we introduce CODA, a novel and trainable compositional framework that integrates a generalist planner (Cerebrum) with a specialist executor (Cerebellum), trained via a dedicated two-stage pipeline. In the first stage, Specialization, we apply a decoupled GRPO approach to train an expert planner for each scientific application individually, bootstrapping from a small set of task trajectories. In the second stage, Generalization, we aggregate all successful trajectories from the specialized experts to build a consolidated dataset, which is then used for supervised fine-tuning of the final planner. This equips CODA with both robust execution and cross-domain generalization. Evaluated on four challenging applications from the ScienceBoard benchmark, CODA significantly outperforms baselines and establishes a new state of the art among open-source models.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.20094v1" target="_blank">(Un)solvable Matrix Models for BPS Correlators</a></h3>
                    <p><strong>Authors:</strong> Prokopii Anempodistov, Adolfo Holguin, Vladimir Kazakov, Harish Murali</p>
                    <p><strong>Published:</strong> 8/27/2025</p>
                    <p><strong>Categories:</strong> hep-th</p>
                    <p><strong>Summary:</strong> We propose and study a family of complex matrix models computing the protected two- and three-point correlation functions in $\mathcal{N}=4$ SYM. Our description allows us to directly relate the eigenvalue density of the matrix model for ``Huge operators with $ \Delta \sim N^2$ to the shape of droplets in the dual Lin-Lunin-Maldacena (LLM) geometry. We demonstrate how to determine the eigenvalue distribution for various choices of operators such as those of exponential, character, or coherent state type, which then allows us to efficiently compute one-point functions of light chiral primaries in generic LLM backgrounds. In particular, we successfully match the results for light probes with the supergravity calculations of Skenderis and Taylor. We provide a large $N$ formalism for one-point functions of ``Giant probes, such as operators dual to giant graviton branes in LLM backgrounds, and explicitly apply it for particular backgrounds. We also explicitly compute the correlator of three huge half-BPS operators of exponential type and stacks of determinant operators by reducing them to the known matrix model problems such as the Potts or $O(n)$ model on random planar graphs. Finally, we point out a curious relation between the correlators of $\frac{1}{4}$-BPS and $\frac{1}{8}$-BPS coherent state operators and the Eguchi-Kawai reduction of the Principal Chiral Model in $2D$ and $3D$ correspondingly.</p>
                
            
                
                    <h3><a href="http://dx.doi.org/10.1007/s11365-019-00627-z" target="_blank">From stand-up to start-up: exploring entrepreneurship competences and STEM womens intention</a></h3>
                    <p><strong>Authors:</strong> Armuna Cristina, Ramos Sergio, Juan Jesus, Feijoo Claudio, Arenal Alberto</p>
                    <p><strong>Published:</strong> 8/27/2025</p>
                    <p><strong>Categories:</strong> cs.CE</p>
                    <p><strong>Summary:</strong> This study seeks to explore the relationship between entrepreneurship competencies and intention (EI) of a sample of potential STEM entrepreneurs in order to assess the conventional assumption on women exhibiting lower rates of entrepreneurship intention than men and that the lack of competence perceived is a higher barrier to be an entrepreneur for them. The model used for the analysis takes as reference the Entrepreneurship Competences Framework (EntreComp) proposed by the European Commission (EC) as a common guide to inspire entrepreneurship education. Data gathering is based on a structured questionnaire. The conducted analysis uses Students t test means comparison and factor analysis to define the model of competences, and a multiple regression model to study the relationship between competences and skill factors in EI. Findings do not validate the hypothesis that women have fewer entrepreneurship intentions than men. Also, slight differences on the self-perceived competences are obtained by gender. In addition, the study confirms the hypothesis of a positive relationship between competences and EI, but here gender is not a moderating factor. Results are expected to contribute to the entrepreneurship competences debate and provide useful insights of application in entrepreneurship education with orientation towards the business creation.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.20089v1" target="_blank">Bridging Domain Gaps for Fine-Grained Moth Classification Through Expert-Informed Adaptation and Foundation Model Priors</a></h3>
                    <p><strong>Authors:</strong> Ross J Gardiner, Guillaume Mougeot, Sareh Rowlands, Benno I Simmons, Flemming Helsing, Toke Thomas Høye</p>
                    <p><strong>Published:</strong> 8/27/2025</p>
                    <p><strong>Categories:</strong> cs.CV</p>
                    <p><strong>Summary:</strong> Labelling images of Lepidoptera (moths) from automated camera systems is vital for understanding insect declines. However, accurate species identification is challenging due to domain shifts between curated images and noisy field imagery. We propose a lightweight classification approach, combining limited expert-labelled field data with knowledge distillation from the high-performance BioCLIP2 foundation model into a ConvNeXt-tiny architecture. Experiments on 101 Danish moth species from AMI camera systems demonstrate that BioCLIP2 substantially outperforms other methods and that our distilled lightweight model achieves comparable accuracy with significantly reduced computational cost. These insights offer practical guidelines for the development of efficient insect monitoring systems and bridging domain gaps for fine-grained classification.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.20088v1" target="_blank">AudioStory: Generating Long-Form Narrative Audio with Large Language Models</a></h3>
                    <p><strong>Authors:</strong> Yuxin Guo, Teng Wang, Yuying Ge, Shijie Ma, Yixiao Ge, Wei Zou, Ying Shan</p>
                    <p><strong>Published:</strong> 8/27/2025</p>
                    <p><strong>Categories:</strong> cs.CV, cs.MM, cs.SD</p>
                    <p><strong>Summary:</strong> Recent advances in text-to-audio (TTA) generation excel at synthesizing short audio clips but struggle with long-form narrative audio, which requires temporal coherence and compositional reasoning. To address this gap, we propose AudioStory, a unified framework that integrates large language models (LLMs) with TTA systems to generate structured, long-form audio narratives. AudioStory possesses strong instruction-following reasoning generation capabilities. It employs LLMs to decompose complex narrative queries into temporally ordered sub-tasks with contextual cues, enabling coherent scene transitions and emotional tone consistency. AudioStory has two appealing features: (1) Decoupled bridging mechanism: AudioStory disentangles LLM-diffuser collaboration into two specialized components, i.e., a bridging query for intra-event semantic alignment and a residual query for cross-event coherence preservation. (2) End-to-end training: By unifying instruction comprehension and audio generation within a single end-to-end framework, AudioStory eliminates the need for modular training pipelines while enhancing synergy between components. Furthermore, we establish a benchmark AudioStory-10K, encompassing diverse domains such as animated soundscapes and natural sound narratives. Extensive experiments show the superiority of AudioStory on both single-audio generation and narrative audio generation, surpassing prior TTA baselines in both instruction-following ability and audio fidelity. Our code is available at https://github.com/TencentARC/AudioStory</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.20084v1" target="_blank">Phase transition properties via partition function zeros: The Blume-Capel ferromagnet revisited</a></h3>
                    <p><strong>Authors:</strong> Leïla Moueddene, Nikolaos G Fytas, Bertrand Berche</p>
                    <p><strong>Published:</strong> 8/27/2025</p>
                    <p><strong>Categories:</strong> cond-mat.stat-mech</p>
                    <p><strong>Summary:</strong> Since the landmark work of Lee and Yang, locating the zeros of the partition function in the complex magnetic-field plane has become a powerful method for studying phase transitions. Fisher later extended this approach to complex temperatures, and subsequent generalizations introduced other control parameters, such as the crystal field. In previous works [Moueddene et al, J. Stat. Mech. (2024) 023206; Phys. Rev. E 110, 064144 (2024)] we applied this framework to the two- and three-dimensional Blume-Capel model -- a system with a rich phase structure where a second-order critical line meets a first-order line at a tricritical point. We showed that the scaling of Lee-Yang, Fisher, and crystal-field zeros yields accurate critical exponents even for modest lattice sizes. In the present study, we extend this analysis and demonstrate that simulations need not be performed exactly at the nominal transition point to obtain reliable exponent estimates. Strikingly, small system sizes are sufficient, which not only improves methodological efficiency but also advances the broader goal of reducing the carbon footprint of large-scale computational studies.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.20083v1" target="_blank">Disabling Self-Correction in Retrieval-Augmented Generation via Stealthy Retriever Poisoning</a></h3>
                    <p><strong>Authors:</strong> Yanbo Dai, Zhenlan Ji, Zongjie Li, Kuan Li, Shuai Wang</p>
                    <p><strong>Published:</strong> 8/27/2025</p>
                    <p><strong>Categories:</strong> cs.CR, cs.CL</p>
                    <p><strong>Summary:</strong> Retrieval-Augmented Generation (RAG) has become a standard approach for improving the reliability of large language models (LLMs). Prior work demonstrates the vulnerability of RAG systems by misleading them into generating attacker-chosen outputs through poisoning the knowledge base. However, this paper uncovers that such attacks could be mitigated by the strong \textit{self-correction ability (SCA)} of modern LLMs, which can reject false context once properly configured. This SCA poses a significant challenge for attackers aiming to manipulate RAG systems. In contrast to previous poisoning methods, which primarily target the knowledge base, we introduce \textsc{DisarmRAG}, a new poisoning paradigm that compromises the retriever itself to suppress the SCA and enforce attacker-chosen outputs. This compromisation enables the attacker to straightforwardly embed anti-SCA instructions into the context provided to the generator, thereby bypassing the SCA. To this end, we present a contrastive-learning-based model editing technique that performs localized and stealthy edits, ensuring the retriever returns a malicious instruction only for specific victim queries while preserving benign retrieval behavior. To further strengthen the attack, we design an iterative co-optimization framework that automatically discovers robust instructions capable of bypassing prompt-based defenses. We extensively evaluate DisarmRAG across six LLMs and three QA benchmarks. Our results show near-perfect retrieval of malicious instructions, which successfully suppress SCA and achieve attack success rates exceeding 90\% under diverse defensive prompts. Also, the edited retriever remains stealthy under several detection methods, highlighting the urgent need for retriever-centric defenses.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.20080v1" target="_blank">Seam360GS: Seamless 360° Gaussian Splatting from Real-World Omnidirectional Images</a></h3>
                    <p><strong>Authors:</strong> Changha Shin, Woong Oh Cho, Seon Joo Kim</p>
                    <p><strong>Published:</strong> 8/27/2025</p>
                    <p><strong>Categories:</strong> cs.CV, cs.GR</p>
                    <p><strong>Summary:</strong> 360-degree visual content is widely shared on platforms such as YouTube and plays a central role in virtual reality, robotics, and autonomous navigation. However, consumer-grade dual-fisheye systems consistently yield imperfect panoramas due to inherent lens separation and angular distortions. In this work, we introduce a novel calibration framework that incorporates a dual-fisheye camera model into the 3D Gaussian splatting pipeline. Our approach not only simulates the realistic visual artifacts produced by dual-fisheye cameras but also enables the synthesis of seamlessly rendered 360-degree images. By jointly optimizing 3D Gaussian parameters alongside calibration variables that emulate lens gaps and angular distortions, our framework transforms imperfect omnidirectional inputs into flawless novel view synthesis. Extensive evaluations on real-world datasets confirm that our method produces seamless renderings-even from imperfect images-and outperforms existing 360-degree rendering models.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.20073v1" target="_blank">High-frequency continuous gravitational waves searched in LIGO O3 public data with Einstein@Home</a></h3>
                    <p><strong>Authors:</strong> Brian McGloughlin, Benjamin Steltner, Jasper Martins, Maria Alessandra Papa, Heinz-Bernd Eggenstein, Jing Ming, Bernd Machenschalk, Reinhard Prix, Maximilian Bensch</p>
                    <p><strong>Published:</strong> 8/27/2025</p>
                    <p><strong>Categories:</strong> gr-qc</p>
                    <p><strong>Summary:</strong> We search for nearly-monochromatic gravitational wave signals with frequencies $800.0~\textrm{Hz} \leq f \leq 1686.0~\textrm{Hz}$ and spin-down $-2.7\times10^{-9}~\textrm{Hz}\,\textrm{s}^{-1} \leq \dot f \leq 0.2\times 10^{-9}~\textrm{Hz}\,\textrm{s}^{-1}$. We use LIGO O3 public data from the Hanford and Livingston detectors and deploy this search on the Einstein@Home volunteer-computing project. This is the most sensitive search carried out to date in this parameter space. Our results are consistent with a non-detection. We set upper limits on the gravitational wave amplitude $h_{0}$ and translate these to upper limits on neutron star ellipticity and on r-mode amplitude. The most stringent upper limits are at $800~\textrm{Hz}$ with $h_{0} = 1.32\times10^{-25}$, at the $90\%$ confidence level. Searching in the high frequency bands allows us to probe astrophysically interesting ellipticities with our results excluding isolated neutron stars rotating faster than $2.5~\textrm{ms}$ with ellipticities $\epsilon \geq 1.96 \times 10^{-8}\left[\frac{d}{100~\textrm{pc}}\right]$ within a distance $d$ from Earth. Our results also exclude r-mode amplitudes $\alpha \geq 7 \times 10^{-7}\left[\frac{d}{100~\textrm{pc}}\right]$ for neutron stars stars spinning faster than 400 Hz.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.20072v1" target="_blank">Discrete Diffusion VLA: Bringing Discrete Diffusion to Action Decoding in Vision-Language-Action Policies</a></h3>
                    <p><strong>Authors:</strong> Zhixuan Liang, Yizhuo Li, Tianshuo Yang, Chengyue Wu, Sitong Mao, Liuao Pei, Xiaokang Yang, Jiangmiao Pang, Yao Mu, Ping Luo</p>
                    <p><strong>Published:</strong> 8/27/2025</p>
                    <p><strong>Categories:</strong> cs.CV, cs.LG, cs.RO</p>
                    <p><strong>Summary:</strong> Vision-Language-Action (VLA) models adapt large vision-language backbones to map images and instructions to robot actions. However, prevailing VLA decoders either generate actions autoregressively in a fixed left-to-right order or attach continuous diffusion or flow matching heads outside the backbone, demanding specialized training and iterative sampling that hinder a unified, scalable architecture. We present Discrete Diffusion VLA, a single-transformer policy that models discretized action chunks with discrete diffusion and is trained with the same cross-entropy objective as the VLM backbone. The design retains diffusions progressive refinement paradigm while remaining natively compatible with the discrete token interface of VLMs. Our method achieves an adaptive decoding order that resolves easy action elements before harder ones and uses secondary remasking to revisit uncertain predictions across refinement rounds, which improves consistency and enables robust error correction. This unified decoder preserves pretrained vision language priors, supports parallel decoding, breaks the autoregressive bottleneck, and reduces the number of function evaluations. Discrete Diffusion VLA achieves 96.3% avg. SR on LIBERO, 71.2% visual matching on SimplerEnv Fractal and 49.3% overall on SimplerEnv Bridge, improving over both autoregressive and continuous diffusion baselines. These findings indicate that discrete-diffusion action decoder supports precise action modeling and consistent training, laying groundwork for scaling VLA to larger models and datasets.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.20071v1" target="_blank">A Partially Derivative-Free Proximal Method for Composite Multiobjective Optimization in the Hölder Setting</a></h3>
                    <p><strong>Authors:</strong> V. S. Amaral, P. B. Assunção, D. R. Souza</p>
                    <p><strong>Published:</strong> 8/27/2025</p>
                    <p><strong>Categories:</strong> math.OC</p>
                    <p><strong>Summary:</strong> This paper presents an algorithm for solving multiobjective optimization problems involving composite functions, where we minimize a quadratic model that approximates $F(x) - F(x^k)$ and that can be derivative-free. We establish theoretical assumptions about the component functions of the composition and provide comprehensive convergence and complexity analysis. Specifically, we prove that the proposed method converges to a weakly $\varepsilon$-approximate Pareto point in at most $\mathcal{O}\left(\varepsilon^{-\frac{\beta+1}{\beta}}\right)$ iterations, where $\beta$ denotes the H\{o}lder exponent of the gradient. The algorithm incorporates gradient approximations and a scaling matrix $B_k$ to achieve an optimal balance between computational accuracy and efficiency. Numerical experiments on robust biobjective instances with Lipschitz and H\{o}lder-gradient components illustrate the methods behavior. In these tests, the proposed approach was able to approximate the Pareto front under different levels of uncertainty and consistently recovered distinct solutions, even in challenging cases where the objectives have only H\{o}lder continuous gradients.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.20068v1" target="_blank">11Plus-Bench: Demystifying Multimodal LLM Spatial Reasoning with Cognitive-Inspired Analysis</a></h3>
                    <p><strong>Authors:</strong> Chengzu Li, Wenshan Wu, Huanyu Zhang, Qingtao Li, Zeyu Gao, Yan Xia, José Hernández-Orallo, Ivan Vulić, Furu Wei</p>
                    <p><strong>Published:</strong> 8/27/2025</p>
                    <p><strong>Categories:</strong> cs.CL, cs.CV, cs.LG</p>
                    <p><strong>Summary:</strong> For human cognitive process, spatial reasoning and perception are closely entangled, yet the nature of this interplay remains underexplored in the evaluation of multimodal large language models (MLLMs). While recent MLLM advancements show impressive performance on reasoning, their capacity for human-like spatial cognition remains an open question. In this work, we introduce a systematic evaluation framework to assess the spatial reasoning abilities of state-of-the-art MLLMs relative to human performance. Central to our work is 11Plus-Bench, a high-quality benchmark derived from realistic standardized spatial aptitude tests. 11Plus-Bench also features fine-grained expert annotations of both perceptual complexity and reasoning process, enabling detailed instance-level analysis of model behavior. Through extensive experiments across 14 MLLMs and human evaluation, we find that current MLLMs exhibit early signs of spatial cognition. Despite a large performance gap compared to humans, MLLMs cognitive profiles resemble those of humans in that cognitive effort correlates strongly with reasoning-related complexity. However, instance-level performance in MLLMs remains largely random, whereas human correctness is highly predictable and shaped by abstract pattern complexity. These findings highlight both emerging capabilities and limitations in current MLLMs spatial reasoning capabilities and provide actionable insights for advancing model design.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.20066v1" target="_blank">PAUL: Uncertainty-Guided Partition and Augmentation for Robust Cross-View Geo-Localization under Noisy Correspondence</a></h3>
                    <p><strong>Authors:</strong> Zheng Li, Yanming Guo, WenZhe Liu, Xueyi Zhang, Zhaoyun Ding, Long Xu, Mingrui Lao</p>
                    <p><strong>Published:</strong> 8/27/2025</p>
                    <p><strong>Categories:</strong> cs.CV</p>
                    <p><strong>Summary:</strong> Cross-view geo-localization is a critical task for UAV navigation, event detection, and aerial surveying, as it enables matching between drone-captured and satellite imagery. Most existing approaches embed multi-modal data into a joint feature space to maximize the similarity of paired images. However, these methods typically assume perfect alignment of image pairs during training, which rarely holds true in real-world scenarios. In practice, factors such as urban canyon effects, electromagnetic interference, and adverse weather frequently induce GPS drift, resulting in systematic alignment shifts where only partial correspondences exist between pairs. Despite its prevalence, this source of noisy correspondence has received limited attention in current research. In this paper, we formally introduce and address the Noisy Correspondence on Cross-View Geo-Localization (NC-CVGL) problem, aiming to bridge the gap between idealized benchmarks and practical applications. To this end, we propose PAUL (Partition and Augmentation by Uncertainty Learning), a novel framework that partitions and augments training data based on estimated data uncertainty through uncertainty-aware co-augmentation and evidential co-training. Specifically, PAUL selectively augments regions with high correspondence confidence and utilizes uncertainty estimation to refine feature learning, effectively suppressing noise from misaligned pairs. Distinct from traditional filtering or label correction, PAUL leverages both data uncertainty and loss discrepancy for targeted partitioning and augmentation, thus providing robust supervision for noisy samples. Comprehensive experiments validate the effectiveness of individual components in PAUL,which consistently achieves superior performance over other competitive noisy-correspondence-driven methods in various noise ratios.</p>
                
            
                
                    <h3><a href="http://dx.doi.org/10.1007/978-3-031-86651-7_9" target="_blank">Patch Progression Masked Autoencoder with Fusion CNN Network for Classifying Evolution Between Two Pairs of 2D OCT Slices</a></h3>
                    <p><strong>Authors:</strong> Philippe Zhang, Weili Jiang, Yihao Li, Jing Zhang, Sarah Matta, Yubo Tan, Hui Lin, Haoshen Wang, Jiangtian Pan, Hui Xu, Laurent Borderie, Alexandre Le Guilcher, Béatrice Cochener, Chubin Ou, Gwenolé Quellec, Mathieu Lamard</p>
                    <p><strong>Published:</strong> 8/27/2025</p>
                    <p><strong>Categories:</strong> cs.CV, cs.AI</p>
                    <p><strong>Summary:</strong> Age-related Macular Degeneration (AMD) is a prevalent eye condition affecting visual acuity. Anti-vascular endothelial growth factor (anti-VEGF) treatments have been effective in slowing the progression of neovascular AMD, with better outcomes achieved through timely diagnosis and consistent monitoring. Tracking the progression of neovascular activity in OCT scans of patients with exudative AMD allows for the development of more personalized and effective treatment plans. This was the focus of the Monitoring Age-related Macular Degeneration Progression in Optical Coherence Tomography (MARIO) challenge, in which we participated. In Task 1, which involved classifying the evolution between two pairs of 2D slices from consecutive OCT acquisitions, we employed a fusion CNN network with model ensembling to further enhance the models performance. For Task 2, which focused on predicting progression over the next three months based on current exam data, we proposed the Patch Progression Masked Autoencoder that generates an OCT for the next exam and then classifies the evolution between the current OCT and the one generated using our solution from Task 1. The results we achieved allowed us to place in the Top 10 for both tasks. Some team members are part of the same organization as the challenge organizers; therefore, we are not eligible to compete for the prize.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.20063v1" target="_blank">OpenM3D: Open Vocabulary Multi-view Indoor 3D Object Detection without Human Annotations</a></h3>
                    <p><strong>Authors:</strong> Peng-Hao Hsu, Ke Zhang, Fu-En Wang, Tao Tu, Ming-Feng Li, Yu-Lun Liu, Albert Y. C. Chen, Min Sun, Cheng-Hao Kuo</p>
                    <p><strong>Published:</strong> 8/27/2025</p>
                    <p><strong>Categories:</strong> cs.CV</p>
                    <p><strong>Summary:</strong> Open-vocabulary (OV) 3D object detection is an emerging field, yet its exploration through image-based methods remains limited compared to 3D point cloud-based methods. We introduce OpenM3D, a novel open-vocabulary multi-view indoor 3D object detector trained without human annotations. In particular, OpenM3D is a single-stage detector adapting the 2D-induced voxel features from the ImGeoNet model. To support OV, it is jointly trained with a class-agnostic 3D localization loss requiring high-quality 3D pseudo boxes and a voxel-semantic alignment loss requiring diverse pre-trained CLIP features. We follow the training setting of OV-3DET where posed RGB-D images are given but no human annotations of 3D boxes or classes are available. We propose a 3D Pseudo Box Generation method using a graph embedding technique that combines 2D segments into coherent 3D structures. Our pseudo-boxes achieve higher precision and recall than other methods, including the method proposed in OV-3DET. We further sample diverse CLIP features from 2D segments associated with each coherent 3D structure to align with the corresponding voxel feature. The key to training a highly accurate single-stage detector requires both losses to be learned toward high-quality targets. At inference, OpenM3D, a highly efficient detector, requires only multi-view images for input and demonstrates superior accuracy and speed (0.3 sec. per scene) on ScanNet200 and ARKitScenes indoor benchmarks compared to existing methods. We outperform a strong two-stage method that leverages our class-agnostic detector with a ViT CLIP-based OV classifier and a baseline incorporating multi-view depth estimator on both accuracy and speed.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.20059v1" target="_blank">Moment Constrained Optimal Transport for Thermostatically Controlled Loads</a></h3>
                    <p><strong>Authors:</strong> Thomas Le Corre, Julien Cardinal, Ana Bušić</p>
                    <p><strong>Published:</strong> 8/27/2025</p>
                    <p><strong>Categories:</strong> math.OC</p>
                    <p><strong>Summary:</strong> Controlling large populations of thermostatically controlled loads (TCLs), such as water heaters, poses significant challenges due to the need to balance global constraints (e.g., grid stability) with individual requirements (e.g., physical limits and quality of service). In this work, we introduce a novel framework based on Moment Constrained Optimal Transport (MCOT) for distributed control of TCLs. By formulating the control problem as an optimal transport problem with moment constraints, our approach integrates global consumption constraints and physical feasibility conditions into the control design. This problem with high (or infinite) dimensionality can be reduced to a much lower finite-dimensional problem. The structure of this problem allows for computing the gradient with Monte Carlo methods by generating trajectories of TCLs. Contrary to all previous work, in our MCOT framework, it is possible to choose the sampling law, which considerably speeds up the calculations. This algorithm mitigates the need for extensive state-space discretization and significantly reduces computational complexity compared to existing methods. Numerical experiments in a water heater case study demonstrate that our MCOT-based method effectively coordinates TCLs under various constraints. We further extend our approach to an online setting, illustrating its practical applicability on simulated data from the SMACH (Multi-agent Simulation of Human Activity in the Household) platform.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.20057v1" target="_blank">ProMSC-MIS: Prompt-based Multimodal Semantic Communication for Multi-Spectral Image Segmentation</a></h3>
                    <p><strong>Authors:</strong> Haoshuo Zhang, Yufei Bo, Meixia Tao</p>
                    <p><strong>Published:</strong> 8/27/2025</p>
                    <p><strong>Categories:</strong> cs.MM</p>
                    <p><strong>Summary:</strong> Multimodal semantic communication has great potential to enhance downstream task performance by integrating complementary information across modalities. This paper introduces ProMSC-MIS, a novel Prompt-based Multimodal Semantic Communication framework for Multi-Spectral Image Segmentation. It enables efficient task-oriented transmission of spatially aligned RGB and thermal images over band-limited channels. Our framework has two main design novelties. First, by leveraging prompt learning and contrastive learning, unimodal semantic encoders are pre-trained to learn diverse and complementary semantic representations by using features from one modality as prompts for another. Second, a semantic fusion module that combines cross-attention mechanism and squeeze-and-excitation (SE) networks is designed to effectively fuse cross-modal features. Experimental results demonstrate that ProMSC-MIS substantially outperforms conventional image transmission combined with state-of-the-art segmentation methods. Notably, it reduces the required channel bandwidth by 50%--70% at the same segmentation performance, while also decreasing the storage overhead and computational complexity by 26% and 37%, respectively. Ablation studies also validate the effectiveness of the proposed pre-training and semantic fusion strategies. Our scheme is highly suitable for applications such as autonomous driving and nighttime surveillance.</p>
                
            
                
                    <h3><a href="http://dx.doi.org/10.1016/j.cie.2025.111413" target="_blank">Reinforcement Learning for Search Tree Size Minimization in Constraint Programming: New Results on Scheduling Benchmarks</a></h3>
                    <p><strong>Authors:</strong> Vilém Heinz, Petr Vilím, Zdeněk Hanzálek</p>
                    <p><strong>Published:</strong> 8/27/2025</p>
                    <p><strong>Categories:</strong> cs.LG, 90-08, 90B35, 90C59, 90C99, 68T20, 90C27</p>
                    <p><strong>Summary:</strong> Failure-Directed Search (FDS) is a significant complete generic search algorithm used in Constraint Programming (CP) to efficiently explore the search space, proven particularly effective on scheduling problems. This paper analyzes FDSs properties, showing that minimizing the size of its search tree guided by ranked branching decisions is closely related to the Multi-armed bandit (MAB) problem. Building on this insight, MAB reinforcement learning algorithms are applied to FDS, extended with problem-specific refinements and parameter tuning, and evaluated on the two most fundamental scheduling problems, the Job Shop Scheduling Problem (JSSP) and Resource-Constrained Project Scheduling Problem (RCPSP). The resulting enhanced FDS, using the best extended MAB algorithm and configuration, performs 1.7 times faster on the JSSP and 2.1 times faster on the RCPSP benchmarks compared to the original implementation in a new solver called OptalCP, while also being 3.5 times faster on the JSSP and 2.1 times faster on the RCPSP benchmarks than the current state-of-the-art FDS algorithm in IBM CP Optimizer 22.1. Furthermore, using only a 900-second time limit per instance, the enhanced FDS improved the existing state-of-the-art lower bounds of 78 of 84 JSSP and 226 of 393 RCPSP standard open benchmark instances while also completely closing a few of them.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.20054v1" target="_blank">Between Markov and restriction: Two more monads on categories for relations</a></h3>
                    <p><strong>Authors:</strong> Cipriano Junior Cioffo, Fabio Gadducci, Davide Trotta</p>
                    <p><strong>Published:</strong> 8/27/2025</p>
                    <p><strong>Categories:</strong> cs.LO, math.CT</p>
                    <p><strong>Summary:</strong> The study of categories abstracting the structural properties of relations has been extensively developed over the years, resulting in a rich and diverse body of work. A previous paper offered a survey providing a modern and comprehensive presentation of these ``categories for relations as instances of gs-monoidal categories, showing how they arise as Kleisli categories of suitable symmetric monoidal monads. The end result was a taxonomy that organised numerous related concepts in the literature, including in particular Markov and restriction categories. This paper further enriches the taxonomy: it proposes two categories that are once more instances of gs-monoidal categories, yet more abstract than Markov and restriction categories. They are characterised by an axiomatic notion of mass and domain of an arrow, the latter one of the key ingredient of restriction categories, which generalises the domain of partial functions. The paper then introduces mass and domain preserving monads, proving that the associated Kleisli categories in fact preserve the corresponding equations and that these monads arise naturally for the categories of semiring-weighted relations.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.20050v1" target="_blank">Correlated decoherence and thermometry with mobile impurities in a 1D Fermi gas</a></h3>
                    <p><strong>Authors:</strong> Sindre Brattegard, Thomás Fogarty, Thomas Busch, Mark T. Mitchison</p>
                    <p><strong>Published:</strong> 8/27/2025</p>
                    <p><strong>Categories:</strong> cond-mat.quant-gas, cond-mat.stat-mech, quant-ph</p>
                    <p><strong>Summary:</strong> We theoretically investigate the correlated decoherence dynamics of two mobile impurities trapped within a gas of ultracold fermionic atoms. We use a mean-field approximation to self-consistently describe the effect of impurity-gas collisions on impurity motion, while decoherence of the impurities internal state is computed exactly within a functional determinant approach. At equilibrium, we find that the impurities undergo bath-induced localisation as the impurity-gas interaction strength is increased. We then study the non-equilibrium dynamics induced by a sudden change of the impurities internal state, which can be experimentally probed by Ramsey interferometry. Our theoretical approach allows us to investigate the effect of impurity motion on decoherence dynamics, finding strong deviations from the universal behaviour associated with Andersons orthogonality catastrophe when the mass imbalance between impurity and gas atoms is small. Finally, we show that mobile impurities can be used as thermometers of their environment and that bath-mediated correlations can be beneficial for thermometric performance at low temperatures, even in the presence of non-trivial impurity motion. Our results showcase the interesting open quantum dynamics of mobile impurities dephasing in a common environment, and could help provide more precise temperature estimates of ultracold fermionic mixtures.</p>
                
            
                
                    <h3><a href="http://dx.doi.org/10.1051/0004-6361/202554721" target="_blank">Constraining the TeV gamma-ray emission of SN 2024bch, a possible type IIn-L from a red supergiant progenitor. Multiwavelength observations and analysis of the progenitor</a></h3>
                    <p><strong>Authors:</strong> The CTAO-LST Project, :, K. Abe, S. Abe, A. Abhishek, F. Acero, A. Aguasca-Cabot, I. Agudo, C. Alispach, D. Ambrosino, F. Ambrosino, L. A. Antonelli, C. Aramo, A. Arbet-Engels, C. Arcaro, T. T. H. Arnesen, K. Asano, P. Aubert, A. Baktash, M. Balbo, A. Bamba, A. Baquero-Larriva, U. Barresde-Almeida, J. A. Barrio, L. Barrios-Jiménez, I. Batkovic, J. Baxter, J. BecerraGonzález, E. Bernardini, J. Bernete, A. Berti, I. Bezshyiko, C. Bigongiari, E. Bissaldi, O. Blanch, G. Bonnoli, P. Bordas, G. Borkowski, G. Brunelli, A. Bulgarelli, M. Bunse, I. Burelli, L. Burmistrov, M. Cardillo, S. Caroff, A. Carosi, R. Carraro, M. S. Carrasco, F. Cassol, N. Castrejón, D. Cerasole, G. Ceribella, A. Cerviño-Cortínez, Y. Chai, K. Cheng, A. Chiavassa, M. Chikawa, G. Chon, L. Chytka, G. M. Cicciari, A. Cifuentes, J. L. Contreras, J. Cortina, H. Costantini, M. Dalchenko, P. DaVela, F. Dazzi, A. DeAngelis, M. de Bony de Lavergne, R. Del Burgo, C. Delgado, J. Delgado Mengual, M. Dellaiera, D. della Volpe, B. De Lotto, L. Del Peral, R. de Menezes, G. De Palma, C. Díaz, A. Di Piano, F. Di Pierro, R. Di Tria, L. Di Venere, R. M. Dominik, D. Dominis Prester, A. Donini, D. Dore, D. Dorner, M. Doro, L. Eisenberger, D. Elsässer, G. Emery, J. Escudero, V. Fallah Ramazani, F. Ferrarotto, A. Fiasson, L. Foffano, F. Frías García-Lago, S. Fröse, Y. Fukazawa, S. Gallozzi, R. Garcia López, S. Garcia Soto, C. Gasbarra, D. Gasparrini, D. Geyer, J. Giesbrecht Paiva, N. Giglietto, F. Giordano, N. Godinovic, T. Gradetzke, R. Grau, D. Green, J. Green, S. Gunji, P. Günther, J. Hackfeld, D. Hadasch, A. Hahn, M. Hashizume, T. Hassan, K. Hayashi, L. Heckmann, M. Heller, J. Herrera Llorente, K. Hirotani, D. Hoffmann, D. Horns, J. Houles, M. Hrabovsky, D. Hrupec, D. Hui, M. Iarlori, R. Imazawa, T. Inada, Y. Inome, S. Inoue, K. Ioka, M. Iori, T. Itokawa, A. Iuliano, J. Jahanvi, I. Jimenez Martinez, J. Jimenez Quiles, I. Jorge Rodrigo, J. Jurysek, M. Kagaya, O. Kalashev, V. Karas, H. Katagiri, D. Kerszberg, T. Kiyomot, Y. Kobayashi, K. Kohri, A. Kong, P. Kornecki, H. Kubo, J. Kushida, B. Lacave, M. Lainez, G. Lamanna, A. Lamastra, L. Lemoigne, M. Linhoff, S. Lombardi, F. Longo, R. López-Coto, M. López-Moya, A. López-Oramas, S. Loporchio, A. Lorini, J. LozanoBahilo, F. Lucarelli, H. Luciani, P. L. Luque-Escamilla, P. Majumdar, M. Makariev, M. Mallamaci, D. Mandat, M. Manganaro, D. K. Maniadakis, G. Manicò, K. Mannheim, S. Marchesi, F. Marini, M. Mariotti, P. Marquez, G. Marsella, J. Martí, O. Martinez, G. Martínez, M. Martínez, A. Mas-Aguilar, M. Massa, G. Maurin, D. Mazin, J. Méndez-Gallego, S. Menon, E. MestreGuillen, S. Micanovic, D. Miceli, T. Miener, J. M. Miranda, R. Mirzoyan, M. Mizote, T. Mizuno, M. Molero Gonzalez, E. Molina, T. Montaruli, A. Moralejo, D. Morcuende, A. Moreno Ramos, A. Morselli, V. Moya, H. Muraishi, S. Nagataki, T. Nakamori, A. Neronov, D. Nieto Castaño, M. Nievas Rosillo, L. Nikolic, K. Nishijima, K. Noda, D. Nosek, V. Novotny, S. Nozaki, M. Ohishi, Y. Ohtani, T. Oka, A. Okumura, R. Orito, L. Orsini, J. Otero-Santos, P. Ottanelli, M. Palatiello, G. Panebianco, D. Paneque, F. R. Pantaleo, R. Paoletti, J. M. Paredes, M. Pech, M. Pecimotika, M. Peresano, F. Pfeifle, E. Pietropaolo, M. Pihet, G. Pirola, C. Plard, F. Podobnik, M. Polo, E. Prandini, M. Prouza, S. Rainò, R. Rando, W. Rhode, M. Ribó, V. Rizi, G. Rodriguez Fernandez, M. D. Rodríguez Frías, P. Romano, A. Roy, A. Ruina, E. Ruiz-Velasco, T. Saito, S. Sakurai, D. A. Sanchez, H. Sano, T. Šarić, Y. Sato, F. G. Saturni, V. Savchenko, F. Schiavone, B. Schleicher, F. Schmuckermaier, J. L. Schubert, F. Schussler, T. Schweizer, M. Seglar Arroyo, T. Siegert, G. Silvestri, A. Simongini, J. Sitarek, V. Sliusar, A. Stamerra, J. Strišković, M. Strzys, Y. Suda, A. Sunny, H. Tajima, M. Takahashi, J. Takata, R. Takeishi, P. H. T. Tam, S. J. Tanaka, D. Tateishi, T. Tavernier, P. Temnikov, Y. Terada, K. Terauchi, T. Terzic, M. Teshima, M. Tluczykont, F. Tokanai, T. Tomura, D. F. Torres, F. Tramonti, P. Travnicek, G. Tripodo, A. Tutone, M. Vacula, J. van Scherpenberg, M. Vázquez Acosta, S. Ventura, S. Vercellone, G. Verna, I. Viale, A. Vigliano, C. F. Vigorito, E. Visentin, V. Vitale, V. Voitsekhovskyi, G. Voutsinas, I. Vovk, T. Vuillaume, R. Walter, L. Wan, M. Will, J. Wójtowicz, T. Yamamoto, R. Yamazaki, Y. Yao, P. K. H. Yeung, T. Yoshida, T. Yoshikoshi, W. Zhang</p>
                    <p><strong>Published:</strong> 8/27/2025</p>
                    <p><strong>Categories:</strong> astro-ph.HE</p>
                    <p><strong>Summary:</strong> We present very high-energy optical photometry and spectroscopic observations of SN 2024bch in the nearby galaxy NGC 3206 (\sim 20 Mpc). We used gamma-ray observations performed with the first Large-Sized Telescope (LST-1) of the Cherenkov Telescope Array Observatory (CTAO) and optical observations with the Liverpool Telescope (LT) combined with data from public repositories to evaluate the general properties of the event and the progenitor star. No significant emission above the LST-1 energy threshold for this observation (\sim 100 GeV) was detected in the direction of SN 2024bch, and we computed an integral upper limit on the photon flux of F_\gamma(100 GeV) \le 3.61 \times 10^{-12} cm^{-2} s^{-1} based on six nonconsecutive nights of observations with the LST-1, between 16 and 38 days after the explosion. Employing a general model for the gamma-ray flux emission, we found an upper limit on the mass-loss-rate to wind-velocity ratio of \dot M/u_{w} \le 10^{-4} \frac{M_\odot}{yr}\frac{s}{km}, although gamma-gamma absorption could potentially have skewed this estimation, effectively weakening our constraint. From spectro-photometric observations we found progenitor parameters of M_{pr} = 11 - 20 M_\odot and R_{pr} = 531 \pm 125 R_\odot. Finally, using archival images from the Hubble Space Telescope, we constrained the luminosity of the progenitor star to log(L_{pr}/L_\odot) \le 4.82 and its effective temperature to T_{pr} \le 4000 K. Our results suggest that SN 2024bch is a type IIn-L supernova that originated from a progenitor star consistent with a red supergiant. We show how the correct estimation of the mass-loss history of a supernova will play a major role in future multiwavelength observations.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.20047v1" target="_blank">AraHealthQA 2025 Shared Task Description Paper</a></h3>
                    <p><strong>Authors:</strong> Hassan Alhuzali, Farah Shamout, Muhammad Abdul-Mageed, Chaimae Abouzahir, Mouath Abu-Daoud, Ashwag Alasmari, Walid Al-Eisawi, Renad Al-Monef, Ali Alqahtani, Lama Ayash, Nizar Habash, Leen Kharouf</p>
                    <p><strong>Published:</strong> 8/27/2025</p>
                    <p><strong>Categories:</strong> cs.CL</p>
                    <p><strong>Summary:</strong> We introduce {AraHealthQA 2025}, the {Comprehensive Arabic Health Question Answering Shared Task}, held in conjunction with {ArabicNLP 2025} (co-located with EMNLP 2025). This shared task addresses the paucity of high-quality Arabic medical QA resources by offering two complementary tracks: {MentalQA}, focusing on Arabic mental health Q\A (e.g., anxiety, depression, stigma reduction), and {MedArabiQ}, covering broader medical domains such as internal medicine, pediatrics, and clinical decision making. Each track comprises multiple subtasks, evaluation datasets, and standardized metrics, facilitating fair benchmarking. The task was structured to promote modeling under realistic, multilingual, and culturally nuanced healthcare contexts. We outline the dataset creation, task design and evaluation framework, participation statistics, baseline systems, and summarize the overall outcomes. We conclude with reflections on the performance trends observed and prospects for future iterations in Arabic health QA.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.20042v1" target="_blank">CHEMSMART: Chemistry Simulation and Modeling Automation Toolkit for High-Efficiency Computational Chemistry Workflows</a></h3>
                    <p><strong>Authors:</strong> Xinglong Zhang, Huiwen Tan, Jingyi Liu, Zihan Li, Lewen Wang, Benjamin W. J. Chen</p>
                    <p><strong>Published:</strong> 8/27/2025</p>
                    <p><strong>Categories:</strong> physics.chem-ph, physics.comp-ph</p>
                    <p><strong>Summary:</strong> CHEMSMART (Chemistry Simulation and Modeling Automation Toolkit) is an open-source, Python-based framework designed to streamline quantum chemistry workflows for homogeneous catalysis and molecular modeling. By integrating job preparation, submission, execution, results analysis, and visualization, CHEMSMART addresses the inefficiencies of manual workflow management in computational chemistry by ensuring seamless interoperability with quantum chemistry packages and cheminformatics platforms. Its modular architecture supports automated job submission and execution tasks for geometry optimization, transition state searches, thermochemical analysis, and non-covalent interaction plotting, while auxiliary scripts facilitate file conversion, data organization, and electronic structure analysis. Future developments aim to expand compatibility with additional software, incorporate QM/MM and classical MD, and align with FAIR data principles for enhanced reproducibility and data reuse. Available on GitHub, CHEMSMART empowers researchers with a robust, user-friendly platform for efficient and reproducible computational chemistry.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.20039v1" target="_blank">Robust Paths: Geometry and Computation</a></h3>
                    <p><strong>Authors:</strong> Hao Hao, Peter Zhang</p>
                    <p><strong>Published:</strong> 8/27/2025</p>
                    <p><strong>Categories:</strong> math.OC</p>
                    <p><strong>Summary:</strong> Applying robust optimization often requires selecting an appropriate uncertainty set both in shape and size, a choice that directly affects the trade-off between average-case and worst-case performances. In practice, this calibration is usually done via trial-and-error: solving the robust optimization problem many times with different uncertainty set shapes and sizes, and examining their performance trade-off. This process is computationally expensive and ad hoc. In this work, we take a principled approach to study this issue for robust optimization problems with linear objective functions, convex feasible regions, and convex uncertainty sets. We introduce and study what we define as the robust path: a set of robust solutions obtained by varying the uncertainty sets parameters. Our central geometric insight is that a robust path can be characterized as a Bregman projection of a curve (whose geometry is defined by the uncertainty set) onto the feasible region. This leads to a surprising discovery that the robust path can be approximated via the trajectories of standard optimization algorithms, such as the proximal point method, of the deterministic counterpart problem. We give a sharp approximation error bound and show it depends on the geometry of the feasible region and the uncertainty set. We also illustrate two special cases where the approximation error is zero: the feasible region is polyhedrally monotone (e.g., a simplex feasible region under an ellipsoidal uncertainty set), or the feasible region and the uncertainty set follow a dual relationship. We demonstrate the practical impact of this approach in two settings: portfolio optimization and adversarial deep learning.</p>
                
            
                
                    <h3><a href="http://arxiv.org/abs/2508.20038v1" target="_blank">Forewarned is Forearmed: Pre-Synthesizing Jailbreak-like Instructions to Enhance LLM Safety Guardrail to Potential Attacks</a></h3>
                    <p><strong>Authors:</strong> Sheng Liu, Qiang Sheng, Danding Wang, Yang Li, Guang Yang, Juan Cao</p>
                    <p><strong>Published:</strong> 8/27/2025</p>
                    <p><strong>Categories:</strong> cs.CL</p>
                    <p><strong>Summary:</strong> Despite advances in improving large language model(LLM) to refuse to answer malicious instructions, widely used LLMs remain vulnerable to jailbreak attacks where attackers generate instructions with distributions differing from safety alignment corpora. New attacks expose LLMs inability to recognize unseen malicious instructions, highlighting a critical distributional mismatch between training data and real-world attacks that forces developers into reactive patching cycles. To tackle this challenge, we propose IMAGINE, a synthesis framework that leverages embedding space distribution analysis to generate jailbreak-like instructions. This approach effectively fills the distributional gap between authentic jailbreak patterns and safety alignment corpora. IMAGINE follows an iterative optimization process that dynamically evolves text generation distributions across iterations, thereby augmenting the coverage of safety alignment data distributions through synthesized data examples. Based on the safety-aligned corpus enhanced through IMAGINE, our framework demonstrates significant decreases in attack success rate on Qwen2.5, Llama3.1, and Llama3.2 without compromising their utility.</p>
                
            
        
        
            <p><em>Generated by AI News Agent</em></p>
        
    

